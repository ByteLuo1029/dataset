2025-04-11T04:12:14.6717019Z ##[group]Run CURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \
2025-04-11T04:12:14.6717491Z [36;1mCURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \[0m
2025-04-11T04:12:14.6717822Z [36;1m-m "not largedist" \[0m
2025-04-11T04:12:14.6718064Z [36;1m--durations=0 \[0m
2025-04-11T04:12:14.6718307Z [36;1m--ignore tests/test_analyzer \[0m
2025-04-11T04:12:14.6718590Z [36;1m--ignore tests/test_auto_parallel \[0m
2025-04-11T04:12:14.6718863Z [36;1m--ignore tests/test_fx \[0m
2025-04-11T04:12:14.6719115Z [36;1m--ignore tests/test_autochunk \[0m
2025-04-11T04:12:14.6719382Z [36;1m--ignore tests/test_gptq \[0m
2025-04-11T04:12:14.6719641Z [36;1m--ignore tests/test_infer_ops \[0m
2025-04-11T04:12:14.6719902Z [36;1m--ignore tests/test_legacy \[0m
2025-04-11T04:12:14.6720168Z [36;1m--ignore tests/test_smoothquant \[0m
2025-04-11T04:12:14.6720416Z [36;1mtests/[0m
2025-04-11T04:12:14.6720752Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T04:12:14.6721027Z env:
2025-04-11T04:12:14.6721406Z   LD_LIBRARY_PATH: /github/home/.tensornvme/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2025-04-11T04:12:14.6721798Z   LLAMA_PATH: /data/scratch/llama-tiny
2025-04-11T04:12:14.6722243Z   MOE_TENSOR_PATH: /data/scratch/moe_tensors
2025-04-11T04:12:14.6722534Z   HF_ENDPOINT: https://hf-mirror.com
2025-04-11T04:12:14.6722791Z ##[endgroup]
2025-04-11T04:12:24.7053668Z ============================= test session starts ==============================
2025-04-11T04:12:24.7054115Z platform linux -- Python 3.10.14, pytest-7.4.4, pluggy-1.5.0
2025-04-11T04:12:24.7054456Z rootdir: /__w/ColossalAI/ColossalAI
2025-04-11T04:12:24.7054747Z configfile: pytest.ini
2025-04-11T04:12:24.7055037Z plugins: hypothesis-6.131.0, anyio-4.9.0, testmon-2.0.7b1
2025-04-11T04:12:24.7055368Z collected 865 items / 23 deselected / 842 selected
2025-04-11T04:12:24.7055576Z 
2025-04-11T04:12:24.9018240Z tests/test_booster/test_accelerator.py F                                 [  0%]
2025-04-11T04:12:24.9024552Z tests/test_booster/test_mixed_precision/test_fp16_torch.py s             [  0%]
2025-04-11T04:12:33.4974001Z tests/test_booster/test_plugin/test_3d_plugin.py F                       [  0%]
2025-04-11T04:12:37.7207489Z tests/test_booster/test_plugin/test_dp_plugin_base.py F                  [  0%]
2025-04-11T04:12:46.4076227Z tests/test_booster/test_plugin/test_gemini_plugin.py F                   [  0%]
2025-04-11T04:12:52.9323105Z tests/test_booster/test_plugin/test_low_level_zero_plugin.py F           [  0%]
2025-04-11T04:12:59.9410326Z tests/test_booster/test_plugin/test_torch_ddp_plugin.py F                [  0%]
2025-04-11T04:13:06.5194336Z tests/test_booster/test_plugin/test_torch_fsdp_plugin.py F               [  0%]
2025-04-11T04:13:15.0019590Z tests/test_checkpoint_io/test_gemini_checkpoint_io.py F                  [  1%]
2025-04-11T04:13:22.5340168Z tests/test_checkpoint_io/test_gemini_torch_compability.py F              [  1%]
2025-04-11T04:13:27.4920249Z tests/test_checkpoint_io/test_general_checkpoint_io.py F..FFFFFF         [  2%]
2025-04-11T04:13:36.2931598Z tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py F  [  2%]
2025-04-11T04:13:36.5921918Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py F          [  2%]
2025-04-11T04:13:44.2187882Z tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py F     [  2%]
2025-04-11T04:13:45.6676895Z tests/test_checkpoint_io/test_safetensors_async_io.py FFFFF              [  3%]
2025-04-11T04:13:51.2937047Z tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py F               [  3%]
2025-04-11T04:13:55.8908948Z tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py F              [  3%]
2025-04-11T04:14:00.6559537Z tests/test_cluster/test_device_mesh_manager.py .                         [  3%]
2025-04-11T04:14:04.8567999Z tests/test_cluster/test_process_group_mesh.py .                          [  3%]
2025-04-11T04:14:04.9700835Z tests/test_config/test_load_config.py .                                  [  3%]
2025-04-11T04:14:04.9706706Z tests/test_device/test_alpha_beta.py s                                   [  3%]
2025-04-11T04:14:10.0923032Z tests/test_device/test_device_mesh.py ..                                 [  4%]
2025-04-11T04:14:10.0928783Z tests/test_device/test_extract_alpha_beta.py s                           [  4%]
2025-04-11T04:14:15.2943988Z tests/test_device/test_init_logical_pg.py F                              [  4%]
2025-04-11T04:14:15.2949517Z tests/test_device/test_search_logical_device_mesh.py s                   [  4%]
2025-04-11T04:14:20.8161331Z tests/test_fp8/test_all_to_all_single.py F                               [  4%]
2025-04-11T04:14:25.3461304Z tests/test_fp8/test_fp8_all_to_all.py F                                  [  4%]
2025-04-11T04:14:29.7709511Z tests/test_fp8/test_fp8_all_to_all_single.py F                           [  4%]
2025-04-11T04:14:35.2929550Z tests/test_fp8/test_fp8_allgather.py F                                   [  4%]
2025-04-11T04:14:40.5670544Z tests/test_fp8/test_fp8_allreduce.py F                                   [  5%]
2025-04-11T04:14:40.9027731Z tests/test_fp8/test_fp8_cast.py F                                        [  5%]
2025-04-11T04:14:48.0862200Z tests/test_fp8/test_fp8_fsdp_comm_hook.py F                              [  5%]
2025-04-11T04:14:48.2924540Z tests/test_fp8/test_fp8_hook.py F                                        [  5%]
2025-04-11T04:14:49.1002386Z tests/test_fp8/test_fp8_linear.py FFFF                                   [  5%]
2025-04-11T04:14:54.9228297Z tests/test_fp8/test_fp8_reduce_scatter.py F                              [  6%]
2025-04-11T04:14:55.1395012Z tests/test_infer/test_batch_bucket.py F                                  [  6%]
2025-04-11T04:14:58.9163230Z tests/test_infer/test_config_and_struct.py .                             [  6%]
2025-04-11T04:15:14.3817599Z tests/test_infer/test_continuous_batching.py F                           [  6%]
2025-04-11T04:15:25.2014547Z tests/test_infer/test_drafter.py FF                                      [  6%]
2025-04-11T04:15:29.3620689Z tests/test_infer/test_kvcache_manager.py .F                              [  6%]
2025-04-11T04:15:33.2856725Z tests/test_infer/test_request_handler.py F                               [  7%]
2025-04-11T04:15:39.2988670Z tests/test_infer/test_streamingllm.py F                                  [  7%]
2025-04-11T04:15:39.4378922Z tests/test_infer/test_async_engine/test_async_engine.py s                [  7%]
2025-04-11T04:15:39.5409120Z tests/test_infer/test_async_engine/test_request_tracer.py .              [  7%]
2025-04-11T04:15:39.5485698Z tests/test_infer/test_kernels/cuda/test_convert_fp8.py sssssssssssssssss [  9%]
2025-04-11T04:15:39.5792599Z ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 17%]
2025-04-11T04:15:39.5873613Z sssssssssssssssssss                                                      [ 20%]
2025-04-11T04:15:39.9633786Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py FF   [ 20%]
2025-04-11T04:15:40.3238892Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py FF            [ 20%]
2025-04-11T04:15:42.7169947Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py FFFFFFFFFFFFF [ 22%]
2025-04-11T04:15:46.9371063Z FFFFFFFFFFFFFFFFFFFFFFF                                                  [ 24%]
2025-04-11T04:15:49.6671883Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py FFFFFFFFFFFFFFF [ 26%]
2025-04-11T04:15:49.8480157Z F                                                                        [ 26%]
2025-04-11T04:15:50.6893293Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py FFFF    [ 27%]
2025-04-11T04:15:51.0549907Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py FF               [ 27%]
2025-04-11T04:15:51.7352071Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py ........ [ 28%]
2025-04-11T04:16:02.6155678Z ........................FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 37%]
2025-04-11T04:16:11.4779761Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                         [ 42%]
2025-04-11T04:16:12.6140265Z tests/test_infer/test_kernels/triton/test_decoding_attn.py sssssssssssss [ 44%]
2025-04-11T04:16:20.9794867Z sssssssssssssssssssssssssssssssssssssssssssssssssssFFFFFFFFFFFFFFFFFFFFF [ 52%]
2025-04-11T04:16:34.2605231Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 61%]
2025-04-11T04:16:47.5659389Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 69%]
2025-04-11T04:16:52.5962525Z FFFFFFFFFFFFFFFFFFFFFFFFFFF                                              [ 73%]
2025-04-11T04:16:52.5968561Z tests/test_infer/test_kernels/triton/test_fused_rotary_embedding.py s    [ 73%]
2025-04-11T04:16:55.2424395Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py FFFFFFFFFFFFFF [ 74%]
2025-04-11T04:17:01.5967761Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                                       [ 78%]
2025-04-11T04:17:01.7845811Z tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py F            [ 79%]
2025-04-11T04:17:02.1914858Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py FF    [ 79%]
2025-04-11T04:17:02.3792421Z tests/test_infer/test_kernels/triton/test_xine_copy.py F                 [ 79%]
2025-04-11T04:17:02.3810569Z tests/test_infer/test_models/test_attention.py ssss                      [ 79%]
2025-04-11T04:17:02.4708320Z tests/test_infer/test_models/test_custom_model.py s                      [ 80%]
2025-04-11T04:17:07.2731322Z tests/test_lazy/test_from_pretrained.py .                                [ 80%]
2025-04-11T04:17:24.8380875Z tests/test_lazy/test_models.py .F                                        [ 80%]
2025-04-11T04:17:25.0779916Z tests/test_lazy/test_ops.py F                                            [ 80%]
2025-04-11T04:17:32.5810890Z tests/test_lora/test_lora.py F                                           [ 80%]
2025-04-11T04:17:32.5817007Z tests/test_moe/test_deepseek_layer.py s                                  [ 80%]
2025-04-11T04:17:33.0623999Z tests/test_moe/test_kernel.py FF                                         [ 80%]
2025-04-11T04:17:33.0630095Z tests/test_moe/test_mixtral_layer.py s                                   [ 81%]
2025-04-11T04:17:39.4381052Z tests/test_moe/test_moe_checkpoint.py F                                  [ 81%]
2025-04-11T04:17:39.4386281Z tests/test_moe/test_moe_ep_tp.py s                                       [ 81%]
2025-04-11T04:17:39.4392906Z tests/test_moe/test_moe_ep_zero.py s                                     [ 81%]
2025-04-11T04:17:44.6190293Z tests/test_optimizer/test_adam_kernel.py FFFFFFFFFFFFFFFFFFFF........... [ 85%]
2025-04-11T04:17:44.7110172Z .                                                                        [ 85%]
2025-04-11T04:17:54.2324718Z tests/test_optimizer/test_adam_optim.py F.FFFF.FFFF.FFFF.FFFF.FFFF.FFF   [ 88%]
2025-04-11T04:18:03.2867366Z tests/test_optimizer/test_dist_adafactor.py F                            [ 88%]
2025-04-11T04:18:12.3188368Z tests/test_optimizer/test_dist_came.py F                                 [ 89%]
2025-04-11T04:18:21.2138513Z tests/test_optimizer/test_dist_galore.py F                               [ 89%]
2025-04-11T04:18:29.5554233Z tests/test_optimizer/test_dist_lamb.py F                                 [ 89%]
2025-04-11T04:18:29.7183858Z tests/test_optimizer/test_lr_scheduler.py .                              [ 89%]
2025-04-11T04:18:29.7190236Z tests/test_optimizer/test_nvme.py s                                      [ 89%]
2025-04-11T04:18:33.8709293Z tests/test_pipeline/test_p2p_communication.py F                          [ 89%]
2025-04-11T04:18:40.2515999Z tests/test_pipeline/test_stage_manager.py F                              [ 89%]
2025-04-11T04:18:40.5212588Z tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py ..     [ 90%]
2025-04-11T04:18:40.7044254Z tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py . [ 90%]
2025-04-11T04:18:40.8065207Z .                                                                        [ 90%]
2025-04-11T04:19:01.0590315Z tests/test_pipeline/test_schedule/test_interleaved.py FFFF               [ 90%]
2025-04-11T04:19:23.0158824Z tests/test_pipeline/test_schedule/test_oneF_oneB.py FFFF                 [ 91%]
2025-04-11T04:19:23.3599679Z tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py ...    [ 91%]
2025-04-11T04:19:28.7028054Z tests/test_pipeline/test_schedule/test_zerobubble_pp.py F                [ 91%]
2025-04-11T04:19:28.9431353Z tests/test_shardformer/test_flash_attention.py F                         [ 91%]
2025-04-11T04:19:29.1888119Z tests/test_shardformer/test_shard_utils.py F                             [ 91%]
2025-04-11T04:19:29.5615718Z tests/test_shardformer/test_with_torch_ddp.py F                          [ 92%]
2025-04-11T04:19:29.7685555Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py F [ 92%]
2025-04-11T04:19:29.7687171Z                                                                          [ 92%]
2025-04-11T04:19:29.9728766Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py F [ 92%]
2025-04-11T04:19:29.9729228Z                                                                          [ 92%]
2025-04-11T04:19:30.1773797Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py F [ 92%]
2025-04-11T04:19:30.1775105Z                                                                          [ 92%]
2025-04-11T04:19:35.4275191Z tests/test_shardformer/test_layer/test_dist_crossentropy.py F            [ 92%]
2025-04-11T04:19:39.6004030Z tests/test_shardformer/test_layer/test_dropout.py F                      [ 92%]
2025-04-11T04:19:43.8541055Z tests/test_shardformer/test_layer/test_embedding.py F                    [ 92%]
2025-04-11T04:19:49.1545288Z tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py F     [ 92%]
2025-04-11T04:19:53.3552420Z tests/test_shardformer/test_layer/test_layernorm.py F                    [ 92%]
2025-04-11T04:19:57.6679014Z tests/test_shardformer/test_layer/test_linear_1d.py F                    [ 93%]
2025-04-11T04:20:01.8960424Z tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py F          [ 93%]
2025-04-11T04:20:11.3312064Z tests/test_shardformer/test_layer/test_ring_attn.py FF                   [ 93%]
2025-04-11T04:20:16.6309825Z tests/test_shardformer/test_layer/test_sequence_parallel.py F            [ 93%]
2025-04-11T04:20:20.8815291Z tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py F  [ 93%]
2025-04-11T04:20:21.1274987Z tests/test_shardformer/test_model/test_shard_bert.py F                   [ 93%]
2025-04-11T04:20:21.3301830Z tests/test_shardformer/test_model/test_shard_blip2.py F                  [ 93%]
2025-04-11T04:20:21.5316011Z tests/test_shardformer/test_model/test_shard_bloom.py F                  [ 94%]
2025-04-11T04:20:21.7328537Z tests/test_shardformer/test_model/test_shard_chatglm2.py F               [ 94%]
2025-04-11T04:20:21.9338245Z tests/test_shardformer/test_model/test_shard_command.py F                [ 94%]
2025-04-11T04:20:30.7694332Z tests/test_shardformer/test_model/test_shard_deepseek.py F               [ 94%]
2025-04-11T04:20:41.8757267Z tests/test_shardformer/test_model/test_shard_deepseek_v3.py F            [ 94%]
2025-04-11T04:20:42.1233330Z tests/test_shardformer/test_model/test_shard_falcon.py F                 [ 94%]
2025-04-11T04:20:42.3249558Z tests/test_shardformer/test_model/test_shard_gpt2.py F                   [ 94%]
2025-04-11T04:20:42.3254767Z tests/test_shardformer/test_model/test_shard_gptj.py s                   [ 94%]
2025-04-11T04:20:42.5259798Z tests/test_shardformer/test_model/test_shard_llama.py F                  [ 95%]
2025-04-11T04:20:42.7264793Z tests/test_shardformer/test_model/test_shard_mistral.py F                [ 95%]
2025-04-11T04:20:48.1881334Z tests/test_shardformer/test_model/test_shard_mixtral.py F                [ 95%]
2025-04-11T04:20:48.4267818Z tests/test_shardformer/test_model/test_shard_opt.py F                    [ 95%]
2025-04-11T04:20:48.6282542Z tests/test_shardformer/test_model/test_shard_qwen2.py F                  [ 95%]
2025-04-11T04:20:48.8310463Z tests/test_shardformer/test_model/test_shard_sam.py F                    [ 95%]
2025-04-11T04:20:49.0316412Z tests/test_shardformer/test_model/test_shard_t5.py F                     [ 95%]
2025-04-11T04:20:49.2323801Z tests/test_shardformer/test_model/test_shard_vit.py F                    [ 95%]
2025-04-11T04:20:49.4337918Z tests/test_shardformer/test_model/test_shard_whisper.py F                [ 95%]
2025-04-11T04:20:54.9206485Z tests/test_tensor/test_comm_spec_apply.py F                              [ 96%]
2025-04-11T04:20:54.9211789Z tests/test_tensor/test_mix_gather.py s                                   [ 96%]
2025-04-11T04:21:00.0951728Z tests/test_tensor/test_padded_tensor.py F                                [ 96%]
2025-04-11T04:21:00.3365559Z tests/test_tensor/test_shape_consistency.py ..                           [ 96%]
2025-04-11T04:21:04.7300336Z tests/test_tensor/test_shape_consistency_apply.py F                      [ 96%]
2025-04-11T04:21:04.8695025Z tests/test_tensor/test_sharding_spec.py .                                [ 96%]
2025-04-11T04:21:10.0368703Z tests/test_tensor/test_dtensor/test_comm_spec.py F                       [ 96%]
2025-04-11T04:21:14.5622836Z tests/test_tensor/test_dtensor/test_dtensor.py F                         [ 97%]
2025-04-11T04:21:14.7010798Z tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py .           [ 97%]
2025-04-11T04:21:29.9140092Z tests/test_tensor/test_dtensor/test_layout_converter.py F                [ 97%]
2025-04-11T04:21:34.1305369Z tests/test_zero/test_gemini/test_chunk_mgrv2.py F                        [ 97%]
2025-04-11T04:21:49.2087821Z tests/test_zero/test_gemini/test_chunkv2.py FFF                          [ 97%]
2025-04-11T04:21:49.2097057Z tests/test_zero/test_gemini/test_gemini_use_rmt.py ss                    [ 97%]
2025-04-11T04:21:55.7792013Z tests/test_zero/test_gemini/test_grad_accum.py F                         [ 98%]
2025-04-11T04:22:08.7688077Z tests/test_zero/test_gemini/test_grad_clip.py FF                         [ 98%]
2025-04-11T04:22:25.0198614Z tests/test_zero/test_gemini/test_inference.py FF                         [ 98%]
2025-04-11T04:22:33.0884540Z tests/test_zero/test_gemini/test_optim.py F                              [ 98%]
2025-04-11T04:22:33.0889627Z tests/test_zero/test_gemini/test_runtime_mem_tracer.py s                 [ 98%]
2025-04-11T04:22:43.4814673Z tests/test_zero/test_gemini/test_search.py FF                            [ 99%]
2025-04-11T04:22:50.8777054Z tests/test_zero/test_gemini/test_zeroddp_state_dict.py F                 [ 99%]
2025-04-11T04:22:50.8786990Z tests/test_zero/test_gemini/test_zerooptim_state_dict.py ss              [ 99%]
2025-04-11T04:22:56.8249746Z tests/test_zero/test_low_level/test_coll_nd.py F                         [ 99%]
2025-04-11T04:23:01.9996874Z tests/test_zero/test_low_level/test_grad_acc.py F                        [ 99%]
2025-04-11T04:23:06.3249201Z tests/test_zero/test_low_level/test_mem_leak.py F                        [ 99%]
2025-04-11T04:23:12.4346974Z tests/test_zero/test_low_level/test_zero1_2.py F                         [ 99%]
2025-04-11T04:23:18.3942153Z tests/test_zero/test_low_level/test_zero_ckpt.py F                       [100%]
2025-04-11T04:23:18.3942499Z 
2025-04-11T04:23:18.3942620Z =================================== FAILURES ===================================
2025-04-11T04:23:18.3943972Z _______________________________ test_accelerator _______________________________
2025-04-11T04:23:18.3944289Z 
2025-04-11T04:23:18.3944400Z args = (), kwargs = {}
2025-04-11T04:23:18.3944545Z 
2025-04-11T04:23:18.3944653Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.3944945Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.3945247Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.3945560Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.3945871Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.3946148Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.3946729Z 
2025-04-11T04:23:18.3946859Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.3947168Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.3947525Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.3947847Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.3948122Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.3948312Z 
2025-04-11T04:23:18.3948392Z device = None
2025-04-11T04:23:18.3948561Z 
2025-04-11T04:23:18.3948708Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.3949058Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.3949373Z     
2025-04-11T04:23:18.3949541Z         Args:
2025-04-11T04:23:18.3949829Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.3950248Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.3950610Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.3951038Z         """
2025-04-11T04:23:18.3951229Z         _lazy_init()
2025-04-11T04:23:18.3951463Z         with torch.cuda.device(device):
2025-04-11T04:23:18.3951746Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.3952041Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.3952551Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.3953075Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.3953460Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.3953704Z 
2025-04-11T04:23:18.3953966Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.3954436Z ________________________________ test_3d_plugin ________________________________
2025-04-11T04:23:18.3954649Z 
2025-04-11T04:23:18.3954755Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.3955546Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.3956234Z 
2025-04-11T04:23:18.3956350Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.3956610Z         try_count = 0
2025-04-11T04:23:18.3956858Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.3957139Z             max_try, int
2025-04-11T04:23:18.3957432Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.3957734Z     
2025-04-11T04:23:18.3957954Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.3958213Z             try:
2025-04-11T04:23:18.3958435Z                 try_count += 1
2025-04-11T04:23:18.3958687Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.3958941Z                 return ret
2025-04-11T04:23:18.3959181Z             except exception_type as e:
2025-04-11T04:23:18.3959446Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.3959813Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.3960197Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.3960545Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.3960961Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.3961286Z                     continue
2025-04-11T04:23:18.3961503Z                 else:
2025-04-11T04:23:18.3961847Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.3962328Z >                   raise e
2025-04-11T04:23:18.3962460Z 
2025-04-11T04:23:18.3962563Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.3962841Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.3963153Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.3963441Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.3963757Z tests/test_booster/test_plugin/test_3d_plugin.py:277: in test_3d_plugin
2025-04-11T04:23:18.3964113Z     spawn(run_dist, 4, early_stop=early_stop)
2025-04-11T04:23:18.3964397Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.3964671Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.3965104Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.3965622Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.3966160Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.3966620Z     while not context.join():
2025-04-11T04:23:18.3966886Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.3967160Z 
2025-04-11T04:23:18.3967364Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8493580>
2025-04-11T04:23:18.3967708Z timeout = None
2025-04-11T04:23:18.3967831Z 
2025-04-11T04:23:18.3967925Z     def join(self, timeout=None):
2025-04-11T04:23:18.3968207Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.3968477Z     
2025-04-11T04:23:18.3968735Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.3969275Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.3969661Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.3970026Z         of the first process exiting.
2025-04-11T04:23:18.3970306Z     
2025-04-11T04:23:18.3970563Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.3970918Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.3971210Z     
2025-04-11T04:23:18.3971404Z         Args:
2025-04-11T04:23:18.3971658Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.3971941Z         """
2025-04-11T04:23:18.3972174Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.3972470Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.3972711Z             return True
2025-04-11T04:23:18.3972924Z     
2025-04-11T04:23:18.3973151Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.3973463Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.3973780Z             self.sentinels.keys(),
2025-04-11T04:23:18.3974042Z             timeout=timeout,
2025-04-11T04:23:18.3974262Z         )
2025-04-11T04:23:18.3974449Z     
2025-04-11T04:23:18.3974622Z         error_index = None
2025-04-11T04:23:18.3974848Z         for sentinel in ready:
2025-04-11T04:23:18.3975106Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.3975384Z             process = self.processes[index]
2025-04-11T04:23:18.3975644Z             process.join()
2025-04-11T04:23:18.3975869Z             if process.exitcode != 0:
2025-04-11T04:23:18.3976118Z                 error_index = index
2025-04-11T04:23:18.3976350Z                 break
2025-04-11T04:23:18.3976542Z     
2025-04-11T04:23:18.3976730Z         # Return if there was no error.
2025-04-11T04:23:18.3976972Z         if error_index is None:
2025-04-11T04:23:18.3977260Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.3977567Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.3977803Z     
2025-04-11T04:23:18.3978037Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.3978445Z         for process in self.processes:
2025-04-11T04:23:18.3978699Z             if process.is_alive():
2025-04-11T04:23:18.3978955Z                 process.terminate()
2025-04-11T04:23:18.3979212Z             process.join()
2025-04-11T04:23:18.3979432Z     
2025-04-11T04:23:18.3979670Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.3980010Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.3980318Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.3980637Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.3980929Z             if exitcode < 0:
2025-04-11T04:23:18.3981181Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.3981474Z                 raise ProcessExitedException(
2025-04-11T04:23:18.3981806Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.3982130Z                     error_index=error_index,
2025-04-11T04:23:18.3982408Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.3982673Z                     exit_code=exitcode,
2025-04-11T04:23:18.3982936Z                     signal_name=name,
2025-04-11T04:23:18.3983268Z                 )
2025-04-11T04:23:18.3983459Z             else:
2025-04-11T04:23:18.3983705Z                 raise ProcessExitedException(
2025-04-11T04:23:18.3984046Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.3984379Z                     error_index=error_index,
2025-04-11T04:23:18.3984642Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.3984895Z                     exit_code=exitcode,
2025-04-11T04:23:18.3985125Z                 )
2025-04-11T04:23:18.3985305Z     
2025-04-11T04:23:18.3985529Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.3985902Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.3986233Z         msg += original_trace
2025-04-11T04:23:18.3986551Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.3986962Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.3987265Z E       
2025-04-11T04:23:18.3987493Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.3987779Z E       Traceback (most recent call last):
2025-04-11T04:23:18.3988247Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.3988759Z E           fn(i, *args)
2025-04-11T04:23:18.3989160Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
2025-04-11T04:23:18.3989624Z E           check_3d_plugin(early_stop=early_stop)
2025-04-11T04:23:18.3990065Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.3990507Z E           partial_func(**kwargs)
2025-04-11T04:23:18.3990945Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
2025-04-11T04:23:18.3991487Z E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:18.3991948Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.3992344Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.3992767Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.3993216Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.3993664Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.3994116Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.3994394Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.3995010Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.3995529Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.3995898Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.3996142Z 
2025-04-11T04:23:18.3996452Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.3997009Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.3997405Z [04/11/25 04:12:32] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.3997772Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.3998088Z                              :75 launch                                         
2025-04-11T04:23:18.3998399Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.3998739Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.3999251Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.3999667Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4001034Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4002411Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4003777Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4005125Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4006477Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4007825Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4009164Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4010484Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4011383Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4012209Z   warnings.warn(
2025-04-11T04:23:18.4013000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4013922Z   warnings.warn(
2025-04-11T04:23:18.4014712Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4015542Z   warnings.warn(
2025-04-11T04:23:18.4016329Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4017153Z   warnings.warn(
2025-04-11T04:23:18.4018096Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4019153Z   warnings.warn(
2025-04-11T04:23:18.4020055Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4021005Z   warnings.warn(
2025-04-11T04:23:18.4021935Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4022886Z   warnings.warn(
2025-04-11T04:23:18.4023797Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4024742Z   warnings.warn(
2025-04-11T04:23:18.4025646Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4026599Z   warnings.warn(
2025-04-11T04:23:18.4027499Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4028485Z   warnings.warn(
2025-04-11T04:23:18.4029403Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4030353Z   warnings.warn(
2025-04-11T04:23:18.4031262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4032206Z   warnings.warn(
2025-04-11T04:23:18.4033125Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4034183Z   warnings.warn(
2025-04-11T04:23:18.4035083Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4036019Z   warnings.warn(
2025-04-11T04:23:18.4036672Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4037367Z   warnings.warn(
2025-04-11T04:23:18.4038023Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4038813Z   warnings.warn(
2025-04-11T04:23:18.4039733Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4040678Z   warnings.warn(
2025-04-11T04:23:18.4041334Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4042016Z   warnings.warn(
2025-04-11T04:23:18.4042660Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4043355Z   warnings.warn(
2025-04-11T04:23:18.4044265Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4045220Z   warnings.warn(
2025-04-11T04:23:18.4045880Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4046571Z   warnings.warn(
2025-04-11T04:23:18.4047210Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4047897Z   warnings.warn(
2025-04-11T04:23:18.4048558Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4049259Z   warnings.warn(
2025-04-11T04:23:18.4049908Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4050593Z   warnings.warn(
2025-04-11T04:23:18.4051248Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4051937Z   warnings.warn(
2025-04-11T04:23:18.4052657Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4053364Z   warnings.warn(
2025-04-11T04:23:18.4053626Z __________________________ test_dp_plugin_dataloader ___________________________
2025-04-11T04:23:18.4053848Z 
2025-04-11T04:23:18.4053943Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4054728Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4055382Z 
2025-04-11T04:23:18.4055496Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4055752Z         try_count = 0
2025-04-11T04:23:18.4055977Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4056237Z             max_try, int
2025-04-11T04:23:18.4056516Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4056804Z     
2025-04-11T04:23:18.4057122Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4057397Z             try:
2025-04-11T04:23:18.4057603Z                 try_count += 1
2025-04-11T04:23:18.4057847Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4058104Z                 return ret
2025-04-11T04:23:18.4058345Z             except exception_type as e:
2025-04-11T04:23:18.4058614Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4058984Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4059373Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4059722Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4060112Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4060437Z                     continue
2025-04-11T04:23:18.4060664Z                 else:
2025-04-11T04:23:18.4061039Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4061422Z >                   raise e
2025-04-11T04:23:18.4061557Z 
2025-04-11T04:23:18.4061662Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4061941Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4062257Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4062553Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4062912Z tests/test_booster/test_plugin/test_dp_plugin_base.py:94: in test_dp_plugin_dataloader
2025-04-11T04:23:18.4063281Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4063519Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4063788Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4064225Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4064740Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4065312Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4065786Z     while not context.join():
2025-04-11T04:23:18.4066054Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4066239Z 
2025-04-11T04:23:18.4066442Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb83b3179a0>
2025-04-11T04:23:18.4066803Z timeout = None
2025-04-11T04:23:18.4066928Z 
2025-04-11T04:23:18.4067022Z     def join(self, timeout=None):
2025-04-11T04:23:18.4067312Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4067589Z     
2025-04-11T04:23:18.4067833Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4068295Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4068744Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4069086Z         of the first process exiting.
2025-04-11T04:23:18.4069327Z     
2025-04-11T04:23:18.4069570Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4069955Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4070256Z     
2025-04-11T04:23:18.4070443Z         Args:
2025-04-11T04:23:18.4070707Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4071015Z         """
2025-04-11T04:23:18.4071256Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4071562Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4071808Z             return True
2025-04-11T04:23:18.4072012Z     
2025-04-11T04:23:18.4072236Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4072564Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4072851Z             self.sentinels.keys(),
2025-04-11T04:23:18.4073207Z             timeout=timeout,
2025-04-11T04:23:18.4073433Z         )
2025-04-11T04:23:18.4073607Z     
2025-04-11T04:23:18.4073801Z         error_index = None
2025-04-11T04:23:18.4074039Z         for sentinel in ready:
2025-04-11T04:23:18.4074310Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4074608Z             process = self.processes[index]
2025-04-11T04:23:18.4074870Z             process.join()
2025-04-11T04:23:18.4075117Z             if process.exitcode != 0:
2025-04-11T04:23:18.4075384Z                 error_index = index
2025-04-11T04:23:18.4075629Z                 break
2025-04-11T04:23:18.4075833Z     
2025-04-11T04:23:18.4076025Z         # Return if there was no error.
2025-04-11T04:23:18.4076285Z         if error_index is None:
2025-04-11T04:23:18.4076586Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4076909Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4077161Z     
2025-04-11T04:23:18.4077410Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4077739Z         for process in self.processes:
2025-04-11T04:23:18.4078007Z             if process.is_alive():
2025-04-11T04:23:18.4078262Z                 process.terminate()
2025-04-11T04:23:18.4078534Z             process.join()
2025-04-11T04:23:18.4078746Z     
2025-04-11T04:23:18.4078995Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4079333Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4079638Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4079961Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4080244Z             if exitcode < 0:
2025-04-11T04:23:18.4080507Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4080816Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4081154Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4081493Z                     error_index=error_index,
2025-04-11T04:23:18.4081772Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4082041Z                     exit_code=exitcode,
2025-04-11T04:23:18.4082298Z                     signal_name=name,
2025-04-11T04:23:18.4082539Z                 )
2025-04-11T04:23:18.4082738Z             else:
2025-04-11T04:23:18.4082966Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4083300Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4083647Z                     error_index=error_index,
2025-04-11T04:23:18.4083924Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4084198Z                     exit_code=exitcode,
2025-04-11T04:23:18.4084539Z                 )
2025-04-11T04:23:18.4084718Z     
2025-04-11T04:23:18.4084952Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4085330Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4085660Z         msg += original_trace
2025-04-11T04:23:18.4085984Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4086408Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4086719Z E       
2025-04-11T04:23:18.4086957Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4087255Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4087728Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4088172Z E           fn(i, *args)
2025-04-11T04:23:18.4088587Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
2025-04-11T04:23:18.4089033Z E           check_dataloader_sharding()
2025-04-11T04:23:18.4089510Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
2025-04-11T04:23:18.4090133Z E           batch = next(iter(train_dataloader))[0].cuda()
2025-04-11T04:23:18.4090438Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4090898Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4091408Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4091787Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4092030Z 
2025-04-11T04:23:18.4092345Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4092908Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4093307Z [04/11/25 04:12:37] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4093672Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4093999Z                              :75 launch                                         
2025-04-11T04:23:18.4094333Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4094676Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4095084Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4095500Z ______________________________ test_gemini_plugin ______________________________
2025-04-11T04:23:18.4095709Z 
2025-04-11T04:23:18.4095816Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4096587Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4097273Z 
2025-04-11T04:23:18.4097381Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4097639Z         try_count = 0
2025-04-11T04:23:18.4097875Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4098135Z             max_try, int
2025-04-11T04:23:18.4098416Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4098702Z     
2025-04-11T04:23:18.4098917Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4099181Z             try:
2025-04-11T04:23:18.4099389Z                 try_count += 1
2025-04-11T04:23:18.4099619Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4099875Z                 return ret
2025-04-11T04:23:18.4100123Z             except exception_type as e:
2025-04-11T04:23:18.4100481Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4100840Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4101217Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4101544Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4101920Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4102246Z                     continue
2025-04-11T04:23:18.4102479Z                 else:
2025-04-11T04:23:18.4102837Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4103209Z >                   raise e
2025-04-11T04:23:18.4103371Z 
2025-04-11T04:23:18.4103469Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4103750Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4104075Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4104369Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4104711Z tests/test_booster/test_plugin/test_gemini_plugin.py:172: in test_gemini_plugin
2025-04-11T04:23:18.4105168Z     spawn(run_dist, 4, early_stop=early_stop)
2025-04-11T04:23:18.4105451Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4105726Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4106167Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4106689Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4107226Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4107700Z     while not context.join():
2025-04-11T04:23:18.4107974Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4108190Z 
2025-04-11T04:23:18.4108407Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84917e0>
2025-04-11T04:23:18.4108828Z timeout = None
2025-04-11T04:23:18.4108950Z 
2025-04-11T04:23:18.4109053Z     def join(self, timeout=None):
2025-04-11T04:23:18.4109343Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4109615Z     
2025-04-11T04:23:18.4109875Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4110247Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4110648Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4110996Z         of the first process exiting.
2025-04-11T04:23:18.4111231Z     
2025-04-11T04:23:18.4111484Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4111842Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4112123Z     
2025-04-11T04:23:18.4112301Z         Args:
2025-04-11T04:23:18.4112545Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4112837Z         """
2025-04-11T04:23:18.4113083Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4113386Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4113660Z             return True
2025-04-11T04:23:18.4113854Z     
2025-04-11T04:23:18.4114085Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4114414Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4114704Z             self.sentinels.keys(),
2025-04-11T04:23:18.4114950Z             timeout=timeout,
2025-04-11T04:23:18.4115158Z         )
2025-04-11T04:23:18.4115332Z     
2025-04-11T04:23:18.4115520Z         error_index = None
2025-04-11T04:23:18.4115752Z         for sentinel in ready:
2025-04-11T04:23:18.4115998Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4116383Z             process = self.processes[index]
2025-04-11T04:23:18.4116649Z             process.join()
2025-04-11T04:23:18.4116885Z             if process.exitcode != 0:
2025-04-11T04:23:18.4117151Z                 error_index = index
2025-04-11T04:23:18.4117385Z                 break
2025-04-11T04:23:18.4117589Z     
2025-04-11T04:23:18.4117788Z         # Return if there was no error.
2025-04-11T04:23:18.4118049Z         if error_index is None:
2025-04-11T04:23:18.4118348Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4118655Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4118899Z     
2025-04-11T04:23:18.4119140Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4119455Z         for process in self.processes:
2025-04-11T04:23:18.4119716Z             if process.is_alive():
2025-04-11T04:23:18.4119955Z                 process.terminate()
2025-04-11T04:23:18.4120199Z             process.join()
2025-04-11T04:23:18.4120419Z     
2025-04-11T04:23:18.4120663Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4120999Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4121411Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4121721Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4122005Z             if exitcode < 0:
2025-04-11T04:23:18.4122268Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4122562Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4122885Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4123216Z                     error_index=error_index,
2025-04-11T04:23:18.4123498Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4123769Z                     exit_code=exitcode,
2025-04-11T04:23:18.4124028Z                     signal_name=name,
2025-04-11T04:23:18.4124263Z                 )
2025-04-11T04:23:18.4124451Z             else:
2025-04-11T04:23:18.4124673Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4125010Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4125347Z                     error_index=error_index,
2025-04-11T04:23:18.4125610Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4125866Z                     exit_code=exitcode,
2025-04-11T04:23:18.4126098Z                 )
2025-04-11T04:23:18.4126284Z     
2025-04-11T04:23:18.4126515Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4126890Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4127212Z         msg += original_trace
2025-04-11T04:23:18.4127532Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4127935Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4128243Z E       
2025-04-11T04:23:18.4128476Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4128767Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4129241Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4129697Z E           fn(i, *args)
2025-04-11T04:23:18.4130110Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
2025-04-11T04:23:18.4130593Z E           check_gemini_plugin(early_stop=early_stop)
2025-04-11T04:23:18.4131039Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4131452Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4131861Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4132267Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4132770Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4133180Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4133433Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.4133933Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
2025-04-11T04:23:18.4134542Z E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
2025-04-11T04:23:18.4135049Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4135466Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4135912Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4136385Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4136839Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4137329Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4137714Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4138196Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4138717Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4139091Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4139336Z 
2025-04-11T04:23:18.4139650Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4140217Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4140616Z [04/11/25 04:12:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4140991Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4141317Z                              :75 launch                                         
2025-04-11T04:23:18.4141653Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4141991Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4142397Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4142826Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4144207Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4145588Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4146995Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4148363Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4149747Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4151199Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4152549Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4153887Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4154808Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4155660Z   warnings.warn(
2025-04-11T04:23:18.4156555Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4157388Z   warnings.warn(
2025-04-11T04:23:18.4158180Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4159012Z   warnings.warn(
2025-04-11T04:23:18.4159810Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4160655Z   warnings.warn(
2025-04-11T04:23:18.4161605Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4162573Z   warnings.warn(
2025-04-11T04:23:18.4163488Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4164443Z   warnings.warn(
2025-04-11T04:23:18.4165357Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4166307Z   warnings.warn(
2025-04-11T04:23:18.4167217Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4168167Z   warnings.warn(
2025-04-11T04:23:18.4169076Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4170111Z   warnings.warn(
2025-04-11T04:23:18.4171010Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4171957Z   warnings.warn(
2025-04-11T04:23:18.4172875Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4173814Z   warnings.warn(
2025-04-11T04:23:18.4174720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4175760Z   warnings.warn(
2025-04-11T04:23:18.4176674Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4177622Z   warnings.warn(
2025-04-11T04:23:18.4178542Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4179488Z   warnings.warn(
2025-04-11T04:23:18.4180392Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4181344Z   warnings.warn(
2025-04-11T04:23:18.4182255Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4183191Z   warnings.warn(
2025-04-11T04:23:18.4183612Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25212 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4184264Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25212 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4184920Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25212 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4185841Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4186533Z   warnings.warn(
2025-04-11T04:23:18.4187191Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4187878Z   warnings.warn(
2025-04-11T04:23:18.4188606Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4189428Z   warnings.warn(
2025-04-11T04:23:18.4190080Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4190772Z   warnings.warn(
2025-04-11T04:23:18.4191423Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4192114Z   warnings.warn(
2025-04-11T04:23:18.4192770Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4193462Z   warnings.warn(
2025-04-11T04:23:18.4194137Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4194840Z   warnings.warn(
2025-04-11T04:23:18.4195585Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4196267Z   warnings.warn(
2025-04-11T04:23:18.4196929Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4197636Z   warnings.warn(
2025-04-11T04:23:18.4198309Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4198998Z   warnings.warn(
2025-04-11T04:23:18.4199666Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4200365Z   warnings.warn(
2025-04-11T04:23:18.4201007Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4201690Z   warnings.warn(
2025-04-11T04:23:18.4202341Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4203029Z   warnings.warn(
2025-04-11T04:23:18.4203674Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4204358Z   warnings.warn(
2025-04-11T04:23:18.4204632Z __________________________ test_low_level_zero_plugin __________________________
2025-04-11T04:23:18.4204856Z 
2025-04-11T04:23:18.4204954Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4205724Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4206390Z 
2025-04-11T04:23:18.4206500Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4206764Z         try_count = 0
2025-04-11T04:23:18.4206999Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4207268Z             max_try, int
2025-04-11T04:23:18.4207559Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4207954Z     
2025-04-11T04:23:18.4208176Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4208441Z             try:
2025-04-11T04:23:18.4208644Z                 try_count += 1
2025-04-11T04:23:18.4208877Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4209125Z                 return ret
2025-04-11T04:23:18.4209358Z             except exception_type as e:
2025-04-11T04:23:18.4209627Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4209988Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4210359Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4210695Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4211070Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4211379Z                     continue
2025-04-11T04:23:18.4211599Z                 else:
2025-04-11T04:23:18.4211933Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4212404Z >                   raise e
2025-04-11T04:23:18.4212545Z 
2025-04-11T04:23:18.4212642Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4212915Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4213235Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4213524Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4213898Z tests/test_booster/test_plugin/test_low_level_zero_plugin.py:141: in test_low_level_zero_plugin
2025-04-11T04:23:18.4214307Z     spawn(run_dist, 2, early_stop=early_stop)
2025-04-11T04:23:18.4214585Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4214861Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4215291Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4215805Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4216335Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4216788Z     while not context.join():
2025-04-11T04:23:18.4217046Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4217226Z 
2025-04-11T04:23:18.4217431Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb83b36d6c0>
2025-04-11T04:23:18.4217775Z timeout = None
2025-04-11T04:23:18.4217887Z 
2025-04-11T04:23:18.4217986Z     def join(self, timeout=None):
2025-04-11T04:23:18.4218261Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4218528Z     
2025-04-11T04:23:18.4218776Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4219137Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4219526Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4219849Z         of the first process exiting.
2025-04-11T04:23:18.4220093Z     
2025-04-11T04:23:18.4220336Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4220697Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4220983Z     
2025-04-11T04:23:18.4221144Z         Args:
2025-04-11T04:23:18.4221395Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4221679Z         """
2025-04-11T04:23:18.4221919Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4222217Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4222448Z             return True
2025-04-11T04:23:18.4222646Z     
2025-04-11T04:23:18.4222872Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4223191Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4223578Z             self.sentinels.keys(),
2025-04-11T04:23:18.4223816Z             timeout=timeout,
2025-04-11T04:23:18.4224033Z         )
2025-04-11T04:23:18.4224204Z     
2025-04-11T04:23:18.4224386Z         error_index = None
2025-04-11T04:23:18.4224615Z         for sentinel in ready:
2025-04-11T04:23:18.4224866Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4225147Z             process = self.processes[index]
2025-04-11T04:23:18.4225406Z             process.join()
2025-04-11T04:23:18.4225638Z             if process.exitcode != 0:
2025-04-11T04:23:18.4225884Z                 error_index = index
2025-04-11T04:23:18.4226108Z                 break
2025-04-11T04:23:18.4226300Z     
2025-04-11T04:23:18.4226486Z         # Return if there was no error.
2025-04-11T04:23:18.4226737Z         if error_index is None:
2025-04-11T04:23:18.4227006Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4227310Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4227552Z     
2025-04-11T04:23:18.4227789Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4228195Z         for process in self.processes:
2025-04-11T04:23:18.4228500Z             if process.is_alive():
2025-04-11T04:23:18.4228741Z                 process.terminate()
2025-04-11T04:23:18.4228982Z             process.join()
2025-04-11T04:23:18.4229189Z     
2025-04-11T04:23:18.4229423Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4229740Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4230033Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4230339Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4230617Z             if exitcode < 0:
2025-04-11T04:23:18.4230861Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4231145Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4231465Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4231781Z                     error_index=error_index,
2025-04-11T04:23:18.4232060Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4232321Z                     exit_code=exitcode,
2025-04-11T04:23:18.4232577Z                     signal_name=name,
2025-04-11T04:23:18.4232800Z                 )
2025-04-11T04:23:18.4233005Z             else:
2025-04-11T04:23:18.4233223Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4233567Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4233896Z                     error_index=error_index,
2025-04-11T04:23:18.4234145Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4234404Z                     exit_code=exitcode,
2025-04-11T04:23:18.4234633Z                 )
2025-04-11T04:23:18.4234816Z     
2025-04-11T04:23:18.4235049Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4235414Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4235748Z         msg += original_trace
2025-04-11T04:23:18.4236063Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4236460Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4236772Z E       
2025-04-11T04:23:18.4236995Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4237287Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4237756Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4238216Z E           fn(i, *args)
2025-04-11T04:23:18.4238646Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
2025-04-11T04:23:18.4239156Z E           check_low_level_zero_plugin(early_stop=early_stop)
2025-04-11T04:23:18.4239709Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4240154Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4240666Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
2025-04-11T04:23:18.4241235Z E           err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:18.4241679Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4242083Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4242505Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4242947Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4243411Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4243886Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4244268Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4244739Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4245237Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4245611Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4245849Z 
2025-04-11T04:23:18.4246158Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4246696Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4247084Z [04/11/25 04:12:52] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4247441Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4247753Z                              :75 launch                                         
2025-04-11T04:23:18.4248076Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4248412Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4248808Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4249227Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4250600Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4251980Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4253334Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4254703Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4255616Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4256550Z   warnings.warn(
2025-04-11T04:23:18.4257348Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4258181Z   warnings.warn(
2025-04-11T04:23:18.4259122Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4260085Z   warnings.warn(
2025-04-11T04:23:18.4261007Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4261954Z   warnings.warn(
2025-04-11T04:23:18.4262872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4263905Z   warnings.warn(
2025-04-11T04:23:18.4264810Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4265752Z   warnings.warn(
2025-04-11T04:23:18.4266661Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4267608Z   warnings.warn(
2025-04-11T04:23:18.4268543Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4269479Z   warnings.warn(
2025-04-11T04:23:18.4270146Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4270835Z   warnings.warn(
2025-04-11T04:23:18.4271475Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4272178Z   warnings.warn(
2025-04-11T04:23:18.4272850Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4273537Z   warnings.warn(
2025-04-11T04:23:18.4274188Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4274870Z   warnings.warn(
2025-04-11T04:23:18.4275130Z ____________________________ test_torch_ddp_plugin _____________________________
2025-04-11T04:23:18.4275349Z 
2025-04-11T04:23:18.4275443Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4276295Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4276959Z 
2025-04-11T04:23:18.4277065Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4277322Z         try_count = 0
2025-04-11T04:23:18.4277551Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4277809Z             max_try, int
2025-04-11T04:23:18.4278083Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4278373Z     
2025-04-11T04:23:18.4278584Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4278847Z             try:
2025-04-11T04:23:18.4279044Z                 try_count += 1
2025-04-11T04:23:18.4279272Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4279518Z                 return ret
2025-04-11T04:23:18.4279751Z             except exception_type as e:
2025-04-11T04:23:18.4280017Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4280371Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4323224Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4323646Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4324053Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4324388Z                     continue
2025-04-11T04:23:18.4324609Z                 else:
2025-04-11T04:23:18.4324982Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4325372Z >                   raise e
2025-04-11T04:23:18.4325518Z 
2025-04-11T04:23:18.4325625Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4325920Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4326273Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4326569Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4326946Z tests/test_booster/test_plugin/test_torch_ddp_plugin.py:119: in test_torch_ddp_plugin
2025-04-11T04:23:18.4327323Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4327576Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4327857Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4328316Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4328868Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4329419Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4329898Z     while not context.join():
2025-04-11T04:23:18.4330176Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4330366Z 
2025-04-11T04:23:18.4330585Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84938e0>
2025-04-11T04:23:18.4330964Z timeout = None
2025-04-11T04:23:18.4331095Z 
2025-04-11T04:23:18.4331193Z     def join(self, timeout=None):
2025-04-11T04:23:18.4331484Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4331760Z     
2025-04-11T04:23:18.4332016Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4332375Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4332754Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4333088Z         of the first process exiting.
2025-04-11T04:23:18.4333332Z     
2025-04-11T04:23:18.4333584Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4333946Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4334435Z     
2025-04-11T04:23:18.4334616Z         Args:
2025-04-11T04:23:18.4334876Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4335173Z         """
2025-04-11T04:23:18.4335418Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4335722Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4335972Z             return True
2025-04-11T04:23:18.4336171Z     
2025-04-11T04:23:18.4336404Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4336717Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4337009Z             self.sentinels.keys(),
2025-04-11T04:23:18.4337249Z             timeout=timeout,
2025-04-11T04:23:18.4337475Z         )
2025-04-11T04:23:18.4337650Z     
2025-04-11T04:23:18.4337826Z         error_index = None
2025-04-11T04:23:18.4338103Z         for sentinel in ready:
2025-04-11T04:23:18.4338356Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4338645Z             process = self.processes[index]
2025-04-11T04:23:18.4338909Z             process.join()
2025-04-11T04:23:18.4339133Z             if process.exitcode != 0:
2025-04-11T04:23:18.4339504Z                 error_index = index
2025-04-11T04:23:18.4339746Z                 break
2025-04-11T04:23:18.4339945Z     
2025-04-11T04:23:18.4340130Z         # Return if there was no error.
2025-04-11T04:23:18.4340382Z         if error_index is None:
2025-04-11T04:23:18.4340664Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4340969Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4341211Z     
2025-04-11T04:23:18.4341443Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4341755Z         for process in self.processes:
2025-04-11T04:23:18.4342008Z             if process.is_alive():
2025-04-11T04:23:18.4342253Z                 process.terminate()
2025-04-11T04:23:18.4342492Z             process.join()
2025-04-11T04:23:18.4342697Z     
2025-04-11T04:23:18.4342929Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4343259Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4343556Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4343862Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4344135Z             if exitcode < 0:
2025-04-11T04:23:18.4344378Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4344664Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4344987Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4345302Z                     error_index=error_index,
2025-04-11T04:23:18.4345570Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4345824Z                     exit_code=exitcode,
2025-04-11T04:23:18.4346068Z                     signal_name=name,
2025-04-11T04:23:18.4346301Z                 )
2025-04-11T04:23:18.4346491Z             else:
2025-04-11T04:23:18.4346713Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4347057Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4347397Z                     error_index=error_index,
2025-04-11T04:23:18.4347657Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4347925Z                     exit_code=exitcode,
2025-04-11T04:23:18.4348159Z                 )
2025-04-11T04:23:18.4348337Z     
2025-04-11T04:23:18.4348627Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4349005Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4349334Z         msg += original_trace
2025-04-11T04:23:18.4349654Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4350056Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4350466Z E       
2025-04-11T04:23:18.4350696Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4350991Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4351474Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4351931Z E           fn(i, *args)
2025-04-11T04:23:18.4352348Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
2025-04-11T04:23:18.4352799Z E           check_torch_ddp_plugin()
2025-04-11T04:23:18.4353281Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
2025-04-11T04:23:18.4353800Z E           run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:18.4354216Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4354617Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4355041Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4355606Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4356062Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4356538Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4356820Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4357297Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4357803Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4358175Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4358414Z 
2025-04-11T04:23:18.4358725Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4359263Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4359657Z [04/11/25 04:12:59] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4360009Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4360310Z                              :75 launch                                         
2025-04-11T04:23:18.4360625Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4360954Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4361344Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4361759Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4363106Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4364480Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4365809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4367153Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4368152Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4368993Z   warnings.warn(
2025-04-11T04:23:18.4369783Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4370611Z   warnings.warn(
2025-04-11T04:23:18.4371539Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4372488Z   warnings.warn(
2025-04-11T04:23:18.4373383Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4374403Z   warnings.warn(
2025-04-11T04:23:18.4375307Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4376254Z   warnings.warn(
2025-04-11T04:23:18.4377154Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4378096Z   warnings.warn(
2025-04-11T04:23:18.4379001Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4379968Z   warnings.warn(
2025-04-11T04:23:18.4380876Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4381813Z   warnings.warn(
2025-04-11T04:23:18.4382497Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4383208Z   warnings.warn(
2025-04-11T04:23:18.4383846Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4384527Z   warnings.warn(
2025-04-11T04:23:18.4385180Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4385867Z   warnings.warn(
2025-04-11T04:23:18.4386509Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4387275Z   warnings.warn(
2025-04-11T04:23:18.4387534Z ____________________________ test_torch_fsdp_plugin ____________________________
2025-04-11T04:23:18.4387769Z 
2025-04-11T04:23:18.4387869Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4388675Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4389334Z 
2025-04-11T04:23:18.4389442Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4389701Z         try_count = 0
2025-04-11T04:23:18.4389934Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4390201Z             max_try, int
2025-04-11T04:23:18.4390478Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4390782Z     
2025-04-11T04:23:18.4390997Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4391266Z             try:
2025-04-11T04:23:18.4391458Z                 try_count += 1
2025-04-11T04:23:18.4391806Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4392068Z                 return ret
2025-04-11T04:23:18.4392315Z             except exception_type as e:
2025-04-11T04:23:18.4392579Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4392945Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4393317Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4393650Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4394020Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4394319Z                     continue
2025-04-11T04:23:18.4394528Z                 else:
2025-04-11T04:23:18.4394866Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4395228Z >                   raise e
2025-04-11T04:23:18.4395364Z 
2025-04-11T04:23:18.4395458Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4395722Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4396034Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4396314Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4396654Z tests/test_booster/test_plugin/test_torch_fsdp_plugin.py:83: in test_torch_fsdp_plugin
2025-04-11T04:23:18.4397010Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4397232Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4397498Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4397920Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4398418Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4398941Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4399391Z     while not context.join():
2025-04-11T04:23:18.4399640Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4399822Z 
2025-04-11T04:23:18.4400020Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b838ccd0>
2025-04-11T04:23:18.4400360Z timeout = None
2025-04-11T04:23:18.4400470Z 
2025-04-11T04:23:18.4400566Z     def join(self, timeout=None):
2025-04-11T04:23:18.4400833Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4401095Z     
2025-04-11T04:23:18.4401333Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4401699Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4402092Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4402519Z         of the first process exiting.
2025-04-11T04:23:18.4402744Z     
2025-04-11T04:23:18.4402987Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4403340Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4403611Z     
2025-04-11T04:23:18.4403770Z         Args:
2025-04-11T04:23:18.4404016Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4404291Z         """
2025-04-11T04:23:18.4404530Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4404822Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4405052Z             return True
2025-04-11T04:23:18.4405246Z     
2025-04-11T04:23:18.4405463Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4405772Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4406046Z             self.sentinels.keys(),
2025-04-11T04:23:18.4406280Z             timeout=timeout,
2025-04-11T04:23:18.4406489Z         )
2025-04-11T04:23:18.4406653Z     
2025-04-11T04:23:18.4406926Z         error_index = None
2025-04-11T04:23:18.4407145Z         for sentinel in ready:
2025-04-11T04:23:18.4407392Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4407664Z             process = self.processes[index]
2025-04-11T04:23:18.4407913Z             process.join()
2025-04-11T04:23:18.4408138Z             if process.exitcode != 0:
2025-04-11T04:23:18.4408379Z                 error_index = index
2025-04-11T04:23:18.4408600Z                 break
2025-04-11T04:23:18.4408790Z     
2025-04-11T04:23:18.4408972Z         # Return if there was no error.
2025-04-11T04:23:18.4409216Z         if error_index is None:
2025-04-11T04:23:18.4410916Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4411205Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4411449Z     
2025-04-11T04:23:18.4411679Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4411994Z         for process in self.processes:
2025-04-11T04:23:18.4412245Z             if process.is_alive():
2025-04-11T04:23:18.4412490Z                 process.terminate()
2025-04-11T04:23:18.4412724Z             process.join()
2025-04-11T04:23:18.4412932Z     
2025-04-11T04:23:18.4413161Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4413481Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4413762Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4414063Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4414331Z             if exitcode < 0:
2025-04-11T04:23:18.4414574Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4414850Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4415163Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4415474Z                     error_index=error_index,
2025-04-11T04:23:18.4415734Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4415993Z                     exit_code=exitcode,
2025-04-11T04:23:18.4416231Z                     signal_name=name,
2025-04-11T04:23:18.4416450Z                 )
2025-04-11T04:23:18.4416632Z             else:
2025-04-11T04:23:18.4416838Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4417160Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4417483Z                     error_index=error_index,
2025-04-11T04:23:18.4417731Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4417981Z                     exit_code=exitcode,
2025-04-11T04:23:18.4418205Z                 )
2025-04-11T04:23:18.4418380Z     
2025-04-11T04:23:18.4418600Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4419082Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4419404Z         msg += original_trace
2025-04-11T04:23:18.4419719Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4420116Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4420425Z E       
2025-04-11T04:23:18.4420658Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4420948Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4421420Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4421862Z E           fn(i, *args)
2025-04-11T04:23:18.4422275Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
2025-04-11T04:23:18.4422715Z E           check_torch_fsdp_plugin()
2025-04-11T04:23:18.4423191Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
2025-04-11T04:23:18.4423325Z E           run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:18.4423642Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4423745Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4424002Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4424105Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4424383Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4424489Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4424593Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4424880Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4425024Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4425188Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4425193Z 
2025-04-11T04:23:18.4425508Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4425664Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4425821Z [04/11/25 04:13:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4425950Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4426062Z                              :75 launch                                         
2025-04-11T04:23:18.4426200Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4426328Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4426420Z custom_hanging_param_model
2025-04-11T04:23:18.4426508Z custom_hanging_param_model
2025-04-11T04:23:18.4426711Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4426858Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4427987Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4428154Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4429300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4429561Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4430248Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4430335Z   warnings.warn(
2025-04-11T04:23:18.4431006Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4431181Z   warnings.warn(
2025-04-11T04:23:18.4431994Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4432078Z   warnings.warn(
2025-04-11T04:23:18.4432870Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4432953Z   warnings.warn(
2025-04-11T04:23:18.4433731Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4433821Z   warnings.warn(
2025-04-11T04:23:18.4434606Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4434687Z   warnings.warn(
2025-04-11T04:23:18.4435467Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4435554Z   warnings.warn(
2025-04-11T04:23:18.4436337Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4436425Z   warnings.warn(
2025-04-11T04:23:18.4436973Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4437057Z   warnings.warn(
2025-04-11T04:23:18.4437582Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4437660Z   warnings.warn(
2025-04-11T04:23:18.4438288Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4438370Z   warnings.warn(
2025-04-11T04:23:18.4438897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4438972Z   warnings.warn(
2025-04-11T04:23:18.4439112Z ______________________________ test_gemini_ckpIO _______________________________
2025-04-11T04:23:18.4439116Z 
2025-04-11T04:23:18.4439207Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4439808Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4439818Z 
2025-04-11T04:23:18.4439921Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4440090Z         try_count = 0
2025-04-11T04:23:18.4440192Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4440272Z             max_try, int
2025-04-11T04:23:18.4440423Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4440496Z     
2025-04-11T04:23:18.4440630Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4440707Z             try:
2025-04-11T04:23:18.4440795Z                 try_count += 1
2025-04-11T04:23:18.4440892Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4440971Z                 return ret
2025-04-11T04:23:18.4441078Z             except exception_type as e:
2025-04-11T04:23:18.4441185Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4441420Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4441565Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4441717Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4441883Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4441963Z                     continue
2025-04-11T04:23:18.4442044Z                 else:
2025-04-11T04:23:18.4442273Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4442359Z >                   raise e
2025-04-11T04:23:18.4442364Z 
2025-04-11T04:23:18.4442456Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4442570Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4442701Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4442787Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4442984Z tests/test_checkpoint_io/test_gemini_checkpoint_io.py:220: in test_gemini_ckpIO
2025-04-11T04:23:18.4443067Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4443174Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4443271Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4443528Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4443717Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4444002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4444097Z     while not context.join():
2025-04-11T04:23:18.4444203Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4444208Z 
2025-04-11T04:23:18.4444408Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8493940>
2025-04-11T04:23:18.4444486Z timeout = None
2025-04-11T04:23:18.4444580Z 
2025-04-11T04:23:18.4444676Z     def join(self, timeout=None):
2025-04-11T04:23:18.4444802Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4444876Z     
2025-04-11T04:23:18.4445027Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4445170Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4445339Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4445433Z         of the first process exiting.
2025-04-11T04:23:18.4445510Z     
2025-04-11T04:23:18.4445659Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4445799Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4445872Z     
2025-04-11T04:23:18.4445944Z         Args:
2025-04-11T04:23:18.4446085Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4446160Z         """
2025-04-11T04:23:18.4446298Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4446395Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4446563Z             return True
2025-04-11T04:23:18.4446639Z     
2025-04-11T04:23:18.4446772Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4446891Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4446983Z             self.sentinels.keys(),
2025-04-11T04:23:18.4447069Z             timeout=timeout,
2025-04-11T04:23:18.4447145Z         )
2025-04-11T04:23:18.4447214Z     
2025-04-11T04:23:18.4447300Z         error_index = None
2025-04-11T04:23:18.4447386Z         for sentinel in ready:
2025-04-11T04:23:18.4447491Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4447594Z             process = self.processes[index]
2025-04-11T04:23:18.4447678Z             process.join()
2025-04-11T04:23:18.4447775Z             if process.exitcode != 0:
2025-04-11T04:23:18.4447866Z                 error_index = index
2025-04-11T04:23:18.4447940Z                 break
2025-04-11T04:23:18.4448014Z     
2025-04-11T04:23:18.4448107Z         # Return if there was no error.
2025-04-11T04:23:18.4448196Z         if error_index is None:
2025-04-11T04:23:18.4448345Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4448445Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4448531Z     
2025-04-11T04:23:18.4448677Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4448778Z         for process in self.processes:
2025-04-11T04:23:18.4448867Z             if process.is_alive():
2025-04-11T04:23:18.4448963Z                 process.terminate()
2025-04-11T04:23:18.4449046Z             process.join()
2025-04-11T04:23:18.4449117Z     
2025-04-11T04:23:18.4449263Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4449376Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4449490Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4449611Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4449698Z             if exitcode < 0:
2025-04-11T04:23:18.4449805Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4449909Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4450066Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4450160Z                     error_index=error_index,
2025-04-11T04:23:18.4450265Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4450352Z                     exit_code=exitcode,
2025-04-11T04:23:18.4450436Z                     signal_name=name,
2025-04-11T04:23:18.4450517Z                 )
2025-04-11T04:23:18.4450589Z             else:
2025-04-11T04:23:18.4450695Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4450857Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4451071Z                     error_index=error_index,
2025-04-11T04:23:18.4451175Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4451259Z                     exit_code=exitcode,
2025-04-11T04:23:18.4451339Z                 )
2025-04-11T04:23:18.4451408Z     
2025-04-11T04:23:18.4451544Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4451711Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4451795Z         msg += original_trace
2025-04-11T04:23:18.4451966Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4452123Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4452206Z E       
2025-04-11T04:23:18.4452331Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4452436Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4452739Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4452904Z E           fn(i, *args)
2025-04-11T04:23:18.4453189Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
2025-04-11T04:23:18.4453273Z E           exam_state_dict()
2025-04-11T04:23:18.4453499Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4453599Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4453864Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4453963Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4454240Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4454351Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4454461Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4454753Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4454891Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4455054Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4455060Z 
2025-04-11T04:23:18.4455367Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4455522Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4455677Z [04/11/25 04:13:14] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4455808Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4455917Z                              :75 launch                                         
2025-04-11T04:23:18.4456059Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4456193Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4456392Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4456544Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4457687Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4457867Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4459089Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4459265Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4460384Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4460555Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4461662Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4461911Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4462603Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4462691Z   warnings.warn(
2025-04-11T04:23:18.4463380Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4463470Z   warnings.warn(
2025-04-11T04:23:18.4464159Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4464240Z   warnings.warn(
2025-04-11T04:23:18.4464929Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4465015Z   warnings.warn(
2025-04-11T04:23:18.4465828Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4465912Z   warnings.warn(
2025-04-11T04:23:18.4466712Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4466789Z   warnings.warn(
2025-04-11T04:23:18.4467573Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4467749Z   warnings.warn(
2025-04-11T04:23:18.4468605Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4468692Z   warnings.warn(
2025-04-11T04:23:18.4469505Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4469584Z   warnings.warn(
2025-04-11T04:23:18.4470381Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4470563Z   warnings.warn(
2025-04-11T04:23:18.4471369Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4471446Z   warnings.warn(
2025-04-11T04:23:18.4472254Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4472334Z   warnings.warn(
2025-04-11T04:23:18.4473146Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4473228Z   warnings.warn(
2025-04-11T04:23:18.4474038Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4474119Z   warnings.warn(
2025-04-11T04:23:18.4474926Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4475010Z   warnings.warn(
2025-04-11T04:23:18.4475826Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4475907Z   warnings.warn(
2025-04-11T04:23:18.4476206Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:55733 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4476781Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4476857Z   warnings.warn(
2025-04-11T04:23:18.4477512Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4477594Z   warnings.warn(
2025-04-11T04:23:18.4478137Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4478216Z   warnings.warn(
2025-04-11T04:23:18.4478742Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4478818Z   warnings.warn(
2025-04-11T04:23:18.4479353Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4479438Z   warnings.warn(
2025-04-11T04:23:18.4479959Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4480145Z   warnings.warn(
2025-04-11T04:23:18.4480675Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4480757Z   warnings.warn(
2025-04-11T04:23:18.4481282Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4481365Z   warnings.warn(
2025-04-11T04:23:18.4481537Z Exception ignored in: <function GeminiDDP.__del__ at 0x7fc3b46f5f30>
2025-04-11T04:23:18.4481634Z Traceback (most recent call last):
2025-04-11T04:23:18.4481875Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.4481964Z     self.remove_hooks()
2025-04-11T04:23:18.4482215Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.4482319Z     for p in self.module.parameters():
2025-04-11T04:23:18.4482617Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.4482810Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.4482965Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.4483126Z Exception ignored in: <function GeminiDDP.__del__ at 0x7fec640edf30>
2025-04-11T04:23:18.4483219Z Traceback (most recent call last):
2025-04-11T04:23:18.4483457Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.4483541Z     self.remove_hooks()
2025-04-11T04:23:18.4483791Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.4483891Z     for p in self.module.parameters():
2025-04-11T04:23:18.4484182Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.4484371Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.4484521Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.4484687Z Exception ignored in: <function GeminiDDP.__del__ at 0x7fd220b15ea0>
2025-04-11T04:23:18.4484777Z Traceback (most recent call last):
2025-04-11T04:23:18.4485010Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.4485186Z     self.remove_hooks()
2025-04-11T04:23:18.4485422Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.4485528Z     for p in self.module.parameters():
2025-04-11T04:23:18.4485815Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.4486012Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.4486161Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.4486315Z _____________________________ test_gemini_ckpIO[2] _____________________________
2025-04-11T04:23:18.4486319Z 
2025-04-11T04:23:18.4486444Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.4487052Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4487062Z 
2025-04-11T04:23:18.4487163Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4487339Z         try_count = 0
2025-04-11T04:23:18.4487441Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4487522Z             max_try, int
2025-04-11T04:23:18.4487675Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4487748Z     
2025-04-11T04:23:18.4487862Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4487936Z             try:
2025-04-11T04:23:18.4488020Z                 try_count += 1
2025-04-11T04:23:18.4488115Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4488195Z                 return ret
2025-04-11T04:23:18.4488291Z             except exception_type as e:
2025-04-11T04:23:18.4488390Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4488579Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4488700Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4488854Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4489016Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4489099Z                     continue
2025-04-11T04:23:18.4489185Z                 else:
2025-04-11T04:23:18.4489413Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4489503Z >                   raise e
2025-04-11T04:23:18.4489507Z 
2025-04-11T04:23:18.4489601Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4489715Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4489849Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4489935Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4490146Z tests/test_checkpoint_io/test_gemini_torch_compability.py:175: in test_gemini_ckpIO
2025-04-11T04:23:18.4490235Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.4490343Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4490443Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4490697Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4490879Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4491161Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4491255Z     while not context.join():
2025-04-11T04:23:18.4491366Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4491371Z 
2025-04-11T04:23:18.4491575Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb83b36d570>
2025-04-11T04:23:18.4491746Z timeout = None
2025-04-11T04:23:18.4491751Z 
2025-04-11T04:23:18.4491841Z     def join(self, timeout=None):
2025-04-11T04:23:18.4491967Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4492043Z     
2025-04-11T04:23:18.4492193Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4492340Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4492507Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4492601Z         of the first process exiting.
2025-04-11T04:23:18.4492669Z     
2025-04-11T04:23:18.4492820Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4492957Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4493030Z     
2025-04-11T04:23:18.4493103Z         Args:
2025-04-11T04:23:18.4493237Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4493317Z         """
2025-04-11T04:23:18.4493456Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4493640Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4493720Z             return True
2025-04-11T04:23:18.4493793Z     
2025-04-11T04:23:18.4493925Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4494040Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4494134Z             self.sentinels.keys(),
2025-04-11T04:23:18.4494219Z             timeout=timeout,
2025-04-11T04:23:18.4494296Z         )
2025-04-11T04:23:18.4494366Z     
2025-04-11T04:23:18.4494448Z         error_index = None
2025-04-11T04:23:18.4494535Z         for sentinel in ready:
2025-04-11T04:23:18.4494641Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4494743Z             process = self.processes[index]
2025-04-11T04:23:18.4494829Z             process.join()
2025-04-11T04:23:18.4494924Z             if process.exitcode != 0:
2025-04-11T04:23:18.4495023Z                 error_index = index
2025-04-11T04:23:18.4495100Z                 break
2025-04-11T04:23:18.4495181Z     
2025-04-11T04:23:18.4495273Z         # Return if there was no error.
2025-04-11T04:23:18.4495363Z         if error_index is None:
2025-04-11T04:23:18.4495505Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4495609Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4495682Z     
2025-04-11T04:23:18.4495818Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4495917Z         for process in self.processes:
2025-04-11T04:23:18.4496005Z             if process.is_alive():
2025-04-11T04:23:18.4496105Z                 process.terminate()
2025-04-11T04:23:18.4496193Z             process.join()
2025-04-11T04:23:18.4496264Z     
2025-04-11T04:23:18.4496418Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4496535Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4496643Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4496762Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4496850Z             if exitcode < 0:
2025-04-11T04:23:18.4496964Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4497067Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4497219Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4497316Z                     error_index=error_index,
2025-04-11T04:23:18.4497419Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4497506Z                     exit_code=exitcode,
2025-04-11T04:23:18.4497594Z                     signal_name=name,
2025-04-11T04:23:18.4497678Z                 )
2025-04-11T04:23:18.4497753Z             else:
2025-04-11T04:23:18.4497858Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4498116Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4498210Z                     error_index=error_index,
2025-04-11T04:23:18.4498319Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4498404Z                     exit_code=exitcode,
2025-04-11T04:23:18.4498485Z                 )
2025-04-11T04:23:18.4498555Z     
2025-04-11T04:23:18.4498684Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4498858Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4498944Z         msg += original_trace
2025-04-11T04:23:18.4499121Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4499279Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4499358Z E       
2025-04-11T04:23:18.4499484Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4499588Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4499888Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4500057Z E           fn(i, *args)
2025-04-11T04:23:18.4500351Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
2025-04-11T04:23:18.4500446Z E           exam_torch_load_from_gemini()
2025-04-11T04:23:18.4500670Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4500770Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4501029Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4501132Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4501409Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4501522Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4501628Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4501923Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4502055Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4502224Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4502230Z 
2025-04-11T04:23:18.4502536Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4502686Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4502846Z [04/11/25 04:13:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4502976Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4503091Z                              :75 launch                                         
2025-04-11T04:23:18.4503228Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4503359Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4503557Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4503701Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4504840Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4505113Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4506234Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4506408Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4507086Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4507175Z   warnings.warn(
2025-04-11T04:23:18.4507847Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4508041Z   warnings.warn(
2025-04-11T04:23:18.4508906Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4508987Z   warnings.warn(
2025-04-11T04:23:18.4509791Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4509873Z   warnings.warn(
2025-04-11T04:23:18.4510671Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4510754Z   warnings.warn(
2025-04-11T04:23:18.4511551Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4511631Z   warnings.warn(
2025-04-11T04:23:18.4512426Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4512510Z   warnings.warn(
2025-04-11T04:23:18.4513314Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4513392Z   warnings.warn(
2025-04-11T04:23:18.4513948Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4514025Z   warnings.warn(
2025-04-11T04:23:18.4514563Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4514744Z   warnings.warn(
2025-04-11T04:23:18.4515277Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4515363Z   warnings.warn(
2025-04-11T04:23:18.4515887Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4515968Z   warnings.warn(
2025-04-11T04:23:18.4516134Z Exception ignored in: <function GeminiDDP.__del__ at 0x7f4b31341750>
2025-04-11T04:23:18.4516232Z Traceback (most recent call last):
2025-04-11T04:23:18.4516469Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.4516556Z     self.remove_hooks()
2025-04-11T04:23:18.4516802Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.4516991Z     for p in self.module.parameters():
2025-04-11T04:23:18.4517288Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.4517479Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.4517634Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.4517780Z __________________________ test_unsharded_checkpoint ___________________________
2025-04-11T04:23:18.4517785Z 
2025-04-11T04:23:18.4517869Z args = (), kwargs = {}
2025-04-11T04:23:18.4517873Z 
2025-04-11T04:23:18.4517965Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.4518059Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.4518174Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.4518293Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.4518402Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.4518499Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.4518504Z 
2025-04-11T04:23:18.4518604Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.4518717Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4518871Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4518969Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4519080Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4519085Z 
2025-04-11T04:23:18.4519167Z device = None
2025-04-11T04:23:18.4519171Z 
2025-04-11T04:23:18.4519293Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4519459Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4519535Z     
2025-04-11T04:23:18.4519612Z         Args:
2025-04-11T04:23:18.4519786Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4519963Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4520075Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4520148Z         """
2025-04-11T04:23:18.4520231Z         _lazy_init()
2025-04-11T04:23:18.4520324Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4520427Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4520538Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4520830Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4520971Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4521130Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4521225Z 
2025-04-11T04:23:18.4521487Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4521648Z ___________________ test_sharded_model_checkpoint[True-True] ___________________
2025-04-11T04:23:18.4521653Z 
2025-04-11T04:23:18.4521749Z use_safetensors = True, use_async = True
2025-04-11T04:23:18.4521757Z 
2025-04-11T04:23:18.4521908Z     @pytest.mark.parametrize("use_safetensors", [True, False])
2025-04-11T04:23:18.4522042Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4522223Z     def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
2025-04-11T04:23:18.4522314Z         # create a model and optimizer
2025-04-11T04:23:18.4522402Z         model = resnet18()
2025-04-11T04:23:18.4522518Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4522608Z         # create test data sample
2025-04-11T04:23:18.4522698Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4522773Z     
2025-04-11T04:23:18.4522862Z         # run fwd and bwd
2025-04-11T04:23:18.4522938Z         y = model(x)
2025-04-11T04:23:18.4523100Z         loss = y.sum()
2025-04-11T04:23:18.4523184Z         loss.backward()
2025-04-11T04:23:18.4523273Z         optimizer.step()
2025-04-11T04:23:18.4523348Z     
2025-04-11T04:23:18.4523472Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4523623Z         optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T04:23:18.4523694Z     
2025-04-11T04:23:18.4523783Z         # save the model and optimizer
2025-04-11T04:23:18.4523882Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4523952Z     
2025-04-11T04:23:18.4524046Z >       ckpt_io.save_model(
2025-04-11T04:23:18.4524269Z             model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
2025-04-11T04:23:18.4524344Z         )
2025-04-11T04:23:18.4524352Z 
2025-04-11T04:23:18.4524504Z tests/test_checkpoint_io/test_general_checkpoint_io.py:96: 
2025-04-11T04:23:18.4524614Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4524788Z colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
2025-04-11T04:23:18.4524875Z     self.save_sharded_model(
2025-04-11T04:23:18.4525071Z colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
2025-04-11T04:23:18.4525252Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T04:23:18.4525443Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T04:23:18.4525583Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T04:23:18.4525747Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4525987Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4526224Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4526366Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4526606Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4526742Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4526861Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4527090Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4527206Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4527211Z 
2025-04-11T04:23:18.4527363Z tensor = tensor([[[[-7.9533e-03,  2.4479e-03,  2.2101e-02,  ...,  2.2613e-02,
2025-04-11T04:23:18.4527461Z            -7.4241e-03,  4.8188e-02],
2025-04-11T04:23:18.4527539Z           [...594e-02],
2025-04-11T04:23:18.4527670Z           [-3.3366e-02, -1.8918e-02, -3.1868e-02,  ..., -2.9370e-02,
2025-04-11T04:23:18.4527850Z            -1.6090e-02, -4.3804e-02]]]])
2025-04-11T04:23:18.4527932Z empty = True
2025-04-11T04:23:18.4527940Z 
2025-04-11T04:23:18.4528120Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4528197Z         if empty:
2025-04-11T04:23:18.4528357Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4528464Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4528763Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4528904Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4529068Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4529076Z 
2025-04-11T04:23:18.4529200Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4529359Z __________________ test_sharded_model_checkpoint[True-False] ___________________
2025-04-11T04:23:18.4529363Z 
2025-04-11T04:23:18.4529468Z use_safetensors = False, use_async = True
2025-04-11T04:23:18.4529559Z 
2025-04-11T04:23:18.4529707Z     @pytest.mark.parametrize("use_safetensors", [True, False])
2025-04-11T04:23:18.4529840Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4530015Z     def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
2025-04-11T04:23:18.4530109Z         # create a model and optimizer
2025-04-11T04:23:18.4530192Z         model = resnet18()
2025-04-11T04:23:18.4530307Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4530400Z         # create test data sample
2025-04-11T04:23:18.4530487Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4530560Z     
2025-04-11T04:23:18.4530640Z         # run fwd and bwd
2025-04-11T04:23:18.4530716Z         y = model(x)
2025-04-11T04:23:18.4530802Z         loss = y.sum()
2025-04-11T04:23:18.4530890Z         loss.backward()
2025-04-11T04:23:18.4530985Z         optimizer.step()
2025-04-11T04:23:18.4531055Z     
2025-04-11T04:23:18.4531184Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4531329Z         optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T04:23:18.4531398Z     
2025-04-11T04:23:18.4531496Z         # save the model and optimizer
2025-04-11T04:23:18.4531591Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4531666Z     
2025-04-11T04:23:18.4531751Z >       ckpt_io.save_model(
2025-04-11T04:23:18.4531967Z             model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
2025-04-11T04:23:18.4532045Z         )
2025-04-11T04:23:18.4532050Z 
2025-04-11T04:23:18.4532195Z tests/test_checkpoint_io/test_general_checkpoint_io.py:96: 
2025-04-11T04:23:18.4532308Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4532472Z colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
2025-04-11T04:23:18.4532566Z     self.save_sharded_model(
2025-04-11T04:23:18.4532751Z colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
2025-04-11T04:23:18.4532937Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T04:23:18.4533123Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T04:23:18.4533263Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T04:23:18.4533430Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4533657Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4533896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4534027Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4534380Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4534517Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4534637Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4534868Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4534977Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4534982Z 
2025-04-11T04:23:18.4535136Z tensor = tensor([[[[ 1.0525e-02,  3.3759e-02, -3.8922e-03,  ...,  1.3324e-02,
2025-04-11T04:23:18.4535227Z            -1.5804e-03, -4.7921e-03],
2025-04-11T04:23:18.4535307Z           [...171e-03],
2025-04-11T04:23:18.4535429Z           [-3.2414e-03, -5.0440e-02,  3.3628e-02,  ..., -8.8520e-03,
2025-04-11T04:23:18.4535515Z             4.6587e-03,  4.6623e-02]]]])
2025-04-11T04:23:18.4535600Z empty = True
2025-04-11T04:23:18.4535605Z 
2025-04-11T04:23:18.4535779Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4535860Z         if empty:
2025-04-11T04:23:18.4536013Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4536211Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4536496Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4536634Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4536798Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4536803Z 
2025-04-11T04:23:18.4536923Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4537077Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4537244Z [04/11/25 04:13:24] WARNING  colossalai - colossalai - WARNING:                 
2025-04-11T04:23:18.4537375Z                              /__w/ColossalAI/ColossalAI/colossalai/checkpoint_io
2025-04-11T04:23:18.4537499Z                              /checkpoint_io_base.py:183 save_model              
2025-04-11T04:23:18.4537646Z                     WARNING  colossalai - colossalai - WARNING: Async save is   
2025-04-11T04:23:18.4537778Z                              only supported when use_safetensors is set to True.
2025-04-11T04:23:18.4537900Z                              Setting use_safetensors to True for async save.    
2025-04-11T04:23:18.4538057Z ___________________ test_sharded_optimizer_checkpoint[False] ___________________
2025-04-11T04:23:18.4538061Z 
2025-04-11T04:23:18.4538139Z use_async = False
2025-04-11T04:23:18.4538144Z 
2025-04-11T04:23:18.4538278Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4538412Z     def test_sharded_optimizer_checkpoint(use_async: bool):
2025-04-11T04:23:18.4538508Z         # create a model and optimizer
2025-04-11T04:23:18.4538598Z         model = resnet18()
2025-04-11T04:23:18.4538713Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4538790Z     
2025-04-11T04:23:18.4538880Z         # create test data sample
2025-04-11T04:23:18.4538972Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4539044Z     
2025-04-11T04:23:18.4539124Z         # run fwd and bwd
2025-04-11T04:23:18.4539205Z         y = model(x)
2025-04-11T04:23:18.4539281Z         loss = y.sum()
2025-04-11T04:23:18.4539366Z         loss.backward()
2025-04-11T04:23:18.4539449Z         optimizer.step()
2025-04-11T04:23:18.4539524Z     
2025-04-11T04:23:18.4539636Z         # create temp directories for checkpoint
2025-04-11T04:23:18.4539758Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4539893Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4539963Z     
2025-04-11T04:23:18.4540057Z         # save the model and optimizer
2025-04-11T04:23:18.4540156Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4540326Z     
2025-04-11T04:23:18.4540537Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T04:23:18.4540811Z         ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4540884Z     
2025-04-11T04:23:18.4540969Z         ckpt_io._sync_d2h()
2025-04-11T04:23:18.4541056Z         ckpt_io._sync_io()
2025-04-11T04:23:18.4541129Z     
2025-04-11T04:23:18.4541208Z         # create new model
2025-04-11T04:23:18.4541319Z         new_model = resnet18()
2025-04-11T04:23:18.4541451Z         new_optimizer = Adam(new_model.parameters(), lr=0.001)
2025-04-11T04:23:18.4541525Z     
2025-04-11T04:23:18.4541678Z         ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
2025-04-11T04:23:18.4541835Z >       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T04:23:18.4541840Z 
2025-04-11T04:23:18.4541998Z tests/test_checkpoint_io/test_general_checkpoint_io.py:149: 
2025-04-11T04:23:18.4542110Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4542380Z colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
2025-04-11T04:23:18.4542475Z     self.load_sharded_optimizer(
2025-04-11T04:23:18.4542675Z colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
2025-04-11T04:23:18.4542812Z     load_states_into_optimizer(optimizer, state_dict, id_map)
2025-04-11T04:23:18.4542981Z colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
2025-04-11T04:23:18.4543081Z     get_accelerator().synchronize()
2025-04-11T04:23:18.4543234Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4543332Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4543442Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4543446Z 
2025-04-11T04:23:18.4543524Z device = None
2025-04-11T04:23:18.4543533Z 
2025-04-11T04:23:18.4543651Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4543804Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4543882Z     
2025-04-11T04:23:18.4543956Z         Args:
2025-04-11T04:23:18.4544129Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4544299Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4544413Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4544486Z         """
2025-04-11T04:23:18.4544565Z         _lazy_init()
2025-04-11T04:23:18.4544662Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4544764Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4544874Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4545161Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4545302Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4545465Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4545470Z 
2025-04-11T04:23:18.4545712Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4545874Z ___________________ test_sharded_optimizer_checkpoint[True] ____________________
2025-04-11T04:23:18.4545878Z 
2025-04-11T04:23:18.4545957Z use_async = True
2025-04-11T04:23:18.4545962Z 
2025-04-11T04:23:18.4546095Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4546226Z     def test_sharded_optimizer_checkpoint(use_async: bool):
2025-04-11T04:23:18.4546323Z         # create a model and optimizer
2025-04-11T04:23:18.4546409Z         model = resnet18()
2025-04-11T04:23:18.4546522Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4546687Z     
2025-04-11T04:23:18.4546777Z         # create test data sample
2025-04-11T04:23:18.4546870Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4546943Z     
2025-04-11T04:23:18.4547024Z         # run fwd and bwd
2025-04-11T04:23:18.4547103Z         y = model(x)
2025-04-11T04:23:18.4547179Z         loss = y.sum()
2025-04-11T04:23:18.4547262Z         loss.backward()
2025-04-11T04:23:18.4547346Z         optimizer.step()
2025-04-11T04:23:18.4547418Z     
2025-04-11T04:23:18.4547520Z         # create temp directories for checkpoint
2025-04-11T04:23:18.4547641Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4547775Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4547844Z     
2025-04-11T04:23:18.4547940Z         # save the model and optimizer
2025-04-11T04:23:18.4548036Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4548106Z     
2025-04-11T04:23:18.4548313Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T04:23:18.4548616Z >       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4548715Z 
2025-04-11T04:23:18.4548872Z tests/test_checkpoint_io/test_general_checkpoint_io.py:139: 
2025-04-11T04:23:18.4548984Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4549160Z colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
2025-04-11T04:23:18.4549253Z     self.save_sharded_optimizer(
2025-04-11T04:23:18.4549446Z colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
2025-04-11T04:23:18.4549637Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T04:23:18.4549826Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T04:23:18.4549972Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T04:23:18.4550137Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4550381Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4550627Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4550773Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4551014Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4551146Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4551270Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4551497Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4551611Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4551620Z 
2025-04-11T04:23:18.4551711Z tensor = tensor(1.), empty = True
2025-04-11T04:23:18.4551716Z 
2025-04-11T04:23:18.4551889Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4551970Z         if empty:
2025-04-11T04:23:18.4552128Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4552238Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4552542Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4552685Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4552843Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4552848Z 
2025-04-11T04:23:18.4552969Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4553138Z _____________ test_sharded_optimizer_multiple_param_groups[False] ______________
2025-04-11T04:23:18.4553238Z 
2025-04-11T04:23:18.4553323Z use_async = False
2025-04-11T04:23:18.4553328Z 
2025-04-11T04:23:18.4553461Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4553624Z     def test_sharded_optimizer_multiple_param_groups(use_async: bool):
2025-04-11T04:23:18.4553719Z         # create a model and optimizer
2025-04-11T04:23:18.4553803Z         model = resnet18()
2025-04-11T04:23:18.4553894Z         optimizer = Adam(
2025-04-11T04:23:18.4554130Z             [{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T04:23:18.4554209Z         )
2025-04-11T04:23:18.4554280Z     
2025-04-11T04:23:18.4554369Z         # create test data sample
2025-04-11T04:23:18.4554463Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4554530Z     
2025-04-11T04:23:18.4554617Z         # run fwd and bwd
2025-04-11T04:23:18.4554694Z         y = model(x)
2025-04-11T04:23:18.4554772Z         loss = y.sum()
2025-04-11T04:23:18.4554865Z         loss.backward()
2025-04-11T04:23:18.4554950Z         optimizer.step()
2025-04-11T04:23:18.4555029Z     
2025-04-11T04:23:18.4555137Z         # create temp directories for checkpoint
2025-04-11T04:23:18.4555346Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4555481Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4555550Z     
2025-04-11T04:23:18.4555644Z         # save the model and optimizer
2025-04-11T04:23:18.4555741Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4555814Z     
2025-04-11T04:23:18.4556017Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T04:23:18.4556274Z         ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4556350Z     
2025-04-11T04:23:18.4556434Z         ckpt_io._sync_d2h()
2025-04-11T04:23:18.4556521Z         ckpt_io._sync_io()
2025-04-11T04:23:18.4556594Z     
2025-04-11T04:23:18.4556674Z         # create new model
2025-04-11T04:23:18.4556762Z         new_model = resnet18()
2025-04-11T04:23:18.4556849Z         new_optimizer = Adam(
2025-04-11T04:23:18.4557106Z             [{"params": new_model.layer1.parameters()}, {"params": new_model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T04:23:18.4557180Z         )
2025-04-11T04:23:18.4557253Z     
2025-04-11T04:23:18.4557413Z         ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
2025-04-11T04:23:18.4557572Z >       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T04:23:18.4557577Z 
2025-04-11T04:23:18.4557727Z tests/test_checkpoint_io/test_general_checkpoint_io.py:222: 
2025-04-11T04:23:18.4557837Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4558011Z colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
2025-04-11T04:23:18.4558104Z     self.load_sharded_optimizer(
2025-04-11T04:23:18.4558304Z colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
2025-04-11T04:23:18.4558441Z     load_states_into_optimizer(optimizer, state_dict, id_map)
2025-04-11T04:23:18.4558610Z colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
2025-04-11T04:23:18.4558709Z     get_accelerator().synchronize()
2025-04-11T04:23:18.4558860Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4558957Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4559065Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4559069Z 
2025-04-11T04:23:18.4559155Z device = None
2025-04-11T04:23:18.4559160Z 
2025-04-11T04:23:18.4559288Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4559448Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4559521Z     
2025-04-11T04:23:18.4559596Z         Args:
2025-04-11T04:23:18.4559777Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4560061Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4560176Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4560250Z         """
2025-04-11T04:23:18.4560329Z         _lazy_init()
2025-04-11T04:23:18.4560430Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4560528Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4560637Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4560925Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4561065Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4561223Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4561227Z 
2025-04-11T04:23:18.4561473Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4561642Z ______________ test_sharded_optimizer_multiple_param_groups[True] ______________
2025-04-11T04:23:18.4561734Z 
2025-04-11T04:23:18.4561815Z use_async = True
2025-04-11T04:23:18.4561820Z 
2025-04-11T04:23:18.4561951Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4562108Z     def test_sharded_optimizer_multiple_param_groups(use_async: bool):
2025-04-11T04:23:18.4562203Z         # create a model and optimizer
2025-04-11T04:23:18.4562287Z         model = resnet18()
2025-04-11T04:23:18.4562370Z         optimizer = Adam(
2025-04-11T04:23:18.4562600Z             [{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T04:23:18.4562673Z         )
2025-04-11T04:23:18.4562747Z     
2025-04-11T04:23:18.4562835Z         # create test data sample
2025-04-11T04:23:18.4562927Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4562999Z     
2025-04-11T04:23:18.4563079Z         # run fwd and bwd
2025-04-11T04:23:18.4563163Z         y = model(x)
2025-04-11T04:23:18.4563239Z         loss = y.sum()
2025-04-11T04:23:18.4563327Z         loss.backward()
2025-04-11T04:23:18.4563412Z         optimizer.step()
2025-04-11T04:23:18.4563484Z     
2025-04-11T04:23:18.4563592Z         # create temp directories for checkpoint
2025-04-11T04:23:18.4563716Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4563852Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4563925Z     
2025-04-11T04:23:18.4564015Z         # save the model and optimizer
2025-04-11T04:23:18.4564114Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4564183Z     
2025-04-11T04:23:18.4564385Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T04:23:18.4564639Z >       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4564648Z 
2025-04-11T04:23:18.4564809Z tests/test_checkpoint_io/test_general_checkpoint_io.py:210: 
2025-04-11T04:23:18.4564924Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4565101Z colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
2025-04-11T04:23:18.4565196Z     self.save_sharded_optimizer(
2025-04-11T04:23:18.4565388Z colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
2025-04-11T04:23:18.4565575Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T04:23:18.4565756Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T04:23:18.4565900Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T04:23:18.4566065Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4566300Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4566625Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4566763Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4567003Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4567134Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4567256Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4567481Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4567593Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4567597Z 
2025-04-11T04:23:18.4567689Z tensor = tensor(1.), empty = True
2025-04-11T04:23:18.4567694Z 
2025-04-11T04:23:18.4567863Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4567947Z         if empty:
2025-04-11T04:23:18.4568095Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4568301Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4568593Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4568741Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4568898Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4568903Z 
2025-04-11T04:23:18.4569026Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4569160Z _____________________________ test_hybrid_ckpIO[4] _____________________________
2025-04-11T04:23:18.4569165Z 
2025-04-11T04:23:18.4569282Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.4569897Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4569908Z 
2025-04-11T04:23:18.4570015Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4570100Z         try_count = 0
2025-04-11T04:23:18.4570200Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4570287Z             max_try, int
2025-04-11T04:23:18.4570433Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4570511Z     
2025-04-11T04:23:18.4570622Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4570699Z             try:
2025-04-11T04:23:18.4570793Z                 try_count += 1
2025-04-11T04:23:18.4570886Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4570969Z                 return ret
2025-04-11T04:23:18.4571061Z             except exception_type as e:
2025-04-11T04:23:18.4571163Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4571351Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4571470Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4571619Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4571772Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4571858Z                     continue
2025-04-11T04:23:18.4571933Z                 else:
2025-04-11T04:23:18.4572156Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4572238Z >                   raise e
2025-04-11T04:23:18.4572242Z 
2025-04-11T04:23:18.4572333Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4572447Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4572674Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4572764Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4573001Z tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py:155: in test_hybrid_ckpIO
2025-04-11T04:23:18.4573090Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.4573195Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4573294Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4573552Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4573731Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4574018Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4574106Z     while not context.join():
2025-04-11T04:23:18.4574213Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4574221Z 
2025-04-11T04:23:18.4574424Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840d6f0>
2025-04-11T04:23:18.4574502Z timeout = None
2025-04-11T04:23:18.4574591Z 
2025-04-11T04:23:18.4574687Z     def join(self, timeout=None):
2025-04-11T04:23:18.4574812Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4574892Z     
2025-04-11T04:23:18.4575035Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4575180Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4575347Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4575440Z         of the first process exiting.
2025-04-11T04:23:18.4575514Z     
2025-04-11T04:23:18.4575660Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4575800Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4575875Z     
2025-04-11T04:23:18.4575947Z         Args:
2025-04-11T04:23:18.4576087Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4576165Z         """
2025-04-11T04:23:18.4576303Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4576395Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4576472Z             return True
2025-04-11T04:23:18.4576545Z     
2025-04-11T04:23:18.4576674Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4576793Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4576884Z             self.sentinels.keys(),
2025-04-11T04:23:18.4576969Z             timeout=timeout,
2025-04-11T04:23:18.4577042Z         )
2025-04-11T04:23:18.4577112Z     
2025-04-11T04:23:18.4577196Z         error_index = None
2025-04-11T04:23:18.4577281Z         for sentinel in ready:
2025-04-11T04:23:18.4577390Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4577490Z             process = self.processes[index]
2025-04-11T04:23:18.4577574Z             process.join()
2025-04-11T04:23:18.4577682Z             if process.exitcode != 0:
2025-04-11T04:23:18.4577771Z                 error_index = index
2025-04-11T04:23:18.4577858Z                 break
2025-04-11T04:23:18.4577930Z     
2025-04-11T04:23:18.4578020Z         # Return if there was no error.
2025-04-11T04:23:18.4578109Z         if error_index is None:
2025-04-11T04:23:18.4578241Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4578347Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4578417Z     
2025-04-11T04:23:18.4578556Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4578652Z         for process in self.processes:
2025-04-11T04:23:18.4578743Z             if process.is_alive():
2025-04-11T04:23:18.4578837Z                 process.terminate()
2025-04-11T04:23:18.4578922Z             process.join()
2025-04-11T04:23:18.4579090Z     
2025-04-11T04:23:18.4579230Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4579343Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4579458Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4579577Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4579666Z             if exitcode < 0:
2025-04-11T04:23:18.4579771Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4579879Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4580027Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4580121Z                     error_index=error_index,
2025-04-11T04:23:18.4580224Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4580312Z                     exit_code=exitcode,
2025-04-11T04:23:18.4580404Z                     signal_name=name,
2025-04-11T04:23:18.4580482Z                 )
2025-04-11T04:23:18.4580553Z             else:
2025-04-11T04:23:18.4580660Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4580926Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4581024Z                     error_index=error_index,
2025-04-11T04:23:18.4581125Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4581213Z                     exit_code=exitcode,
2025-04-11T04:23:18.4581286Z                 )
2025-04-11T04:23:18.4581355Z     
2025-04-11T04:23:18.4581490Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4581660Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4581750Z         msg += original_trace
2025-04-11T04:23:18.4581920Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4582082Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4582164Z E       
2025-04-11T04:23:18.4582290Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4582391Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4582690Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4582775Z E           fn(i, *args)
2025-04-11T04:23:18.4583099Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
2025-04-11T04:23:18.4583184Z E           exam_state_dict()
2025-04-11T04:23:18.4583447Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4583535Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4583792Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4583879Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4584131Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4584220Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4584323Z E         [Previous line repeated 3 more times]
2025-04-11T04:23:18.4584546Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4584645Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4584904Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4585002Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4585282Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4585384Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4585493Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4585879Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4586016Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4586188Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4586193Z 
2025-04-11T04:23:18.4586496Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4586653Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4586811Z [04/11/25 04:13:35] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4586974Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4587092Z                              :75 launch                                         
2025-04-11T04:23:18.4587234Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4587366Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4587651Z [04/11/25 04:13:35] WARNING  colossalai - colossalai.shardformer.modeling.llama 
2025-04-11T04:23:18.4587780Z                              - WARNING: `use_cache=True` is incompatible with   
2025-04-11T04:23:18.4587904Z                              pipeline parallelism. Setting `use_cache=False`... 
2025-04-11T04:23:18.4588103Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4588248Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4589422Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4589595Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4590716Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4590881Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4591987Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4592153Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4593255Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4593412Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4594105Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4594293Z   warnings.warn(
2025-04-11T04:23:18.4594982Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4595063Z   warnings.warn(
2025-04-11T04:23:18.4595746Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4595824Z   warnings.warn(
2025-04-11T04:23:18.4596521Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4596705Z   warnings.warn(
2025-04-11T04:23:18.4597537Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4597620Z   warnings.warn(
2025-04-11T04:23:18.4598436Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4598516Z   warnings.warn(
2025-04-11T04:23:18.4599323Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4599411Z   warnings.warn(
2025-04-11T04:23:18.4600209Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4600293Z   warnings.warn(
2025-04-11T04:23:18.4601096Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4601183Z   warnings.warn(
2025-04-11T04:23:18.4601988Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4602072Z   warnings.warn(
2025-04-11T04:23:18.4602880Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4602962Z   warnings.warn(
2025-04-11T04:23:18.4603765Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4603938Z   warnings.warn(
2025-04-11T04:23:18.4604738Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4604820Z   warnings.warn(
2025-04-11T04:23:18.4605620Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4605701Z   warnings.warn(
2025-04-11T04:23:18.4606500Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4606668Z   warnings.warn(
2025-04-11T04:23:18.4607495Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4607572Z   warnings.warn(
2025-04-11T04:23:18.4607877Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57270 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4608162Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57270 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4608745Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4608829Z   warnings.warn(
2025-04-11T04:23:18.4609378Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4609455Z   warnings.warn(
2025-04-11T04:23:18.4610011Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4610088Z   warnings.warn(
2025-04-11T04:23:18.4610628Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4610712Z   warnings.warn(
2025-04-11T04:23:18.4611262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4611344Z   warnings.warn(
2025-04-11T04:23:18.4611882Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4611964Z   warnings.warn(
2025-04-11T04:23:18.4612507Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4612673Z   warnings.warn(
2025-04-11T04:23:18.4613197Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4613281Z   warnings.warn(
2025-04-11T04:23:18.4613433Z _______________________ test_low_level_zero_checkpointIO _______________________
2025-04-11T04:23:18.4613437Z 
2025-04-11T04:23:18.4613534Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4613539Z 
2025-04-11T04:23:18.4613641Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4613721Z         try_count = 0
2025-04-11T04:23:18.4613826Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4613906Z             max_try, int
2025-04-11T04:23:18.4614055Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4614126Z     
2025-04-11T04:23:18.4614238Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4614318Z             try:
2025-04-11T04:23:18.4614401Z                 try_count += 1
2025-04-11T04:23:18.4614498Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.4614693Z 
2025-04-11T04:23:18.4614790Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.4614909Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4615024Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.4615118Z     get_accelerator().synchronize()
2025-04-11T04:23:18.4615277Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4615381Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4615515Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4615519Z 
2025-04-11T04:23:18.4615597Z device = None
2025-04-11T04:23:18.4615601Z 
2025-04-11T04:23:18.4615736Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4615897Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4615968Z     
2025-04-11T04:23:18.4616054Z         Args:
2025-04-11T04:23:18.4616225Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4616401Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4616511Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4616588Z         """
2025-04-11T04:23:18.4616667Z         _lazy_init()
2025-04-11T04:23:18.4616765Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4616873Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4616979Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4617274Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4617415Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4617585Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4617594Z 
2025-04-11T04:23:18.4617833Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4617985Z ______________________ test_huggingface_compatibility[2] _______________________
2025-04-11T04:23:18.4617994Z 
2025-04-11T04:23:18.4618109Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.4618713Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4618723Z 
2025-04-11T04:23:18.4618825Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4618905Z         try_count = 0
2025-04-11T04:23:18.4619009Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4619178Z             max_try, int
2025-04-11T04:23:18.4619326Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4619400Z     
2025-04-11T04:23:18.4619509Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4619588Z             try:
2025-04-11T04:23:18.4619672Z                 try_count += 1
2025-04-11T04:23:18.4619766Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4619845Z                 return ret
2025-04-11T04:23:18.4619938Z             except exception_type as e:
2025-04-11T04:23:18.4620041Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4620230Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4620356Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4620500Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4620662Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4620743Z                     continue
2025-04-11T04:23:18.4620900Z                 else:
2025-04-11T04:23:18.4621129Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4621208Z >                   raise e
2025-04-11T04:23:18.4621213Z 
2025-04-11T04:23:18.4621309Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4621420Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4621554Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4621640Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4621909Z tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py:79: in test_huggingface_compatibility
2025-04-11T04:23:18.4622003Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.4622101Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4622208Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4622466Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4622651Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4622938Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4623027Z     while not context.join():
2025-04-11T04:23:18.4623140Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4623144Z 
2025-04-11T04:23:18.4623341Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840ea10>
2025-04-11T04:23:18.4623426Z timeout = None
2025-04-11T04:23:18.4623431Z 
2025-04-11T04:23:18.4623520Z     def join(self, timeout=None):
2025-04-11T04:23:18.4623649Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4623720Z     
2025-04-11T04:23:18.4623869Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4624018Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4624186Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4624283Z         of the first process exiting.
2025-04-11T04:23:18.4624352Z     
2025-04-11T04:23:18.4624502Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4624644Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4624717Z     
2025-04-11T04:23:18.4624808Z         Args:
2025-04-11T04:23:18.4624957Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4625038Z         """
2025-04-11T04:23:18.4625177Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4625272Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4625350Z             return True
2025-04-11T04:23:18.4625509Z     
2025-04-11T04:23:18.4625646Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4625763Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4625860Z             self.sentinels.keys(),
2025-04-11T04:23:18.4625943Z             timeout=timeout,
2025-04-11T04:23:18.4626016Z         )
2025-04-11T04:23:18.4626093Z     
2025-04-11T04:23:18.4626178Z         error_index = None
2025-04-11T04:23:18.4626269Z         for sentinel in ready:
2025-04-11T04:23:18.4626372Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4626472Z             process = self.processes[index]
2025-04-11T04:23:18.4626561Z             process.join()
2025-04-11T04:23:18.4626653Z             if process.exitcode != 0:
2025-04-11T04:23:18.4626744Z                 error_index = index
2025-04-11T04:23:18.4626818Z                 break
2025-04-11T04:23:18.4626901Z     
2025-04-11T04:23:18.4626997Z         # Return if there was no error.
2025-04-11T04:23:18.4627088Z         if error_index is None:
2025-04-11T04:23:18.4627234Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4627332Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4627498Z     
2025-04-11T04:23:18.4627642Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4627741Z         for process in self.processes:
2025-04-11T04:23:18.4627841Z             if process.is_alive():
2025-04-11T04:23:18.4627936Z                 process.terminate()
2025-04-11T04:23:18.4628030Z             process.join()
2025-04-11T04:23:18.4628103Z     
2025-04-11T04:23:18.4628245Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4628369Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4628545Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4628678Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4628765Z             if exitcode < 0:
2025-04-11T04:23:18.4628885Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4628993Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4629148Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4629254Z                     error_index=error_index,
2025-04-11T04:23:18.4629356Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4629454Z                     exit_code=exitcode,
2025-04-11T04:23:18.4629542Z                     signal_name=name,
2025-04-11T04:23:18.4629626Z                 )
2025-04-11T04:23:18.4629704Z             else:
2025-04-11T04:23:18.4629806Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4629979Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4630074Z                     error_index=error_index,
2025-04-11T04:23:18.4630181Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4630274Z                     exit_code=exitcode,
2025-04-11T04:23:18.4630348Z                 )
2025-04-11T04:23:18.4630428Z     
2025-04-11T04:23:18.4630561Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4630741Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4630828Z         msg += original_trace
2025-04-11T04:23:18.4631008Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4631172Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4631247Z E       
2025-04-11T04:23:18.4631381Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4631481Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4631790Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4631873Z E           fn(i, *args)
2025-04-11T04:23:18.4632296Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
2025-04-11T04:23:18.4632414Z E           exam_from_pretrained()
2025-04-11T04:23:18.4632654Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4632762Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4633022Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4633129Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4633409Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4633523Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4633630Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4633922Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4634074Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4634240Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4634339Z 
2025-04-11T04:23:18.4634655Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4634809Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4634974Z [04/11/25 04:13:43] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4635105Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4635221Z                              :75 launch                                         
2025-04-11T04:23:18.4635359Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4635486Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4635696Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4635847Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4636991Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4637164Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4638280Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4638456Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4639141Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4639228Z   warnings.warn(
2025-04-11T04:23:18.4639931Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4640119Z   warnings.warn(
2025-04-11T04:23:18.4640953Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4641038Z   warnings.warn(
2025-04-11T04:23:18.4641861Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4641950Z   warnings.warn(
2025-04-11T04:23:18.4642773Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4642859Z   warnings.warn(
2025-04-11T04:23:18.4643748Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4643827Z   warnings.warn(
2025-04-11T04:23:18.4644623Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4644709Z   warnings.warn(
2025-04-11T04:23:18.4645504Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4645600Z   warnings.warn(
2025-04-11T04:23:18.4646151Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4646238Z   warnings.warn(
2025-04-11T04:23:18.4646765Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4646854Z   warnings.warn(
2025-04-11T04:23:18.4647384Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4647472Z   warnings.warn(
2025-04-11T04:23:18.4648000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4648086Z   warnings.warn(
2025-04-11T04:23:18.4648229Z ___________________________ test_create_pin[1-True] ____________________________
2025-04-11T04:23:18.4648234Z 
2025-04-11T04:23:18.4648325Z empty = True, num_threads = 1
2025-04-11T04:23:18.4648337Z 
2025-04-11T04:23:18.4648463Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T04:23:18.4648582Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T04:23:18.4648720Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T04:23:18.4648825Z         model_state_dict = gen_model_state_dict()
2025-04-11T04:23:18.4649182Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4649188Z 
2025-04-11T04:23:18.4649340Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T04:23:18.4649464Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4649635Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4649874Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4650120Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4650262Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4650508Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4650644Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4650777Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4651009Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4651211Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4651223Z 
2025-04-11T04:23:18.4651404Z tensor = tensor([[8.6847e-01, 8.1748e-04, 4.0391e-02,  ..., 7.9576e-01, 7.4807e-01,
2025-04-11T04:23:18.4651485Z          1.9106e-01],
2025-04-11T04:23:18.4651587Z         [8.6654e-02, ...         9.2385e-01],
2025-04-11T04:23:18.4651728Z         [3.8017e-01, 2.4513e-01, 9.6436e-01,  ..., 3.2863e-01, 2.9029e-01,
2025-04-11T04:23:18.4651817Z          5.3874e-01]])
2025-04-11T04:23:18.4651898Z empty = True
2025-04-11T04:23:18.4651902Z 
2025-04-11T04:23:18.4652078Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4652173Z         if empty:
2025-04-11T04:23:18.4652334Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4652453Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4652743Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4652894Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4653055Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4653060Z 
2025-04-11T04:23:18.4653190Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4653335Z ___________________________ test_create_pin[1-False] ___________________________
2025-04-11T04:23:18.4653339Z 
2025-04-11T04:23:18.4653433Z empty = False, num_threads = 1
2025-04-11T04:23:18.4653445Z 
2025-04-11T04:23:18.4653567Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T04:23:18.4653684Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T04:23:18.4653817Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T04:23:18.4653923Z         model_state_dict = gen_model_state_dict()
2025-04-11T04:23:18.4654176Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4654185Z 
2025-04-11T04:23:18.4654333Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T04:23:18.4654450Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4654614Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4654842Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4655086Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4655225Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4655469Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4655692Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4655821Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4656050Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4656162Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4656173Z 
2025-04-11T04:23:18.4656318Z tensor = tensor([[0.3194, 0.1471, 0.8299,  ..., 0.0654, 0.6019, 0.1725],
2025-04-11T04:23:18.4656490Z         [0.6552, 0.0854, 0.7144,  ..., 0.4938, 0.4092,...0.2009, 0.3128, 0.3861,  ..., 0.5467, 0.1594, 0.9723],
2025-04-11T04:23:18.4656610Z         [0.6342, 0.6712, 0.4807,  ..., 0.1087, 0.7685, 0.4644]])
2025-04-11T04:23:18.4656691Z empty = False
2025-04-11T04:23:18.4656696Z 
2025-04-11T04:23:18.4656881Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4656965Z         if empty:
2025-04-11T04:23:18.4657125Z             return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4657310Z >       return tensor.pin_memory()
2025-04-11T04:23:18.4657420Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4657716Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4657856Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4658026Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4658031Z 
2025-04-11T04:23:18.4658150Z colossalai/checkpoint_io/utils.py:968: RuntimeError
2025-04-11T04:23:18.4658299Z ___________________________ test_create_pin[4-True] ____________________________
2025-04-11T04:23:18.4658303Z 
2025-04-11T04:23:18.4658394Z empty = True, num_threads = 4
2025-04-11T04:23:18.4658399Z 
2025-04-11T04:23:18.4658519Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T04:23:18.4658643Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T04:23:18.4658764Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T04:23:18.4658875Z         model_state_dict = gen_model_state_dict()
2025-04-11T04:23:18.4659120Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4659125Z 
2025-04-11T04:23:18.4659273Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T04:23:18.4659380Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4659548Z colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
2025-04-11T04:23:18.4659639Z     elems[idx] = future.result()
2025-04-11T04:23:18.4659841Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
2025-04-11T04:23:18.4659935Z     return self.__get_result()
2025-04-11T04:23:18.4660151Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
2025-04-11T04:23:18.4660244Z     raise self._exception
2025-04-11T04:23:18.4660434Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
2025-04-11T04:23:18.4660550Z     result = self.fn(*self.args, **self.kwargs)
2025-04-11T04:23:18.4660658Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4660662Z 
2025-04-11T04:23:18.4660801Z tensor = tensor([[0.5821, 0.7269, 0.9384,  ..., 0.4998, 0.7969, 0.4132],
2025-04-11T04:23:18.4660987Z         [0.1225, 0.3994, 0.1804,  ..., 0.1278, 0.2117,...0.5724, 0.2939, 0.7177,  ..., 0.9228, 0.0104, 0.5268],
2025-04-11T04:23:18.4661104Z         [0.9206, 0.8477, 0.4199,  ..., 0.2250, 0.8432, 0.0800]])
2025-04-11T04:23:18.4661186Z empty = True
2025-04-11T04:23:18.4661191Z 
2025-04-11T04:23:18.4661363Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4661450Z         if empty:
2025-04-11T04:23:18.4661681Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4661795Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4662090Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4662227Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4662389Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4662394Z 
2025-04-11T04:23:18.4662510Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4662650Z ___________________________ test_create_pin[4-False] ___________________________
2025-04-11T04:23:18.4662655Z 
2025-04-11T04:23:18.4662742Z empty = False, num_threads = 4
2025-04-11T04:23:18.4662747Z 
2025-04-11T04:23:18.4662871Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T04:23:18.4662984Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T04:23:18.4663108Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T04:23:18.4663215Z         model_state_dict = gen_model_state_dict()
2025-04-11T04:23:18.4663559Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4663565Z 
2025-04-11T04:23:18.4663712Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T04:23:18.4663819Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4663984Z colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
2025-04-11T04:23:18.4664076Z     elems[idx] = future.result()
2025-04-11T04:23:18.4664275Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
2025-04-11T04:23:18.4664367Z     return self.__get_result()
2025-04-11T04:23:18.4664580Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
2025-04-11T04:23:18.4664672Z     raise self._exception
2025-04-11T04:23:18.4664860Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
2025-04-11T04:23:18.4664975Z     result = self.fn(*self.args, **self.kwargs)
2025-04-11T04:23:18.4665082Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4665087Z 
2025-04-11T04:23:18.4665223Z tensor = tensor([[0.6302, 0.2935, 0.7903,  ..., 0.0028, 0.9982, 0.4133],
2025-04-11T04:23:18.4665391Z         [0.3542, 0.4418, 0.9797,  ..., 0.7308, 0.7955,...0.9666, 0.5375, 0.3321,  ..., 0.7164, 0.7701, 0.3787],
2025-04-11T04:23:18.4665501Z         [0.0016, 0.1229, 0.2857,  ..., 0.3524, 0.9990, 0.6592]])
2025-04-11T04:23:18.4665582Z empty = False
2025-04-11T04:23:18.4665587Z 
2025-04-11T04:23:18.4665756Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4665834Z         if empty:
2025-04-11T04:23:18.4665983Z             return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4666081Z >       return tensor.pin_memory()
2025-04-11T04:23:18.4666186Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4666472Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4666612Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4666770Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4666775Z 
2025-04-11T04:23:18.4666896Z colossalai/checkpoint_io/utils.py:968: RuntimeError
2025-04-11T04:23:18.4667031Z ________________________________ test_save_load ________________________________
2025-04-11T04:23:18.4667036Z 
2025-04-11T04:23:18.4667125Z args = (), kwargs = {}
2025-04-11T04:23:18.4667130Z 
2025-04-11T04:23:18.4667224Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.4667317Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.4667426Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.4667645Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.4667753Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.4667848Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.4667853Z 
2025-04-11T04:23:18.4667949Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.4668059Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4668211Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4668309Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4668472Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4668479Z 
2025-04-11T04:23:18.4668562Z device = None
2025-04-11T04:23:18.4668566Z 
2025-04-11T04:23:18.4668687Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4668848Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4668924Z     
2025-04-11T04:23:18.4668997Z         Args:
2025-04-11T04:23:18.4669172Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4669438Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4669554Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4669628Z         """
2025-04-11T04:23:18.4669710Z         _lazy_init()
2025-04-11T04:23:18.4669801Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4669906Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4670014Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4670301Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4670441Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4670599Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4670608Z 
2025-04-11T04:23:18.4670903Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4671061Z _________________________ test_torch_ddp_checkpointIO __________________________
2025-04-11T04:23:18.4671066Z 
2025-04-11T04:23:18.4671160Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4671776Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4671783Z 
2025-04-11T04:23:18.4671890Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4671974Z         try_count = 0
2025-04-11T04:23:18.4672074Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4672161Z             max_try, int
2025-04-11T04:23:18.4672309Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4672388Z     
2025-04-11T04:23:18.4672501Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4672577Z             try:
2025-04-11T04:23:18.4672664Z                 try_count += 1
2025-04-11T04:23:18.4672753Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4672836Z                 return ret
2025-04-11T04:23:18.4672927Z             except exception_type as e:
2025-04-11T04:23:18.4673027Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4673215Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4673332Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4673482Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4673635Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4673822Z                     continue
2025-04-11T04:23:18.4673899Z                 else:
2025-04-11T04:23:18.4674124Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4674210Z >                   raise e
2025-04-11T04:23:18.4674216Z 
2025-04-11T04:23:18.4674307Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4674421Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4674550Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4674638Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4674859Z tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py:81: in test_torch_ddp_checkpointIO
2025-04-11T04:23:18.4674942Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4675042Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4675143Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4675405Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4675587Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4675965Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4676055Z     while not context.join():
2025-04-11T04:23:18.4676166Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4676174Z 
2025-04-11T04:23:18.4676376Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b7870280>
2025-04-11T04:23:18.4676455Z timeout = None
2025-04-11T04:23:18.4676459Z 
2025-04-11T04:23:18.4676554Z     def join(self, timeout=None):
2025-04-11T04:23:18.4676678Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4676751Z     
2025-04-11T04:23:18.4676900Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4677047Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4677214Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4677310Z         of the first process exiting.
2025-04-11T04:23:18.4677383Z     
2025-04-11T04:23:18.4677533Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4677672Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4677741Z     
2025-04-11T04:23:18.4677813Z         Args:
2025-04-11T04:23:18.4677954Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4678027Z         """
2025-04-11T04:23:18.4678170Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4678262Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4678345Z             return True
2025-04-11T04:23:18.4678414Z     
2025-04-11T04:23:18.4678543Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4678667Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4678758Z             self.sentinels.keys(),
2025-04-11T04:23:18.4678848Z             timeout=timeout,
2025-04-11T04:23:18.4678924Z         )
2025-04-11T04:23:18.4678992Z     
2025-04-11T04:23:18.4679079Z         error_index = None
2025-04-11T04:23:18.4679164Z         for sentinel in ready:
2025-04-11T04:23:18.4679275Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4679374Z             process = self.processes[index]
2025-04-11T04:23:18.4679459Z             process.join()
2025-04-11T04:23:18.4679554Z             if process.exitcode != 0:
2025-04-11T04:23:18.4679641Z                 error_index = index
2025-04-11T04:23:18.4679720Z                 break
2025-04-11T04:23:18.4679791Z     
2025-04-11T04:23:18.4679885Z         # Return if there was no error.
2025-04-11T04:23:18.4679973Z         if error_index is None:
2025-04-11T04:23:18.4680105Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4680304Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4680380Z     
2025-04-11T04:23:18.4680524Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4680624Z         for process in self.processes:
2025-04-11T04:23:18.4680719Z             if process.is_alive():
2025-04-11T04:23:18.4680815Z                 process.terminate()
2025-04-11T04:23:18.4680899Z             process.join()
2025-04-11T04:23:18.4680974Z     
2025-04-11T04:23:18.4681132Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4681250Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4681357Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4681475Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4681563Z             if exitcode < 0:
2025-04-11T04:23:18.4681670Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4681783Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4681931Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4682120Z                     error_index=error_index,
2025-04-11T04:23:18.4682225Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4682314Z                     exit_code=exitcode,
2025-04-11T04:23:18.4682421Z                     signal_name=name,
2025-04-11T04:23:18.4682499Z                 )
2025-04-11T04:23:18.4682582Z             else:
2025-04-11T04:23:18.4682689Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4682855Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4682959Z                     error_index=error_index,
2025-04-11T04:23:18.4683060Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4683157Z                     exit_code=exitcode,
2025-04-11T04:23:18.4683234Z                 )
2025-04-11T04:23:18.4683309Z     
2025-04-11T04:23:18.4683451Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4683627Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4683731Z         msg += original_trace
2025-04-11T04:23:18.4683907Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4684081Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4684158Z E       
2025-04-11T04:23:18.4684285Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4684394Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4684714Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4684804Z E           fn(i, *args)
2025-04-11T04:23:18.4685093Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
2025-04-11T04:23:18.4685204Z E           check_torch_ddp_checkpointIO()
2025-04-11T04:23:18.4685472Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4685565Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4685833Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4685924Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4686186Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4686273Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4686384Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.4686724Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
2025-04-11T04:23:18.4686978Z E           model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
2025-04-11T04:23:18.4687280Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T04:23:18.4687481Z E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T04:23:18.4687757Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
2025-04-11T04:23:18.4687865Z E           model = model.to(get_current_device())
2025-04-11T04:23:18.4688138Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.4688233Z E           return self._apply(convert)
2025-04-11T04:23:18.4688514Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.4688604Z E           module._apply(fn)
2025-04-11T04:23:18.4688875Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.4688980Z E           param_applied = fn(param)
2025-04-11T04:23:18.4689374Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.4689597Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.4689707Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4690010Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4690150Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4690319Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4690324Z 
2025-04-11T04:23:18.4690636Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4690797Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4690964Z [04/11/25 04:13:50] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4691099Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4691219Z                              :75 launch                                         
2025-04-11T04:23:18.4691360Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4691494Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4691694Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4691843Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4692147Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:28471 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4692292Z _____________________________ test_torch_fsdp_ckpt _____________________________
2025-04-11T04:23:18.4692296Z 
2025-04-11T04:23:18.4692403Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4693013Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4693020Z 
2025-04-11T04:23:18.4693130Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4693213Z         try_count = 0
2025-04-11T04:23:18.4693323Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4693409Z             max_try, int
2025-04-11T04:23:18.4693558Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4693640Z     
2025-04-11T04:23:18.4693754Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4693925Z             try:
2025-04-11T04:23:18.4694014Z                 try_count += 1
2025-04-11T04:23:18.4694117Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4694203Z                 return ret
2025-04-11T04:23:18.4694298Z             except exception_type as e:
2025-04-11T04:23:18.4694408Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4694596Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4694721Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4694867Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4695030Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4695112Z                     continue
2025-04-11T04:23:18.4695188Z                 else:
2025-04-11T04:23:18.4695419Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4695503Z >                   raise e
2025-04-11T04:23:18.4695508Z 
2025-04-11T04:23:18.4695608Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4695810Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4695950Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4696038Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4696249Z tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py:162: in test_torch_fsdp_ckpt
2025-04-11T04:23:18.4696343Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4696447Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4696555Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4696809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4696995Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4697292Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4697392Z     while not context.join():
2025-04-11T04:23:18.4697518Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4697523Z 
2025-04-11T04:23:18.4697747Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5e6115c90>
2025-04-11T04:23:18.4697836Z timeout = None
2025-04-11T04:23:18.4697842Z 
2025-04-11T04:23:18.4697935Z     def join(self, timeout=None):
2025-04-11T04:23:18.4698070Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4698145Z     
2025-04-11T04:23:18.4698294Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4698453Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4698620Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4698720Z         of the first process exiting.
2025-04-11T04:23:18.4698795Z     
2025-04-11T04:23:18.4698951Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4699095Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4699166Z     
2025-04-11T04:23:18.4699247Z         Args:
2025-04-11T04:23:18.4699387Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4699467Z         """
2025-04-11T04:23:18.4699607Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4699700Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4699788Z             return True
2025-04-11T04:23:18.4699861Z     
2025-04-11T04:23:18.4700001Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4700119Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4700219Z             self.sentinels.keys(),
2025-04-11T04:23:18.4700304Z             timeout=timeout,
2025-04-11T04:23:18.4700484Z         )
2025-04-11T04:23:18.4700566Z     
2025-04-11T04:23:18.4700651Z         error_index = None
2025-04-11T04:23:18.4700742Z         for sentinel in ready:
2025-04-11T04:23:18.4700852Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4700952Z             process = self.processes[index]
2025-04-11T04:23:18.4701046Z             process.join()
2025-04-11T04:23:18.4701138Z             if process.exitcode != 0:
2025-04-11T04:23:18.4701233Z                 error_index = index
2025-04-11T04:23:18.4701310Z                 break
2025-04-11T04:23:18.4701383Z     
2025-04-11T04:23:18.4701483Z         # Return if there was no error.
2025-04-11T04:23:18.4701568Z         if error_index is None:
2025-04-11T04:23:18.4701708Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4701806Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4701887Z     
2025-04-11T04:23:18.4702027Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4702128Z         for process in self.processes:
2025-04-11T04:23:18.4702224Z             if process.is_alive():
2025-04-11T04:23:18.4702410Z                 process.terminate()
2025-04-11T04:23:18.4702503Z             process.join()
2025-04-11T04:23:18.4702574Z     
2025-04-11T04:23:18.4702717Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4702841Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4702950Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4703077Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4703162Z             if exitcode < 0:
2025-04-11T04:23:18.4703271Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4703385Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4703539Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4703646Z                     error_index=error_index,
2025-04-11T04:23:18.4703749Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4703847Z                     exit_code=exitcode,
2025-04-11T04:23:18.4703940Z                     signal_name=name,
2025-04-11T04:23:18.4704018Z                 )
2025-04-11T04:23:18.4704103Z             else:
2025-04-11T04:23:18.4704207Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4704385Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4704484Z                     error_index=error_index,
2025-04-11T04:23:18.4704595Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4704684Z                     exit_code=exitcode,
2025-04-11T04:23:18.4704761Z                 )
2025-04-11T04:23:18.4704839Z     
2025-04-11T04:23:18.4704974Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4705155Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4705245Z         msg += original_trace
2025-04-11T04:23:18.4705422Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4705596Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4705671Z E       
2025-04-11T04:23:18.4705807Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4705908Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4706225Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4706308Z E           fn(i, *args)
2025-04-11T04:23:18.4706635Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
2025-04-11T04:23:18.4706751Z E           check_torch_fsdp_ckpt()
2025-04-11T04:23:18.4707014Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4707209Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4707527Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
2025-04-11T04:23:18.4707745Z E           fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
2025-04-11T04:23:18.4707952Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T04:23:18.4708156Z E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T04:23:18.4708463Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
2025-04-11T04:23:18.4708690Z E           fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
2025-04-11T04:23:18.4708965Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
2025-04-11T04:23:18.4709104Z E           self.module = FSDP(module, *args, **kwargs)
2025-04-11T04:23:18.4709484Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
2025-04-11T04:23:18.4709693Z E           _init_param_handle_from_module(
2025-04-11T04:23:18.4710077Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
2025-04-11T04:23:18.4710170Z E           _move_module_to_device(
2025-04-11T04:23:18.4710529Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
2025-04-11T04:23:18.4710710Z E           _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
2025-04-11T04:23:18.4711060Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
2025-04-11T04:23:18.4711191Z E           param.data = param.to(device_from_device_id)
2025-04-11T04:23:18.4711298Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4711596Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4711733Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4711901Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4711906Z 
2025-04-11T04:23:18.4712211Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4712368Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4712523Z [04/11/25 04:13:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4712653Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4712774Z                              :75 launch                                         
2025-04-11T04:23:18.4712917Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4713054Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4713251Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4713394Z _______________________________ test_logical_pg ________________________________
2025-04-11T04:23:18.4713398Z 
2025-04-11T04:23:18.4713494Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4714111Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4714122Z 
2025-04-11T04:23:18.4714318Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4714400Z         try_count = 0
2025-04-11T04:23:18.4714508Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4714595Z             max_try, int
2025-04-11T04:23:18.4714751Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4714825Z     
2025-04-11T04:23:18.4714942Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4715019Z             try:
2025-04-11T04:23:18.4715104Z                 try_count += 1
2025-04-11T04:23:18.4715203Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4715286Z                 return ret
2025-04-11T04:23:18.4715387Z             except exception_type as e:
2025-04-11T04:23:18.4715489Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4715676Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4715800Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4715949Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4716121Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4716311Z                     continue
2025-04-11T04:23:18.4716395Z                 else:
2025-04-11T04:23:18.4716620Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4716702Z >                   raise e
2025-04-11T04:23:18.4716713Z 
2025-04-11T04:23:18.4716809Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4716922Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4717060Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4717148Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4717312Z tests/test_device/test_init_logical_pg.py:33: in test_logical_pg
2025-04-11T04:23:18.4717405Z     spawn(check_layer, 4)
2025-04-11T04:23:18.4717506Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4717620Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4717882Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4718067Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4718352Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4718450Z     while not context.join():
2025-04-11T04:23:18.4718562Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4718567Z 
2025-04-11T04:23:18.4718766Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b841ba00>
2025-04-11T04:23:18.4718858Z timeout = None
2025-04-11T04:23:18.4718862Z 
2025-04-11T04:23:18.4718955Z     def join(self, timeout=None):
2025-04-11T04:23:18.4719089Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4719167Z     
2025-04-11T04:23:18.4719319Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4719466Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4719630Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4719730Z         of the first process exiting.
2025-04-11T04:23:18.4719802Z     
2025-04-11T04:23:18.4719956Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4720093Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4720171Z     
2025-04-11T04:23:18.4720246Z         Args:
2025-04-11T04:23:18.4720385Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4720467Z         """
2025-04-11T04:23:18.4720608Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4720802Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4720884Z             return True
2025-04-11T04:23:18.4720958Z     
2025-04-11T04:23:18.4721103Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4721222Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4721323Z             self.sentinels.keys(),
2025-04-11T04:23:18.4721409Z             timeout=timeout,
2025-04-11T04:23:18.4721492Z         )
2025-04-11T04:23:18.4721564Z     
2025-04-11T04:23:18.4721648Z         error_index = None
2025-04-11T04:23:18.4721741Z         for sentinel in ready:
2025-04-11T04:23:18.4721846Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4721953Z             process = self.processes[index]
2025-04-11T04:23:18.4722040Z             process.join()
2025-04-11T04:23:18.4722134Z             if process.exitcode != 0:
2025-04-11T04:23:18.4722230Z                 error_index = index
2025-04-11T04:23:18.4722308Z                 break
2025-04-11T04:23:18.4722388Z     
2025-04-11T04:23:18.4722483Z         # Return if there was no error.
2025-04-11T04:23:18.4722571Z         if error_index is None:
2025-04-11T04:23:18.4722901Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4722998Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4723080Z     
2025-04-11T04:23:18.4723220Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4723325Z         for process in self.processes:
2025-04-11T04:23:18.4723418Z             if process.is_alive():
2025-04-11T04:23:18.4723510Z                 process.terminate()
2025-04-11T04:23:18.4723605Z             process.join()
2025-04-11T04:23:18.4723677Z     
2025-04-11T04:23:18.4723826Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4723942Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4724052Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4724187Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4724274Z             if exitcode < 0:
2025-04-11T04:23:18.4724385Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4724494Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4724651Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4724750Z                     error_index=error_index,
2025-04-11T04:23:18.4724849Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4724947Z                     exit_code=exitcode,
2025-04-11T04:23:18.4725034Z                     signal_name=name,
2025-04-11T04:23:18.4725116Z                 )
2025-04-11T04:23:18.4725193Z             else:
2025-04-11T04:23:18.4725295Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4725465Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4725565Z                     error_index=error_index,
2025-04-11T04:23:18.4725691Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4725778Z                     exit_code=exitcode,
2025-04-11T04:23:18.4725870Z                 )
2025-04-11T04:23:18.4725942Z     
2025-04-11T04:23:18.4726076Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4726253Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4726341Z         msg += original_trace
2025-04-11T04:23:18.4726520Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4726680Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4726761Z E       
2025-04-11T04:23:18.4726888Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4726988Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4727293Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4727465Z E           fn(i, *args)
2025-04-11T04:23:18.4727727Z E         File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
2025-04-11T04:23:18.4727857Z E           tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
2025-04-11T04:23:18.4727969Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4728254Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4728391Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4728560Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4728565Z 
2025-04-11T04:23:18.4728872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4729030Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4729191Z [04/11/25 04:14:14] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4729326Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4729521Z                              :75 launch                                         
2025-04-11T04:23:18.4729661Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4729794Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4729993Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4730148Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4730450Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:64432 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4730742Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:64432 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4730885Z ____________________________ test_all_to_all_single ____________________________
2025-04-11T04:23:18.4730893Z 
2025-04-11T04:23:18.4730992Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4731596Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4731601Z 
2025-04-11T04:23:18.4731710Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4731791Z         try_count = 0
2025-04-11T04:23:18.4731893Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4731983Z             max_try, int
2025-04-11T04:23:18.4732136Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4732216Z     
2025-04-11T04:23:18.4732337Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4732425Z             try:
2025-04-11T04:23:18.4732511Z                 try_count += 1
2025-04-11T04:23:18.4732612Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4732701Z                 return ret
2025-04-11T04:23:18.4732797Z             except exception_type as e:
2025-04-11T04:23:18.4732908Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4733095Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4733213Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4733366Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4733521Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4733612Z                     continue
2025-04-11T04:23:18.4733689Z                 else:
2025-04-11T04:23:18.4734010Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4734090Z >                   raise e
2025-04-11T04:23:18.4734099Z 
2025-04-11T04:23:18.4734196Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4734316Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4734448Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4734541Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4734709Z tests/test_fp8/test_all_to_all_single.py:73: in test_all_to_all_single
2025-04-11T04:23:18.4734802Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4734906Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4735011Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4735300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4735480Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4735776Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4735950Z     while not context.join():
2025-04-11T04:23:18.4736071Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4736076Z 
2025-04-11T04:23:18.4736277Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8236230>
2025-04-11T04:23:18.4736359Z timeout = None
2025-04-11T04:23:18.4736371Z 
2025-04-11T04:23:18.4736463Z     def join(self, timeout=None):
2025-04-11T04:23:18.4736587Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4736667Z     
2025-04-11T04:23:18.4736814Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4736966Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4737130Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4737227Z         of the first process exiting.
2025-04-11T04:23:18.4737306Z     
2025-04-11T04:23:18.4737455Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4737605Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4737678Z     
2025-04-11T04:23:18.4737760Z         Args:
2025-04-11T04:23:18.4737901Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4737977Z         """
2025-04-11T04:23:18.4738127Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4738223Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4738314Z             return True
2025-04-11T04:23:18.4738387Z     
2025-04-11T04:23:18.4738520Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4738645Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4738738Z             self.sentinels.keys(),
2025-04-11T04:23:18.4738833Z             timeout=timeout,
2025-04-11T04:23:18.4738907Z         )
2025-04-11T04:23:18.4738979Z     
2025-04-11T04:23:18.4739070Z         error_index = None
2025-04-11T04:23:18.4739163Z         for sentinel in ready:
2025-04-11T04:23:18.4739276Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4739380Z             process = self.processes[index]
2025-04-11T04:23:18.4739471Z             process.join()
2025-04-11T04:23:18.4739567Z             if process.exitcode != 0:
2025-04-11T04:23:18.4739657Z                 error_index = index
2025-04-11T04:23:18.4739740Z                 break
2025-04-11T04:23:18.4739813Z     
2025-04-11T04:23:18.4739913Z         # Return if there was no error.
2025-04-11T04:23:18.4740001Z         if error_index is None:
2025-04-11T04:23:18.4740135Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4740240Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4740313Z     
2025-04-11T04:23:18.4740569Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4740669Z         for process in self.processes:
2025-04-11T04:23:18.4740760Z             if process.is_alive():
2025-04-11T04:23:18.4740866Z                 process.terminate()
2025-04-11T04:23:18.4740953Z             process.join()
2025-04-11T04:23:18.4741034Z     
2025-04-11T04:23:18.4741177Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4741301Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4741411Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4741535Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4741630Z             if exitcode < 0:
2025-04-11T04:23:18.4741741Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4741858Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4742008Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4742122Z                     error_index=error_index,
2025-04-11T04:23:18.4742225Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4742399Z                     exit_code=exitcode,
2025-04-11T04:23:18.4742498Z                     signal_name=name,
2025-04-11T04:23:18.4742575Z                 )
2025-04-11T04:23:18.4742663Z             else:
2025-04-11T04:23:18.4742768Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4742933Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4743036Z                     error_index=error_index,
2025-04-11T04:23:18.4743138Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4743235Z                     exit_code=exitcode,
2025-04-11T04:23:18.4743310Z                 )
2025-04-11T04:23:18.4743389Z     
2025-04-11T04:23:18.4743521Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4743691Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4743791Z         msg += original_trace
2025-04-11T04:23:18.4743967Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4744143Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4744218Z E       
2025-04-11T04:23:18.4744345Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4744453Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4744770Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4744867Z E           fn(i, *args)
2025-04-11T04:23:18.4745104Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
2025-04-11T04:23:18.4745195Z E           check_all2all()
2025-04-11T04:23:18.4745418Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4745521Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4745791Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4745897Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4746183Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4746288Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4746402Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4746687Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4746828Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4746991Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4746996Z 
2025-04-11T04:23:18.4747388Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4747546Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4747704Z [04/11/25 04:14:20] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4747840Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4747947Z                              :75 launch                                         
2025-04-11T04:23:18.4748090Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4748216Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4748457Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4748611Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4748914Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:62642 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4749215Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:62642 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4749877Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4749972Z   warnings.warn(
2025-04-11T04:23:18.4750525Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4750620Z   warnings.warn(
2025-04-11T04:23:18.4751180Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4751275Z   warnings.warn(
2025-04-11T04:23:18.4751829Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4751915Z   warnings.warn(
2025-04-11T04:23:18.4752482Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4752562Z   warnings.warn(
2025-04-11T04:23:18.4753106Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4753184Z   warnings.warn(
2025-04-11T04:23:18.4753741Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4753827Z   warnings.warn(
2025-04-11T04:23:18.4754376Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4754457Z   warnings.warn(
2025-04-11T04:23:18.4754604Z _______________________________ test_all_to_all ________________________________
2025-04-11T04:23:18.4754608Z 
2025-04-11T04:23:18.4754701Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4755313Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4755421Z 
2025-04-11T04:23:18.4755527Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4755612Z         try_count = 0
2025-04-11T04:23:18.4755731Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4755820Z             max_try, int
2025-04-11T04:23:18.4755983Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4756059Z     
2025-04-11T04:23:18.4756178Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4756264Z             try:
2025-04-11T04:23:18.4756355Z                 try_count += 1
2025-04-11T04:23:18.4756461Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4756548Z                 return ret
2025-04-11T04:23:18.4756655Z             except exception_type as e:
2025-04-11T04:23:18.4756761Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4756957Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4757091Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4757242Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4757490Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4757572Z                     continue
2025-04-11T04:23:18.4757657Z                 else:
2025-04-11T04:23:18.4757884Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4757966Z >                   raise e
2025-04-11T04:23:18.4757970Z 
2025-04-11T04:23:18.4758074Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4758186Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4758323Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4758411Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4758571Z tests/test_fp8/test_fp8_all_to_all.py:36: in test_all_to_all
2025-04-11T04:23:18.4758656Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4758759Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4758870Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4759125Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4759311Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4759600Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4759701Z     while not context.join():
2025-04-11T04:23:18.4759816Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4759820Z 
2025-04-11T04:23:18.4760019Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b841a410>
2025-04-11T04:23:18.4760108Z timeout = None
2025-04-11T04:23:18.4760112Z 
2025-04-11T04:23:18.4760208Z     def join(self, timeout=None):
2025-04-11T04:23:18.4760340Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4760412Z     
2025-04-11T04:23:18.4760565Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4760709Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4760872Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4760972Z         of the first process exiting.
2025-04-11T04:23:18.4761045Z     
2025-04-11T04:23:18.4761199Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4761337Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4761408Z     
2025-04-11T04:23:18.4761489Z         Args:
2025-04-11T04:23:18.4761637Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4761725Z         """
2025-04-11T04:23:18.4761965Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4762066Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4762146Z             return True
2025-04-11T04:23:18.4762221Z     
2025-04-11T04:23:18.4762361Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4762482Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4762579Z             self.sentinels.keys(),
2025-04-11T04:23:18.4762665Z             timeout=timeout,
2025-04-11T04:23:18.4762738Z         )
2025-04-11T04:23:18.4762816Z     
2025-04-11T04:23:18.4762900Z         error_index = None
2025-04-11T04:23:18.4762994Z         for sentinel in ready:
2025-04-11T04:23:18.4763102Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4763201Z             process = self.processes[index]
2025-04-11T04:23:18.4763295Z             process.join()
2025-04-11T04:23:18.4763387Z             if process.exitcode != 0:
2025-04-11T04:23:18.4763484Z                 error_index = index
2025-04-11T04:23:18.4763567Z                 break
2025-04-11T04:23:18.4763647Z     
2025-04-11T04:23:18.4763740Z         # Return if there was no error.
2025-04-11T04:23:18.4763915Z         if error_index is None:
2025-04-11T04:23:18.4764056Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4764154Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4764232Z     
2025-04-11T04:23:18.4764372Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4764470Z         for process in self.processes:
2025-04-11T04:23:18.4764568Z             if process.is_alive():
2025-04-11T04:23:18.4764660Z                 process.terminate()
2025-04-11T04:23:18.4764752Z             process.join()
2025-04-11T04:23:18.4764824Z     
2025-04-11T04:23:18.4764973Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4765088Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4765199Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4765326Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4765414Z             if exitcode < 0:
2025-04-11T04:23:18.4765527Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4765633Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4765786Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4765888Z                     error_index=error_index,
2025-04-11T04:23:18.4765989Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4766086Z                     exit_code=exitcode,
2025-04-11T04:23:18.4766174Z                     signal_name=name,
2025-04-11T04:23:18.4766260Z                 )
2025-04-11T04:23:18.4766337Z             else:
2025-04-11T04:23:18.4766442Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4766617Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4766714Z                     error_index=error_index,
2025-04-11T04:23:18.4766821Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4766912Z                     exit_code=exitcode,
2025-04-11T04:23:18.4766988Z                 )
2025-04-11T04:23:18.4767066Z     
2025-04-11T04:23:18.4767198Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4767376Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4767464Z         msg += original_trace
2025-04-11T04:23:18.4767641Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4767802Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4767878Z E       
2025-04-11T04:23:18.4768011Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4768110Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4768525Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4768607Z E           fn(i, *args)
2025-04-11T04:23:18.4768849Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
2025-04-11T04:23:18.4768937Z E           check_4gpu()
2025-04-11T04:23:18.4769162Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4769272Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4769534Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4769644Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4769925Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4770047Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4770159Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4770447Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4770685Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4770848Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4770853Z 
2025-04-11T04:23:18.4771169Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4771322Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4771490Z [04/11/25 04:14:24] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4771620Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4771736Z                              :75 launch                                         
2025-04-11T04:23:18.4771878Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4772002Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4772215Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4772361Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4772928Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4773012Z   warnings.warn(
2025-04-11T04:23:18.4773557Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4773639Z   warnings.warn(
2025-04-11T04:23:18.4774190Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4774276Z   warnings.warn(
2025-04-11T04:23:18.4774806Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4774893Z   warnings.warn(
2025-04-11T04:23:18.4775430Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4775518Z   warnings.warn(
2025-04-11T04:23:18.4776042Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4776215Z   warnings.warn(
2025-04-11T04:23:18.4776755Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4776845Z   warnings.warn(
2025-04-11T04:23:18.4777368Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4777455Z   warnings.warn(
2025-04-11T04:23:18.4777591Z ____________________________ test_all_to_all_single ____________________________
2025-04-11T04:23:18.4777596Z 
2025-04-11T04:23:18.4777688Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4778290Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4778382Z 
2025-04-11T04:23:18.4778488Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4778577Z         try_count = 0
2025-04-11T04:23:18.4778679Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4778768Z             max_try, int
2025-04-11T04:23:18.4778916Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4778990Z     
2025-04-11T04:23:18.4779109Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4779184Z             try:
2025-04-11T04:23:18.4779276Z                 try_count += 1
2025-04-11T04:23:18.4779369Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4779458Z                 return ret
2025-04-11T04:23:18.4779553Z             except exception_type as e:
2025-04-11T04:23:18.4779654Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4779852Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4779974Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4780128Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4780284Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4780374Z                     continue
2025-04-11T04:23:18.4780451Z                 else:
2025-04-11T04:23:18.4780681Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4780777Z >                   raise e
2025-04-11T04:23:18.4780782Z 
2025-04-11T04:23:18.4780884Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4781002Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4781138Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4781232Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4781405Z tests/test_fp8/test_fp8_all_to_all_single.py:34: in test_all_to_all_single
2025-04-11T04:23:18.4781493Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4781604Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4781706Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4781969Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4782149Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4782440Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4782531Z     while not context.join():
2025-04-11T04:23:18.4782642Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4782646Z 
2025-04-11T04:23:18.4782856Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b835abf0>
2025-04-11T04:23:18.4783027Z timeout = None
2025-04-11T04:23:18.4783032Z 
2025-04-11T04:23:18.4783130Z     def join(self, timeout=None):
2025-04-11T04:23:18.4783260Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4783338Z     
2025-04-11T04:23:18.4783485Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4783630Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4783803Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4783896Z         of the first process exiting.
2025-04-11T04:23:18.4783981Z     
2025-04-11T04:23:18.4784130Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4784277Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4784352Z     
2025-04-11T04:23:18.4784429Z         Args:
2025-04-11T04:23:18.4784584Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4784662Z         """
2025-04-11T04:23:18.4784815Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4784998Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4785081Z             return True
2025-04-11T04:23:18.4785161Z     
2025-04-11T04:23:18.4785292Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4785416Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4785510Z             self.sentinels.keys(),
2025-04-11T04:23:18.4785597Z             timeout=timeout,
2025-04-11T04:23:18.4785678Z         )
2025-04-11T04:23:18.4785749Z     
2025-04-11T04:23:18.4785840Z         error_index = None
2025-04-11T04:23:18.4785927Z         for sentinel in ready:
2025-04-11T04:23:18.4786038Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4786136Z             process = self.processes[index]
2025-04-11T04:23:18.4786226Z             process.join()
2025-04-11T04:23:18.4786325Z             if process.exitcode != 0:
2025-04-11T04:23:18.4786413Z                 error_index = index
2025-04-11T04:23:18.4786500Z                 break
2025-04-11T04:23:18.4786572Z     
2025-04-11T04:23:18.4786664Z         # Return if there was no error.
2025-04-11T04:23:18.4786759Z         if error_index is None:
2025-04-11T04:23:18.4786894Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4786997Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4787069Z     
2025-04-11T04:23:18.4787209Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4787314Z         for process in self.processes:
2025-04-11T04:23:18.4787402Z             if process.is_alive():
2025-04-11T04:23:18.4787501Z                 process.terminate()
2025-04-11T04:23:18.4787585Z             process.join()
2025-04-11T04:23:18.4787660Z     
2025-04-11T04:23:18.4787802Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4787920Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4788031Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4788155Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4788244Z             if exitcode < 0:
2025-04-11T04:23:18.4788352Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4788507Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4788670Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4788768Z                     error_index=error_index,
2025-04-11T04:23:18.4788877Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4788967Z                     exit_code=exitcode,
2025-04-11T04:23:18.4789063Z                     signal_name=name,
2025-04-11T04:23:18.4789139Z                 )
2025-04-11T04:23:18.4789216Z             else:
2025-04-11T04:23:18.4789428Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4789594Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4789699Z                     error_index=error_index,
2025-04-11T04:23:18.4789800Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4789895Z                     exit_code=exitcode,
2025-04-11T04:23:18.4789971Z                 )
2025-04-11T04:23:18.4790044Z     
2025-04-11T04:23:18.4790189Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4790365Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4790470Z         msg += original_trace
2025-04-11T04:23:18.4790655Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4790823Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4790909Z E       
2025-04-11T04:23:18.4791037Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4791151Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4791453Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4791661Z E           fn(i, *args)
2025-04-11T04:23:18.4791912Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
2025-04-11T04:23:18.4791999Z E           check_4gpu()
2025-04-11T04:23:18.4792232Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4792335Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4792603Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4792706Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4792989Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4793099Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4793207Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4793509Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4793646Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4793823Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4793828Z 
2025-04-11T04:23:18.4794130Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4794292Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4794448Z [04/11/25 04:14:29] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4794586Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4794699Z                              :75 launch                                         
2025-04-11T04:23:18.4794842Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4794981Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4795182Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4795336Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4795908Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4795997Z   warnings.warn(
2025-04-11T04:23:18.4796543Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4796727Z   warnings.warn(
2025-04-11T04:23:18.4797286Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4797367Z   warnings.warn(
2025-04-11T04:23:18.4797917Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4797998Z   warnings.warn(
2025-04-11T04:23:18.4798571Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4798652Z   warnings.warn(
2025-04-11T04:23:18.4799222Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4799389Z   warnings.warn(
2025-04-11T04:23:18.4799968Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4800050Z   warnings.warn(
2025-04-11T04:23:18.4800602Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4800681Z   warnings.warn(
2025-04-11T04:23:18.4800816Z _______________________________ test_all_gather ________________________________
2025-04-11T04:23:18.4800821Z 
2025-04-11T04:23:18.4800927Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4801528Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4801537Z 
2025-04-11T04:23:18.4801645Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4801727Z         try_count = 0
2025-04-11T04:23:18.4801834Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4801914Z             max_try, int
2025-04-11T04:23:18.4802071Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4802145Z     
2025-04-11T04:23:18.4802261Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4802344Z             try:
2025-04-11T04:23:18.4802431Z                 try_count += 1
2025-04-11T04:23:18.4802531Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4802615Z                 return ret
2025-04-11T04:23:18.4802711Z             except exception_type as e:
2025-04-11T04:23:18.4802818Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4803009Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4803133Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4803279Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4803441Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4803523Z                     continue
2025-04-11T04:23:18.4803602Z                 else:
2025-04-11T04:23:18.4803832Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4803912Z >                   raise e
2025-04-11T04:23:18.4803917Z 
2025-04-11T04:23:18.4804019Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4804227Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4804364Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4804458Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4804606Z tests/test_fp8/test_fp8_allgather.py:42: in test_all_gather
2025-04-11T04:23:18.4804698Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4804800Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4804908Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4805165Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4805350Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4805636Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4805728Z     while not context.join():
2025-04-11T04:23:18.4805847Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4805855Z 
2025-04-11T04:23:18.4806056Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8571db0>
2025-04-11T04:23:18.4806226Z timeout = None
2025-04-11T04:23:18.4806232Z 
2025-04-11T04:23:18.4806325Z     def join(self, timeout=None):
2025-04-11T04:23:18.4806456Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4806531Z     
2025-04-11T04:23:18.4806681Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4806834Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4806999Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4807100Z         of the first process exiting.
2025-04-11T04:23:18.4807174Z     
2025-04-11T04:23:18.4807329Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4807469Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4807543Z     
2025-04-11T04:23:18.4807625Z         Args:
2025-04-11T04:23:18.4807765Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4807849Z         """
2025-04-11T04:23:18.4807991Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4808086Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4808176Z             return True
2025-04-11T04:23:18.4808250Z     
2025-04-11T04:23:18.4808390Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4808508Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4808609Z             self.sentinels.keys(),
2025-04-11T04:23:18.4808694Z             timeout=timeout,
2025-04-11T04:23:18.4808767Z         )
2025-04-11T04:23:18.4808846Z     
2025-04-11T04:23:18.4808932Z         error_index = None
2025-04-11T04:23:18.4809026Z         for sentinel in ready:
2025-04-11T04:23:18.4809144Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4809245Z             process = self.processes[index]
2025-04-11T04:23:18.4809338Z             process.join()
2025-04-11T04:23:18.4809443Z             if process.exitcode != 0:
2025-04-11T04:23:18.4809540Z                 error_index = index
2025-04-11T04:23:18.4809618Z                 break
2025-04-11T04:23:18.4809691Z     
2025-04-11T04:23:18.4809793Z         # Return if there was no error.
2025-04-11T04:23:18.4809882Z         if error_index is None:
2025-04-11T04:23:18.4810028Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4810126Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4810206Z     
2025-04-11T04:23:18.4810352Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4810452Z         for process in self.processes:
2025-04-11T04:23:18.4810549Z             if process.is_alive():
2025-04-11T04:23:18.4810641Z                 process.terminate()
2025-04-11T04:23:18.4810834Z             process.join()
2025-04-11T04:23:18.4810908Z     
2025-04-11T04:23:18.4811051Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4811180Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4811288Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4811416Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4811500Z             if exitcode < 0:
2025-04-11T04:23:18.4811614Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4811719Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4811868Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4811971Z                     error_index=error_index,
2025-04-11T04:23:18.4812072Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4812167Z                     exit_code=exitcode,
2025-04-11T04:23:18.4812259Z                     signal_name=name,
2025-04-11T04:23:18.4812334Z                 )
2025-04-11T04:23:18.4812418Z             else:
2025-04-11T04:23:18.4812521Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4812780Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4812876Z                     error_index=error_index,
2025-04-11T04:23:18.4812983Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4813072Z                     exit_code=exitcode,
2025-04-11T04:23:18.4813147Z                 )
2025-04-11T04:23:18.4813226Z     
2025-04-11T04:23:18.4813357Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4813533Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4813623Z         msg += original_trace
2025-04-11T04:23:18.4813795Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4813965Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4814040Z E       
2025-04-11T04:23:18.4814174Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4814277Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4814578Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4814659Z E           fn(i, *args)
2025-04-11T04:23:18.4814889Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
2025-04-11T04:23:18.4814981Z E           check_4gpu()
2025-04-11T04:23:18.4815202Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4815309Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4815572Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4815683Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4815963Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4816070Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4816181Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4816466Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4816655Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4816817Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4816822Z 
2025-04-11T04:23:18.4817136Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4817291Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4817454Z [04/11/25 04:14:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4817667Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4817782Z                              :75 launch                                         
2025-04-11T04:23:18.4817929Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4818057Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4818261Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4818407Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4818714Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60836 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4819276Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4819373Z   warnings.warn(
2025-04-11T04:23:18.4820010Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4820095Z   warnings.warn(
2025-04-11T04:23:18.4820641Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4820723Z   warnings.warn(
2025-04-11T04:23:18.4821259Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4821340Z   warnings.warn(
2025-04-11T04:23:18.4821889Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4821973Z   warnings.warn(
2025-04-11T04:23:18.4822504Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4822582Z   warnings.warn(
2025-04-11T04:23:18.4823125Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4823205Z   warnings.warn(
2025-04-11T04:23:18.4823728Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4823822Z   warnings.warn(
2025-04-11T04:23:18.4823961Z _______________________________ test_all_reduce ________________________________
2025-04-11T04:23:18.4823966Z 
2025-04-11T04:23:18.4824067Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4824658Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4824664Z 
2025-04-11T04:23:18.4824775Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4824858Z         try_count = 0
2025-04-11T04:23:18.4824972Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4825056Z             max_try, int
2025-04-11T04:23:18.4825205Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4825374Z     
2025-04-11T04:23:18.4825489Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4825571Z             try:
2025-04-11T04:23:18.4825661Z                 try_count += 1
2025-04-11T04:23:18.4825753Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4825843Z                 return ret
2025-04-11T04:23:18.4825938Z             except exception_type as e:
2025-04-11T04:23:18.4826045Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4826240Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4826362Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4826508Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4826673Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4826762Z                     continue
2025-04-11T04:23:18.4826843Z                 else:
2025-04-11T04:23:18.4827081Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4827249Z >                   raise e
2025-04-11T04:23:18.4827254Z 
2025-04-11T04:23:18.4827356Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4827469Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4827601Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4827696Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4827846Z tests/test_fp8/test_fp8_allreduce.py:52: in test_all_reduce
2025-04-11T04:23:18.4827939Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4828043Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4828150Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4828403Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4828610Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4828903Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4828997Z     while not context.join():
2025-04-11T04:23:18.4829115Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4829120Z 
2025-04-11T04:23:18.4829321Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e1fc0>
2025-04-11T04:23:18.4829408Z timeout = None
2025-04-11T04:23:18.4829413Z 
2025-04-11T04:23:18.4829504Z     def join(self, timeout=None):
2025-04-11T04:23:18.4829635Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4829707Z     
2025-04-11T04:23:18.4829855Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4830006Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4830172Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4830276Z         of the first process exiting.
2025-04-11T04:23:18.4830347Z     
2025-04-11T04:23:18.4830498Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4830644Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4830718Z     
2025-04-11T04:23:18.4830799Z         Args:
2025-04-11T04:23:18.4830940Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4831021Z         """
2025-04-11T04:23:18.4831163Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4831258Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4831348Z             return True
2025-04-11T04:23:18.4831420Z     
2025-04-11T04:23:18.4831558Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4831675Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4831960Z             self.sentinels.keys(),
2025-04-11T04:23:18.4832055Z             timeout=timeout,
2025-04-11T04:23:18.4832130Z         )
2025-04-11T04:23:18.4832212Z     
2025-04-11T04:23:18.4832304Z         error_index = None
2025-04-11T04:23:18.4832394Z         for sentinel in ready:
2025-04-11T04:23:18.4832510Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4832610Z             process = self.processes[index]
2025-04-11T04:23:18.4832707Z             process.join()
2025-04-11T04:23:18.4832802Z             if process.exitcode != 0:
2025-04-11T04:23:18.4832899Z                 error_index = index
2025-04-11T04:23:18.4832978Z                 break
2025-04-11T04:23:18.4833052Z     
2025-04-11T04:23:18.4833156Z         # Return if there was no error.
2025-04-11T04:23:18.4833244Z         if error_index is None:
2025-04-11T04:23:18.4833387Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4833486Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4833564Z     
2025-04-11T04:23:18.4833713Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4833812Z         for process in self.processes:
2025-04-11T04:23:18.4834004Z             if process.is_alive():
2025-04-11T04:23:18.4834098Z                 process.terminate()
2025-04-11T04:23:18.4834184Z             process.join()
2025-04-11T04:23:18.4834264Z     
2025-04-11T04:23:18.4834406Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4834527Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4834636Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4834763Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4834860Z             if exitcode < 0:
2025-04-11T04:23:18.4834969Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4835098Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4835252Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4835367Z                     error_index=error_index,
2025-04-11T04:23:18.4835480Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4835596Z                     exit_code=exitcode,
2025-04-11T04:23:18.4835689Z                     signal_name=name,
2025-04-11T04:23:18.4835771Z                 )
2025-04-11T04:23:18.4835869Z             else:
2025-04-11T04:23:18.4835985Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4836160Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4836257Z                     error_index=error_index,
2025-04-11T04:23:18.4836359Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4836457Z                     exit_code=exitcode,
2025-04-11T04:23:18.4836532Z                 )
2025-04-11T04:23:18.4836616Z     
2025-04-11T04:23:18.4836748Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4836931Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4837020Z         msg += original_trace
2025-04-11T04:23:18.4837198Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4837370Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4837447Z E       
2025-04-11T04:23:18.4837581Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4837681Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4837982Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4838073Z E           fn(i, *args)
2025-04-11T04:23:18.4838301Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
2025-04-11T04:23:18.4838396Z E           check_4gpu()
2025-04-11T04:23:18.4838655Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4838842Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4839063Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4839177Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4839435Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4839538Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4839821Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4839929Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4840046Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4840332Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4840483Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4840648Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4840735Z 
2025-04-11T04:23:18.4841044Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4841206Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4841362Z [04/11/25 04:14:40] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4841503Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4841614Z                              :75 launch                                         
2025-04-11T04:23:18.4841765Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4841894Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4842105Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4842251Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4842552Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26964 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4843118Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4843203Z   warnings.warn(
2025-04-11T04:23:18.4843750Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4843834Z   warnings.warn(
2025-04-11T04:23:18.4844374Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4844464Z   warnings.warn(
2025-04-11T04:23:18.4845004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4845112Z   warnings.warn(
2025-04-11T04:23:18.4845665Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4845760Z   warnings.warn(
2025-04-11T04:23:18.4846289Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4846492Z   warnings.warn(
2025-04-11T04:23:18.4847030Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4847125Z   warnings.warn(
2025-04-11T04:23:18.4847647Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4847739Z   warnings.warn(
2025-04-11T04:23:18.4847876Z ________________________________ test_fp8_cast _________________________________
2025-04-11T04:23:18.4847881Z 
2025-04-11T04:23:18.4847975Z args = (), kwargs = {}
2025-04-11T04:23:18.4847980Z 
2025-04-11T04:23:18.4848079Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.4848180Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.4848314Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.4848438Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.4848640Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.4848736Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.4848741Z 
2025-04-11T04:23:18.4848847Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.4848964Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4849123Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4849233Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4849346Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4849350Z 
2025-04-11T04:23:18.4849444Z device = None
2025-04-11T04:23:18.4849449Z 
2025-04-11T04:23:18.4849572Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4849740Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4849818Z     
2025-04-11T04:23:18.4849897Z         Args:
2025-04-11T04:23:18.4850085Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4850263Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4850387Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4850464Z         """
2025-04-11T04:23:18.4850557Z         _lazy_init()
2025-04-11T04:23:18.4850658Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4850766Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4850894Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4851198Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4851361Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4851533Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4851538Z 
2025-04-11T04:23:18.4851788Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4851925Z __________________________________ test_fsdp ___________________________________
2025-04-11T04:23:18.4851930Z 
2025-04-11T04:23:18.4852033Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4852632Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4852638Z 
2025-04-11T04:23:18.4852749Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4852830Z         try_count = 0
2025-04-11T04:23:18.4852931Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4853024Z             max_try, int
2025-04-11T04:23:18.4853258Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4853343Z     
2025-04-11T04:23:18.4853459Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4853538Z             try:
2025-04-11T04:23:18.4853636Z                 try_count += 1
2025-04-11T04:23:18.4853731Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4853822Z                 return ret
2025-04-11T04:23:18.4853917Z             except exception_type as e:
2025-04-11T04:23:18.4854021Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4854219Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4854340Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4854497Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4854654Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4854760Z                     continue
2025-04-11T04:23:18.4854839Z                 else:
2025-04-11T04:23:18.4855079Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4855264Z >                   raise e
2025-04-11T04:23:18.4855270Z 
2025-04-11T04:23:18.4855366Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4855487Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4855619Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4855724Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4855874Z tests/test_fp8/test_fp8_fsdp_comm_hook.py:104: in test_fsdp
2025-04-11T04:23:18.4855966Z     spawn(demo_basic, n_gpus)
2025-04-11T04:23:18.4856077Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4856179Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4856444Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4856629Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4856925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4857017Z     while not context.join():
2025-04-11T04:23:18.4857129Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4857142Z 
2025-04-11T04:23:18.4857340Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e3ee0>
2025-04-11T04:23:18.4857424Z timeout = None
2025-04-11T04:23:18.4857429Z 
2025-04-11T04:23:18.4857532Z     def join(self, timeout=None):
2025-04-11T04:23:18.4857661Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4857747Z     
2025-04-11T04:23:18.4857898Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4858054Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4858224Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4858322Z         of the first process exiting.
2025-04-11T04:23:18.4858404Z     
2025-04-11T04:23:18.4858552Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4858701Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4858775Z     
2025-04-11T04:23:18.4858851Z         Args:
2025-04-11T04:23:18.4858999Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4859076Z         """
2025-04-11T04:23:18.4859226Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4859322Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4859414Z             return True
2025-04-11T04:23:18.4859488Z     
2025-04-11T04:23:18.4859621Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4859839Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4859936Z             self.sentinels.keys(),
2025-04-11T04:23:18.4860043Z             timeout=timeout,
2025-04-11T04:23:18.4860122Z         )
2025-04-11T04:23:18.4860197Z     
2025-04-11T04:23:18.4860292Z         error_index = None
2025-04-11T04:23:18.4860383Z         for sentinel in ready:
2025-04-11T04:23:18.4860501Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4860604Z             process = self.processes[index]
2025-04-11T04:23:18.4860693Z             process.join()
2025-04-11T04:23:18.4860798Z             if process.exitcode != 0:
2025-04-11T04:23:18.4860889Z                 error_index = index
2025-04-11T04:23:18.4860978Z                 break
2025-04-11T04:23:18.4861053Z     
2025-04-11T04:23:18.4861154Z         # Return if there was no error.
2025-04-11T04:23:18.4861247Z         if error_index is None:
2025-04-11T04:23:18.4861387Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4861506Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4861585Z     
2025-04-11T04:23:18.4861743Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4861939Z         for process in self.processes:
2025-04-11T04:23:18.4862031Z             if process.is_alive():
2025-04-11T04:23:18.4862138Z                 process.terminate()
2025-04-11T04:23:18.4862227Z             process.join()
2025-04-11T04:23:18.4862311Z     
2025-04-11T04:23:18.4862456Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4862573Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4862692Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4862815Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4862909Z             if exitcode < 0:
2025-04-11T04:23:18.4863018Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4863140Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4863292Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4863393Z                     error_index=error_index,
2025-04-11T04:23:18.4863504Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4863594Z                     exit_code=exitcode,
2025-04-11T04:23:18.4863695Z                     signal_name=name,
2025-04-11T04:23:18.4863772Z                 )
2025-04-11T04:23:18.4863861Z             else:
2025-04-11T04:23:18.4863968Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4864134Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4864239Z                     error_index=error_index,
2025-04-11T04:23:18.4864342Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4864475Z                     exit_code=exitcode,
2025-04-11T04:23:18.4864561Z                 )
2025-04-11T04:23:18.4864637Z     
2025-04-11T04:23:18.4864789Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4864961Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4865070Z         msg += original_trace
2025-04-11T04:23:18.4865246Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4865421Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4865496Z E       
2025-04-11T04:23:18.4865623Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4865733Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4866032Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4866124Z E           fn(i, *args)
2025-04-11T04:23:18.4866372Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
2025-04-11T04:23:18.4866558Z E           run_model()
2025-04-11T04:23:18.4866780Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4866886Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4867155Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4867260Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4867546Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4867653Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4867767Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4868054Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4868191Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4868367Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4868372Z 
2025-04-11T04:23:18.4868701Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4868969Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4869073Z Running basic FSDP example on rank 1.
2025-04-11T04:23:18.4869184Z Running basic FSDP example on rank 5.
2025-04-11T04:23:18.4869281Z Running basic FSDP example on rank 0.
2025-04-11T04:23:18.4869441Z [04/11/25 04:14:47] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4869586Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4869697Z                              :75 launch                                         
2025-04-11T04:23:18.4869850Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4869982Z                              environment is initialized, world size: 8          
2025-04-11T04:23:18.4870086Z Running basic FSDP example on rank 6.
2025-04-11T04:23:18.4870185Z Running basic FSDP example on rank 4.
2025-04-11T04:23:18.4870279Z Running basic FSDP example on rank 7.
2025-04-11T04:23:18.4870382Z Running basic FSDP example on rank 2.
2025-04-11T04:23:18.4870475Z Running basic FSDP example on rank 3.
2025-04-11T04:23:18.4870691Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4870843Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4871159Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4871450Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4871744Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4872027Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4872308Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4872879Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4872965Z   warnings.warn(
2025-04-11T04:23:18.4873509Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4873600Z   warnings.warn(
2025-04-11T04:23:18.4874308Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4874394Z   warnings.warn(
2025-04-11T04:23:18.4874942Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4875023Z   warnings.warn(
2025-04-11T04:23:18.4875591Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4875672Z   warnings.warn(
2025-04-11T04:23:18.4876220Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4876316Z   warnings.warn(
2025-04-11T04:23:18.4876958Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4877052Z   warnings.warn(
2025-04-11T04:23:18.4877592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4877686Z   warnings.warn(
2025-04-11T04:23:18.4878244Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4878336Z   warnings.warn(
2025-04-11T04:23:18.4878880Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4878974Z   warnings.warn(
2025-04-11T04:23:18.4879522Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4879602Z   warnings.warn(
2025-04-11T04:23:18.4880153Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4880233Z   warnings.warn(
2025-04-11T04:23:18.4880796Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4880880Z   warnings.warn(
2025-04-11T04:23:18.4881422Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4881519Z   warnings.warn(
2025-04-11T04:23:18.4882084Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4882164Z   warnings.warn(
2025-04-11T04:23:18.4882706Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4882792Z   warnings.warn(
2025-04-11T04:23:18.4883565Z [rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T04:23:18.4884399Z [rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T04:23:18.4885135Z [rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T04:23:18.4885881Z [rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T04:23:18.4886123Z ________________________________ test_fp8_hook _________________________________
2025-04-11T04:23:18.4886128Z 
2025-04-11T04:23:18.4886410Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4886506Z     def test_fp8_hook():
2025-04-11T04:23:18.4886589Z         # create tensors
2025-04-11T04:23:18.4886784Z >       w = nn.Parameter(torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE))
2025-04-11T04:23:18.4886902Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4887191Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4887338Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4887502Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4887507Z 
2025-04-11T04:23:18.4887631Z tests/test_fp8/test_fp8_hook.py:41: RuntimeError
2025-04-11T04:23:18.4887774Z __________________________ test_fp8_linear[True-True] __________________________
2025-04-11T04:23:18.4887779Z 
2025-04-11T04:23:18.4887872Z use_bias = True, use_batch = True
2025-04-11T04:23:18.4887883Z 
2025-04-11T04:23:18.4888154Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4888285Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T04:23:18.4888422Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T04:23:18.4888549Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T04:23:18.4888640Z         # create tensors
2025-04-11T04:23:18.4888837Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T04:23:18.4888946Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4889233Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4889366Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4889531Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4889536Z 
2025-04-11T04:23:18.4889656Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T04:23:18.4889806Z _________________________ test_fp8_linear[True-False] __________________________
2025-04-11T04:23:18.4889811Z 
2025-04-11T04:23:18.4889907Z use_bias = False, use_batch = True
2025-04-11T04:23:18.4889911Z 
2025-04-11T04:23:18.4890183Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4890396Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T04:23:18.4890531Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T04:23:18.4890662Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T04:23:18.4890744Z         # create tensors
2025-04-11T04:23:18.4890946Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T04:23:18.4891053Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4891344Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4891484Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4891655Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4891660Z 
2025-04-11T04:23:18.4891779Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T04:23:18.4891927Z _________________________ test_fp8_linear[False-True] __________________________
2025-04-11T04:23:18.4891939Z 
2025-04-11T04:23:18.4892118Z use_bias = True, use_batch = False
2025-04-11T04:23:18.4892123Z 
2025-04-11T04:23:18.4892394Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4892527Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T04:23:18.4892652Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T04:23:18.4892783Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T04:23:18.4892866Z         # create tensors
2025-04-11T04:23:18.4893056Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T04:23:18.4893168Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4893449Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4893592Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4893752Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4893757Z 
2025-04-11T04:23:18.4893881Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T04:23:18.4894025Z _________________________ test_fp8_linear[False-False] _________________________
2025-04-11T04:23:18.4894030Z 
2025-04-11T04:23:18.4894134Z use_bias = False, use_batch = False
2025-04-11T04:23:18.4894139Z 
2025-04-11T04:23:18.4894406Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4894534Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T04:23:18.4894660Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T04:23:18.4894782Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T04:23:18.4894879Z         # create tensors
2025-04-11T04:23:18.4895080Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T04:23:18.4895196Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4895477Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4895616Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4895774Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4895780Z 
2025-04-11T04:23:18.4895896Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T04:23:18.4896042Z _____________________________ test_reduce_scatter ______________________________
2025-04-11T04:23:18.4896047Z 
2025-04-11T04:23:18.4896139Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4896760Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4896854Z 
2025-04-11T04:23:18.4896960Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4897050Z         try_count = 0
2025-04-11T04:23:18.4897151Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4897237Z             max_try, int
2025-04-11T04:23:18.4897387Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4897460Z     
2025-04-11T04:23:18.4897580Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4897657Z             try:
2025-04-11T04:23:18.4897749Z                 try_count += 1
2025-04-11T04:23:18.4897844Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4897925Z                 return ret
2025-04-11T04:23:18.4898029Z             except exception_type as e:
2025-04-11T04:23:18.4898131Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4898329Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4898538Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4898693Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4898854Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4898936Z                     continue
2025-04-11T04:23:18.4899020Z                 else:
2025-04-11T04:23:18.4899249Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4899337Z >                   raise e
2025-04-11T04:23:18.4899342Z 
2025-04-11T04:23:18.4899436Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4899559Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4899697Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4899785Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4899959Z tests/test_fp8/test_fp8_reduce_scatter.py:41: in test_reduce_scatter
2025-04-11T04:23:18.4900048Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4900158Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4900261Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4900541Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4900735Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4901032Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4901129Z     while not context.join():
2025-04-11T04:23:18.4901241Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4901246Z 
2025-04-11T04:23:18.4901453Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e06d0>
2025-04-11T04:23:18.4901540Z timeout = None
2025-04-11T04:23:18.4901545Z 
2025-04-11T04:23:18.4901641Z     def join(self, timeout=None):
2025-04-11T04:23:18.4901771Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4901845Z     
2025-04-11T04:23:18.4901999Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4902145Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4902319Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4902415Z         of the first process exiting.
2025-04-11T04:23:18.4902495Z     
2025-04-11T04:23:18.4902642Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4902782Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4902861Z     
2025-04-11T04:23:18.4902938Z         Args:
2025-04-11T04:23:18.4903171Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4903247Z         """
2025-04-11T04:23:18.4903389Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4903494Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4903577Z             return True
2025-04-11T04:23:18.4903658Z     
2025-04-11T04:23:18.4903791Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4903918Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4904013Z             self.sentinels.keys(),
2025-04-11T04:23:18.4904099Z             timeout=timeout,
2025-04-11T04:23:18.4904181Z         )
2025-04-11T04:23:18.4904253Z     
2025-04-11T04:23:18.4904344Z         error_index = None
2025-04-11T04:23:18.4904432Z         for sentinel in ready:
2025-04-11T04:23:18.4904538Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4904649Z             process = self.processes[index]
2025-04-11T04:23:18.4904742Z             process.join()
2025-04-11T04:23:18.4904846Z             if process.exitcode != 0:
2025-04-11T04:23:18.4904936Z                 error_index = index
2025-04-11T04:23:18.4905100Z                 break
2025-04-11T04:23:18.4905182Z     
2025-04-11T04:23:18.4905276Z         # Return if there was no error.
2025-04-11T04:23:18.4905371Z         if error_index is None:
2025-04-11T04:23:18.4905505Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4905607Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4905680Z     
2025-04-11T04:23:18.4905820Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4905925Z         for process in self.processes:
2025-04-11T04:23:18.4906015Z             if process.is_alive():
2025-04-11T04:23:18.4906114Z                 process.terminate()
2025-04-11T04:23:18.4906200Z             process.join()
2025-04-11T04:23:18.4906272Z     
2025-04-11T04:23:18.4906420Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4906540Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4906659Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4906785Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4906882Z             if exitcode < 0:
2025-04-11T04:23:18.4906992Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4907101Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4907260Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4907358Z                     error_index=error_index,
2025-04-11T04:23:18.4907464Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4907555Z                     exit_code=exitcode,
2025-04-11T04:23:18.4907642Z                     signal_name=name,
2025-04-11T04:23:18.4907725Z                 )
2025-04-11T04:23:18.4907800Z             else:
2025-04-11T04:23:18.4907913Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4908078Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4908182Z                     error_index=error_index,
2025-04-11T04:23:18.4908282Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4908373Z                     exit_code=exitcode,
2025-04-11T04:23:18.4908522Z                 )
2025-04-11T04:23:18.4908599Z     
2025-04-11T04:23:18.4908743Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4908917Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4909012Z         msg += original_trace
2025-04-11T04:23:18.4909188Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4909353Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4909438Z E       
2025-04-11T04:23:18.4909565Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4909766Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4910070Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4910158Z E           fn(i, *args)
2025-04-11T04:23:18.4910412Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
2025-04-11T04:23:18.4910497Z E           check_4gpu()
2025-04-11T04:23:18.4910723Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4910820Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4911090Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4911191Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4911470Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4911594Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4911701Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4912088Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4912226Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4912395Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4912400Z 
2025-04-11T04:23:18.4912709Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4912869Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4913029Z [04/11/25 04:14:54] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4913410Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4913734Z                              :75 launch                                         
2025-04-11T04:23:18.4914068Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4914411Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4914804Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4915233Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4915764Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24751 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4916439Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24751 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4917106Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24751 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4918120Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4918855Z   warnings.warn(
2025-04-11T04:23:18.4919528Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4920241Z   warnings.warn(
2025-04-11T04:23:18.4920919Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4921620Z   warnings.warn(
2025-04-11T04:23:18.4922288Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4923079Z   warnings.warn(
2025-04-11T04:23:18.4923747Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4924443Z   warnings.warn(
2025-04-11T04:23:18.4925094Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4925763Z   warnings.warn(
2025-04-11T04:23:18.4926409Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4927098Z   warnings.warn(
2025-04-11T04:23:18.4927742Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4928515Z   warnings.warn(
2025-04-11T04:23:18.4928760Z _________________________________ test_bucket __________________________________
2025-04-11T04:23:18.4928974Z 
2025-04-11T04:23:18.4929052Z kwargs = {}
2025-04-11T04:23:18.4929368Z val = {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}
2025-04-11T04:23:18.4929875Z arg_map = {'test_config': {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}}
2025-04-11T04:23:18.4930630Z partial_func = functools.partial(<function test_bucket at 0x7fb5e55d57e0>, test_config={'block_size': 4, 'max_batch_size': 4, 'max_input_len': 32, 'max_output_len': 8, 'dtype': torch.float16, 'tp_size': 1})
2025-04-11T04:23:18.4931140Z 
2025-04-11T04:23:18.4931257Z     def _execute_function_by_param(**kwargs):
2025-04-11T04:23:18.4931518Z         for val in values:
2025-04-11T04:23:18.4931762Z             arg_map = {argument: val}
2025-04-11T04:23:18.4932033Z             partial_func = partial(func, **arg_map)
2025-04-11T04:23:18.4932306Z >           partial_func(**kwargs)
2025-04-11T04:23:18.4932457Z 
2025-04-11T04:23:18.4932563Z colossalai/testing/utils.py:64: 
2025-04-11T04:23:18.4932825Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4933152Z tests/test_infer/test_batch_bucket.py:42: in test_bucket
2025-04-11T04:23:18.4933514Z     cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T04:23:18.4933906Z colossalai/inference/kv_cache/kvcache_manager.py:105: in __init__
2025-04-11T04:23:18.4934298Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:18.4934643Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4934831Z 
2025-04-11T04:23:18.4935069Z self = <colossalai.inference.kv_cache.kvcache_manager.KVCacheManager object at 0x7fb5b8491750>
2025-04-11T04:23:18.4935508Z kalloc_shape = (40, 4, 4, 32), valloc_shape = (40, 4, 4, 32)
2025-04-11T04:23:18.4935713Z 
2025-04-11T04:23:18.4935802Z     def _init_device_caches(
2025-04-11T04:23:18.4936105Z         self, kalloc_shape: Tuple[int, ...], valloc_shape: Tuple[int, ...]
2025-04-11T04:23:18.4936437Z     ) -> Tuple[torch.Tensor, torch.Tensor]:
2025-04-11T04:23:18.4936727Z         """Initialize the physical cache on the device.
2025-04-11T04:23:18.4936996Z     
2025-04-11T04:23:18.4937288Z         For each layer of the model, we allocate two tensors for key and value respectively,
2025-04-11T04:23:18.4937705Z         with shape of [num_blocks, num_kv_heads, block_size, head_size]
2025-04-11T04:23:18.4937995Z         """
2025-04-11T04:23:18.4938195Z         k_cache: List[torch.Tensor] = []
2025-04-11T04:23:18.4938450Z         v_cache: List[torch.Tensor] = []
2025-04-11T04:23:18.4938809Z         for _ in range(self.num_layers):
2025-04-11T04:23:18.4939184Z >           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:18.4939582Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4940053Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4940547Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4940905Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4941143Z 
2025-04-11T04:23:18.4941308Z colossalai/inference/kv_cache/kvcache_manager.py:519: RuntimeError
2025-04-11T04:23:18.4941688Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4942056Z [04/11/25 04:14:55] INFO     colossalai -                                       
2025-04-11T04:23:18.4942407Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.4942713Z                              INFO:                                              
2025-04-11T04:23:18.4943115Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T04:23:18.4943435Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T04:23:18.4943751Z                     INFO     colossalai -                                       
2025-04-11T04:23:18.4944072Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.4944395Z                              INFO: Allocating KV cache with shape: (40, 4, 4,   
2025-04-11T04:23:18.4944706Z                              32) consisting of 40 blocks.                       
2025-04-11T04:23:18.4945028Z ___________________________ test_continuous_batching ___________________________
2025-04-11T04:23:18.4945247Z 
2025-04-11T04:23:18.4945347Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4946124Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4946798Z 
2025-04-11T04:23:18.4946903Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4947190Z         try_count = 0
2025-04-11T04:23:18.4947421Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4947668Z             max_try, int
2025-04-11T04:23:18.4947947Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4948247Z     
2025-04-11T04:23:18.4948582Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4948857Z             try:
2025-04-11T04:23:18.4949065Z                 try_count += 1
2025-04-11T04:23:18.4949323Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4949591Z                 return ret
2025-04-11T04:23:18.4949840Z             except exception_type as e:
2025-04-11T04:23:18.4950121Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4950485Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4950880Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4951236Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4951620Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4951941Z                     continue
2025-04-11T04:23:18.4952164Z                 else:
2025-04-11T04:23:18.4952504Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4952886Z >                   raise e
2025-04-11T04:23:18.4953028Z 
2025-04-11T04:23:18.4953225Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4953498Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4953825Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4954117Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4954444Z tests/test_infer/test_continuous_batching.py:67: in test_continuous_batching
2025-04-11T04:23:18.4954788Z     spawn(run_dist, 1)
2025-04-11T04:23:18.4955028Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4955307Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4955742Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4956335Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4956881Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4957353Z     while not context.join():
2025-04-11T04:23:18.4957616Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4957795Z 
2025-04-11T04:23:18.4958000Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5e5782c50>
2025-04-11T04:23:18.4958467Z timeout = None
2025-04-11T04:23:18.4958583Z 
2025-04-11T04:23:18.4958675Z     def join(self, timeout=None):
2025-04-11T04:23:18.4958958Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4959222Z     
2025-04-11T04:23:18.4959467Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4959825Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4960200Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4960534Z         of the first process exiting.
2025-04-11T04:23:18.4960774Z     
2025-04-11T04:23:18.4961028Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4961394Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4961674Z     
2025-04-11T04:23:18.4961837Z         Args:
2025-04-11T04:23:18.4962120Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4962406Z         """
2025-04-11T04:23:18.4962650Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4962942Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4963183Z             return True
2025-04-11T04:23:18.4963384Z     
2025-04-11T04:23:18.4963610Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4963929Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4964202Z             self.sentinels.keys(),
2025-04-11T04:23:18.4964442Z             timeout=timeout,
2025-04-11T04:23:18.4964657Z         )
2025-04-11T04:23:18.4964833Z     
2025-04-11T04:23:18.4965016Z         error_index = None
2025-04-11T04:23:18.4965235Z         for sentinel in ready:
2025-04-11T04:23:18.4965490Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4965780Z             process = self.processes[index]
2025-04-11T04:23:18.4966049Z             process.join()
2025-04-11T04:23:18.4966283Z             if process.exitcode != 0:
2025-04-11T04:23:18.4966527Z                 error_index = index
2025-04-11T04:23:18.4966764Z                 break
2025-04-11T04:23:18.4966966Z     
2025-04-11T04:23:18.4967159Z         # Return if there was no error.
2025-04-11T04:23:18.4967411Z         if error_index is None:
2025-04-11T04:23:18.4967685Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4967988Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4968227Z     
2025-04-11T04:23:18.4968467Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4968770Z         for process in self.processes:
2025-04-11T04:23:18.4969018Z             if process.is_alive():
2025-04-11T04:23:18.4969359Z                 process.terminate()
2025-04-11T04:23:18.4969601Z             process.join()
2025-04-11T04:23:18.4969814Z     
2025-04-11T04:23:18.4970050Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4970376Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4970680Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4970990Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4971269Z             if exitcode < 0:
2025-04-11T04:23:18.4971517Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4971800Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4972130Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4972450Z                     error_index=error_index,
2025-04-11T04:23:18.4972721Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4972988Z                     exit_code=exitcode,
2025-04-11T04:23:18.4973232Z                     signal_name=name,
2025-04-11T04:23:18.4973464Z                 )
2025-04-11T04:23:18.4973655Z             else:
2025-04-11T04:23:18.4973965Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4974302Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4974632Z                     error_index=error_index,
2025-04-11T04:23:18.4974887Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4975164Z                     exit_code=exitcode,
2025-04-11T04:23:18.4975413Z                 )
2025-04-11T04:23:18.4975600Z     
2025-04-11T04:23:18.4975826Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4976206Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4976538Z         msg += original_trace
2025-04-11T04:23:18.4976854Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4977260Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4977563Z E       
2025-04-11T04:23:18.4977784Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4978080Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4978551Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4979003Z E           fn(i, *args)
2025-04-11T04:23:18.4979384Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
2025-04-11T04:23:18.4979782Z E           check_inference_engine()
2025-04-11T04:23:18.4980196Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4980614Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4981020Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4981440Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4981849Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4982258Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4982517Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.4982982Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
2025-04-11T04:23:18.4983502Z E           model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
2025-04-11T04:23:18.4984026Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.4984491Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.4984935Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.4985587Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.4986069Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.4986506Z E           module._apply(fn)
2025-04-11T04:23:18.4986921Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.4987348Z E           module._apply(fn)
2025-04-11T04:23:18.4987754Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.4988192Z E           param_applied = fn(param)
2025-04-11T04:23:18.4988669Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.4989137Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.4989430Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4989905Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4990497Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4990863Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4991100Z 
2025-04-11T04:23:18.4991411Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4991941Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4992341Z [04/11/25 04:15:02] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4992714Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4993022Z                              :75 launch                                         
2025-04-11T04:23:18.4993345Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4993687Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.4994084Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4994498Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4995868Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4997236Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4998149Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4998984Z   warnings.warn(
2025-04-11T04:23:18.4999934Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5000885Z   warnings.warn(
2025-04-11T04:23:18.5001799Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5002747Z   warnings.warn(
2025-04-11T04:23:18.5003766Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5004706Z   warnings.warn(
2025-04-11T04:23:18.5005617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5006552Z   warnings.warn(
2025-04-11T04:23:18.5007442Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5008386Z   warnings.warn(
2025-04-11T04:23:18.5009288Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5010320Z   warnings.warn(
2025-04-11T04:23:18.5011243Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5012195Z   warnings.warn(
2025-04-11T04:23:18.5013112Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5014069Z   warnings.warn(
2025-04-11T04:23:18.5014977Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5015924Z   warnings.warn(
2025-04-11T04:23:18.5016167Z _______________________________ test_drafter[5] ________________________________
2025-04-11T04:23:18.5016381Z 
2025-04-11T04:23:18.5016755Z tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2025-04-11T04:23:18.5017429Z 	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
2025-04-11T04:23:18.5017809Z }
2025-04-11T04:23:18.5017988Z spec_num = 5
2025-04-11T04:23:18.5018102Z 
2025-04-11T04:23:18.5018239Z     @pytest.mark.parametrize("spec_num", [SPEC_NUM])
2025-04-11T04:23:18.5018544Z     def test_drafter(tokenizer, spec_num: int):
2025-04-11T04:23:18.5018819Z         torch.manual_seed(123)
2025-04-11T04:23:18.5019041Z     
2025-04-11T04:23:18.5019230Z         device = get_current_device()
2025-04-11T04:23:18.5019518Z         toy_config = LlamaConfig(num_hidden_layers=NUM_LAYERS)
2025-04-11T04:23:18.5019844Z         toy_config.pad_token_id = tokenizer.eos_token_id
2025-04-11T04:23:18.5020155Z         drafter_model = LlamaForCausalLM(toy_config)
2025-04-11T04:23:18.5020452Z >       drafter_model = drafter_model.eval().cuda()
2025-04-11T04:23:18.5020632Z 
2025-04-11T04:23:18.5020778Z tests/test_infer/test_drafter.py:27: 
2025-04-11T04:23:18.5021084Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5021609Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2548: in cuda
2025-04-11T04:23:18.5022036Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.5022436Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
2025-04-11T04:23:18.5022857Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.5023273Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.5023668Z     module._apply(fn)
2025-04-11T04:23:18.5024026Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.5024416Z     module._apply(fn)
2025-04-11T04:23:18.5024775Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.5025179Z     param_applied = fn(param)
2025-04-11T04:23:18.5025442Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5025628Z 
2025-04-11T04:23:18.5025718Z t = Parameter containing:
2025-04-11T04:23:18.5025990Z tensor([[-0.0259,  0.0026,  0.0006,  ...,  0.0104,  0.0194,  0.0062],
2025-04-11T04:23:18.5026375Z         [-0.0076,  0.0020,...5,  0.0329,  0.0046],
2025-04-11T04:23:18.5026670Z         [-0.0124,  0.0230, -0.0264,  ..., -0.0224, -0.0274, -0.0157]],
2025-04-11T04:23:18.5026947Z        requires_grad=True)
2025-04-11T04:23:18.5027083Z 
2025-04-11T04:23:18.5027198Z >   return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.5027474Z E   RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5027937Z E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5028460Z E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5028839Z E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5029062Z 
2025-04-11T04:23:18.5029327Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
2025-04-11T04:23:18.5029784Z ________________________________ test_spec_dec _________________________________
2025-04-11T04:23:18.5029993Z 
2025-04-11T04:23:18.5030357Z tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2025-04-11T04:23:18.5031022Z 	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
2025-04-11T04:23:18.5031389Z }
2025-04-11T04:23:18.5031493Z 
2025-04-11T04:23:18.5031589Z     def test_spec_dec(tokenizer):
2025-04-11T04:23:18.5031838Z         spec_num = SPEC_NUM
2025-04-11T04:23:18.5032077Z         device = get_current_device()
2025-04-11T04:23:18.5032344Z         tokenizer.pad_token = tokenizer.eos_token
2025-04-11T04:23:18.5032601Z     
2025-04-11T04:23:18.5032801Z         # Dummy config for Glide Model
2025-04-11T04:23:18.5033068Z         glide_config = GlideLlamaConfig(
2025-04-11T04:23:18.5033326Z             intermediate_size=8192,
2025-04-11T04:23:18.5033568Z             large_hidden_size=4096,
2025-04-11T04:23:18.5033821Z             large_num_attention_heads=32,
2025-04-11T04:23:18.5034081Z             num_hidden_layers=NUM_LAYERS,
2025-04-11T04:23:18.5034319Z         )
2025-04-11T04:23:18.5034549Z         drafter_model = GlideLlamaForCausalLM(glide_config)
2025-04-11T04:23:18.5034813Z     
2025-04-11T04:23:18.5035013Z         assert hasattr(drafter_model, "model")
2025-04-11T04:23:18.5035299Z         assert hasattr(drafter_model.model, "layers")
2025-04-11T04:23:18.5035616Z         for _, layer in enumerate(drafter_model.model.layers):
2025-04-11T04:23:18.5035917Z             assert hasattr(layer, "cross_attn")
2025-04-11T04:23:18.5036156Z     
2025-04-11T04:23:18.5036389Z         # Init the Drafter by providing the sharded drafter model
2025-04-11T04:23:18.5036785Z >       drafter = Drafter(drafter_model, tokenizer, device=device, dtype=torch.float16)
2025-04-11T04:23:18.5037148Z 
2025-04-11T04:23:18.5037255Z tests/test_infer/test_drafter.py:65: 
2025-04-11T04:23:18.5037528Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5037844Z colossalai/inference/spec/drafter.py:31: in __init__
2025-04-11T04:23:18.5038148Z     self._drafter_model = model.to(self._device)
2025-04-11T04:23:18.5038569Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.5038976Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.5039364Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.5039757Z     return self._apply(convert)
2025-04-11T04:23:18.5040156Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.5040547Z     module._apply(fn)
2025-04-11T04:23:18.5040905Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.5041294Z     module._apply(fn)
2025-04-11T04:23:18.5041648Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.5042127Z     param_applied = fn(param)
2025-04-11T04:23:18.5042391Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5042586Z 
2025-04-11T04:23:18.5042680Z t = Parameter containing:
2025-04-11T04:23:18.5042965Z tensor([[-0.0389,  0.0039, -0.0004,  ...,  0.0133,  0.0029, -0.0177],
2025-04-11T04:23:18.5043269Z         [-0.0144,  0.0054,...4,  0.0227,  0.0264],
2025-04-11T04:23:18.5043562Z         [ 0.0320, -0.0080,  0.0294,  ...,  0.0173,  0.0005, -0.0045]],
2025-04-11T04:23:18.5043837Z        requires_grad=True)
2025-04-11T04:23:18.5043982Z 
2025-04-11T04:23:18.5044066Z     def convert(t):
2025-04-11T04:23:18.5044327Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.5044716Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.5045096Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.5045502Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.5045887Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5046355Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5046863Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5047234Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5047467Z 
2025-04-11T04:23:18.5047731Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.5048203Z ______________________________ test_cache_manager ______________________________
2025-04-11T04:23:18.5048417Z 
2025-04-11T04:23:18.5048517Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.5049286Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.5049999Z 
2025-04-11T04:23:18.5050108Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.5050378Z         try_count = 0
2025-04-11T04:23:18.5050614Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.5050889Z             max_try, int
2025-04-11T04:23:18.5051166Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.5051464Z     
2025-04-11T04:23:18.5051685Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.5051955Z             try:
2025-04-11T04:23:18.5052164Z                 try_count += 1
2025-04-11T04:23:18.5052402Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.5052734Z                 return ret
2025-04-11T04:23:18.5052964Z             except exception_type as e:
2025-04-11T04:23:18.5053231Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.5053591Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.5053955Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.5054288Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.5054659Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.5054970Z                     continue
2025-04-11T04:23:18.5055185Z                 else:
2025-04-11T04:23:18.5055528Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.5055888Z >                   raise e
2025-04-11T04:23:18.5056026Z 
2025-04-11T04:23:18.5056125Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.5056390Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5056815Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.5057113Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.5057422Z tests/test_infer/test_kvcache_manager.py:174: in test_cache_manager
2025-04-11T04:23:18.5057734Z     spawn(run_dist, 1)
2025-04-11T04:23:18.5057962Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.5058233Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.5058658Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.5059164Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.5059689Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.5060139Z     while not context.join():
2025-04-11T04:23:18.5060397Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5060576Z 
2025-04-11T04:23:18.5060782Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840c1f0>
2025-04-11T04:23:18.5061129Z timeout = None
2025-04-11T04:23:18.5061243Z 
2025-04-11T04:23:18.5061342Z     def join(self, timeout=None):
2025-04-11T04:23:18.5061615Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.5061880Z     
2025-04-11T04:23:18.5062126Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.5062485Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.5062864Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.5063183Z         of the first process exiting.
2025-04-11T04:23:18.5063414Z     
2025-04-11T04:23:18.5063658Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.5064019Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.5064297Z     
2025-04-11T04:23:18.5064461Z         Args:
2025-04-11T04:23:18.5064718Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.5065002Z         """
2025-04-11T04:23:18.5065248Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.5065541Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.5065772Z             return True
2025-04-11T04:23:18.5065973Z     
2025-04-11T04:23:18.5066195Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.5066520Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.5066806Z             self.sentinels.keys(),
2025-04-11T04:23:18.5067041Z             timeout=timeout,
2025-04-11T04:23:18.5067262Z         )
2025-04-11T04:23:18.5067434Z     
2025-04-11T04:23:18.5067609Z         error_index = None
2025-04-11T04:23:18.5067830Z         for sentinel in ready:
2025-04-11T04:23:18.5068213Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.5068527Z             process = self.processes[index]
2025-04-11T04:23:18.5068789Z             process.join()
2025-04-11T04:23:18.5069020Z             if process.exitcode != 0:
2025-04-11T04:23:18.5069266Z                 error_index = index
2025-04-11T04:23:18.5069487Z                 break
2025-04-11T04:23:18.5069679Z     
2025-04-11T04:23:18.5069866Z         # Return if there was no error.
2025-04-11T04:23:18.5070117Z         if error_index is None:
2025-04-11T04:23:18.5070391Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.5070691Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.5070931Z     
2025-04-11T04:23:18.5071168Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.5071475Z         for process in self.processes:
2025-04-11T04:23:18.5071719Z             if process.is_alive():
2025-04-11T04:23:18.5071966Z                 process.terminate()
2025-04-11T04:23:18.5072202Z             process.join()
2025-04-11T04:23:18.5072412Z     
2025-04-11T04:23:18.5072642Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.5073065Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.5073358Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.5073661Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.5073939Z             if exitcode < 0:
2025-04-11T04:23:18.5074188Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.5074477Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5074799Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.5075119Z                     error_index=error_index,
2025-04-11T04:23:18.5075387Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5075655Z                     exit_code=exitcode,
2025-04-11T04:23:18.5075915Z                     signal_name=name,
2025-04-11T04:23:18.5076170Z                 )
2025-04-11T04:23:18.5076354Z             else:
2025-04-11T04:23:18.5076592Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5076928Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.5077259Z                     error_index=error_index,
2025-04-11T04:23:18.5077509Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5077760Z                     exit_code=exitcode,
2025-04-11T04:23:18.5077985Z                 )
2025-04-11T04:23:18.5078161Z     
2025-04-11T04:23:18.5078381Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.5078744Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.5079070Z         msg += original_trace
2025-04-11T04:23:18.5079377Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.5079775Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.5080080Z E       
2025-04-11T04:23:18.5080303Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.5080590Z E       Traceback (most recent call last):
2025-04-11T04:23:18.5081057Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.5081516Z E           fn(i, *args)
2025-04-11T04:23:18.5081882Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
2025-04-11T04:23:18.5082279Z E           check_cache_manager()
2025-04-11T04:23:18.5082681Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.5083098Z E           partial_func(**kwargs)
2025-04-11T04:23:18.5117599Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
2025-04-11T04:23:18.5118367Z E           cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T04:23:18.5118892Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T04:23:18.5119461Z E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:18.5120050Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T04:23:18.5120670Z E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:18.5121094Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5121608Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5122119Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5122516Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5122747Z 
2025-04-11T04:23:18.5123104Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.5123802Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.5124183Z [04/11/25 04:15:28] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.5124532Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.5124841Z                              :75 launch                                         
2025-04-11T04:23:18.5125160Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.5125490Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.5125814Z [04/11/25 04:15:28] INFO     colossalai -                                       
2025-04-11T04:23:18.5126156Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.5126460Z                              INFO:                                              
2025-04-11T04:23:18.5126757Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T04:23:18.5127070Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T04:23:18.5127380Z                     INFO     colossalai -                                       
2025-04-11T04:23:18.5127693Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.5128011Z                              INFO: Allocating KV cache with shape: (80, 16, 8,  
2025-04-11T04:23:18.5128314Z                              32) consisting of 80 blocks.                       
2025-04-11T04:23:18.5128696Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.5129110Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.5130466Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.5131826Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.5132209Z ____________________ test_running_list_and_request_handler _____________________
2025-04-11T04:23:18.5132430Z 
2025-04-11T04:23:18.5132526Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.5133291Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.5134066Z 
2025-04-11T04:23:18.5134178Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.5134434Z         try_count = 0
2025-04-11T04:23:18.5134666Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.5134919Z             max_try, int
2025-04-11T04:23:18.5135186Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.5135477Z     
2025-04-11T04:23:18.5135690Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.5135963Z             try:
2025-04-11T04:23:18.5136163Z                 try_count += 1
2025-04-11T04:23:18.5136397Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.5136641Z                 return ret
2025-04-11T04:23:18.5136868Z             except exception_type as e:
2025-04-11T04:23:18.5137127Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.5137484Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.5137610Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.5137847Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.5138008Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.5138095Z                     continue
2025-04-11T04:23:18.5138177Z                 else:
2025-04-11T04:23:18.5138399Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.5138479Z >                   raise e
2025-04-11T04:23:18.5138490Z 
2025-04-11T04:23:18.5138586Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.5138698Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5138834Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.5138925Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.5139133Z tests/test_infer/test_request_handler.py:101: in test_running_list_and_request_handler
2025-04-11T04:23:18.5139220Z     spawn(run_dist, 1)
2025-04-11T04:23:18.5139322Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.5139431Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.5139687Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.5139874Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.5140163Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.5140283Z     while not context.join():
2025-04-11T04:23:18.5140397Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5140401Z 
2025-04-11T04:23:18.5140608Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e17b0>
2025-04-11T04:23:18.5140694Z timeout = None
2025-04-11T04:23:18.5140699Z 
2025-04-11T04:23:18.5140787Z     def join(self, timeout=None):
2025-04-11T04:23:18.5140921Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.5140993Z     
2025-04-11T04:23:18.5141148Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.5141291Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.5141455Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.5141547Z         of the first process exiting.
2025-04-11T04:23:18.5141619Z     
2025-04-11T04:23:18.5141768Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.5141906Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.5141980Z     
2025-04-11T04:23:18.5142053Z         Args:
2025-04-11T04:23:18.5142191Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.5142368Z         """
2025-04-11T04:23:18.5142509Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.5142612Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.5142690Z             return True
2025-04-11T04:23:18.5142766Z     
2025-04-11T04:23:18.5142898Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.5143016Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.5143113Z             self.sentinels.keys(),
2025-04-11T04:23:18.5143197Z             timeout=timeout,
2025-04-11T04:23:18.5143274Z         )
2025-04-11T04:23:18.5143345Z     
2025-04-11T04:23:18.5143428Z         error_index = None
2025-04-11T04:23:18.5143518Z         for sentinel in ready:
2025-04-11T04:23:18.5143622Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.5143728Z             process = self.processes[index]
2025-04-11T04:23:18.5143814Z             process.join()
2025-04-11T04:23:18.5143910Z             if process.exitcode != 0:
2025-04-11T04:23:18.5144006Z                 error_index = index
2025-04-11T04:23:18.5144084Z                 break
2025-04-11T04:23:18.5144264Z     
2025-04-11T04:23:18.5144359Z         # Return if there was no error.
2025-04-11T04:23:18.5144447Z         if error_index is None:
2025-04-11T04:23:18.5144585Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.5144683Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.5144760Z     
2025-04-11T04:23:18.5144896Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.5144997Z         for process in self.processes:
2025-04-11T04:23:18.5145088Z             if process.is_alive():
2025-04-11T04:23:18.5145179Z                 process.terminate()
2025-04-11T04:23:18.5145270Z             process.join()
2025-04-11T04:23:18.5145342Z     
2025-04-11T04:23:18.5145489Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.5145610Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.5145718Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.5145849Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.5145933Z             if exitcode < 0:
2025-04-11T04:23:18.5146046Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.5146154Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5146312Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.5146409Z                     error_index=error_index,
2025-04-11T04:23:18.5146509Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5146604Z                     exit_code=exitcode,
2025-04-11T04:23:18.5146691Z                     signal_name=name,
2025-04-11T04:23:18.5146771Z                 )
2025-04-11T04:23:18.5146846Z             else:
2025-04-11T04:23:18.5146950Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5147122Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.5147216Z                     error_index=error_index,
2025-04-11T04:23:18.5147322Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5147411Z                     exit_code=exitcode,
2025-04-11T04:23:18.5147488Z                 )
2025-04-11T04:23:18.5147558Z     
2025-04-11T04:23:18.5147689Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.5147868Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.5147956Z         msg += original_trace
2025-04-11T04:23:18.5148135Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.5148300Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.5148378Z E       
2025-04-11T04:23:18.5148547Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.5148743Z E       Traceback (most recent call last):
2025-04-11T04:23:18.5149053Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.5149141Z E           fn(i, *args)
2025-04-11T04:23:18.5149385Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
2025-04-11T04:23:18.5149477Z E           check_request_handler()
2025-04-11T04:23:18.5149752Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
2025-04-11T04:23:18.5149915Z E           request_handler = RequestHandler(inference_config, model_config)
2025-04-11T04:23:18.5150180Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
2025-04-11T04:23:18.5150284Z E           self._init_cache(model_config)
2025-04-11T04:23:18.5150550Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
2025-04-11T04:23:18.5150736Z E           self.cache_manager = KVCacheManager(self.inference_config, model_config)
2025-04-11T04:23:18.5151097Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T04:23:18.5151257Z E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:18.5151552Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T04:23:18.5151774Z E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:18.5151884Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5152168Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5152309Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5152473Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5152478Z 
2025-04-11T04:23:18.5152794Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.5152946Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.5153108Z [04/11/25 04:15:32] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.5153234Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.5153338Z                              :75 launch                                         
2025-04-11T04:23:18.5153476Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.5153596Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.5153738Z [04/11/25 04:15:32] INFO     colossalai -                                       
2025-04-11T04:23:18.5153872Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.5153978Z                              INFO:                                              
2025-04-11T04:23:18.5154100Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T04:23:18.5154224Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T04:23:18.5154342Z                     INFO     colossalai -                                       
2025-04-11T04:23:18.5154468Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.5154594Z                              INFO: Allocating KV cache with shape: (24, 4, 8, 8)
2025-04-11T04:23:18.5154707Z                              consisting of 24 blocks.                           
2025-04-11T04:23:18.5154906Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.5155134Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.5156276Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.5156450Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.5156585Z _________________________________ test_engine __________________________________
2025-04-11T04:23:18.5156589Z 
2025-04-11T04:23:18.5156681Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.5157277Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.5157376Z 
2025-04-11T04:23:18.5157480Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.5157563Z         try_count = 0
2025-04-11T04:23:18.5157663Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.5157744Z             max_try, int
2025-04-11T04:23:18.5157890Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.5157964Z     
2025-04-11T04:23:18.5158077Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.5158153Z             try:
2025-04-11T04:23:18.5158237Z                 try_count += 1
2025-04-11T04:23:18.5158333Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.5158411Z                 return ret
2025-04-11T04:23:18.5158509Z             except exception_type as e:
2025-04-11T04:23:18.5158609Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.5158799Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.5158921Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.5159069Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.5159228Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.5159310Z                     continue
2025-04-11T04:23:18.5159392Z                 else:
2025-04-11T04:23:18.5159621Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.5159708Z >                   raise e
2025-04-11T04:23:18.5159717Z 
2025-04-11T04:23:18.5159810Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.5159919Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5160055Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.5160150Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.5160293Z tests/test_infer/test_streamingllm.py:117: in test_engine
2025-04-11T04:23:18.5160454Z     spawn(run_dist, 1, func_to_run=check_streamingllm, ret=result_list)
2025-04-11T04:23:18.5160553Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.5160654Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.5160908Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.5161086Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.5161366Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.5161455Z     while not context.join():
2025-04-11T04:23:18.5161561Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5161565Z 
2025-04-11T04:23:18.5161770Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8477e50>
2025-04-11T04:23:18.5161945Z timeout = None
2025-04-11T04:23:18.5161950Z 
2025-04-11T04:23:18.5162044Z     def join(self, timeout=None):
2025-04-11T04:23:18.5162175Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.5162248Z     
2025-04-11T04:23:18.5162396Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.5162541Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.5162707Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.5162799Z         of the first process exiting.
2025-04-11T04:23:18.5162870Z     
2025-04-11T04:23:18.5163018Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.5163154Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.5163227Z     
2025-04-11T04:23:18.5163301Z         Args:
2025-04-11T04:23:18.5163442Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.5163520Z         """
2025-04-11T04:23:18.5163656Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.5163839Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.5163919Z             return True
2025-04-11T04:23:18.5163997Z     
2025-04-11T04:23:18.5164130Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.5164245Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.5164342Z             self.sentinels.keys(),
2025-04-11T04:23:18.5164429Z             timeout=timeout,
2025-04-11T04:23:18.5164506Z         )
2025-04-11T04:23:18.5164577Z     
2025-04-11T04:23:18.5164659Z         error_index = None
2025-04-11T04:23:18.5164750Z         for sentinel in ready:
2025-04-11T04:23:18.5164855Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.5164958Z             process = self.processes[index]
2025-04-11T04:23:18.5165043Z             process.join()
2025-04-11T04:23:18.5165135Z             if process.exitcode != 0:
2025-04-11T04:23:18.5165224Z                 error_index = index
2025-04-11T04:23:18.5165302Z                 break
2025-04-11T04:23:18.5165376Z     
2025-04-11T04:23:18.5165466Z         # Return if there was no error.
2025-04-11T04:23:18.5165551Z         if error_index is None:
2025-04-11T04:23:18.5165689Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.5165784Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.5165860Z     
2025-04-11T04:23:18.5166000Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.5166100Z         for process in self.processes:
2025-04-11T04:23:18.5166188Z             if process.is_alive():
2025-04-11T04:23:18.5166279Z                 process.terminate()
2025-04-11T04:23:18.5166369Z             process.join()
2025-04-11T04:23:18.5166440Z     
2025-04-11T04:23:18.5166583Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.5166701Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.5166810Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.5166934Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.5167016Z             if exitcode < 0:
2025-04-11T04:23:18.5167125Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.5167228Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5167382Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.5167477Z                     error_index=error_index,
2025-04-11T04:23:18.5167576Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5167666Z                     exit_code=exitcode,
2025-04-11T04:23:18.5167752Z                     signal_name=name,
2025-04-11T04:23:18.5167829Z                 )
2025-04-11T04:23:18.5167903Z             else:
2025-04-11T04:23:18.5168098Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5168260Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.5168357Z                     error_index=error_index,
2025-04-11T04:23:18.5168461Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5168547Z                     exit_code=exitcode,
2025-04-11T04:23:18.5168623Z                 )
2025-04-11T04:23:18.5168691Z     
2025-04-11T04:23:18.5168830Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.5169011Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.5169104Z         msg += original_trace
2025-04-11T04:23:18.5169286Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.5169449Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.5169531Z E       
2025-04-11T04:23:18.5169662Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.5169771Z E       Traceback (most recent call last):
2025-04-11T04:23:18.5170072Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.5170267Z E           fn(i, *args)
2025-04-11T04:23:18.5170506Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
2025-04-11T04:23:18.5170606Z E           ret[rank] = func_to_run(**kwargs)
2025-04-11T04:23:18.5170864Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
2025-04-11T04:23:18.5170942Z E           ).cuda()
2025-04-11T04:23:18.5171229Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.5171331Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.5171594Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.5171724Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.5171997Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.5172097Z E           module._apply(fn)
2025-04-11T04:23:18.5172362Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.5172453Z E           module._apply(fn)
2025-04-11T04:23:18.5172718Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.5172813Z E           param_applied = fn(param)
2025-04-11T04:23:18.5173090Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.5173205Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.5173322Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5173604Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5173748Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5173907Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5173912Z 
2025-04-11T04:23:18.5174215Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.5174365Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.5174517Z [04/11/25 04:15:36] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.5174647Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.5174752Z                              :75 launch                                         
2025-04-11T04:23:18.5174993Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.5175117Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.5175321Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.5175465Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.5176610Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.5176782Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.5177490Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.5177665Z   warnings.warn(
2025-04-11T04:23:18.5178507Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5178597Z   warnings.warn(
2025-04-11T04:23:18.5178743Z ________________________ test_flash_decoding_attention _________________________
2025-04-11T04:23:18.5178754Z 
2025-04-11T04:23:18.5178837Z args = (), kwargs = {}
2025-04-11T04:23:18.5178842Z 
2025-04-11T04:23:18.5178938Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.5179043Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.5179154Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.5179279Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.5179384Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.5179478Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.5179486Z 
2025-04-11T04:23:18.5179581Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.5179693Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5179857Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.5179955Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.5180069Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5180074Z 
2025-04-11T04:23:18.5180152Z device = None
2025-04-11T04:23:18.5180156Z 
2025-04-11T04:23:18.5180287Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5180448Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5180522Z     
2025-04-11T04:23:18.5180606Z         Args:
2025-04-11T04:23:18.5180780Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5180955Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5181062Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5181136Z         """
2025-04-11T04:23:18.5181220Z         _lazy_init()
2025-04-11T04:23:18.5181315Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5181426Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5181531Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5181830Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5182062Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5182224Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5182238Z 
2025-04-11T04:23:18.5182477Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5182627Z ______________________ test_vllm_flash_decoding_attention ______________________
2025-04-11T04:23:18.5182632Z 
2025-04-11T04:23:18.5182719Z args = (), kwargs = {}
2025-04-11T04:23:18.5182723Z 
2025-04-11T04:23:18.5182814Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.5182912Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.5183020Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.5183137Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.5183238Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.5183327Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.5183334Z 
2025-04-11T04:23:18.5183433Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.5183541Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5183791Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.5183886Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.5183996Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5184001Z 
2025-04-11T04:23:18.5184079Z device = None
2025-04-11T04:23:18.5184083Z 
2025-04-11T04:23:18.5184199Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5184356Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5184429Z     
2025-04-11T04:23:18.5184509Z         Args:
2025-04-11T04:23:18.5184676Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5184849Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5184963Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5185036Z         """
2025-04-11T04:23:18.5185123Z         _lazy_init()
2025-04-11T04:23:18.5185215Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5185327Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5185430Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5185714Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5185858Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5186075Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5186080Z 
2025-04-11T04:23:18.5186328Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5186482Z _____________________ test_get_cos_and_sin[dtype0-64-64-4] _____________________
2025-04-11T04:23:18.5186490Z 
2025-04-11T04:23:18.5186649Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float16
2025-04-11T04:23:18.5186657Z 
2025-04-11T04:23:18.5186768Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5186886Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T04:23:18.5186992Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T04:23:18.5187166Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5187314Z     def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T04:23:18.5187414Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T04:23:18.5187608Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T04:23:18.5187710Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5188000Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5188228Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5188392Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5188400Z 
2025-04-11T04:23:18.5188638Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
2025-04-11T04:23:18.5188781Z _____________________ test_get_cos_and_sin[dtype1-64-64-4] _____________________
2025-04-11T04:23:18.5188790Z 
2025-04-11T04:23:18.5188933Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T04:23:18.5188938Z 
2025-04-11T04:23:18.5189044Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5189163Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T04:23:18.5189269Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T04:23:18.5189433Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5189580Z     def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T04:23:18.5189693Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T04:23:18.5189874Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T04:23:18.5190190Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5190481Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5190615Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5190778Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5190783Z 
2025-04-11T04:23:18.5190956Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
2025-04-11T04:23:18.5191108Z ____________________ test_kv_cache_memcopy[True-16-8-16-4] _____________________
2025-04-11T04:23:18.5191112Z 
2025-04-11T04:23:18.5191263Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5191357Z same_context_len = True
2025-04-11T04:23:18.5191361Z 
2025-04-11T04:23:18.5191463Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5191594Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5191741Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5191852Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5191996Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5192091Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5192170Z         bsz: int,
2025-04-11T04:23:18.5192259Z         block_size: int,
2025-04-11T04:23:18.5192353Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5192443Z         num_kv_heads: int,
2025-04-11T04:23:18.5192530Z         same_context_len: bool,
2025-04-11T04:23:18.5192609Z     ):
2025-04-11T04:23:18.5192834Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5192843Z 
2025-04-11T04:23:18.5192997Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5193118Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5193122Z 
2025-04-11T04:23:18.5193268Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5193358Z same_context_len = True
2025-04-11T04:23:18.5193362Z 
2025-04-11T04:23:18.5193455Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5193535Z         bsz: int,
2025-04-11T04:23:18.5193616Z         block_size: int,
2025-04-11T04:23:18.5193706Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5193793Z         num_kv_heads: int,
2025-04-11T04:23:18.5193879Z         same_context_len: bool,
2025-04-11T04:23:18.5193956Z     ):
2025-04-11T04:23:18.5194043Z         torch.manual_seed(123)
2025-04-11T04:23:18.5194114Z     
2025-04-11T04:23:18.5194322Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5194554Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5194654Z         dtype = torch.float16
2025-04-11T04:23:18.5194747Z         device = get_current_device()
2025-04-11T04:23:18.5194823Z     
2025-04-11T04:23:18.5194905Z         if same_context_len:
2025-04-11T04:23:18.5195136Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5195249Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5195534Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5195691Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5195856Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5195860Z 
2025-04-11T04:23:18.5196063Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5196210Z ____________________ test_kv_cache_memcopy[True-16-8-16-7] _____________________
2025-04-11T04:23:18.5196301Z 
2025-04-11T04:23:18.5196454Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5196537Z same_context_len = True
2025-04-11T04:23:18.5196542Z 
2025-04-11T04:23:18.5196644Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5196777Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5196918Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5197036Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5197173Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5197267Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5197342Z         bsz: int,
2025-04-11T04:23:18.5197424Z         block_size: int,
2025-04-11T04:23:18.5197524Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5197605Z         num_kv_heads: int,
2025-04-11T04:23:18.5197695Z         same_context_len: bool,
2025-04-11T04:23:18.5197771Z     ):
2025-04-11T04:23:18.5197993Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5198004Z 
2025-04-11T04:23:18.5198155Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5198264Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5198268Z 
2025-04-11T04:23:18.5198414Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5198499Z same_context_len = True
2025-04-11T04:23:18.5198503Z 
2025-04-11T04:23:18.5198601Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5198676Z         bsz: int,
2025-04-11T04:23:18.5198760Z         block_size: int,
2025-04-11T04:23:18.5198849Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5198935Z         num_kv_heads: int,
2025-04-11T04:23:18.5199023Z         same_context_len: bool,
2025-04-11T04:23:18.5199096Z     ):
2025-04-11T04:23:18.5199183Z         torch.manual_seed(123)
2025-04-11T04:23:18.5199257Z     
2025-04-11T04:23:18.5199454Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5199582Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5199670Z         dtype = torch.float16
2025-04-11T04:23:18.5199767Z         device = get_current_device()
2025-04-11T04:23:18.5199839Z     
2025-04-11T04:23:18.5199922Z         if same_context_len:
2025-04-11T04:23:18.5200155Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5200259Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5200545Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5200774Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5200938Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5200946Z 
2025-04-11T04:23:18.5201124Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5201274Z ____________________ test_kv_cache_memcopy[True-16-8-16-32] ____________________
2025-04-11T04:23:18.5201279Z 
2025-04-11T04:23:18.5201427Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5201510Z same_context_len = True
2025-04-11T04:23:18.5201514Z 
2025-04-11T04:23:18.5201624Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5201746Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5201889Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5202000Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5202145Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5202233Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5202397Z         bsz: int,
2025-04-11T04:23:18.5202484Z         block_size: int,
2025-04-11T04:23:18.5202576Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5202662Z         num_kv_heads: int,
2025-04-11T04:23:18.5202746Z         same_context_len: bool,
2025-04-11T04:23:18.5202819Z     ):
2025-04-11T04:23:18.5203049Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5203054Z 
2025-04-11T04:23:18.5203202Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5203316Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5203320Z 
2025-04-11T04:23:18.5203462Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5203550Z same_context_len = True
2025-04-11T04:23:18.5203558Z 
2025-04-11T04:23:18.5203647Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5203727Z         bsz: int,
2025-04-11T04:23:18.5203812Z         block_size: int,
2025-04-11T04:23:18.5203901Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5203989Z         num_kv_heads: int,
2025-04-11T04:23:18.5204072Z         same_context_len: bool,
2025-04-11T04:23:18.5204150Z     ):
2025-04-11T04:23:18.5204237Z         torch.manual_seed(123)
2025-04-11T04:23:18.5204308Z     
2025-04-11T04:23:18.5204513Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5204630Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5204725Z         dtype = torch.float16
2025-04-11T04:23:18.5204815Z         device = get_current_device()
2025-04-11T04:23:18.5204892Z     
2025-04-11T04:23:18.5204977Z         if same_context_len:
2025-04-11T04:23:18.5205205Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5205317Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5205609Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5205748Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5205904Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5205909Z 
2025-04-11T04:23:18.5206094Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5206237Z ____________________ test_kv_cache_memcopy[True-16-8-32-4] _____________________
2025-04-11T04:23:18.5206242Z 
2025-04-11T04:23:18.5206383Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5206475Z same_context_len = True
2025-04-11T04:23:18.5206480Z 
2025-04-11T04:23:18.5206581Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5206800Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5206938Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5207056Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5207193Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5207281Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5207363Z         bsz: int,
2025-04-11T04:23:18.5207444Z         block_size: int,
2025-04-11T04:23:18.5207540Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5207621Z         num_kv_heads: int,
2025-04-11T04:23:18.5207712Z         same_context_len: bool,
2025-04-11T04:23:18.5207783Z     ):
2025-04-11T04:23:18.5208008Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5208013Z 
2025-04-11T04:23:18.5208169Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5208284Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5208288Z 
2025-04-11T04:23:18.5208433Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5208598Z same_context_len = True
2025-04-11T04:23:18.5208602Z 
2025-04-11T04:23:18.5208698Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5208775Z         bsz: int,
2025-04-11T04:23:18.5208855Z         block_size: int,
2025-04-11T04:23:18.5208948Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5209030Z         num_kv_heads: int,
2025-04-11T04:23:18.5209120Z         same_context_len: bool,
2025-04-11T04:23:18.5209192Z     ):
2025-04-11T04:23:18.5209278Z         torch.manual_seed(123)
2025-04-11T04:23:18.5209355Z     
2025-04-11T04:23:18.5209554Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5209678Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5209769Z         dtype = torch.float16
2025-04-11T04:23:18.5209864Z         device = get_current_device()
2025-04-11T04:23:18.5209936Z     
2025-04-11T04:23:18.5210023Z         if same_context_len:
2025-04-11T04:23:18.5210251Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5210355Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5210645Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5210783Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5210947Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5210952Z 
2025-04-11T04:23:18.5211128Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5211273Z ____________________ test_kv_cache_memcopy[True-16-8-32-7] _____________________
2025-04-11T04:23:18.5211284Z 
2025-04-11T04:23:18.5211424Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5211509Z same_context_len = True
2025-04-11T04:23:18.5211514Z 
2025-04-11T04:23:18.5211625Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5211746Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5211893Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5212005Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5212148Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5212234Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5212309Z         bsz: int,
2025-04-11T04:23:18.5212393Z         block_size: int,
2025-04-11T04:23:18.5212481Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5212568Z         num_kv_heads: int,
2025-04-11T04:23:18.5212652Z         same_context_len: bool,
2025-04-11T04:23:18.5212809Z     ):
2025-04-11T04:23:18.5213042Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5213051Z 
2025-04-11T04:23:18.5213201Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5213316Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5213320Z 
2025-04-11T04:23:18.5213459Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5213546Z same_context_len = True
2025-04-11T04:23:18.5213550Z 
2025-04-11T04:23:18.5213642Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5213723Z         bsz: int,
2025-04-11T04:23:18.5213802Z         block_size: int,
2025-04-11T04:23:18.5213888Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5213973Z         num_kv_heads: int,
2025-04-11T04:23:18.5214055Z         same_context_len: bool,
2025-04-11T04:23:18.5214131Z     ):
2025-04-11T04:23:18.5214220Z         torch.manual_seed(123)
2025-04-11T04:23:18.5214292Z     
2025-04-11T04:23:18.5214493Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5214700Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5214800Z         dtype = torch.float16
2025-04-11T04:23:18.5214891Z         device = get_current_device()
2025-04-11T04:23:18.5214960Z     
2025-04-11T04:23:18.5215049Z         if same_context_len:
2025-04-11T04:23:18.5215284Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5215395Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5215677Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5215816Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5215974Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5215982Z 
2025-04-11T04:23:18.5216162Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5216312Z ____________________ test_kv_cache_memcopy[True-16-8-32-32] ____________________
2025-04-11T04:23:18.5216316Z 
2025-04-11T04:23:18.5216463Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5216554Z same_context_len = True
2025-04-11T04:23:18.5216559Z 
2025-04-11T04:23:18.5216662Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5216802Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5216943Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5217062Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5217199Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5217287Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5217373Z         bsz: int,
2025-04-11T04:23:18.5217457Z         block_size: int,
2025-04-11T04:23:18.5217554Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5217640Z         num_kv_heads: int,
2025-04-11T04:23:18.5217728Z         same_context_len: bool,
2025-04-11T04:23:18.5217809Z     ):
2025-04-11T04:23:18.5218031Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5218035Z 
2025-04-11T04:23:18.5218196Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5218307Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5218311Z 
2025-04-11T04:23:18.5218458Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5218541Z same_context_len = True
2025-04-11T04:23:18.5218545Z 
2025-04-11T04:23:18.5218643Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5218719Z         bsz: int,
2025-04-11T04:23:18.5218910Z         block_size: int,
2025-04-11T04:23:18.5219005Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5219085Z         num_kv_heads: int,
2025-04-11T04:23:18.5219182Z         same_context_len: bool,
2025-04-11T04:23:18.5219257Z     ):
2025-04-11T04:23:18.5219345Z         torch.manual_seed(123)
2025-04-11T04:23:18.5219425Z     
2025-04-11T04:23:18.5219621Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5219745Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5219834Z         dtype = torch.float16
2025-04-11T04:23:18.5219933Z         device = get_current_device()
2025-04-11T04:23:18.5220006Z     
2025-04-11T04:23:18.5220090Z         if same_context_len:
2025-04-11T04:23:18.5220323Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5220431Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5220727Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5220865Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5221118Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5221123Z 
2025-04-11T04:23:18.5221302Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5221447Z ____________________ test_kv_cache_memcopy[True-16-8-64-4] _____________________
2025-04-11T04:23:18.5221457Z 
2025-04-11T04:23:18.5221599Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5221682Z same_context_len = True
2025-04-11T04:23:18.5221687Z 
2025-04-11T04:23:18.5221802Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5221925Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5222071Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5222186Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5222323Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5222417Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5222492Z         bsz: int,
2025-04-11T04:23:18.5222577Z         block_size: int,
2025-04-11T04:23:18.5222664Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5222749Z         num_kv_heads: int,
2025-04-11T04:23:18.5222835Z         same_context_len: bool,
2025-04-11T04:23:18.5222906Z     ):
2025-04-11T04:23:18.5223136Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5223140Z 
2025-04-11T04:23:18.5223291Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5223405Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5223409Z 
2025-04-11T04:23:18.5223556Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5223645Z same_context_len = True
2025-04-11T04:23:18.5223649Z 
2025-04-11T04:23:18.5223741Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5223817Z         bsz: int,
2025-04-11T04:23:18.5223903Z         block_size: int,
2025-04-11T04:23:18.5223991Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5224085Z         num_kv_heads: int,
2025-04-11T04:23:18.5224177Z         same_context_len: bool,
2025-04-11T04:23:18.5224262Z     ):
2025-04-11T04:23:18.5224353Z         torch.manual_seed(123)
2025-04-11T04:23:18.5224425Z     
2025-04-11T04:23:18.5224627Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5224741Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5224830Z         dtype = torch.float16
2025-04-11T04:23:18.5224920Z         device = get_current_device()
2025-04-11T04:23:18.5224991Z     
2025-04-11T04:23:18.5225180Z         if same_context_len:
2025-04-11T04:23:18.5225404Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5225519Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5225804Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5225945Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5226104Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5226109Z 
2025-04-11T04:23:18.5226285Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5226436Z ____________________ test_kv_cache_memcopy[True-16-8-64-7] _____________________
2025-04-11T04:23:18.5226440Z 
2025-04-11T04:23:18.5226581Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5226673Z same_context_len = True
2025-04-11T04:23:18.5226678Z 
2025-04-11T04:23:18.5226785Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5226914Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5227137Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5227250Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5227384Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5227470Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5227554Z         bsz: int,
2025-04-11T04:23:18.5227635Z         block_size: int,
2025-04-11T04:23:18.5227728Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5227808Z         num_kv_heads: int,
2025-04-11T04:23:18.5227891Z         same_context_len: bool,
2025-04-11T04:23:18.5227969Z     ):
2025-04-11T04:23:18.5228186Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5228194Z 
2025-04-11T04:23:18.5228349Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5228503Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5228511Z 
2025-04-11T04:23:18.5228663Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5228745Z same_context_len = True
2025-04-11T04:23:18.5228749Z 
2025-04-11T04:23:18.5228845Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5228922Z         bsz: int,
2025-04-11T04:23:18.5229003Z         block_size: int,
2025-04-11T04:23:18.5229098Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5229181Z         num_kv_heads: int,
2025-04-11T04:23:18.5229271Z         same_context_len: bool,
2025-04-11T04:23:18.5229344Z     ):
2025-04-11T04:23:18.5229431Z         torch.manual_seed(123)
2025-04-11T04:23:18.5229512Z     
2025-04-11T04:23:18.5229711Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5229841Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5229929Z         dtype = torch.float16
2025-04-11T04:23:18.5230026Z         device = get_current_device()
2025-04-11T04:23:18.5230106Z     
2025-04-11T04:23:18.5230195Z         if same_context_len:
2025-04-11T04:23:18.5230425Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5230532Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5230816Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5230950Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5231109Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5231120Z 
2025-04-11T04:23:18.5231295Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5231541Z ____________________ test_kv_cache_memcopy[True-16-8-64-32] ____________________
2025-04-11T04:23:18.5231545Z 
2025-04-11T04:23:18.5231706Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5231790Z same_context_len = True
2025-04-11T04:23:18.5231795Z 
2025-04-11T04:23:18.5231905Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5232028Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5232174Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5232287Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5232426Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5232522Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5232599Z         bsz: int,
2025-04-11T04:23:18.5232689Z         block_size: int,
2025-04-11T04:23:18.5232778Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5232863Z         num_kv_heads: int,
2025-04-11T04:23:18.5232955Z         same_context_len: bool,
2025-04-11T04:23:18.5233029Z     ):
2025-04-11T04:23:18.5233256Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5233353Z 
2025-04-11T04:23:18.5233509Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5233627Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5233631Z 
2025-04-11T04:23:18.5233777Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5233884Z same_context_len = True
2025-04-11T04:23:18.5233894Z 
2025-04-11T04:23:18.5234003Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5234084Z         bsz: int,
2025-04-11T04:23:18.5234185Z         block_size: int,
2025-04-11T04:23:18.5234270Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5234355Z         num_kv_heads: int,
2025-04-11T04:23:18.5234445Z         same_context_len: bool,
2025-04-11T04:23:18.5234517Z     ):
2025-04-11T04:23:18.5234608Z         torch.manual_seed(123)
2025-04-11T04:23:18.5234681Z     
2025-04-11T04:23:18.5234885Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5235004Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5235098Z         dtype = torch.float16
2025-04-11T04:23:18.5235187Z         device = get_current_device()
2025-04-11T04:23:18.5235260Z     
2025-04-11T04:23:18.5235348Z         if same_context_len:
2025-04-11T04:23:18.5235572Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5235683Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5235965Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5236112Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5236270Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5236278Z 
2025-04-11T04:23:18.5236455Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5236606Z ____________________ test_kv_cache_memcopy[True-16-32-16-4] ____________________
2025-04-11T04:23:18.5236610Z 
2025-04-11T04:23:18.5236755Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5236844Z same_context_len = True
2025-04-11T04:23:18.5236849Z 
2025-04-11T04:23:18.5236955Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5237084Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5237224Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5237337Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5237482Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5237655Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5237736Z         bsz: int,
2025-04-11T04:23:18.5237821Z         block_size: int,
2025-04-11T04:23:18.5237912Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5237991Z         num_kv_heads: int,
2025-04-11T04:23:18.5238076Z         same_context_len: bool,
2025-04-11T04:23:18.5238156Z     ):
2025-04-11T04:23:18.5238374Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5238379Z 
2025-04-11T04:23:18.5238531Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5238640Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5238644Z 
2025-04-11T04:23:18.5238794Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5238877Z same_context_len = True
2025-04-11T04:23:18.5238881Z 
2025-04-11T04:23:18.5238974Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5239057Z         bsz: int,
2025-04-11T04:23:18.5239136Z         block_size: int,
2025-04-11T04:23:18.5239349Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5239430Z         num_kv_heads: int,
2025-04-11T04:23:18.5239519Z         same_context_len: bool,
2025-04-11T04:23:18.5239592Z     ):
2025-04-11T04:23:18.5239677Z         torch.manual_seed(123)
2025-04-11T04:23:18.5239755Z     
2025-04-11T04:23:18.5239951Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5240070Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5240156Z         dtype = torch.float16
2025-04-11T04:23:18.5240246Z         device = get_current_device()
2025-04-11T04:23:18.5240326Z     
2025-04-11T04:23:18.5240409Z         if same_context_len:
2025-04-11T04:23:18.5240640Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5240750Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5241032Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5241172Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5241329Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5241334Z 
2025-04-11T04:23:18.5241516Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5241665Z ____________________ test_kv_cache_memcopy[True-16-32-16-7] ____________________
2025-04-11T04:23:18.5241669Z 
2025-04-11T04:23:18.5241824Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5241905Z same_context_len = True
2025-04-11T04:23:18.5241910Z 
2025-04-11T04:23:18.5242021Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5242149Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5242293Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5242411Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5242549Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5242644Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5242725Z         bsz: int,
2025-04-11T04:23:18.5242809Z         block_size: int,
2025-04-11T04:23:18.5242899Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5242981Z         num_kv_heads: int,
2025-04-11T04:23:18.5243073Z         same_context_len: bool,
2025-04-11T04:23:18.5243148Z     ):
2025-04-11T04:23:18.5243377Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5243382Z 
2025-04-11T04:23:18.5243532Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5243649Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5243806Z 
2025-04-11T04:23:18.5243952Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5244035Z same_context_len = True
2025-04-11T04:23:18.5244045Z 
2025-04-11T04:23:18.5244136Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5244213Z         bsz: int,
2025-04-11T04:23:18.5244299Z         block_size: int,
2025-04-11T04:23:18.5244387Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5244474Z         num_kv_heads: int,
2025-04-11T04:23:18.5244556Z         same_context_len: bool,
2025-04-11T04:23:18.5244630Z     ):
2025-04-11T04:23:18.5244723Z         torch.manual_seed(123)
2025-04-11T04:23:18.5244794Z     
2025-04-11T04:23:18.5244996Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5245111Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5245200Z         dtype = torch.float16
2025-04-11T04:23:18.5245300Z         device = get_current_device()
2025-04-11T04:23:18.5245380Z     
2025-04-11T04:23:18.5245480Z         if same_context_len:
2025-04-11T04:23:18.5245800Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5245936Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5246231Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5246369Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5246537Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5246542Z 
2025-04-11T04:23:18.5246723Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5246882Z ___________________ test_kv_cache_memcopy[True-16-32-16-32] ____________________
2025-04-11T04:23:18.5246891Z 
2025-04-11T04:23:18.5247042Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5247135Z same_context_len = True
2025-04-11T04:23:18.5247142Z 
2025-04-11T04:23:18.5247250Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5247382Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5247526Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5247640Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5247788Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5247880Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5247969Z         bsz: int,
2025-04-11T04:23:18.5248054Z         block_size: int,
2025-04-11T04:23:18.5248146Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5248236Z         num_kv_heads: int,
2025-04-11T04:23:18.5248325Z         same_context_len: bool,
2025-04-11T04:23:18.5248406Z     ):
2025-04-11T04:23:18.5248638Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5248642Z 
2025-04-11T04:23:18.5248800Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5248918Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5248922Z 
2025-04-11T04:23:18.5249076Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5249163Z same_context_len = True
2025-04-11T04:23:18.5249167Z 
2025-04-11T04:23:18.5249263Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5249349Z         bsz: int,
2025-04-11T04:23:18.5249432Z         block_size: int,
2025-04-11T04:23:18.5249527Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5249611Z         num_kv_heads: int,
2025-04-11T04:23:18.5249699Z         same_context_len: bool,
2025-04-11T04:23:18.5249781Z     ):
2025-04-11T04:23:18.5249871Z         torch.manual_seed(123)
2025-04-11T04:23:18.5250041Z     
2025-04-11T04:23:18.5250243Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5250365Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5250454Z         dtype = torch.float16
2025-04-11T04:23:18.5250544Z         device = get_current_device()
2025-04-11T04:23:18.5250623Z     
2025-04-11T04:23:18.5250706Z         if same_context_len:
2025-04-11T04:23:18.5250932Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5251039Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5251320Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5251459Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5251615Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5251623Z 
2025-04-11T04:23:18.5251806Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5252036Z ____________________ test_kv_cache_memcopy[True-16-32-32-4] ____________________
2025-04-11T04:23:18.5252041Z 
2025-04-11T04:23:18.5252192Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5252274Z same_context_len = True
2025-04-11T04:23:18.5252279Z 
2025-04-11T04:23:18.5252390Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5252514Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5252654Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5252771Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5252907Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5252998Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5253073Z         bsz: int,
2025-04-11T04:23:18.5253162Z         block_size: int,
2025-04-11T04:23:18.5253249Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5253331Z         num_kv_heads: int,
2025-04-11T04:23:18.5253423Z         same_context_len: bool,
2025-04-11T04:23:18.5253495Z     ):
2025-04-11T04:23:18.5253723Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5253728Z 
2025-04-11T04:23:18.5253879Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5253994Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5253999Z 
2025-04-11T04:23:18.5254139Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5254220Z same_context_len = True
2025-04-11T04:23:18.5254224Z 
2025-04-11T04:23:18.5254320Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5254397Z         bsz: int,
2025-04-11T04:23:18.5254481Z         block_size: int,
2025-04-11T04:23:18.5254571Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5254658Z         num_kv_heads: int,
2025-04-11T04:23:18.5254741Z         same_context_len: bool,
2025-04-11T04:23:18.5254815Z     ):
2025-04-11T04:23:18.5254905Z         torch.manual_seed(123)
2025-04-11T04:23:18.5254977Z     
2025-04-11T04:23:18.5255180Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5255296Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5255382Z         dtype = torch.float16
2025-04-11T04:23:18.5255479Z         device = get_current_device()
2025-04-11T04:23:18.5255551Z     
2025-04-11T04:23:18.5255642Z         if same_context_len:
2025-04-11T04:23:18.5255861Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5255966Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5256253Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5256475Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5256645Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5256649Z 
2025-04-11T04:23:18.5256823Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5256990Z ____________________ test_kv_cache_memcopy[True-16-32-32-7] ____________________
2025-04-11T04:23:18.5256993Z 
2025-04-11T04:23:18.5257144Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5257237Z same_context_len = True
2025-04-11T04:23:18.5257241Z 
2025-04-11T04:23:18.5257346Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5257476Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5257618Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5257732Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5257874Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5258053Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5258139Z         bsz: int,
2025-04-11T04:23:18.5258220Z         block_size: int,
2025-04-11T04:23:18.5258311Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5258397Z         num_kv_heads: int,
2025-04-11T04:23:18.5258483Z         same_context_len: bool,
2025-04-11T04:23:18.5258561Z     ):
2025-04-11T04:23:18.5258783Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5258788Z 
2025-04-11T04:23:18.5258939Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5259050Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5259054Z 
2025-04-11T04:23:18.5259194Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5259286Z same_context_len = True
2025-04-11T04:23:18.5259290Z 
2025-04-11T04:23:18.5259382Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5259472Z         bsz: int,
2025-04-11T04:23:18.5259557Z         block_size: int,
2025-04-11T04:23:18.5259658Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5259740Z         num_kv_heads: int,
2025-04-11T04:23:18.5259823Z         same_context_len: bool,
2025-04-11T04:23:18.5259906Z     ):
2025-04-11T04:23:18.5259993Z         torch.manual_seed(123)
2025-04-11T04:23:18.5260069Z     
2025-04-11T04:23:18.5260266Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5260381Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5260472Z         dtype = torch.float16
2025-04-11T04:23:18.5260562Z         device = get_current_device()
2025-04-11T04:23:18.5260639Z     
2025-04-11T04:23:18.5260722Z         if same_context_len:
2025-04-11T04:23:18.5260954Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5261061Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5261348Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5261487Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5261644Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5261649Z 
2025-04-11T04:23:18.5261832Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5261979Z ___________________ test_kv_cache_memcopy[True-16-32-32-32] ____________________
2025-04-11T04:23:18.5261983Z 
2025-04-11T04:23:18.5262137Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5262219Z same_context_len = True
2025-04-11T04:23:18.5262313Z 
2025-04-11T04:23:18.5262423Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5262547Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5262692Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5262812Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5262950Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5263042Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5263119Z         bsz: int,
2025-04-11T04:23:18.5263203Z         block_size: int,
2025-04-11T04:23:18.5263293Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5263373Z         num_kv_heads: int,
2025-04-11T04:23:18.5263462Z         same_context_len: bool,
2025-04-11T04:23:18.5263534Z     ):
2025-04-11T04:23:18.5263760Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5263765Z 
2025-04-11T04:23:18.5263919Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5264028Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5264140Z 
2025-04-11T04:23:18.5264289Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5264373Z same_context_len = True
2025-04-11T04:23:18.5264378Z 
2025-04-11T04:23:18.5264476Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5264553Z         bsz: int,
2025-04-11T04:23:18.5264639Z         block_size: int,
2025-04-11T04:23:18.5264726Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5264806Z         num_kv_heads: int,
2025-04-11T04:23:18.5264896Z         same_context_len: bool,
2025-04-11T04:23:18.5264968Z     ):
2025-04-11T04:23:18.5265057Z         torch.manual_seed(123)
2025-04-11T04:23:18.5265128Z     
2025-04-11T04:23:18.5265330Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5265451Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5265538Z         dtype = torch.float16
2025-04-11T04:23:18.5265636Z         device = get_current_device()
2025-04-11T04:23:18.5265715Z     
2025-04-11T04:23:18.5265806Z         if same_context_len:
2025-04-11T04:23:18.5266031Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5266138Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5266426Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5266562Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5266727Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5266732Z 
2025-04-11T04:23:18.5266906Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5267061Z ____________________ test_kv_cache_memcopy[True-16-32-64-4] ____________________
2025-04-11T04:23:18.5267065Z 
2025-04-11T04:23:18.5267207Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5267298Z same_context_len = True
2025-04-11T04:23:18.5267302Z 
2025-04-11T04:23:18.5267408Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5267530Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5267677Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5267789Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5267930Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5268019Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5268098Z         bsz: int,
2025-04-11T04:23:18.5268178Z         block_size: int,
2025-04-11T04:23:18.5268269Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5268369Z         num_kv_heads: int,
2025-04-11T04:23:18.5268579Z         same_context_len: bool,
2025-04-11T04:23:18.5268657Z     ):
2025-04-11T04:23:18.5268882Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5268890Z 
2025-04-11T04:23:18.5269048Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5269159Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5269163Z 
2025-04-11T04:23:18.5269307Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5269398Z same_context_len = True
2025-04-11T04:23:18.5269402Z 
2025-04-11T04:23:18.5269492Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5269574Z         bsz: int,
2025-04-11T04:23:18.5269652Z         block_size: int,
2025-04-11T04:23:18.5269742Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5269822Z         num_kv_heads: int,
2025-04-11T04:23:18.5269905Z         same_context_len: bool,
2025-04-11T04:23:18.5269993Z     ):
2025-04-11T04:23:18.5270079Z         torch.manual_seed(123)
2025-04-11T04:23:18.5270156Z     
2025-04-11T04:23:18.5270354Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5270575Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5270669Z         dtype = torch.float16
2025-04-11T04:23:18.5270758Z         device = get_current_device()
2025-04-11T04:23:18.5270834Z     
2025-04-11T04:23:18.5270917Z         if same_context_len:
2025-04-11T04:23:18.5271139Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5271251Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5271532Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5271673Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5271836Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5271841Z 
2025-04-11T04:23:18.5272023Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5272169Z ____________________ test_kv_cache_memcopy[True-16-32-64-7] ____________________
2025-04-11T04:23:18.5272173Z 
2025-04-11T04:23:18.5272319Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5272402Z same_context_len = True
2025-04-11T04:23:18.5272407Z 
2025-04-11T04:23:18.5272516Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5272636Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5272775Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5272894Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5273033Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5273130Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5273208Z         bsz: int,
2025-04-11T04:23:18.5273289Z         block_size: int,
2025-04-11T04:23:18.5273387Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5273468Z         num_kv_heads: int,
2025-04-11T04:23:18.5273558Z         same_context_len: bool,
2025-04-11T04:23:18.5273631Z     ):
2025-04-11T04:23:18.5273851Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5273861Z 
2025-04-11T04:23:18.5274011Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5274123Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5274127Z 
2025-04-11T04:23:18.5274274Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5274354Z same_context_len = True
2025-04-11T04:23:18.5274358Z 
2025-04-11T04:23:18.5274453Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5274631Z         bsz: int,
2025-04-11T04:23:18.5274718Z         block_size: int,
2025-04-11T04:23:18.5274806Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5274890Z         num_kv_heads: int,
2025-04-11T04:23:18.5274981Z         same_context_len: bool,
2025-04-11T04:23:18.5275054Z     ):
2025-04-11T04:23:18.5275145Z         torch.manual_seed(123)
2025-04-11T04:23:18.5275216Z     
2025-04-11T04:23:18.5275413Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5275535Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5275622Z         dtype = torch.float16
2025-04-11T04:23:18.5275717Z         device = get_current_device()
2025-04-11T04:23:18.5275788Z     
2025-04-11T04:23:18.5275878Z         if same_context_len:
2025-04-11T04:23:18.5276099Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5276206Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5276498Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5276723Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5276887Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5276892Z 
2025-04-11T04:23:18.5277065Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5277219Z ___________________ test_kv_cache_memcopy[True-16-32-64-32] ____________________
2025-04-11T04:23:18.5277223Z 
2025-04-11T04:23:18.5277368Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5277455Z same_context_len = True
2025-04-11T04:23:18.5277459Z 
2025-04-11T04:23:18.5277563Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5277686Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5277837Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5277957Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5278111Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5278199Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5278289Z         bsz: int,
2025-04-11T04:23:18.5278391Z         block_size: int,
2025-04-11T04:23:18.5278485Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5278574Z         num_kv_heads: int,
2025-04-11T04:23:18.5278658Z         same_context_len: bool,
2025-04-11T04:23:18.5278737Z     ):
2025-04-11T04:23:18.5278959Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5278964Z 
2025-04-11T04:23:18.5279115Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5279232Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5279240Z 
2025-04-11T04:23:18.5279383Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5279476Z same_context_len = True
2025-04-11T04:23:18.5279480Z 
2025-04-11T04:23:18.5279571Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5279652Z         bsz: int,
2025-04-11T04:23:18.5279732Z         block_size: int,
2025-04-11T04:23:18.5279820Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5279906Z         num_kv_heads: int,
2025-04-11T04:23:18.5279992Z         same_context_len: bool,
2025-04-11T04:23:18.5280069Z     ):
2025-04-11T04:23:18.5280152Z         torch.manual_seed(123)
2025-04-11T04:23:18.5280227Z     
2025-04-11T04:23:18.5280421Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5280535Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5280628Z         dtype = torch.float16
2025-04-11T04:23:18.5280718Z         device = get_current_device()
2025-04-11T04:23:18.5280880Z     
2025-04-11T04:23:18.5280964Z         if same_context_len:
2025-04-11T04:23:18.5281188Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5281302Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5281582Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5281720Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5281879Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5281883Z 
2025-04-11T04:23:18.5282061Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5282210Z ____________________ test_kv_cache_memcopy[False-16-8-16-4] ____________________
2025-04-11T04:23:18.5282214Z 
2025-04-11T04:23:18.5282366Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5282453Z same_context_len = False
2025-04-11T04:23:18.5282458Z 
2025-04-11T04:23:18.5282652Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5282782Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5282923Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5283037Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5283173Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5283263Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5283339Z         bsz: int,
2025-04-11T04:23:18.5283419Z         block_size: int,
2025-04-11T04:23:18.5283513Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5283593Z         num_kv_heads: int,
2025-04-11T04:23:18.5283681Z         same_context_len: bool,
2025-04-11T04:23:18.5283755Z     ):
2025-04-11T04:23:18.5283976Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5283990Z 
2025-04-11T04:23:18.5284140Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5284253Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5284257Z 
2025-04-11T04:23:18.5284407Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5284488Z same_context_len = False
2025-04-11T04:23:18.5284492Z 
2025-04-11T04:23:18.5284591Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5284667Z         bsz: int,
2025-04-11T04:23:18.5284754Z         block_size: int,
2025-04-11T04:23:18.5284843Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5284923Z         num_kv_heads: int,
2025-04-11T04:23:18.5285012Z         same_context_len: bool,
2025-04-11T04:23:18.5285084Z     ):
2025-04-11T04:23:18.5285173Z         torch.manual_seed(123)
2025-04-11T04:23:18.5285244Z     
2025-04-11T04:23:18.5285448Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5285572Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5285660Z         dtype = torch.float16
2025-04-11T04:23:18.5285754Z         device = get_current_device()
2025-04-11T04:23:18.5285827Z     
2025-04-11T04:23:18.5285910Z         if same_context_len:
2025-04-11T04:23:18.5286141Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5286216Z         else:
2025-04-11T04:23:18.5286458Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5286567Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5286849Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5286982Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5287249Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5287257Z 
2025-04-11T04:23:18.5287434Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5287580Z ____________________ test_kv_cache_memcopy[False-16-8-16-7] ____________________
2025-04-11T04:23:18.5287590Z 
2025-04-11T04:23:18.5287735Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5287818Z same_context_len = False
2025-04-11T04:23:18.5287822Z 
2025-04-11T04:23:18.5287933Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5288057Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5288199Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5288310Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5288449Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5288544Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5288621Z         bsz: int,
2025-04-11T04:23:18.5288708Z         block_size: int,
2025-04-11T04:23:18.5288984Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5289071Z         num_kv_heads: int,
2025-04-11T04:23:18.5289158Z         same_context_len: bool,
2025-04-11T04:23:18.5289230Z     ):
2025-04-11T04:23:18.5289460Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5289464Z 
2025-04-11T04:23:18.5289623Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5289737Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5289741Z 
2025-04-11T04:23:18.5289891Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5289982Z same_context_len = False
2025-04-11T04:23:18.5289986Z 
2025-04-11T04:23:18.5290090Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5290169Z         bsz: int,
2025-04-11T04:23:18.5290255Z         block_size: int,
2025-04-11T04:23:18.5290344Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5290442Z         num_kv_heads: int,
2025-04-11T04:23:18.5290528Z         same_context_len: bool,
2025-04-11T04:23:18.5290607Z     ):
2025-04-11T04:23:18.5290692Z         torch.manual_seed(123)
2025-04-11T04:23:18.5290765Z     
2025-04-11T04:23:18.5290970Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5291085Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5291176Z         dtype = torch.float16
2025-04-11T04:23:18.5291267Z         device = get_current_device()
2025-04-11T04:23:18.5291338Z     
2025-04-11T04:23:18.5291429Z         if same_context_len:
2025-04-11T04:23:18.5291653Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5291738Z         else:
2025-04-11T04:23:18.5291976Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5292090Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5292369Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5292507Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5292673Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5292677Z 
2025-04-11T04:23:18.5292851Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5293005Z ___________________ test_kv_cache_memcopy[False-16-8-16-32] ____________________
2025-04-11T04:23:18.5293008Z 
2025-04-11T04:23:18.5293151Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5293338Z same_context_len = False
2025-04-11T04:23:18.5293342Z 
2025-04-11T04:23:18.5293447Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5293578Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5293719Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5293832Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5293974Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5294062Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5294144Z         bsz: int,
2025-04-11T04:23:18.5294223Z         block_size: int,
2025-04-11T04:23:18.5294313Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5294401Z         num_kv_heads: int,
2025-04-11T04:23:18.5294485Z         same_context_len: bool,
2025-04-11T04:23:18.5294565Z     ):
2025-04-11T04:23:18.5294785Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5294793Z 
2025-04-11T04:23:18.5294951Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5295060Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5295149Z 
2025-04-11T04:23:18.5295296Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5295386Z same_context_len = False
2025-04-11T04:23:18.5295390Z 
2025-04-11T04:23:18.5295481Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5295563Z         bsz: int,
2025-04-11T04:23:18.5295642Z         block_size: int,
2025-04-11T04:23:18.5295734Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5295814Z         num_kv_heads: int,
2025-04-11T04:23:18.5295897Z         same_context_len: bool,
2025-04-11T04:23:18.5295977Z     ):
2025-04-11T04:23:18.5296062Z         torch.manual_seed(123)
2025-04-11T04:23:18.5296137Z     
2025-04-11T04:23:18.5296332Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5296448Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5296544Z         dtype = torch.float16
2025-04-11T04:23:18.5296638Z         device = get_current_device()
2025-04-11T04:23:18.5296715Z     
2025-04-11T04:23:18.5296797Z         if same_context_len:
2025-04-11T04:23:18.5297021Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5297096Z         else:
2025-04-11T04:23:18.5297330Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5297441Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5297718Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5297858Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5298022Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5298027Z 
2025-04-11T04:23:18.5298209Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5298358Z ____________________ test_kv_cache_memcopy[False-16-8-32-4] ____________________
2025-04-11T04:23:18.5298362Z 
2025-04-11T04:23:18.5298508Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5298591Z same_context_len = False
2025-04-11T04:23:18.5298595Z 
2025-04-11T04:23:18.5298697Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5298824Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5298962Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5299081Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5299219Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5299309Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5299470Z         bsz: int,
2025-04-11T04:23:18.5299551Z         block_size: int,
2025-04-11T04:23:18.5299645Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5299728Z         num_kv_heads: int,
2025-04-11T04:23:18.5299818Z         same_context_len: bool,
2025-04-11T04:23:18.5299891Z     ):
2025-04-11T04:23:18.5300112Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5300123Z 
2025-04-11T04:23:18.5300272Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5300380Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5300384Z 
2025-04-11T04:23:18.5300536Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5300618Z same_context_len = False
2025-04-11T04:23:18.5300622Z 
2025-04-11T04:23:18.5300725Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5300802Z         bsz: int,
2025-04-11T04:23:18.5300893Z         block_size: int,
2025-04-11T04:23:18.5300980Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5301064Z         num_kv_heads: int,
2025-04-11T04:23:18.5301244Z         same_context_len: bool,
2025-04-11T04:23:18.5301329Z     ):
2025-04-11T04:23:18.5301444Z         torch.manual_seed(123)
2025-04-11T04:23:18.5301516Z     
2025-04-11T04:23:18.5301713Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5301836Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5301923Z         dtype = torch.float16
2025-04-11T04:23:18.5302021Z         device = get_current_device()
2025-04-11T04:23:18.5302093Z     
2025-04-11T04:23:18.5302178Z         if same_context_len:
2025-04-11T04:23:18.5302423Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5302501Z         else:
2025-04-11T04:23:18.5302751Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5302865Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5303153Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5303286Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5303451Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5303455Z 
2025-04-11T04:23:18.5303634Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5303781Z ____________________ test_kv_cache_memcopy[False-16-8-32-7] ____________________
2025-04-11T04:23:18.5303785Z 
2025-04-11T04:23:18.5303939Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5304022Z same_context_len = False
2025-04-11T04:23:18.5304026Z 
2025-04-11T04:23:18.5304145Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5304271Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5304421Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5304537Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5304673Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5304767Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5304845Z         bsz: int,
2025-04-11T04:23:18.5304931Z         block_size: int,
2025-04-11T04:23:18.5305021Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5305110Z         num_kv_heads: int,
2025-04-11T04:23:18.5305195Z         same_context_len: bool,
2025-04-11T04:23:18.5305267Z     ):
2025-04-11T04:23:18.5305496Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5305501Z 
2025-04-11T04:23:18.5305653Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5305883Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5305890Z 
2025-04-11T04:23:18.5306037Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5306128Z same_context_len = False
2025-04-11T04:23:18.5306132Z 
2025-04-11T04:23:18.5306224Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5306303Z         bsz: int,
2025-04-11T04:23:18.5306396Z         block_size: int,
2025-04-11T04:23:18.5306485Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5306578Z         num_kv_heads: int,
2025-04-11T04:23:18.5306664Z         same_context_len: bool,
2025-04-11T04:23:18.5306737Z     ):
2025-04-11T04:23:18.5306831Z         torch.manual_seed(123)
2025-04-11T04:23:18.5306904Z     
2025-04-11T04:23:18.5307108Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5307224Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5307324Z         dtype = torch.float16
2025-04-11T04:23:18.5307413Z         device = get_current_device()
2025-04-11T04:23:18.5307578Z     
2025-04-11T04:23:18.5307676Z         if same_context_len:
2025-04-11T04:23:18.5307902Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5307986Z         else:
2025-04-11T04:23:18.5308223Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5308338Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5308679Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5308814Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5308982Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5308990Z 
2025-04-11T04:23:18.5309166Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5309325Z ___________________ test_kv_cache_memcopy[False-16-8-32-32] ____________________
2025-04-11T04:23:18.5309330Z 
2025-04-11T04:23:18.5309474Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5309562Z same_context_len = False
2025-04-11T04:23:18.5309566Z 
2025-04-11T04:23:18.5309674Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5309806Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5309948Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5310062Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5310211Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5310298Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5310382Z         bsz: int,
2025-04-11T04:23:18.5310466Z         block_size: int,
2025-04-11T04:23:18.5310555Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5310643Z         num_kv_heads: int,
2025-04-11T04:23:18.5310734Z         same_context_len: bool,
2025-04-11T04:23:18.5310814Z     ):
2025-04-11T04:23:18.5311039Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5311043Z 
2025-04-11T04:23:18.5311201Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5311311Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5311315Z 
2025-04-11T04:23:18.5311456Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5311549Z same_context_len = False
2025-04-11T04:23:18.5311553Z 
2025-04-11T04:23:18.5311643Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5311732Z         bsz: int,
2025-04-11T04:23:18.5311810Z         block_size: int,
2025-04-11T04:23:18.5312018Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5312102Z         num_kv_heads: int,
2025-04-11T04:23:18.5312189Z         same_context_len: bool,
2025-04-11T04:23:18.5312274Z     ):
2025-04-11T04:23:18.5312361Z         torch.manual_seed(123)
2025-04-11T04:23:18.5312442Z     
2025-04-11T04:23:18.5312645Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5312773Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5312872Z         dtype = torch.float16
2025-04-11T04:23:18.5312973Z         device = get_current_device()
2025-04-11T04:23:18.5313062Z     
2025-04-11T04:23:18.5313147Z         if same_context_len:
2025-04-11T04:23:18.5313378Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5313453Z         else:
2025-04-11T04:23:18.5313688Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5313814Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5314101Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5314335Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5314496Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5314501Z 
2025-04-11T04:23:18.5314681Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5314828Z ____________________ test_kv_cache_memcopy[False-16-8-64-4] ____________________
2025-04-11T04:23:18.5314832Z 
2025-04-11T04:23:18.5314983Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5315067Z same_context_len = False
2025-04-11T04:23:18.5315071Z 
2025-04-11T04:23:18.5315179Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5315317Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5315457Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5315582Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5315725Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5315818Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5315895Z         bsz: int,
2025-04-11T04:23:18.5315975Z         block_size: int,
2025-04-11T04:23:18.5316070Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5316152Z         num_kv_heads: int,
2025-04-11T04:23:18.5316242Z         same_context_len: bool,
2025-04-11T04:23:18.5316315Z     ):
2025-04-11T04:23:18.5316535Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5316547Z 
2025-04-11T04:23:18.5316699Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5316813Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5316817Z 
2025-04-11T04:23:18.5316969Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5317060Z same_context_len = False
2025-04-11T04:23:18.5317064Z 
2025-04-11T04:23:18.5317162Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5317239Z         bsz: int,
2025-04-11T04:23:18.5317326Z         block_size: int,
2025-04-11T04:23:18.5317416Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5317499Z         num_kv_heads: int,
2025-04-11T04:23:18.5317591Z         same_context_len: bool,
2025-04-11T04:23:18.5317665Z     ):
2025-04-11T04:23:18.5317756Z         torch.manual_seed(123)
2025-04-11T04:23:18.5317829Z     
2025-04-11T04:23:18.5318027Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5318154Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5318240Z         dtype = torch.float16
2025-04-11T04:23:18.5318424Z         device = get_current_device()
2025-04-11T04:23:18.5318497Z     
2025-04-11T04:23:18.5318582Z         if same_context_len:
2025-04-11T04:23:18.5318816Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5318891Z         else:
2025-04-11T04:23:18.5319129Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5319236Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5319522Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5319658Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5319819Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5319829Z 
2025-04-11T04:23:18.5320012Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5320163Z ____________________ test_kv_cache_memcopy[False-16-8-64-7] ____________________
2025-04-11T04:23:18.5320257Z 
2025-04-11T04:23:18.5320409Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5320493Z same_context_len = False
2025-04-11T04:23:18.5320497Z 
2025-04-11T04:23:18.5320612Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5320737Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5320884Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5320999Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5321138Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5321231Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5321308Z         bsz: int,
2025-04-11T04:23:18.5321395Z         block_size: int,
2025-04-11T04:23:18.5321484Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5321575Z         num_kv_heads: int,
2025-04-11T04:23:18.5321658Z         same_context_len: bool,
2025-04-11T04:23:18.5321736Z     ):
2025-04-11T04:23:18.5321969Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5321974Z 
2025-04-11T04:23:18.5322130Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5322246Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5322250Z 
2025-04-11T04:23:18.5322391Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5322481Z same_context_len = False
2025-04-11T04:23:18.5322485Z 
2025-04-11T04:23:18.5322579Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5322656Z         bsz: int,
2025-04-11T04:23:18.5322748Z         block_size: int,
2025-04-11T04:23:18.5322838Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5322930Z         num_kv_heads: int,
2025-04-11T04:23:18.5323017Z         same_context_len: bool,
2025-04-11T04:23:18.5323090Z     ):
2025-04-11T04:23:18.5323184Z         torch.manual_seed(123)
2025-04-11T04:23:18.5323266Z     
2025-04-11T04:23:18.5323477Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5323598Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5323693Z         dtype = torch.float16
2025-04-11T04:23:18.5323786Z         device = get_current_device()
2025-04-11T04:23:18.5323857Z     
2025-04-11T04:23:18.5323949Z         if same_context_len:
2025-04-11T04:23:18.5324173Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5324271Z         else:
2025-04-11T04:23:18.5324524Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5324638Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5325010Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5325149Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5325318Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5325322Z 
2025-04-11T04:23:18.5325499Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5325656Z ___________________ test_kv_cache_memcopy[False-16-8-64-32] ____________________
2025-04-11T04:23:18.5325660Z 
2025-04-11T04:23:18.5325804Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5325893Z same_context_len = False
2025-04-11T04:23:18.5325898Z 
2025-04-11T04:23:18.5326005Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5326141Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5326287Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5326399Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5326626Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5326715Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5326801Z         bsz: int,
2025-04-11T04:23:18.5326883Z         block_size: int,
2025-04-11T04:23:18.5326972Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5327063Z         num_kv_heads: int,
2025-04-11T04:23:18.5327149Z         same_context_len: bool,
2025-04-11T04:23:18.5327231Z     ):
2025-04-11T04:23:18.5327454Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5327458Z 
2025-04-11T04:23:18.5327615Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5327727Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5327734Z 
2025-04-11T04:23:18.5327878Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5327967Z same_context_len = False
2025-04-11T04:23:18.5327974Z 
2025-04-11T04:23:18.5328068Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5328151Z         bsz: int,
2025-04-11T04:23:18.5328232Z         block_size: int,
2025-04-11T04:23:18.5328331Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5328413Z         num_kv_heads: int,
2025-04-11T04:23:18.5328499Z         same_context_len: bool,
2025-04-11T04:23:18.5328581Z     ):
2025-04-11T04:23:18.5328667Z         torch.manual_seed(123)
2025-04-11T04:23:18.5328745Z     
2025-04-11T04:23:18.5328942Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5329056Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5329147Z         dtype = torch.float16
2025-04-11T04:23:18.5329241Z         device = get_current_device()
2025-04-11T04:23:18.5329328Z     
2025-04-11T04:23:18.5329411Z         if same_context_len:
2025-04-11T04:23:18.5329640Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5329719Z         else:
2025-04-11T04:23:18.5329954Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5330067Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5330349Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5330490Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5330650Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5330655Z 
2025-04-11T04:23:18.5330840Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5331070Z ___________________ test_kv_cache_memcopy[False-16-32-16-4] ____________________
2025-04-11T04:23:18.5331075Z 
2025-04-11T04:23:18.5331226Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5331319Z same_context_len = False
2025-04-11T04:23:18.5331323Z 
2025-04-11T04:23:18.5331427Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5331558Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5331700Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5331819Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5331955Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5332048Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5332125Z         bsz: int,
2025-04-11T04:23:18.5332206Z         block_size: int,
2025-04-11T04:23:18.5332303Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5332386Z         num_kv_heads: int,
2025-04-11T04:23:18.5332483Z         same_context_len: bool,
2025-04-11T04:23:18.5332558Z     ):
2025-04-11T04:23:18.5332778Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5332888Z 
2025-04-11T04:23:18.5333049Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5333162Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5333166Z 
2025-04-11T04:23:18.5333317Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5333402Z same_context_len = False
2025-04-11T04:23:18.5333406Z 
2025-04-11T04:23:18.5333509Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5333588Z         bsz: int,
2025-04-11T04:23:18.5333666Z         block_size: int,
2025-04-11T04:23:18.5333762Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5333844Z         num_kv_heads: int,
2025-04-11T04:23:18.5333937Z         same_context_len: bool,
2025-04-11T04:23:18.5334014Z     ):
2025-04-11T04:23:18.5334110Z         torch.manual_seed(123)
2025-04-11T04:23:18.5334183Z     
2025-04-11T04:23:18.5334383Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5334507Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5334595Z         dtype = torch.float16
2025-04-11T04:23:18.5334697Z         device = get_current_device()
2025-04-11T04:23:18.5334769Z     
2025-04-11T04:23:18.5334857Z         if same_context_len:
2025-04-11T04:23:18.5335089Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5335166Z         else:
2025-04-11T04:23:18.5335414Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5335529Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5335825Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5335959Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5336122Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5336131Z 
2025-04-11T04:23:18.5336309Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5336458Z ___________________ test_kv_cache_memcopy[False-16-32-16-7] ____________________
2025-04-11T04:23:18.5336462Z 
2025-04-11T04:23:18.5336612Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5336700Z same_context_len = False
2025-04-11T04:23:18.5336705Z 
2025-04-11T04:23:18.5336816Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5336940Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5337087Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5337289Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5337426Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5337523Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5337602Z         bsz: int,
2025-04-11T04:23:18.5337687Z         block_size: int,
2025-04-11T04:23:18.5337776Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5337860Z         num_kv_heads: int,
2025-04-11T04:23:18.5337946Z         same_context_len: bool,
2025-04-11T04:23:18.5338018Z     ):
2025-04-11T04:23:18.5338245Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5338250Z 
2025-04-11T04:23:18.5338397Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5338513Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5338517Z 
2025-04-11T04:23:18.5338658Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5338749Z same_context_len = False
2025-04-11T04:23:18.5338753Z 
2025-04-11T04:23:18.5338929Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5339007Z         bsz: int,
2025-04-11T04:23:18.5339094Z         block_size: int,
2025-04-11T04:23:18.5339181Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5339268Z         num_kv_heads: int,
2025-04-11T04:23:18.5339351Z         same_context_len: bool,
2025-04-11T04:23:18.5339425Z     ):
2025-04-11T04:23:18.5339514Z         torch.manual_seed(123)
2025-04-11T04:23:18.5339587Z     
2025-04-11T04:23:18.5339786Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5339901Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5339991Z         dtype = torch.float16
2025-04-11T04:23:18.5340079Z         device = get_current_device()
2025-04-11T04:23:18.5340150Z     
2025-04-11T04:23:18.5340242Z         if same_context_len:
2025-04-11T04:23:18.5340464Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5340548Z         else:
2025-04-11T04:23:18.5340778Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5340883Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5341166Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5341302Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5341466Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5341471Z 
2025-04-11T04:23:18.5341647Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5341800Z ___________________ test_kv_cache_memcopy[False-16-32-16-32] ___________________
2025-04-11T04:23:18.5341807Z 
2025-04-11T04:23:18.5341950Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5342041Z same_context_len = False
2025-04-11T04:23:18.5342046Z 
2025-04-11T04:23:18.5342148Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5342271Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5342415Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5342526Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5342669Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5342756Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5342836Z         bsz: int,
2025-04-11T04:23:18.5342914Z         block_size: int,
2025-04-11T04:23:18.5343001Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5343088Z         num_kv_heads: int,
2025-04-11T04:23:18.5343172Z         same_context_len: bool,
2025-04-11T04:23:18.5343339Z     ):
2025-04-11T04:23:18.5343564Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5343572Z 
2025-04-11T04:23:18.5343729Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5343842Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5343846Z 
2025-04-11T04:23:18.5343991Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5344080Z same_context_len = False
2025-04-11T04:23:18.5344084Z 
2025-04-11T04:23:18.5344176Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5344258Z         bsz: int,
2025-04-11T04:23:18.5344335Z         block_size: int,
2025-04-11T04:23:18.5344430Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5344511Z         num_kv_heads: int,
2025-04-11T04:23:18.5344594Z         same_context_len: bool,
2025-04-11T04:23:18.5344673Z     ):
2025-04-11T04:23:18.5344763Z         torch.manual_seed(123)
2025-04-11T04:23:18.5344840Z     
2025-04-11T04:23:18.5345035Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5345233Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5345325Z         dtype = torch.float16
2025-04-11T04:23:18.5345422Z         device = get_current_device()
2025-04-11T04:23:18.5345504Z     
2025-04-11T04:23:18.5345586Z         if same_context_len:
2025-04-11T04:23:18.5345832Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5345906Z         else:
2025-04-11T04:23:18.5346143Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5346255Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5346535Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5346683Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5346845Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5346850Z 
2025-04-11T04:23:18.5347035Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5347182Z ___________________ test_kv_cache_memcopy[False-16-32-32-4] ____________________
2025-04-11T04:23:18.5347186Z 
2025-04-11T04:23:18.5347332Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5347415Z same_context_len = False
2025-04-11T04:23:18.5347419Z 
2025-04-11T04:23:18.5347522Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5347652Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5347790Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5347907Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5348050Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5348141Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5348236Z         bsz: int,
2025-04-11T04:23:18.5348316Z         block_size: int,
2025-04-11T04:23:18.5348439Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5348523Z         num_kv_heads: int,
2025-04-11T04:23:18.5348610Z         same_context_len: bool,
2025-04-11T04:23:18.5348683Z     ):
2025-04-11T04:23:18.5348902Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5348906Z 
2025-04-11T04:23:18.5349067Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5349177Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5349180Z 
2025-04-11T04:23:18.5349326Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5349510Z same_context_len = False
2025-04-11T04:23:18.5349514Z 
2025-04-11T04:23:18.5349608Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5349688Z         bsz: int,
2025-04-11T04:23:18.5349767Z         block_size: int,
2025-04-11T04:23:18.5349859Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5349938Z         num_kv_heads: int,
2025-04-11T04:23:18.5350027Z         same_context_len: bool,
2025-04-11T04:23:18.5350098Z     ):
2025-04-11T04:23:18.5350186Z         torch.manual_seed(123)
2025-04-11T04:23:18.5350258Z     
2025-04-11T04:23:18.5350451Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5350572Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5350659Z         dtype = torch.float16
2025-04-11T04:23:18.5350754Z         device = get_current_device()
2025-04-11T04:23:18.5350827Z     
2025-04-11T04:23:18.5350911Z         if same_context_len:
2025-04-11T04:23:18.5351139Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5351217Z         else:
2025-04-11T04:23:18.5351550Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5351657Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5351943Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5352077Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5352236Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5352245Z 
2025-04-11T04:23:18.5352418Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5352565Z ___________________ test_kv_cache_memcopy[False-16-32-32-7] ____________________
2025-04-11T04:23:18.5352568Z 
2025-04-11T04:23:18.5352719Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5352801Z same_context_len = False
2025-04-11T04:23:18.5352808Z 
2025-04-11T04:23:18.5352916Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5353041Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5353185Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5353296Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5353433Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5353525Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5353599Z         bsz: int,
2025-04-11T04:23:18.5353682Z         block_size: int,
2025-04-11T04:23:18.5353771Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5353851Z         num_kv_heads: int,
2025-04-11T04:23:18.5353941Z         same_context_len: bool,
2025-04-11T04:23:18.5354014Z     ):
2025-04-11T04:23:18.5354240Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5354249Z 
2025-04-11T04:23:18.5354399Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5354517Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5354522Z 
2025-04-11T04:23:18.5354663Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5354751Z same_context_len = False
2025-04-11T04:23:18.5354756Z 
2025-04-11T04:23:18.5354847Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5354922Z         bsz: int,
2025-04-11T04:23:18.5355007Z         block_size: int,
2025-04-11T04:23:18.5355094Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5355179Z         num_kv_heads: int,
2025-04-11T04:23:18.5355263Z         same_context_len: bool,
2025-04-11T04:23:18.5355336Z     ):
2025-04-11T04:23:18.5355423Z         torch.manual_seed(123)
2025-04-11T04:23:18.5355578Z     
2025-04-11T04:23:18.5355780Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5355895Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5355991Z         dtype = torch.float16
2025-04-11T04:23:18.5356082Z         device = get_current_device()
2025-04-11T04:23:18.5356154Z     
2025-04-11T04:23:18.5356241Z         if same_context_len:
2025-04-11T04:23:18.5356462Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5356541Z         else:
2025-04-11T04:23:18.5356772Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5356878Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5357166Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5357307Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5357473Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5357575Z 
2025-04-11T04:23:18.5357762Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5357922Z ___________________ test_kv_cache_memcopy[False-16-32-32-32] ___________________
2025-04-11T04:23:18.5357926Z 
2025-04-11T04:23:18.5358072Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5358162Z same_context_len = False
2025-04-11T04:23:18.5358166Z 
2025-04-11T04:23:18.5358269Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5358393Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5358536Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5358648Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5358792Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5358880Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5358963Z         bsz: int,
2025-04-11T04:23:18.5359047Z         block_size: int,
2025-04-11T04:23:18.5359135Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5359221Z         num_kv_heads: int,
2025-04-11T04:23:18.5359305Z         same_context_len: bool,
2025-04-11T04:23:18.5359384Z     ):
2025-04-11T04:23:18.5359604Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5359608Z 
2025-04-11T04:23:18.5359761Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5359872Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5359876Z 
2025-04-11T04:23:18.5360018Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5360106Z same_context_len = False
2025-04-11T04:23:18.5360113Z 
2025-04-11T04:23:18.5360204Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5360285Z         bsz: int,
2025-04-11T04:23:18.5360363Z         block_size: int,
2025-04-11T04:23:18.5360457Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5360538Z         num_kv_heads: int,
2025-04-11T04:23:18.5360620Z         same_context_len: bool,
2025-04-11T04:23:18.5360698Z     ):
2025-04-11T04:23:18.5360783Z         torch.manual_seed(123)
2025-04-11T04:23:18.5360858Z     
2025-04-11T04:23:18.5361052Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5361166Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5361257Z         dtype = torch.float16
2025-04-11T04:23:18.5361346Z         device = get_current_device()
2025-04-11T04:23:18.5361422Z     
2025-04-11T04:23:18.5361504Z         if same_context_len:
2025-04-11T04:23:18.5361722Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5361885Z         else:
2025-04-11T04:23:18.5362115Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5362229Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5362506Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5362645Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5362804Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5362808Z 
2025-04-11T04:23:18.5362989Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5363136Z ___________________ test_kv_cache_memcopy[False-16-32-64-4] ____________________
2025-04-11T04:23:18.5363140Z 
2025-04-11T04:23:18.5363284Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5363370Z same_context_len = False
2025-04-11T04:23:18.5363374Z 
2025-04-11T04:23:18.5363477Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5363708Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5363847Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5363963Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5364102Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5364191Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5364268Z         bsz: int,
2025-04-11T04:23:18.5364348Z         block_size: int,
2025-04-11T04:23:18.5364441Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5364520Z         num_kv_heads: int,
2025-04-11T04:23:18.5364608Z         same_context_len: bool,
2025-04-11T04:23:18.5364681Z     ):
2025-04-11T04:23:18.5364902Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5364910Z 
2025-04-11T04:23:18.5365069Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5365184Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5365188Z 
2025-04-11T04:23:18.5365335Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5365418Z same_context_len = False
2025-04-11T04:23:18.5365422Z 
2025-04-11T04:23:18.5365517Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5365593Z         bsz: int,
2025-04-11T04:23:18.5365672Z         block_size: int,
2025-04-11T04:23:18.5365766Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5365846Z         num_kv_heads: int,
2025-04-11T04:23:18.5365935Z         same_context_len: bool,
2025-04-11T04:23:18.5366008Z     ):
2025-04-11T04:23:18.5366098Z         torch.manual_seed(123)
2025-04-11T04:23:18.5366169Z     
2025-04-11T04:23:18.5366365Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5366491Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5366581Z         dtype = torch.float16
2025-04-11T04:23:18.5366677Z         device = get_current_device()
2025-04-11T04:23:18.5366748Z     
2025-04-11T04:23:18.5366830Z         if same_context_len:
2025-04-11T04:23:18.5367057Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5367131Z         else:
2025-04-11T04:23:18.5367364Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5367467Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5367751Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5367886Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5368138Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5368150Z 
2025-04-11T04:23:18.5368328Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5368474Z ___________________ test_kv_cache_memcopy[False-16-32-64-7] ____________________
2025-04-11T04:23:18.5368479Z 
2025-04-11T04:23:18.5368633Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5368730Z same_context_len = False
2025-04-11T04:23:18.5368734Z 
2025-04-11T04:23:18.5368851Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5368975Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5369128Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5369241Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5369378Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5369477Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5369552Z         bsz: int,
2025-04-11T04:23:18.5369638Z         block_size: int,
2025-04-11T04:23:18.5369809Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5369892Z         num_kv_heads: int,
2025-04-11T04:23:18.5369985Z         same_context_len: bool,
2025-04-11T04:23:18.5370057Z     ):
2025-04-11T04:23:18.5370284Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5370288Z 
2025-04-11T04:23:18.5370437Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5370554Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5370558Z 
2025-04-11T04:23:18.5370698Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5370785Z same_context_len = False
2025-04-11T04:23:18.5370789Z 
2025-04-11T04:23:18.5370881Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5370960Z         bsz: int,
2025-04-11T04:23:18.5371048Z         block_size: int,
2025-04-11T04:23:18.5371135Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5371223Z         num_kv_heads: int,
2025-04-11T04:23:18.5371308Z         same_context_len: bool,
2025-04-11T04:23:18.5371379Z     ):
2025-04-11T04:23:18.5371471Z         torch.manual_seed(123)
2025-04-11T04:23:18.5371542Z     
2025-04-11T04:23:18.5371743Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5371858Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5371950Z         dtype = torch.float16
2025-04-11T04:23:18.5372038Z         device = get_current_device()
2025-04-11T04:23:18.5372110Z     
2025-04-11T04:23:18.5372195Z         if same_context_len:
2025-04-11T04:23:18.5372416Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5372493Z         else:
2025-04-11T04:23:18.5372729Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5372839Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5373124Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5373258Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5373420Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5373425Z 
2025-04-11T04:23:18.5373600Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5373752Z ___________________ test_kv_cache_memcopy[False-16-32-64-32] ___________________
2025-04-11T04:23:18.5373756Z 
2025-04-11T04:23:18.5373907Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5373995Z same_context_len = False
2025-04-11T04:23:18.5374084Z 
2025-04-11T04:23:18.5374191Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5374316Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5374463Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5374573Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5374714Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5374802Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5374884Z         bsz: int,
2025-04-11T04:23:18.5374964Z         block_size: int,
2025-04-11T04:23:18.5375052Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5375139Z         num_kv_heads: int,
2025-04-11T04:23:18.5375222Z         same_context_len: bool,
2025-04-11T04:23:18.5375297Z     ):
2025-04-11T04:23:18.5375514Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5375519Z 
2025-04-11T04:23:18.5375675Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5375787Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5375876Z 
2025-04-11T04:23:18.5376023Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5376110Z same_context_len = False
2025-04-11T04:23:18.5376114Z 
2025-04-11T04:23:18.5376204Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5376285Z         bsz: int,
2025-04-11T04:23:18.5376365Z         block_size: int,
2025-04-11T04:23:18.5376457Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5376536Z         num_kv_heads: int,
2025-04-11T04:23:18.5376619Z         same_context_len: bool,
2025-04-11T04:23:18.5376696Z     ):
2025-04-11T04:23:18.5376782Z         torch.manual_seed(123)
2025-04-11T04:23:18.5376859Z     
2025-04-11T04:23:18.5377052Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5377171Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5377262Z         dtype = torch.float16
2025-04-11T04:23:18.5377352Z         device = get_current_device()
2025-04-11T04:23:18.5377431Z     
2025-04-11T04:23:18.5377514Z         if same_context_len:
2025-04-11T04:23:18.5377734Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5377813Z         else:
2025-04-11T04:23:18.5378045Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5378158Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5378437Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5378577Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5378735Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5378742Z 
2025-04-11T04:23:18.5378924Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5379064Z ___________________________ test_rms_layernorm[64-2] ___________________________
2025-04-11T04:23:18.5379068Z 
2025-04-11T04:23:18.5379143Z M = 2, N = 64
2025-04-11T04:23:18.5379154Z 
2025-04-11T04:23:18.5379263Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5379383Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5379491Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5379578Z         torch.manual_seed(123)
2025-04-11T04:23:18.5379675Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5379778Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5379783Z 
2025-04-11T04:23:18.5379932Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5380046Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5380412Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T04:23:18.5380524Z     with torch.cuda.device(device):
2025-04-11T04:23:18.5380634Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5380638Z 
2025-04-11T04:23:18.5380757Z self = <torch.cuda.device object at 0x7fb5b8706890>
2025-04-11T04:23:18.5380762Z 
2025-04-11T04:23:18.5380843Z     def __enter__(self):
2025-04-11T04:23:18.5380987Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T04:23:18.5381093Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5381386Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5381530Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5381689Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5381698Z 
2025-04-11T04:23:18.5381950Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T04:23:18.5382174Z ___________________________ test_rms_layernorm[64-4] ___________________________
2025-04-11T04:23:18.5382178Z 
2025-04-11T04:23:18.5382260Z M = 4, N = 64
2025-04-11T04:23:18.5382265Z 
2025-04-11T04:23:18.5382372Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5382496Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5382596Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5382682Z         torch.manual_seed(123)
2025-04-11T04:23:18.5382779Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5382872Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5382876Z 
2025-04-11T04:23:18.5383028Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5383135Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5383142Z 
2025-04-11T04:23:18.5383225Z device = None
2025-04-11T04:23:18.5383230Z 
2025-04-11T04:23:18.5383352Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5383508Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5383584Z     
2025-04-11T04:23:18.5383659Z         Args:
2025-04-11T04:23:18.5383833Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5384004Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5384120Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5384194Z         """
2025-04-11T04:23:18.5384273Z         _lazy_init()
2025-04-11T04:23:18.5384376Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5384478Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5384587Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5384869Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5385012Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5385170Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5385174Z 
2025-04-11T04:23:18.5385414Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5385555Z ___________________________ test_rms_layernorm[64-8] ___________________________
2025-04-11T04:23:18.5385559Z 
2025-04-11T04:23:18.5385633Z M = 8, N = 64
2025-04-11T04:23:18.5385637Z 
2025-04-11T04:23:18.5385750Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5385866Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5385967Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5386055Z         torch.manual_seed(123)
2025-04-11T04:23:18.5386319Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5386417Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5386422Z 
2025-04-11T04:23:18.5386576Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5386694Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5386699Z 
2025-04-11T04:23:18.5386777Z device = None
2025-04-11T04:23:18.5386782Z 
2025-04-11T04:23:18.5386908Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5387059Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5387137Z     
2025-04-11T04:23:18.5387211Z         Args:
2025-04-11T04:23:18.5387381Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5387562Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5387670Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5387773Z         """
2025-04-11T04:23:18.5387855Z         _lazy_init()
2025-04-11T04:23:18.5387966Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5388167Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5388273Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5388597Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5388734Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5388898Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5388902Z 
2025-04-11T04:23:18.5389140Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5389286Z __________________________ test_rms_layernorm[64-16] ___________________________
2025-04-11T04:23:18.5389290Z 
2025-04-11T04:23:18.5389369Z M = 16, N = 64
2025-04-11T04:23:18.5389374Z 
2025-04-11T04:23:18.5389479Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5389602Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5389702Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5389793Z         torch.manual_seed(123)
2025-04-11T04:23:18.5389882Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5389991Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5389996Z 
2025-04-11T04:23:18.5390149Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5390261Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5390265Z 
2025-04-11T04:23:18.5390353Z device = None
2025-04-11T04:23:18.5390358Z 
2025-04-11T04:23:18.5390478Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5390635Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5390709Z     
2025-04-11T04:23:18.5390786Z         Args:
2025-04-11T04:23:18.5390957Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5391128Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5391241Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5391315Z         """
2025-04-11T04:23:18.5391404Z         _lazy_init()
2025-04-11T04:23:18.5391498Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5391604Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5391708Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5391989Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5392129Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5392286Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5392383Z 
2025-04-11T04:23:18.5392628Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5392771Z __________________________ test_rms_layernorm[128-2] ___________________________
2025-04-11T04:23:18.5392776Z 
2025-04-11T04:23:18.5392854Z M = 2, N = 128
2025-04-11T04:23:18.5392859Z 
2025-04-11T04:23:18.5392967Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5393100Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5393242Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5393334Z         torch.manual_seed(123)
2025-04-11T04:23:18.5393436Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5393526Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5393530Z 
2025-04-11T04:23:18.5393683Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5393802Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5393809Z 
2025-04-11T04:23:18.5393892Z device = None
2025-04-11T04:23:18.5393896Z 
2025-04-11T04:23:18.5394014Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5394259Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5394338Z     
2025-04-11T04:23:18.5394411Z         Args:
2025-04-11T04:23:18.5394582Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5394749Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5394860Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5394933Z         """
2025-04-11T04:23:18.5395011Z         _lazy_init()
2025-04-11T04:23:18.5395113Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5395214Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5395323Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5395604Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5395741Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5395903Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5395907Z 
2025-04-11T04:23:18.5396144Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5396287Z __________________________ test_rms_layernorm[128-4] ___________________________
2025-04-11T04:23:18.5396292Z 
2025-04-11T04:23:18.5396366Z M = 4, N = 128
2025-04-11T04:23:18.5396371Z 
2025-04-11T04:23:18.5396479Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5396596Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5396698Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5396784Z         torch.manual_seed(123)
2025-04-11T04:23:18.5396875Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5396974Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5396978Z 
2025-04-11T04:23:18.5397128Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5397243Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5397247Z 
2025-04-11T04:23:18.5397324Z device = None
2025-04-11T04:23:18.5397329Z 
2025-04-11T04:23:18.5397448Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5397594Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5397668Z     
2025-04-11T04:23:18.5397741Z         Args:
2025-04-11T04:23:18.5397906Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5398076Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5398180Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5398342Z         """
2025-04-11T04:23:18.5398423Z         _lazy_init()
2025-04-11T04:23:18.5398521Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5398632Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5398736Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5399021Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5399158Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5399322Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5399327Z 
2025-04-11T04:23:18.5399563Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5399701Z __________________________ test_rms_layernorm[128-8] ___________________________
2025-04-11T04:23:18.5399710Z 
2025-04-11T04:23:18.5399788Z M = 8, N = 128
2025-04-11T04:23:18.5399792Z 
2025-04-11T04:23:18.5399897Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5400022Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5400204Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5400301Z         torch.manual_seed(123)
2025-04-11T04:23:18.5400395Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5400492Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5400503Z 
2025-04-11T04:23:18.5400651Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5400760Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5400765Z 
2025-04-11T04:23:18.5400848Z device = None
2025-04-11T04:23:18.5400852Z 
2025-04-11T04:23:18.5400970Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5401124Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5401198Z     
2025-04-11T04:23:18.5401277Z         Args:
2025-04-11T04:23:18.5401443Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5401614Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5401725Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5401799Z         """
2025-04-11T04:23:18.5401885Z         _lazy_init()
2025-04-11T04:23:18.5401982Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5402081Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5402195Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5402476Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5402616Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5402771Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5402779Z 
2025-04-11T04:23:18.5403023Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5403166Z __________________________ test_rms_layernorm[128-16] __________________________
2025-04-11T04:23:18.5403171Z 
2025-04-11T04:23:18.5403249Z M = 16, N = 128
2025-04-11T04:23:18.5403254Z 
2025-04-11T04:23:18.5403358Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5403473Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5403576Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5403662Z         torch.manual_seed(123)
2025-04-11T04:23:18.5403757Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5403847Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5403851Z 
2025-04-11T04:23:18.5404006Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5404115Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5404222Z 
2025-04-11T04:23:18.5404307Z device = None
2025-04-11T04:23:18.5404311Z 
2025-04-11T04:23:18.5404430Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5404582Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5404661Z     
2025-04-11T04:23:18.5404733Z         Args:
2025-04-11T04:23:18.5404903Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5405069Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5405173Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5405251Z         """
2025-04-11T04:23:18.5405330Z         _lazy_init()
2025-04-11T04:23:18.5405428Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5405525Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5405633Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5405920Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5406135Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5406297Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5406302Z 
2025-04-11T04:23:18.5406537Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5406677Z __________________________ test_rms_layernorm[512-2] ___________________________
2025-04-11T04:23:18.5406682Z 
2025-04-11T04:23:18.5406757Z M = 2, N = 512
2025-04-11T04:23:18.5406762Z 
2025-04-11T04:23:18.5406872Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5406987Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5407090Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5407175Z         torch.manual_seed(123)
2025-04-11T04:23:18.5407264Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5407357Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5407361Z 
2025-04-11T04:23:18.5407512Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5407627Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5407631Z 
2025-04-11T04:23:18.5407709Z device = None
2025-04-11T04:23:18.5407713Z 
2025-04-11T04:23:18.5407835Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5407984Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5408056Z     
2025-04-11T04:23:18.5408135Z         Args:
2025-04-11T04:23:18.5408300Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5408472Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5408577Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5408662Z         """
2025-04-11T04:23:18.5408739Z         _lazy_init()
2025-04-11T04:23:18.5408832Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5408941Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5409046Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5409330Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5409466Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5409629Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5409633Z 
2025-04-11T04:23:18.5409867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5410002Z __________________________ test_rms_layernorm[512-4] ___________________________
2025-04-11T04:23:18.5410011Z 
2025-04-11T04:23:18.5410174Z M = 4, N = 512
2025-04-11T04:23:18.5410179Z 
2025-04-11T04:23:18.5410284Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5410408Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5410510Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5410602Z         torch.manual_seed(123)
2025-04-11T04:23:18.5410688Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5410778Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5410782Z 
2025-04-11T04:23:18.5410933Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5411041Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5411045Z 
2025-04-11T04:23:18.5411125Z device = None
2025-04-11T04:23:18.5411129Z 
2025-04-11T04:23:18.5411245Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5411398Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5411473Z     
2025-04-11T04:23:18.5411546Z         Args:
2025-04-11T04:23:18.5411715Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5411967Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5412084Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5412160Z         """
2025-04-11T04:23:18.5412244Z         _lazy_init()
2025-04-11T04:23:18.5412338Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5412438Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5412552Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5412839Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5412976Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5413141Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5413148Z 
2025-04-11T04:23:18.5413388Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5413526Z __________________________ test_rms_layernorm[512-8] ___________________________
2025-04-11T04:23:18.5413530Z 
2025-04-11T04:23:18.5413609Z M = 8, N = 512
2025-04-11T04:23:18.5413613Z 
2025-04-11T04:23:18.5413718Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5413833Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5413939Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5414025Z         torch.manual_seed(123)
2025-04-11T04:23:18.5414121Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5414212Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5414216Z 
2025-04-11T04:23:18.5414367Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5414476Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5414483Z 
2025-04-11T04:23:18.5414561Z device = None
2025-04-11T04:23:18.5414565Z 
2025-04-11T04:23:18.5414686Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5414838Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5414916Z     
2025-04-11T04:23:18.5414989Z         Args:
2025-04-11T04:23:18.5415160Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5415325Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5415430Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5415509Z         """
2025-04-11T04:23:18.5415588Z         _lazy_init()
2025-04-11T04:23:18.5415685Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5415784Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5415886Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5416260Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5416398Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5416564Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5416568Z 
2025-04-11T04:23:18.5416802Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5416943Z __________________________ test_rms_layernorm[512-16] __________________________
2025-04-11T04:23:18.5416946Z 
2025-04-11T04:23:18.5417021Z M = 16, N = 512
2025-04-11T04:23:18.5417026Z 
2025-04-11T04:23:18.5417134Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5417248Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5417344Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5417435Z         torch.manual_seed(123)
2025-04-11T04:23:18.5417526Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5417620Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5417704Z 
2025-04-11T04:23:18.5417856Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5417969Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5417973Z 
2025-04-11T04:23:18.5418051Z device = None
2025-04-11T04:23:18.5418055Z 
2025-04-11T04:23:18.5418174Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5418320Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5418392Z     
2025-04-11T04:23:18.5418469Z         Args:
2025-04-11T04:23:18.5418634Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5418800Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5418902Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5418978Z         """
2025-04-11T04:23:18.5419062Z         _lazy_init()
2025-04-11T04:23:18.5419154Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5419261Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5419363Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5419651Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5419784Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5419941Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5419952Z 
2025-04-11T04:23:18.5420185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5420323Z __________________________ test_rms_layernorm[5120-2] __________________________
2025-04-11T04:23:18.5420327Z 
2025-04-11T04:23:18.5420413Z M = 2, N = 5120
2025-04-11T04:23:18.5420417Z 
2025-04-11T04:23:18.5420520Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5420645Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5420742Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5420833Z         torch.manual_seed(123)
2025-04-11T04:23:18.5420920Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5421010Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5421014Z 
2025-04-11T04:23:18.5421170Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5421279Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5421283Z 
2025-04-11T04:23:18.5421363Z device = None
2025-04-11T04:23:18.5421367Z 
2025-04-11T04:23:18.5421482Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5421633Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5421794Z     
2025-04-11T04:23:18.5421868Z         Args:
2025-04-11T04:23:18.5422040Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5422206Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5422372Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5422459Z         """
2025-04-11T04:23:18.5422557Z         _lazy_init()
2025-04-11T04:23:18.5422651Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5422749Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5422860Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5423141Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5423283Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5423443Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5423450Z 
2025-04-11T04:23:18.5423691Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5423952Z __________________________ test_rms_layernorm[5120-4] __________________________
2025-04-11T04:23:18.5423956Z 
2025-04-11T04:23:18.5424038Z M = 4, N = 5120
2025-04-11T04:23:18.5424043Z 
2025-04-11T04:23:18.5424146Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5424263Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5424368Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5424455Z         torch.manual_seed(123)
2025-04-11T04:23:18.5424551Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5424643Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5424647Z 
2025-04-11T04:23:18.5424794Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5424908Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5424916Z 
2025-04-11T04:23:18.5424994Z device = None
2025-04-11T04:23:18.5424998Z 
2025-04-11T04:23:18.5425122Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5425274Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5425355Z     
2025-04-11T04:23:18.5425428Z         Args:
2025-04-11T04:23:18.5425600Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5425765Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5425869Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5425951Z         """
2025-04-11T04:23:18.5426030Z         _lazy_init()
2025-04-11T04:23:18.5426127Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5426226Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5426330Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5426619Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5426757Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5426915Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5426920Z 
2025-04-11T04:23:18.5427154Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5427293Z __________________________ test_rms_layernorm[5120-8] __________________________
2025-04-11T04:23:18.5427297Z 
2025-04-11T04:23:18.5427371Z M = 8, N = 5120
2025-04-11T04:23:18.5427376Z 
2025-04-11T04:23:18.5427485Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5427602Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5427698Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5427790Z         torch.manual_seed(123)
2025-04-11T04:23:18.5427969Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5428063Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5428074Z 
2025-04-11T04:23:18.5428221Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5428333Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5428337Z 
2025-04-11T04:23:18.5428439Z device = None
2025-04-11T04:23:18.5428444Z 
2025-04-11T04:23:18.5428564Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5428719Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5428791Z     
2025-04-11T04:23:18.5428870Z         Args:
2025-04-11T04:23:18.5429036Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5429207Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5429309Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5429385Z         """
2025-04-11T04:23:18.5429468Z         _lazy_init()
2025-04-11T04:23:18.5429560Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5429759Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5429864Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5430146Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5430278Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5430433Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5430438Z 
2025-04-11T04:23:18.5430678Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5430815Z _________________________ test_rms_layernorm[5120-16] __________________________
2025-04-11T04:23:18.5430823Z 
2025-04-11T04:23:18.5430905Z M = 16, N = 5120
2025-04-11T04:23:18.5430909Z 
2025-04-11T04:23:18.5431013Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5431138Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5431236Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5431326Z         torch.manual_seed(123)
2025-04-11T04:23:18.5431415Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5431504Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5431507Z 
2025-04-11T04:23:18.5431660Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5431767Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5431771Z 
2025-04-11T04:23:18.5431853Z device = None
2025-04-11T04:23:18.5431857Z 
2025-04-11T04:23:18.5431973Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5432128Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5432202Z     
2025-04-11T04:23:18.5432276Z         Args:
2025-04-11T04:23:18.5432447Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5432616Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5432724Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5432799Z         """
2025-04-11T04:23:18.5432877Z         _lazy_init()
2025-04-11T04:23:18.5432976Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5433074Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5433181Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5433463Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5433602Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5433758Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5433856Z 
2025-04-11T04:23:18.5434097Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5434248Z ____________________ test_rotary_emb[dtype0-64-16-32-64-4] _____________________
2025-04-11T04:23:18.5434252Z 
2025-04-11T04:23:18.5434413Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float16
2025-04-11T04:23:18.5434424Z 
2025-04-11T04:23:18.5434533Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5434639Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.5434751Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.5434854Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T04:23:18.5434960Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.5435124Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5435260Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T04:23:18.5435358Z         torch.manual_seed(10)
2025-04-11T04:23:18.5435453Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.5435650Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.5435782Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5435915Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5435990Z     
2025-04-11T04:23:18.5436158Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5436234Z     
2025-04-11T04:23:18.5436331Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5436407Z     
2025-04-11T04:23:18.5436500Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.5436620Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.5436728Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5436825Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5436919Z         cos_2 = cos[:, : D // 2]
2025-04-11T04:23:18.5437000Z         sin_2 = sin[:, : D // 2]
2025-04-11T04:23:18.5437121Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5437254Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.5437467Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.5437600Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5437672Z     
2025-04-11T04:23:18.5437756Z         # create data
2025-04-11T04:23:18.5437838Z         block_size = 32
2025-04-11T04:23:18.5438003Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T04:23:18.5438099Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5438244Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.5438353Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5438636Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5438786Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5438947Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5438952Z 
2025-04-11T04:23:18.5439151Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T04:23:18.5439299Z ____________________ test_rotary_emb[dtype0-64-32-32-64-4] _____________________
2025-04-11T04:23:18.5439303Z 
2025-04-11T04:23:18.5439450Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float16
2025-04-11T04:23:18.5439460Z 
2025-04-11T04:23:18.5439567Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5439671Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.5439772Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.5439871Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T04:23:18.5440060Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.5440224Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5440363Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T04:23:18.5440457Z         torch.manual_seed(10)
2025-04-11T04:23:18.5440551Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.5440658Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.5440783Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5440912Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5440986Z     
2025-04-11T04:23:18.5441154Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5441229Z     
2025-04-11T04:23:18.5441323Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5441400Z     
2025-04-11T04:23:18.5441493Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.5441615Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.5441719Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5441909Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5442000Z         cos_2 = cos[:, : D // 2]
2025-04-11T04:23:18.5442083Z         sin_2 = sin[:, : D // 2]
2025-04-11T04:23:18.5442206Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5442337Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.5442547Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.5442674Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5442746Z     
2025-04-11T04:23:18.5442832Z         # create data
2025-04-11T04:23:18.5442912Z         block_size = 32
2025-04-11T04:23:18.5443073Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T04:23:18.5443177Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5443314Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.5443429Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5443712Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5443851Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5444011Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5444015Z 
2025-04-11T04:23:18.5444215Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T04:23:18.5444359Z ____________________ test_rotary_emb[dtype1-64-16-32-64-4] _____________________
2025-04-11T04:23:18.5444363Z 
2025-04-11T04:23:18.5444509Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float32
2025-04-11T04:23:18.5444521Z 
2025-04-11T04:23:18.5444632Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5444736Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.5444837Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.5444942Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T04:23:18.5445044Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.5445205Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5445340Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T04:23:18.5445435Z         torch.manual_seed(10)
2025-04-11T04:23:18.5445530Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.5445636Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.5445764Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5445905Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5445980Z     
2025-04-11T04:23:18.5446146Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5446331Z     
2025-04-11T04:23:18.5446427Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5446509Z     
2025-04-11T04:23:18.5446602Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.5446719Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.5446823Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5446918Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5447006Z         cos_2 = cos[:, : D // 2]
2025-04-11T04:23:18.5447087Z         sin_2 = sin[:, : D // 2]
2025-04-11T04:23:18.5447208Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5447335Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.5447542Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.5447668Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5447744Z     
2025-04-11T04:23:18.5447832Z         # create data
2025-04-11T04:23:18.5447913Z         block_size = 32
2025-04-11T04:23:18.5448073Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T04:23:18.5448276Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5448411Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.5448516Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5448801Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5448938Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5449095Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5449100Z 
2025-04-11T04:23:18.5449297Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T04:23:18.5449442Z ____________________ test_rotary_emb[dtype1-64-32-32-64-4] _____________________
2025-04-11T04:23:18.5449450Z 
2025-04-11T04:23:18.5449597Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float32
2025-04-11T04:23:18.5449610Z 
2025-04-11T04:23:18.5449717Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5449817Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.5449917Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.5450017Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T04:23:18.5450116Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.5450274Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5450408Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T04:23:18.5450500Z         torch.manual_seed(10)
2025-04-11T04:23:18.5450594Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.5450700Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.5450826Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5450953Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5451029Z     
2025-04-11T04:23:18.5451194Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5451274Z     
2025-04-11T04:23:18.5451365Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5451440Z     
2025-04-11T04:23:18.5451533Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.5451647Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.5451750Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5451845Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5451934Z         cos_2 = cos[:, : D // 2]
2025-04-11T04:23:18.5452013Z         sin_2 = sin[:, : D // 2]
2025-04-11T04:23:18.5452131Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5452258Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.5452554Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.5452683Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5452755Z     
2025-04-11T04:23:18.5452837Z         # create data
2025-04-11T04:23:18.5452918Z         block_size = 32
2025-04-11T04:23:18.5453079Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T04:23:18.5453179Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5453313Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.5453423Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5453705Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5453842Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5453999Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5454007Z 
2025-04-11T04:23:18.5454202Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T04:23:18.5454431Z _____________________ test_silu_and_mul[dtype0-11008-64-2] _____________________
2025-04-11T04:23:18.5454436Z 
2025-04-11T04:23:18.5454577Z SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float32
2025-04-11T04:23:18.5454589Z 
2025-04-11T04:23:18.5454694Z     @pytest.mark.parametrize("SHAPE_X", [2])
2025-04-11T04:23:18.5454796Z     @pytest.mark.parametrize("SHAPE_Y", [64])
2025-04-11T04:23:18.5454908Z     @pytest.mark.parametrize("SHAPE_Z", [11008])
2025-04-11T04:23:18.5455067Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
2025-04-11T04:23:18.5455206Z     def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
2025-04-11T04:23:18.5455294Z         torch.manual_seed(5)
2025-04-11T04:23:18.5455382Z         device = get_current_device()
2025-04-11T04:23:18.5455570Z >       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
2025-04-11T04:23:18.5455672Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5455955Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5456086Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5456247Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5456251Z 
2025-04-11T04:23:18.5456421Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
2025-04-11T04:23:18.5456568Z _____________________ test_silu_and_mul[dtype1-11008-64-2] _____________________
2025-04-11T04:23:18.5456580Z 
2025-04-11T04:23:18.5456717Z SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float16
2025-04-11T04:23:18.5456721Z 
2025-04-11T04:23:18.5456824Z     @pytest.mark.parametrize("SHAPE_X", [2])
2025-04-11T04:23:18.5456936Z     @pytest.mark.parametrize("SHAPE_Y", [64])
2025-04-11T04:23:18.5457044Z     @pytest.mark.parametrize("SHAPE_Z", [11008])
2025-04-11T04:23:18.5457211Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
2025-04-11T04:23:18.5457350Z     def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
2025-04-11T04:23:18.5457439Z         torch.manual_seed(5)
2025-04-11T04:23:18.5457529Z         device = get_current_device()
2025-04-11T04:23:18.5457713Z >       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
2025-04-11T04:23:18.5457825Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5458113Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5458256Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5458412Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5458507Z 
2025-04-11T04:23:18.5458680Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
2025-04-11T04:23:18.5458852Z _____________ test_context_attention[True-False-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.5458860Z 
2025-04-11T04:23:18.5459017Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5459168Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5459257Z use_new_kcache_layout = True
2025-04-11T04:23:18.5459261Z 
2025-04-11T04:23:18.5459469Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5459573Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5459695Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5459837Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5459962Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5460077Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5460220Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5460362Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5460602Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5460699Z     def test_context_attention(
2025-04-11T04:23:18.5460776Z         bsz: int,
2025-04-11T04:23:18.5460859Z         block_size: int,
2025-04-11T04:23:18.5460954Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5461039Z         num_attn_heads: int,
2025-04-11T04:23:18.5461126Z         kv_group_num: int,
2025-04-11T04:23:18.5461213Z         same_context_len: bool,
2025-04-11T04:23:18.5461304Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5461394Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5461468Z     ):
2025-04-11T04:23:18.5461589Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5461786Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5461982Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5462161Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5462329Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5462406Z             return
2025-04-11T04:23:18.5462480Z     
2025-04-11T04:23:18.5462569Z         torch.manual_seed(123)
2025-04-11T04:23:18.5462669Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5462764Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5462856Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5462860Z 
2025-04-11T04:23:18.5463024Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5463142Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5463394Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T04:23:18.5463494Z     with torch.cuda.device(device):
2025-04-11T04:23:18.5463609Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5463614Z 
2025-04-11T04:23:18.5463735Z self = <torch.cuda.device object at 0x7fb5b84e9bd0>
2025-04-11T04:23:18.5463740Z 
2025-04-11T04:23:18.5463822Z     def __enter__(self):
2025-04-11T04:23:18.5463957Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T04:23:18.5464060Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5464339Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5464475Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5464633Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5464728Z 
2025-04-11T04:23:18.5464970Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T04:23:18.5465142Z _____________ test_context_attention[True-False-True-1-16-8-16-32] _____________
2025-04-11T04:23:18.5465147Z 
2025-04-11T04:23:18.5465303Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5465451Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5465543Z use_new_kcache_layout = True
2025-04-11T04:23:18.5465547Z 
2025-04-11T04:23:18.5465745Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5465849Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5465971Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5466109Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5466229Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5466346Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5466488Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5466706Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5466860Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5466956Z     def test_context_attention(
2025-04-11T04:23:18.5467036Z         bsz: int,
2025-04-11T04:23:18.5467123Z         block_size: int,
2025-04-11T04:23:18.5467213Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5467296Z         num_attn_heads: int,
2025-04-11T04:23:18.5467384Z         kv_group_num: int,
2025-04-11T04:23:18.5467476Z         same_context_len: bool,
2025-04-11T04:23:18.5467567Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5467661Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5467739Z     ):
2025-04-11T04:23:18.5467852Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5468055Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5468244Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5468479Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5468659Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5468736Z             return
2025-04-11T04:23:18.5468816Z     
2025-04-11T04:23:18.5468905Z         torch.manual_seed(123)
2025-04-11T04:23:18.5469003Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5469098Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5469191Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5469196Z 
2025-04-11T04:23:18.5469373Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5469484Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5469492Z 
2025-04-11T04:23:18.5469576Z device = None
2025-04-11T04:23:18.5469581Z 
2025-04-11T04:23:18.5469699Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5469858Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5469938Z     
2025-04-11T04:23:18.5470015Z         Args:
2025-04-11T04:23:18.5470192Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5470362Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5470470Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5470552Z         """
2025-04-11T04:23:18.5470629Z         _lazy_init()
2025-04-11T04:23:18.5470731Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5470832Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5470941Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5471348Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5471491Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5471656Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5471660Z 
2025-04-11T04:23:18.5471895Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5472066Z _____________ test_context_attention[True-False-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.5472070Z 
2025-04-11T04:23:18.5472221Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5472378Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5472467Z use_new_kcache_layout = True
2025-04-11T04:23:18.5472471Z 
2025-04-11T04:23:18.5472674Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5472783Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5472901Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5473143Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5473264Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5473384Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5473520Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5473660Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5473813Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5473901Z     def test_context_attention(
2025-04-11T04:23:18.5473983Z         bsz: int,
2025-04-11T04:23:18.5474066Z         block_size: int,
2025-04-11T04:23:18.5474164Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5474247Z         num_attn_heads: int,
2025-04-11T04:23:18.5474335Z         kv_group_num: int,
2025-04-11T04:23:18.5474425Z         same_context_len: bool,
2025-04-11T04:23:18.5474508Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5474609Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5474683Z     ):
2025-04-11T04:23:18.5474802Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5474993Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5475175Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5475356Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5475521Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5475603Z             return
2025-04-11T04:23:18.5475683Z     
2025-04-11T04:23:18.5475776Z         torch.manual_seed(123)
2025-04-11T04:23:18.5475887Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5475979Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5476077Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5476084Z 
2025-04-11T04:23:18.5476259Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5476376Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5476380Z 
2025-04-11T04:23:18.5476459Z device = None
2025-04-11T04:23:18.5476463Z 
2025-04-11T04:23:18.5476585Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5476738Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5476809Z     
2025-04-11T04:23:18.5476889Z         Args:
2025-04-11T04:23:18.5477055Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5477229Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5477424Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5477505Z         """
2025-04-11T04:23:18.5477584Z         _lazy_init()
2025-04-11T04:23:18.5477683Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5477790Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5477895Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5478183Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5478323Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5478483Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5478493Z 
2025-04-11T04:23:18.5478737Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5478906Z _____________ test_context_attention[True-False-True-1-16-8-32-32] _____________
2025-04-11T04:23:18.5478913Z 
2025-04-11T04:23:18.5479072Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5479307Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5479407Z use_new_kcache_layout = True
2025-04-11T04:23:18.5479412Z 
2025-04-11T04:23:18.5479613Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5479722Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5479840Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5479977Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5480098Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5480213Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5480357Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5480494Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5480653Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5480742Z     def test_context_attention(
2025-04-11T04:23:18.5480822Z         bsz: int,
2025-04-11T04:23:18.5480911Z         block_size: int,
2025-04-11T04:23:18.5480999Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5481089Z         num_attn_heads: int,
2025-04-11T04:23:18.5481172Z         kv_group_num: int,
2025-04-11T04:23:18.5481256Z         same_context_len: bool,
2025-04-11T04:23:18.5481345Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5481431Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5481514Z     ):
2025-04-11T04:23:18.5481625Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5481822Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5482004Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5482182Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5482350Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5482430Z             return
2025-04-11T04:23:18.5482507Z     
2025-04-11T04:23:18.5482594Z         torch.manual_seed(123)
2025-04-11T04:23:18.5482698Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5482786Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5482875Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5482879Z 
2025-04-11T04:23:18.5483049Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5483160Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5483164Z 
2025-04-11T04:23:18.5483244Z device = None
2025-04-11T04:23:18.5483248Z 
2025-04-11T04:23:18.5483365Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5483725Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5483801Z     
2025-04-11T04:23:18.5483874Z         Args:
2025-04-11T04:23:18.5484053Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5484219Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5484330Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5484404Z         """
2025-04-11T04:23:18.5484488Z         _lazy_init()
2025-04-11T04:23:18.5484581Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5484683Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5484793Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5485078Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5485223Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5485388Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5485478Z 
2025-04-11T04:23:18.5485733Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5485910Z _____________ test_context_attention[True-False-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.5485915Z 
2025-04-11T04:23:18.5486064Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5486225Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5486314Z use_new_kcache_layout = True
2025-04-11T04:23:18.5486318Z 
2025-04-11T04:23:18.5486520Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5486624Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5486745Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5486889Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5487012Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5487127Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5487262Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5487402Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5487554Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5487646Z     def test_context_attention(
2025-04-11T04:23:18.5487723Z         bsz: int,
2025-04-11T04:23:18.5487806Z         block_size: int,
2025-04-11T04:23:18.5487899Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5487979Z         num_attn_heads: int,
2025-04-11T04:23:18.5488067Z         kv_group_num: int,
2025-04-11T04:23:18.5488149Z         same_context_len: bool,
2025-04-11T04:23:18.5488238Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5488329Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5488403Z     ):
2025-04-11T04:23:18.5488515Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5488709Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5488900Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5489072Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5489240Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5489323Z             return
2025-04-11T04:23:18.5489396Z     
2025-04-11T04:23:18.5489488Z         torch.manual_seed(123)
2025-04-11T04:23:18.5489586Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5489677Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5489766Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5489770Z 
2025-04-11T04:23:18.5490027Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5490142Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5490150Z 
2025-04-11T04:23:18.5490228Z device = None
2025-04-11T04:23:18.5490233Z 
2025-04-11T04:23:18.5490353Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5490503Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5490581Z     
2025-04-11T04:23:18.5490658Z         Args:
2025-04-11T04:23:18.5490829Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5491005Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5491116Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5491197Z         """
2025-04-11T04:23:18.5491280Z         _lazy_init()
2025-04-11T04:23:18.5491384Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5491495Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5491605Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5491976Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5492115Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5492281Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5492285Z 
2025-04-11T04:23:18.5492521Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5492697Z ____________ test_context_attention[True-False-True-1-16-16-16-32] _____________
2025-04-11T04:23:18.5492701Z 
2025-04-11T04:23:18.5492854Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5493006Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5493096Z use_new_kcache_layout = True
2025-04-11T04:23:18.5493101Z 
2025-04-11T04:23:18.5493301Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5493414Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5493529Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5493673Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5493788Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5493906Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5494041Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5494176Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5494335Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5494425Z     def test_context_attention(
2025-04-11T04:23:18.5494508Z         bsz: int,
2025-04-11T04:23:18.5494593Z         block_size: int,
2025-04-11T04:23:18.5494683Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5494771Z         num_attn_heads: int,
2025-04-11T04:23:18.5494856Z         kv_group_num: int,
2025-04-11T04:23:18.5494959Z         same_context_len: bool,
2025-04-11T04:23:18.5495042Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5495141Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5495214Z     ):
2025-04-11T04:23:18.5495323Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5495529Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5495712Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5495892Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5496057Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5496244Z             return
2025-04-11T04:23:18.5496319Z     
2025-04-11T04:23:18.5496405Z         torch.manual_seed(123)
2025-04-11T04:23:18.5496515Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5496605Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5496697Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5496702Z 
2025-04-11T04:23:18.5496868Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5496980Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5496989Z 
2025-04-11T04:23:18.5497066Z device = None
2025-04-11T04:23:18.5497071Z 
2025-04-11T04:23:18.5497186Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5497345Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5497417Z     
2025-04-11T04:23:18.5497494Z         Args:
2025-04-11T04:23:18.5497664Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5497835Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5498037Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5498111Z         """
2025-04-11T04:23:18.5498196Z         _lazy_init()
2025-04-11T04:23:18.5498289Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5498393Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5498499Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5498778Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5498919Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5499077Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5499081Z 
2025-04-11T04:23:18.5499325Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5499497Z _____________ test_context_attention[True-False-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.5499505Z 
2025-04-11T04:23:18.5499660Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5499804Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5499894Z use_new_kcache_layout = True
2025-04-11T04:23:18.5499898Z 
2025-04-11T04:23:18.5500096Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5500197Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5500318Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5500455Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5500573Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5500683Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5500825Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5500958Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5501110Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5501199Z     def test_context_attention(
2025-04-11T04:23:18.5501273Z         bsz: int,
2025-04-11T04:23:18.5501357Z         block_size: int,
2025-04-11T04:23:18.5501443Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5501524Z         num_attn_heads: int,
2025-04-11T04:23:18.5501609Z         kv_group_num: int,
2025-04-11T04:23:18.5501691Z         same_context_len: bool,
2025-04-11T04:23:18.5501778Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5501863Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5501942Z     ):
2025-04-11T04:23:18.5502050Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5502239Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5502511Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5502687Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5502856Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5502933Z             return
2025-04-11T04:23:18.5503008Z     
2025-04-11T04:23:18.5503094Z         torch.manual_seed(123)
2025-04-11T04:23:18.5503190Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5503282Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5503369Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5503373Z 
2025-04-11T04:23:18.5503540Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5503651Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5503655Z 
2025-04-11T04:23:18.5503737Z device = None
2025-04-11T04:23:18.5503741Z 
2025-04-11T04:23:18.5503856Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5504087Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5504166Z     
2025-04-11T04:23:18.5504238Z         Args:
2025-04-11T04:23:18.5504406Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5504570Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5504678Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5504767Z         """
2025-04-11T04:23:18.5504844Z         _lazy_init()
2025-04-11T04:23:18.5504954Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5505053Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5505167Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5505456Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5505594Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5505759Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5505763Z 
2025-04-11T04:23:18.5506002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5506176Z ____________ test_context_attention[True-False-True-1-16-16-32-32] _____________
2025-04-11T04:23:18.5506180Z 
2025-04-11T04:23:18.5506331Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5506479Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5506566Z use_new_kcache_layout = True
2025-04-11T04:23:18.5506570Z 
2025-04-11T04:23:18.5506769Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5506877Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5506992Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5507134Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5507250Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5507364Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5507499Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5507636Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5507787Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5507873Z     def test_context_attention(
2025-04-11T04:23:18.5507953Z         bsz: int,
2025-04-11T04:23:18.5508033Z         block_size: int,
2025-04-11T04:23:18.5508125Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5508205Z         num_attn_heads: int,
2025-04-11T04:23:18.5508284Z         kv_group_num: int,
2025-04-11T04:23:18.5508488Z         same_context_len: bool,
2025-04-11T04:23:18.5508574Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5508665Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5508741Z     ):
2025-04-11T04:23:18.5508854Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5509046Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5509225Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5509396Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5509559Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5509639Z             return
2025-04-11T04:23:18.5509711Z     
2025-04-11T04:23:18.5509800Z         torch.manual_seed(123)
2025-04-11T04:23:18.5509897Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5509988Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5510082Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5510087Z 
2025-04-11T04:23:18.5510357Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5510474Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5510479Z 
2025-04-11T04:23:18.5510557Z device = None
2025-04-11T04:23:18.5510561Z 
2025-04-11T04:23:18.5510683Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5510829Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5510900Z     
2025-04-11T04:23:18.5510979Z         Args:
2025-04-11T04:23:18.5511149Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5511319Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5511426Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5511509Z         """
2025-04-11T04:23:18.5511587Z         _lazy_init()
2025-04-11T04:23:18.5511680Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5511788Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5511892Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5512176Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5512311Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5512472Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5512477Z 
2025-04-11T04:23:18.5512712Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5512889Z _____________ test_context_attention[True-False-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.5512900Z 
2025-04-11T04:23:18.5513060Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5513205Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5513297Z use_new_kcache_layout = True
2025-04-11T04:23:18.5513302Z 
2025-04-11T04:23:18.5513502Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5513608Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5513724Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5513864Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5513978Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5514090Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5514234Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5514369Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5514522Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5514701Z     def test_context_attention(
2025-04-11T04:23:18.5514777Z         bsz: int,
2025-04-11T04:23:18.5514865Z         block_size: int,
2025-04-11T04:23:18.5514957Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5515042Z         num_attn_heads: int,
2025-04-11T04:23:18.5515124Z         kv_group_num: int,
2025-04-11T04:23:18.5515212Z         same_context_len: bool,
2025-04-11T04:23:18.5515293Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5515382Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5515460Z     ):
2025-04-11T04:23:18.5515569Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5515762Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5515942Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5516112Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5516283Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5516461Z             return
2025-04-11T04:23:18.5516539Z     
2025-04-11T04:23:18.5516625Z         torch.manual_seed(123)
2025-04-11T04:23:18.5516726Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5516811Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5516900Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5516903Z 
2025-04-11T04:23:18.5517075Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5517188Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5517192Z 
2025-04-11T04:23:18.5517272Z device = None
2025-04-11T04:23:18.5517276Z 
2025-04-11T04:23:18.5517391Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5517542Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5517619Z     
2025-04-11T04:23:18.5517691Z         Args:
2025-04-11T04:23:18.5517859Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5518027Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5518134Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5518206Z         """
2025-04-11T04:23:18.5518285Z         _lazy_init()
2025-04-11T04:23:18.5518377Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5518476Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5518583Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5518865Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5519003Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5519168Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5519172Z 
2025-04-11T04:23:18.5519419Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5519588Z _____________ test_context_attention[True-False-True-4-16-8-16-32] _____________
2025-04-11T04:23:18.5519592Z 
2025-04-11T04:23:18.5519748Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5519894Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5519980Z use_new_kcache_layout = True
2025-04-11T04:23:18.5519984Z 
2025-04-11T04:23:18.5520191Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5520294Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5520413Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5520551Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5520755Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5520867Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5521008Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5521145Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5521297Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5521387Z     def test_context_attention(
2025-04-11T04:23:18.5521461Z         bsz: int,
2025-04-11T04:23:18.5521543Z         block_size: int,
2025-04-11T04:23:18.5521637Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5521718Z         num_attn_heads: int,
2025-04-11T04:23:18.5521805Z         kv_group_num: int,
2025-04-11T04:23:18.5521890Z         same_context_len: bool,
2025-04-11T04:23:18.5522017Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5522106Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5522177Z     ):
2025-04-11T04:23:18.5522294Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5522482Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5522761Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5522930Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5523105Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5523179Z             return
2025-04-11T04:23:18.5523249Z     
2025-04-11T04:23:18.5523346Z         torch.manual_seed(123)
2025-04-11T04:23:18.5523449Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5523539Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5523628Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5523632Z 
2025-04-11T04:23:18.5523796Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5523913Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5523917Z 
2025-04-11T04:23:18.5523998Z device = None
2025-04-11T04:23:18.5524002Z 
2025-04-11T04:23:18.5524120Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5524268Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5524340Z     
2025-04-11T04:23:18.5524411Z         Args:
2025-04-11T04:23:18.5524575Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5524744Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5524847Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5524922Z         """
2025-04-11T04:23:18.5524997Z         _lazy_init()
2025-04-11T04:23:18.5525094Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5525194Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5525301Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5525586Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5525727Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5525892Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5525897Z 
2025-04-11T04:23:18.5526135Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5526305Z _____________ test_context_attention[True-False-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.5526309Z 
2025-04-11T04:23:18.5526458Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5526608Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5526695Z use_new_kcache_layout = True
2025-04-11T04:23:18.5526783Z 
2025-04-11T04:23:18.5526983Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5527092Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5527207Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5527351Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5527465Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5527579Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5527713Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5527844Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5527998Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5528085Z     def test_context_attention(
2025-04-11T04:23:18.5528163Z         bsz: int,
2025-04-11T04:23:18.5528244Z         block_size: int,
2025-04-11T04:23:18.5528338Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5528418Z         num_attn_heads: int,
2025-04-11T04:23:18.5528498Z         kv_group_num: int,
2025-04-11T04:23:18.5528671Z         same_context_len: bool,
2025-04-11T04:23:18.5528753Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5528841Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5528913Z     ):
2025-04-11T04:23:18.5529020Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5529217Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5529403Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5529575Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5529740Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5529816Z             return
2025-04-11T04:23:18.5529890Z     
2025-04-11T04:23:18.5529973Z         torch.manual_seed(123)
2025-04-11T04:23:18.5530074Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5530164Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5530257Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5530261Z 
2025-04-11T04:23:18.5530426Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5530536Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5530544Z 
2025-04-11T04:23:18.5530619Z device = None
2025-04-11T04:23:18.5530623Z 
2025-04-11T04:23:18.5530736Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5530888Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5530957Z     
2025-04-11T04:23:18.5531032Z         Args:
2025-04-11T04:23:18.5531195Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5531368Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5531472Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5531546Z         """
2025-04-11T04:23:18.5531627Z         _lazy_init()
2025-04-11T04:23:18.5531721Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5531823Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5531927Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5532208Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5532353Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5532513Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5532517Z 
2025-04-11T04:23:18.5532778Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5533037Z _____________ test_context_attention[True-False-True-4-16-8-32-32] _____________
2025-04-11T04:23:18.5533041Z 
2025-04-11T04:23:18.5533208Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5533358Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5533448Z use_new_kcache_layout = True
2025-04-11T04:23:18.5533452Z 
2025-04-11T04:23:18.5533648Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5533750Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5533867Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5534005Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5534123Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5534233Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5534372Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5534508Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5534659Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5534838Z     def test_context_attention(
2025-04-11T04:23:18.5534915Z         bsz: int,
2025-04-11T04:23:18.5534998Z         block_size: int,
2025-04-11T04:23:18.5535086Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5535175Z         num_attn_heads: int,
2025-04-11T04:23:18.5535257Z         kv_group_num: int,
2025-04-11T04:23:18.5535340Z         same_context_len: bool,
2025-04-11T04:23:18.5535430Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5535514Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5535590Z     ):
2025-04-11T04:23:18.5535700Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5535891Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5536084Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5536253Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5536422Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5536495Z             return
2025-04-11T04:23:18.5536568Z     
2025-04-11T04:23:18.5536650Z         torch.manual_seed(123)
2025-04-11T04:23:18.5536745Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5536836Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5536924Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5536928Z 
2025-04-11T04:23:18.5537095Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5537205Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5537209Z 
2025-04-11T04:23:18.5537288Z device = None
2025-04-11T04:23:18.5537292Z 
2025-04-11T04:23:18.5537409Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5537557Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5537633Z     
2025-04-11T04:23:18.5537704Z         Args:
2025-04-11T04:23:18.5537870Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5538032Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5538137Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5538213Z         """
2025-04-11T04:23:18.5538290Z         _lazy_init()
2025-04-11T04:23:18.5538387Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5538485Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5538594Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5538872Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5539097Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5539255Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5539263Z 
2025-04-11T04:23:18.5539501Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5539669Z _____________ test_context_attention[True-False-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.5539673Z 
2025-04-11T04:23:18.5539823Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5539973Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5540059Z use_new_kcache_layout = True
2025-04-11T04:23:18.5540063Z 
2025-04-11T04:23:18.5540263Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5540365Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5540486Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5540623Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5540842Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5540958Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5541092Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5541230Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5541380Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5541466Z     def test_context_attention(
2025-04-11T04:23:18.5541547Z         bsz: int,
2025-04-11T04:23:18.5541626Z         block_size: int,
2025-04-11T04:23:18.5541718Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5541798Z         num_attn_heads: int,
2025-04-11T04:23:18.5541890Z         kv_group_num: int,
2025-04-11T04:23:18.5541980Z         same_context_len: bool,
2025-04-11T04:23:18.5542066Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5542165Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5542237Z     ):
2025-04-11T04:23:18.5542352Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5542543Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5542725Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5542897Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5543062Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5543140Z             return
2025-04-11T04:23:18.5543212Z     
2025-04-11T04:23:18.5543299Z         torch.manual_seed(123)
2025-04-11T04:23:18.5543397Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5543482Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5543582Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5543585Z 
2025-04-11T04:23:18.5543753Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5543873Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5543878Z 
2025-04-11T04:23:18.5543953Z device = None
2025-04-11T04:23:18.5543957Z 
2025-04-11T04:23:18.5544073Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5544224Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5544293Z     
2025-04-11T04:23:18.5544368Z         Args:
2025-04-11T04:23:18.5544534Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5544735Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5544839Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5544913Z         """
2025-04-11T04:23:18.5545111Z         _lazy_init()
2025-04-11T04:23:18.5545210Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5545313Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5545419Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5545701Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5545835Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5545995Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5546000Z 
2025-04-11T04:23:18.5546235Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5546404Z ____________ test_context_attention[True-False-True-4-16-16-16-32] _____________
2025-04-11T04:23:18.5546412Z 
2025-04-11T04:23:18.5546563Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5546710Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5546804Z use_new_kcache_layout = True
2025-04-11T04:23:18.5546889Z 
2025-04-11T04:23:18.5547100Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5547211Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5547324Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5547465Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5547580Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5547690Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5547829Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5547963Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5548114Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5548205Z     def test_context_attention(
2025-04-11T04:23:18.5548286Z         bsz: int,
2025-04-11T04:23:18.5548364Z         block_size: int,
2025-04-11T04:23:18.5548486Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5548573Z         num_attn_heads: int,
2025-04-11T04:23:18.5548655Z         kv_group_num: int,
2025-04-11T04:23:18.5548744Z         same_context_len: bool,
2025-04-11T04:23:18.5548828Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5548916Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5548993Z     ):
2025-04-11T04:23:18.5549104Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5549297Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5549478Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5549648Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5549812Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5549895Z             return
2025-04-11T04:23:18.5549986Z     
2025-04-11T04:23:18.5550069Z         torch.manual_seed(123)
2025-04-11T04:23:18.5550179Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5550267Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5550354Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5550358Z 
2025-04-11T04:23:18.5550537Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5550647Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5550651Z 
2025-04-11T04:23:18.5550731Z device = None
2025-04-11T04:23:18.5550736Z 
2025-04-11T04:23:18.5550850Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5551001Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5551072Z     
2025-04-11T04:23:18.5551255Z         Args:
2025-04-11T04:23:18.5551423Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5551589Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5551698Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5551770Z         """
2025-04-11T04:23:18.5551850Z         _lazy_init()
2025-04-11T04:23:18.5551943Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5552043Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5552150Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5552440Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5552580Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5552737Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5552745Z 
2025-04-11T04:23:18.5552984Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5553247Z _____________ test_context_attention[True-False-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.5553252Z 
2025-04-11T04:23:18.5553406Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5553551Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5553638Z use_new_kcache_layout = True
2025-04-11T04:23:18.5553642Z 
2025-04-11T04:23:18.5553843Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5553945Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5554064Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5554198Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5554317Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5554432Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5554565Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5554705Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5554856Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5554951Z     def test_context_attention(
2025-04-11T04:23:18.5555027Z         bsz: int,
2025-04-11T04:23:18.5555114Z         block_size: int,
2025-04-11T04:23:18.5555206Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5555289Z         num_attn_heads: int,
2025-04-11T04:23:18.5555384Z         kv_group_num: int,
2025-04-11T04:23:18.5555468Z         same_context_len: bool,
2025-04-11T04:23:18.5555564Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5555650Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5555721Z     ):
2025-04-11T04:23:18.5555845Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5556052Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5556235Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5556416Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5556581Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5556660Z             return
2025-04-11T04:23:18.5556732Z     
2025-04-11T04:23:18.5556820Z         torch.manual_seed(123)
2025-04-11T04:23:18.5556917Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5557019Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5557107Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5557110Z 
2025-04-11T04:23:18.5557286Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5557402Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5557493Z 
2025-04-11T04:23:18.5557574Z device = None
2025-04-11T04:23:18.5557578Z 
2025-04-11T04:23:18.5557709Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5557857Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5557941Z     
2025-04-11T04:23:18.5558013Z         Args:
2025-04-11T04:23:18.5558188Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5558364Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5558468Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5558545Z         """
2025-04-11T04:23:18.5558621Z         _lazy_init()
2025-04-11T04:23:18.5558716Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5558816Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5558921Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5559208Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5559440Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5559601Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5559606Z 
2025-04-11T04:23:18.5559842Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5560012Z ____________ test_context_attention[True-False-True-4-16-16-32-32] _____________
2025-04-11T04:23:18.5560016Z 
2025-04-11T04:23:18.5560167Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5560321Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5560415Z use_new_kcache_layout = True
2025-04-11T04:23:18.5560420Z 
2025-04-11T04:23:18.5560626Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5560732Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5560849Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5560991Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5561103Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5561217Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5561351Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5561487Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5561635Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5561726Z     def test_context_attention(
2025-04-11T04:23:18.5561804Z         bsz: int,
2025-04-11T04:23:18.5561884Z         block_size: int,
2025-04-11T04:23:18.5561976Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5562063Z         num_attn_heads: int,
2025-04-11T04:23:18.5562144Z         kv_group_num: int,
2025-04-11T04:23:18.5562234Z         same_context_len: bool,
2025-04-11T04:23:18.5562321Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5562412Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5562484Z     ):
2025-04-11T04:23:18.5562591Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5562785Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5562964Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5563137Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5563298Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5563375Z             return
2025-04-11T04:23:18.5563446Z     
2025-04-11T04:23:18.5563531Z         torch.manual_seed(123)
2025-04-11T04:23:18.5563739Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5563827Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5563925Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5563929Z 
2025-04-11T04:23:18.5564096Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5564210Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5564214Z 
2025-04-11T04:23:18.5564292Z device = None
2025-04-11T04:23:18.5564295Z 
2025-04-11T04:23:18.5564414Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5564563Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5564635Z     
2025-04-11T04:23:18.5564717Z         Args:
2025-04-11T04:23:18.5564883Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5565055Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5565164Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5565237Z         """
2025-04-11T04:23:18.5565406Z         _lazy_init()
2025-04-11T04:23:18.5565499Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5565601Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5565703Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5565988Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5566123Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5566281Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5566293Z 
2025-04-11T04:23:18.5566535Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5566700Z _____________ test_context_attention[True-False-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.5566708Z 
2025-04-11T04:23:18.5566860Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5567011Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5567103Z use_new_kcache_layout = True
2025-04-11T04:23:18.5567108Z 
2025-04-11T04:23:18.5567314Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5567434Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5567555Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5567692Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5567811Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5567925Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5568065Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5568198Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5568354Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5568443Z     def test_context_attention(
2025-04-11T04:23:18.5568518Z         bsz: int,
2025-04-11T04:23:18.5568601Z         block_size: int,
2025-04-11T04:23:18.5568692Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5568778Z         num_attn_heads: int,
2025-04-11T04:23:18.5568859Z         kv_group_num: int,
2025-04-11T04:23:18.5568942Z         same_context_len: bool,
2025-04-11T04:23:18.5569028Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5569114Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5569188Z     ):
2025-04-11T04:23:18.5569298Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5569488Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5569671Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5569931Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5570101Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5570176Z             return
2025-04-11T04:23:18.5570251Z     
2025-04-11T04:23:18.5570337Z         torch.manual_seed(123)
2025-04-11T04:23:18.5570435Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5570527Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5570614Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5570618Z 
2025-04-11T04:23:18.5570788Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5570896Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5570900Z 
2025-04-11T04:23:18.5570979Z device = None
2025-04-11T04:23:18.5570983Z 
2025-04-11T04:23:18.5571095Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5571251Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5571323Z     
2025-04-11T04:23:18.5571481Z         Args:
2025-04-11T04:23:18.5571651Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5571815Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5571925Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5572000Z         """
2025-04-11T04:23:18.5572077Z         _lazy_init()
2025-04-11T04:23:18.5572176Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5572278Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5572387Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5572674Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5572820Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5572983Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5572991Z 
2025-04-11T04:23:18.5573237Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5573407Z ____________ test_context_attention[True-False-False-1-16-8-16-32] _____________
2025-04-11T04:23:18.5573411Z 
2025-04-11T04:23:18.5573563Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5573712Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5573799Z use_new_kcache_layout = True
2025-04-11T04:23:18.5573803Z 
2025-04-11T04:23:18.5574006Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5574109Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5574228Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5574369Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5574486Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5574606Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5574741Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5574880Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5575030Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5575127Z     def test_context_attention(
2025-04-11T04:23:18.5575208Z         bsz: int,
2025-04-11T04:23:18.5575287Z         block_size: int,
2025-04-11T04:23:18.5575383Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5575464Z         num_attn_heads: int,
2025-04-11T04:23:18.5575549Z         kv_group_num: int,
2025-04-11T04:23:18.5575634Z         same_context_len: bool,
2025-04-11T04:23:18.5575717Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5575897Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5575971Z     ):
2025-04-11T04:23:18.5576083Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5576278Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5576461Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5576629Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5576790Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5576869Z             return
2025-04-11T04:23:18.5576940Z     
2025-04-11T04:23:18.5577027Z         torch.manual_seed(123)
2025-04-11T04:23:18.5577126Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5577216Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5577305Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5577313Z 
2025-04-11T04:23:18.5577477Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5577589Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5577770Z 
2025-04-11T04:23:18.5577848Z device = None
2025-04-11T04:23:18.5577852Z 
2025-04-11T04:23:18.5577983Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5578133Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5578208Z     
2025-04-11T04:23:18.5578280Z         Args:
2025-04-11T04:23:18.5578451Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5578617Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5578721Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5578796Z         """
2025-04-11T04:23:18.5578873Z         _lazy_init()
2025-04-11T04:23:18.5578971Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5579077Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5579180Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5579469Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5579606Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5579770Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5579775Z 
2025-04-11T04:23:18.5580010Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5580176Z _____________ test_context_attention[True-False-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.5580180Z 
2025-04-11T04:23:18.5580328Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5580475Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5580568Z use_new_kcache_layout = True
2025-04-11T04:23:18.5580573Z 
2025-04-11T04:23:18.5580767Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5580875Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5580988Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5581127Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5581241Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5581350Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5581487Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5581619Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5581773Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5581861Z     def test_context_attention(
2025-04-11T04:23:18.5582028Z         bsz: int,
2025-04-11T04:23:18.5582110Z         block_size: int,
2025-04-11T04:23:18.5582197Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5582287Z         num_attn_heads: int,
2025-04-11T04:23:18.5582370Z         kv_group_num: int,
2025-04-11T04:23:18.5582459Z         same_context_len: bool,
2025-04-11T04:23:18.5582541Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5582627Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5582702Z     ):
2025-04-11T04:23:18.5582812Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5583028Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5583214Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5583395Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5583559Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5583636Z             return
2025-04-11T04:23:18.5583710Z     
2025-04-11T04:23:18.5583797Z         torch.manual_seed(123)
2025-04-11T04:23:18.5583990Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5584079Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5584173Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5584177Z 
2025-04-11T04:23:18.5584344Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5584454Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5584458Z 
2025-04-11T04:23:18.5584538Z device = None
2025-04-11T04:23:18.5584542Z 
2025-04-11T04:23:18.5584656Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5584807Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5584878Z     
2025-04-11T04:23:18.5584959Z         Args:
2025-04-11T04:23:18.5585124Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5585288Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5585401Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5585474Z         """
2025-04-11T04:23:18.5585555Z         _lazy_init()
2025-04-11T04:23:18.5585649Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5585752Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5585856Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5586137Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5586274Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5586432Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5586436Z 
2025-04-11T04:23:18.5586676Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5586848Z ____________ test_context_attention[True-False-False-1-16-8-32-32] _____________
2025-04-11T04:23:18.5586855Z 
2025-04-11T04:23:18.5587008Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5587153Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5587243Z use_new_kcache_layout = True
2025-04-11T04:23:18.5587246Z 
2025-04-11T04:23:18.5587444Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5587547Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5587664Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5587801Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5587925Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5588037Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5588296Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5588481Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5588643Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5588735Z     def test_context_attention(
2025-04-11T04:23:18.5588811Z         bsz: int,
2025-04-11T04:23:18.5588901Z         block_size: int,
2025-04-11T04:23:18.5588991Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5589072Z         num_attn_heads: int,
2025-04-11T04:23:18.5589159Z         kv_group_num: int,
2025-04-11T04:23:18.5589242Z         same_context_len: bool,
2025-04-11T04:23:18.5589329Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5589415Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5589490Z     ):
2025-04-11T04:23:18.5589599Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5589790Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5589977Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5590241Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5590407Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5590482Z             return
2025-04-11T04:23:18.5590554Z     
2025-04-11T04:23:18.5590641Z         torch.manual_seed(123)
2025-04-11T04:23:18.5590739Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5590830Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5590918Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5590922Z 
2025-04-11T04:23:18.5591089Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5591201Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5591209Z 
2025-04-11T04:23:18.5591285Z device = None
2025-04-11T04:23:18.5591292Z 
2025-04-11T04:23:18.5591405Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5591556Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5591632Z     
2025-04-11T04:23:18.5591706Z         Args:
2025-04-11T04:23:18.5591877Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5592044Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5592146Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5592225Z         """
2025-04-11T04:23:18.5592305Z         _lazy_init()
2025-04-11T04:23:18.5592403Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5592501Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5592609Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5592894Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5593044Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5593216Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5593220Z 
2025-04-11T04:23:18.5593467Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5593639Z ____________ test_context_attention[True-False-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.5593643Z 
2025-04-11T04:23:18.5593793Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5593951Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5594036Z use_new_kcache_layout = True
2025-04-11T04:23:18.5594040Z 
2025-04-11T04:23:18.5594244Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5594451Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5594564Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5594708Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5594824Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5594940Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5595073Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5595212Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5595361Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5595447Z     def test_context_attention(
2025-04-11T04:23:18.5595526Z         bsz: int,
2025-04-11T04:23:18.5595608Z         block_size: int,
2025-04-11T04:23:18.5595700Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5595780Z         num_attn_heads: int,
2025-04-11T04:23:18.5595868Z         kv_group_num: int,
2025-04-11T04:23:18.5595957Z         same_context_len: bool,
2025-04-11T04:23:18.5596042Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5596217Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5596291Z     ):
2025-04-11T04:23:18.5596404Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5596597Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5596777Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5596953Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5597116Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5597196Z             return
2025-04-11T04:23:18.5597268Z     
2025-04-11T04:23:18.5597354Z         torch.manual_seed(123)
2025-04-11T04:23:18.5597451Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5597541Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5597633Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5597637Z 
2025-04-11T04:23:18.5597809Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5597933Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5597937Z 
2025-04-11T04:23:18.5598013Z device = None
2025-04-11T04:23:18.5598018Z 
2025-04-11T04:23:18.5598142Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5598290Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5598361Z     
2025-04-11T04:23:18.5598435Z         Args:
2025-04-11T04:23:18.5598599Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5598767Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5598870Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5598948Z         """
2025-04-11T04:23:18.5599025Z         _lazy_init()
2025-04-11T04:23:18.5599118Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5599223Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5599326Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5599612Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5599745Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5599903Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5599911Z 
2025-04-11T04:23:18.5600146Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5600316Z ____________ test_context_attention[True-False-False-1-16-16-16-32] ____________
2025-04-11T04:23:18.5600320Z 
2025-04-11T04:23:18.5600562Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5600707Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5600803Z use_new_kcache_layout = True
2025-04-11T04:23:18.5600807Z 
2025-04-11T04:23:18.5601003Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5601109Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5601224Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5601362Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5601479Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5601590Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5601729Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5601862Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5602020Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5602106Z     def test_context_attention(
2025-04-11T04:23:18.5602285Z         bsz: int,
2025-04-11T04:23:18.5602375Z         block_size: int,
2025-04-11T04:23:18.5602466Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5602550Z         num_attn_heads: int,
2025-04-11T04:23:18.5602630Z         kv_group_num: int,
2025-04-11T04:23:18.5602713Z         same_context_len: bool,
2025-04-11T04:23:18.5602817Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5602911Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5602985Z     ):
2025-04-11T04:23:18.5603096Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5603299Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5603477Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5603645Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5603816Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5603892Z             return
2025-04-11T04:23:18.5603968Z     
2025-04-11T04:23:18.5604051Z         torch.manual_seed(123)
2025-04-11T04:23:18.5604149Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5604237Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5604325Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5604328Z 
2025-04-11T04:23:18.5604496Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5604608Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5604613Z 
2025-04-11T04:23:18.5604691Z device = None
2025-04-11T04:23:18.5604695Z 
2025-04-11T04:23:18.5604809Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5604961Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5605035Z     
2025-04-11T04:23:18.5605105Z         Args:
2025-04-11T04:23:18.5605274Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5605442Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5605552Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5605625Z         """
2025-04-11T04:23:18.5605703Z         _lazy_init()
2025-04-11T04:23:18.5605796Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5605892Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5606001Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5606282Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5606418Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5606669Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5606673Z 
2025-04-11T04:23:18.5606913Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5607084Z ____________ test_context_attention[True-False-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.5607088Z 
2025-04-11T04:23:18.5607237Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5607390Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5607475Z use_new_kcache_layout = True
2025-04-11T04:23:18.5607478Z 
2025-04-11T04:23:18.5607688Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5607789Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5607910Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5608051Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5608171Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5608283Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5608515Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5608656Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5608806Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5608895Z     def test_context_attention(
2025-04-11T04:23:18.5608972Z         bsz: int,
2025-04-11T04:23:18.5609051Z         block_size: int,
2025-04-11T04:23:18.5609142Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5609222Z         num_attn_heads: int,
2025-04-11T04:23:18.5609308Z         kv_group_num: int,
2025-04-11T04:23:18.5609391Z         same_context_len: bool,
2025-04-11T04:23:18.5609479Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5609564Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5609639Z     ):
2025-04-11T04:23:18.5609754Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5609946Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5610135Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5610303Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5610465Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5610544Z             return
2025-04-11T04:23:18.5610616Z     
2025-04-11T04:23:18.5610703Z         torch.manual_seed(123)
2025-04-11T04:23:18.5610799Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5610887Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5610974Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5610979Z 
2025-04-11T04:23:18.5611143Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5611263Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5611270Z 
2025-04-11T04:23:18.5611346Z device = None
2025-04-11T04:23:18.5611351Z 
2025-04-11T04:23:18.5611469Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5611617Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5611690Z     
2025-04-11T04:23:18.5611762Z         Args:
2025-04-11T04:23:18.5611928Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5612095Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5612196Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5612272Z         """
2025-04-11T04:23:18.5612348Z         _lazy_init()
2025-04-11T04:23:18.5612454Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5612651Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5612754Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5613051Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5613190Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5613353Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5613357Z 
2025-04-11T04:23:18.5613597Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5613770Z ____________ test_context_attention[True-False-False-1-16-16-32-32] ____________
2025-04-11T04:23:18.5613774Z 
2025-04-11T04:23:18.5613924Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5614073Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5614163Z use_new_kcache_layout = True
2025-04-11T04:23:18.5614167Z 
2025-04-11T04:23:18.5614364Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5614556Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5614672Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5614814Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5614929Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5615044Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5615178Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5615314Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5615466Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5615555Z     def test_context_attention(
2025-04-11T04:23:18.5615635Z         bsz: int,
2025-04-11T04:23:18.5615716Z         block_size: int,
2025-04-11T04:23:18.5615807Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5615892Z         num_attn_heads: int,
2025-04-11T04:23:18.5615974Z         kv_group_num: int,
2025-04-11T04:23:18.5616065Z         same_context_len: bool,
2025-04-11T04:23:18.5616147Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5616239Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5616310Z     ):
2025-04-11T04:23:18.5616419Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5616613Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5616792Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5616965Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5617128Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5617207Z             return
2025-04-11T04:23:18.5617283Z     
2025-04-11T04:23:18.5617366Z         torch.manual_seed(123)
2025-04-11T04:23:18.5617466Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5617558Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5617658Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5617662Z 
2025-04-11T04:23:18.5617828Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5617939Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5617947Z 
2025-04-11T04:23:18.5618025Z device = None
2025-04-11T04:23:18.5618029Z 
2025-04-11T04:23:18.5618143Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5618295Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5618365Z     
2025-04-11T04:23:18.5618439Z         Args:
2025-04-11T04:23:18.5618604Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5618856Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5618964Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5619041Z         """
2025-04-11T04:23:18.5619121Z         _lazy_init()
2025-04-11T04:23:18.5619216Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5619318Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5619422Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5619708Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5619847Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5620005Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5620009Z 
2025-04-11T04:23:18.5620252Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5620421Z _____________ test_context_attention[True-False-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.5620425Z 
2025-04-11T04:23:18.5620663Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5620809Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5620902Z use_new_kcache_layout = True
2025-04-11T04:23:18.5620906Z 
2025-04-11T04:23:18.5621103Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5621205Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5621326Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5621461Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5621578Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5621689Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5621827Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5621963Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5622114Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5622203Z     def test_context_attention(
2025-04-11T04:23:18.5622276Z         bsz: int,
2025-04-11T04:23:18.5622360Z         block_size: int,
2025-04-11T04:23:18.5622448Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5622528Z         num_attn_heads: int,
2025-04-11T04:23:18.5622614Z         kv_group_num: int,
2025-04-11T04:23:18.5622697Z         same_context_len: bool,
2025-04-11T04:23:18.5622786Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5622871Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5622948Z     ):
2025-04-11T04:23:18.5623055Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5623253Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5623452Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5623622Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5623795Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5623868Z             return
2025-04-11T04:23:18.5623943Z     
2025-04-11T04:23:18.5624026Z         torch.manual_seed(123)
2025-04-11T04:23:18.5624121Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5624213Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5624302Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5624306Z 
2025-04-11T04:23:18.5624474Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5624586Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5624590Z 
2025-04-11T04:23:18.5624671Z device = None
2025-04-11T04:23:18.5624757Z 
2025-04-11T04:23:18.5624876Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5625025Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5625107Z     
2025-04-11T04:23:18.5625179Z         Args:
2025-04-11T04:23:18.5625349Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5625517Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5625625Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5625702Z         """
2025-04-11T04:23:18.5625778Z         _lazy_init()
2025-04-11T04:23:18.5625880Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5625982Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5626089Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5626374Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5626515Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5626760Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5626765Z 
2025-04-11T04:23:18.5627002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5627175Z ____________ test_context_attention[True-False-False-4-16-8-16-32] _____________
2025-04-11T04:23:18.5627180Z 
2025-04-11T04:23:18.5627328Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5627476Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5627561Z use_new_kcache_layout = True
2025-04-11T04:23:18.5627565Z 
2025-04-11T04:23:18.5627765Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5627869Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5627986Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5628126Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5628242Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5628356Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5628521Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5628663Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5628811Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5628898Z     def test_context_attention(
2025-04-11T04:23:18.5628978Z         bsz: int,
2025-04-11T04:23:18.5629058Z         block_size: int,
2025-04-11T04:23:18.5629149Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5629232Z         num_attn_heads: int,
2025-04-11T04:23:18.5629317Z         kv_group_num: int,
2025-04-11T04:23:18.5629401Z         same_context_len: bool,
2025-04-11T04:23:18.5629487Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5629580Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5629657Z     ):
2025-04-11T04:23:18.5629773Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5629967Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5630147Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5630321Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5630487Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5630568Z             return
2025-04-11T04:23:18.5630639Z     
2025-04-11T04:23:18.5630729Z         torch.manual_seed(123)
2025-04-11T04:23:18.5630828Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5630917Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5631108Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5631112Z 
2025-04-11T04:23:18.5631281Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5631399Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5631404Z 
2025-04-11T04:23:18.5631480Z device = None
2025-04-11T04:23:18.5631484Z 
2025-04-11T04:23:18.5631608Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5631759Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5631829Z     
2025-04-11T04:23:18.5631906Z         Args:
2025-04-11T04:23:18.5632076Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5632244Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5632348Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5632430Z         """
2025-04-11T04:23:18.5632507Z         _lazy_init()
2025-04-11T04:23:18.5632601Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5632704Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5632924Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5633221Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5633364Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5633536Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5633541Z 
2025-04-11T04:23:18.5633816Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5633984Z _____________ test_context_attention[True-False-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.5633993Z 
2025-04-11T04:23:18.5634139Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5634288Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5634381Z use_new_kcache_layout = True
2025-04-11T04:23:18.5634388Z 
2025-04-11T04:23:18.5634585Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5634693Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5634806Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5634946Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5635061Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5635174Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5635311Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5635445Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5635597Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5635688Z     def test_context_attention(
2025-04-11T04:23:18.5635766Z         bsz: int,
2025-04-11T04:23:18.5635854Z         block_size: int,
2025-04-11T04:23:18.5635950Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5636035Z         num_attn_heads: int,
2025-04-11T04:23:18.5636116Z         kv_group_num: int,
2025-04-11T04:23:18.5636202Z         same_context_len: bool,
2025-04-11T04:23:18.5636286Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5636376Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5636455Z     ):
2025-04-11T04:23:18.5636566Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5636763Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5636943Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5637113Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5637370Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5637446Z             return
2025-04-11T04:23:18.5637528Z     
2025-04-11T04:23:18.5637617Z         torch.manual_seed(123)
2025-04-11T04:23:18.5637718Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5637810Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5637898Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5637903Z 
2025-04-11T04:23:18.5638076Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5638187Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5638192Z 
2025-04-11T04:23:18.5638284Z device = None
2025-04-11T04:23:18.5638289Z 
2025-04-11T04:23:18.5638427Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5638588Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5638663Z     
2025-04-11T04:23:18.5638736Z         Args:
2025-04-11T04:23:18.5638910Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5639165Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5639274Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5639346Z         """
2025-04-11T04:23:18.5639427Z         _lazy_init()
2025-04-11T04:23:18.5639522Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5639623Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5639730Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5640009Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5640150Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5640309Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5640316Z 
2025-04-11T04:23:18.5640554Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5640728Z ____________ test_context_attention[True-False-False-4-16-8-32-32] _____________
2025-04-11T04:23:18.5640732Z 
2025-04-11T04:23:18.5640885Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5641031Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5641115Z use_new_kcache_layout = True
2025-04-11T04:23:18.5641119Z 
2025-04-11T04:23:18.5641318Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5641422Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5641542Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5641679Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5641799Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5641915Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5642049Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5642189Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5642341Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5642431Z     def test_context_attention(
2025-04-11T04:23:18.5642506Z         bsz: int,
2025-04-11T04:23:18.5642586Z         block_size: int,
2025-04-11T04:23:18.5642678Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5642759Z         num_attn_heads: int,
2025-04-11T04:23:18.5642844Z         kv_group_num: int,
2025-04-11T04:23:18.5642927Z         same_context_len: bool,
2025-04-11T04:23:18.5643012Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5643098Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5643173Z     ):
2025-04-11T04:23:18.5643322Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5643608Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5643802Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5643971Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5644147Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5644221Z             return
2025-04-11T04:23:18.5644292Z     
2025-04-11T04:23:18.5644382Z         torch.manual_seed(123)
2025-04-11T04:23:18.5644478Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5644566Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5644654Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5644658Z 
2025-04-11T04:23:18.5644825Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5644942Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5644947Z 
2025-04-11T04:23:18.5645023Z device = None
2025-04-11T04:23:18.5645122Z 
2025-04-11T04:23:18.5645245Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5645394Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5645472Z     
2025-04-11T04:23:18.5645543Z         Args:
2025-04-11T04:23:18.5645709Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5645880Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5645986Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5646065Z         """
2025-04-11T04:23:18.5646144Z         _lazy_init()
2025-04-11T04:23:18.5646244Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5646342Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5646449Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5646735Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5646877Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5647041Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5647045Z 
2025-04-11T04:23:18.5647284Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5647455Z ____________ test_context_attention[True-False-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.5647459Z 
2025-04-11T04:23:18.5647609Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5647759Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5647845Z use_new_kcache_layout = True
2025-04-11T04:23:18.5647849Z 
2025-04-11T04:23:18.5648050Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5648164Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5648283Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5648435Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5648556Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5648674Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5648808Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5648942Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5649096Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5649183Z     def test_context_attention(
2025-04-11T04:23:18.5649264Z         bsz: int,
2025-04-11T04:23:18.5649346Z         block_size: int,
2025-04-11T04:23:18.5649438Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5649607Z         num_attn_heads: int,
2025-04-11T04:23:18.5649689Z         kv_group_num: int,
2025-04-11T04:23:18.5649778Z         same_context_len: bool,
2025-04-11T04:23:18.5649866Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5649955Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5650027Z     ):
2025-04-11T04:23:18.5650135Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5650331Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5650510Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5650700Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5650866Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5650944Z             return
2025-04-11T04:23:18.5651016Z     
2025-04-11T04:23:18.5651105Z         torch.manual_seed(123)
2025-04-11T04:23:18.5651210Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5651298Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5651486Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5651491Z 
2025-04-11T04:23:18.5651660Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5651774Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5651778Z 
2025-04-11T04:23:18.5651853Z device = None
2025-04-11T04:23:18.5651857Z 
2025-04-11T04:23:18.5651972Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5652124Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5652194Z     
2025-04-11T04:23:18.5652269Z         Args:
2025-04-11T04:23:18.5652436Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5652606Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5652714Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5652785Z         """
2025-04-11T04:23:18.5652891Z         _lazy_init()
2025-04-11T04:23:18.5652993Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5653106Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5653213Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5653499Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5653639Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5653799Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5653803Z 
2025-04-11T04:23:18.5654050Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5654221Z ____________ test_context_attention[True-False-False-4-16-16-16-32] ____________
2025-04-11T04:23:18.5654229Z 
2025-04-11T04:23:18.5654386Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5654534Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5654622Z use_new_kcache_layout = True
2025-04-11T04:23:18.5654626Z 
2025-04-11T04:23:18.5654827Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5654936Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5655049Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5655187Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5655302Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5655413Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5655550Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5655787Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5655937Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5656031Z     def test_context_attention(
2025-04-11T04:23:18.5656110Z         bsz: int,
2025-04-11T04:23:18.5656197Z         block_size: int,
2025-04-11T04:23:18.5656288Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5656377Z         num_attn_heads: int,
2025-04-11T04:23:18.5656461Z         kv_group_num: int,
2025-04-11T04:23:18.5656548Z         same_context_len: bool,
2025-04-11T04:23:18.5656639Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5656730Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5656808Z     ):
2025-04-11T04:23:18.5656919Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5657113Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5657301Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5657477Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5657729Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5657817Z             return
2025-04-11T04:23:18.5657901Z     
2025-04-11T04:23:18.5657993Z         torch.manual_seed(123)
2025-04-11T04:23:18.5658090Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5658189Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5658279Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5658283Z 
2025-04-11T04:23:18.5658459Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5658569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5658573Z 
2025-04-11T04:23:18.5658653Z device = None
2025-04-11T04:23:18.5658657Z 
2025-04-11T04:23:18.5658770Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5658920Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5658996Z     
2025-04-11T04:23:18.5659071Z         Args:
2025-04-11T04:23:18.5659238Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5659400Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5659508Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5659580Z         """
2025-04-11T04:23:18.5659657Z         _lazy_init()
2025-04-11T04:23:18.5659755Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5659853Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5659958Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5660238Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5660379Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5660538Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5660545Z 
2025-04-11T04:23:18.5660780Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5660952Z ____________ test_context_attention[True-False-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.5660956Z 
2025-04-11T04:23:18.5661105Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5661252Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5661334Z use_new_kcache_layout = True
2025-04-11T04:23:18.5661339Z 
2025-04-11T04:23:18.5661537Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5661639Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5661756Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5661981Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5662093Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5662212Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5662347Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5662481Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5662629Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5662716Z     def test_context_attention(
2025-04-11T04:23:18.5662792Z         bsz: int,
2025-04-11T04:23:18.5662877Z         block_size: int,
2025-04-11T04:23:18.5662973Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5663054Z         num_attn_heads: int,
2025-04-11T04:23:18.5663140Z         kv_group_num: int,
2025-04-11T04:23:18.5663221Z         same_context_len: bool,
2025-04-11T04:23:18.5663302Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5663423Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5663496Z     ):
2025-04-11T04:23:18.5663607Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5663903Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5664082Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5664255Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5664419Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5664499Z             return
2025-04-11T04:23:18.5664569Z     
2025-04-11T04:23:18.5664655Z         torch.manual_seed(123)
2025-04-11T04:23:18.5664753Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5664842Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5664935Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5664943Z 
2025-04-11T04:23:18.5665109Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5665222Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5665231Z 
2025-04-11T04:23:18.5665307Z device = None
2025-04-11T04:23:18.5665311Z 
2025-04-11T04:23:18.5665426Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5665573Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5665646Z     
2025-04-11T04:23:18.5665717Z         Args:
2025-04-11T04:23:18.5665883Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5666052Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5666155Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5666232Z         """
2025-04-11T04:23:18.5666309Z         _lazy_init()
2025-04-11T04:23:18.5666405Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5666504Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5666610Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5666894Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5667027Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5667187Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5667191Z 
2025-04-11T04:23:18.5667428Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5667600Z ____________ test_context_attention[True-False-False-4-16-16-32-32] ____________
2025-04-11T04:23:18.5667604Z 
2025-04-11T04:23:18.5667755Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5667899Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5668093Z use_new_kcache_layout = True
2025-04-11T04:23:18.5668098Z 
2025-04-11T04:23:18.5668297Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5668404Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5668554Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5668694Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5668808Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5668919Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5669058Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5669192Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5669343Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5669428Z     def test_context_attention(
2025-04-11T04:23:18.5669511Z         bsz: int,
2025-04-11T04:23:18.5669599Z         block_size: int,
2025-04-11T04:23:18.5669696Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5669891Z         num_attn_heads: int,
2025-04-11T04:23:18.5669974Z         kv_group_num: int,
2025-04-11T04:23:18.5670060Z         same_context_len: bool,
2025-04-11T04:23:18.5670141Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5670228Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5670303Z     ):
2025-04-11T04:23:18.5670412Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5670608Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5670787Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5670960Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5671123Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5671199Z             return
2025-04-11T04:23:18.5671278Z     
2025-04-11T04:23:18.5671369Z         torch.manual_seed(123)
2025-04-11T04:23:18.5671471Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5671557Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5671648Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5671657Z 
2025-04-11T04:23:18.5671823Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5671934Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5671939Z 
2025-04-11T04:23:18.5672018Z device = None
2025-04-11T04:23:18.5672022Z 
2025-04-11T04:23:18.5672137Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5672290Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5672360Z     
2025-04-11T04:23:18.5672436Z         Args:
2025-04-11T04:23:18.5672607Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5672771Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5672884Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5672956Z         """
2025-04-11T04:23:18.5676955Z         _lazy_init()
2025-04-11T04:23:18.5677049Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5677150Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5677253Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5677533Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5677673Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5677835Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5677840Z 
2025-04-11T04:23:18.5678196Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5678362Z _____________ test_context_attention[False-True-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.5678370Z 
2025-04-11T04:23:18.5678522Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5678666Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5678759Z use_new_kcache_layout = False
2025-04-11T04:23:18.5678763Z 
2025-04-11T04:23:18.5678960Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5679064Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5679185Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5679321Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5679440Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5679555Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5679701Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5680016Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5680166Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5680270Z     def test_context_attention(
2025-04-11T04:23:18.5680345Z         bsz: int,
2025-04-11T04:23:18.5680429Z         block_size: int,
2025-04-11T04:23:18.5680516Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5680596Z         num_attn_heads: int,
2025-04-11T04:23:18.5680687Z         kv_group_num: int,
2025-04-11T04:23:18.5680769Z         same_context_len: bool,
2025-04-11T04:23:18.5680853Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5680962Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5681036Z     ):
2025-04-11T04:23:18.5681141Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5681332Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5681521Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5681692Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5681856Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5681930Z             return
2025-04-11T04:23:18.5682006Z     
2025-04-11T04:23:18.5682090Z         torch.manual_seed(123)
2025-04-11T04:23:18.5682186Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5682279Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5682367Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5682371Z 
2025-04-11T04:23:18.5682538Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5682648Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5682656Z 
2025-04-11T04:23:18.5682736Z device = None
2025-04-11T04:23:18.5682741Z 
2025-04-11T04:23:18.5682856Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5683005Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5683081Z     
2025-04-11T04:23:18.5683152Z         Args:
2025-04-11T04:23:18.5683320Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5683484Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5683590Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5683665Z         """
2025-04-11T04:23:18.5683741Z         _lazy_init()
2025-04-11T04:23:18.5683838Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5683935Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5684041Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5684438Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5684576Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5684738Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5684742Z 
2025-04-11T04:23:18.5684982Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5685152Z _____________ test_context_attention[False-True-True-1-16-8-16-32] _____________
2025-04-11T04:23:18.5685156Z 
2025-04-11T04:23:18.5685305Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5685451Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5685542Z use_new_kcache_layout = False
2025-04-11T04:23:18.5685546Z 
2025-04-11T04:23:18.5685747Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5685855Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5685971Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5686194Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5686310Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5686427Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5686565Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5686713Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5686870Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5686960Z     def test_context_attention(
2025-04-11T04:23:18.5687040Z         bsz: int,
2025-04-11T04:23:18.5687121Z         block_size: int,
2025-04-11T04:23:18.5687211Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5687296Z         num_attn_heads: int,
2025-04-11T04:23:18.5687381Z         kv_group_num: int,
2025-04-11T04:23:18.5687471Z         same_context_len: bool,
2025-04-11T04:23:18.5687559Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5687658Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5687731Z     ):
2025-04-11T04:23:18.5687845Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5688034Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5688218Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5688392Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5688553Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5688633Z             return
2025-04-11T04:23:18.5688706Z     
2025-04-11T04:23:18.5688796Z         torch.manual_seed(123)
2025-04-11T04:23:18.5688899Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5688988Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5689083Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5689090Z 
2025-04-11T04:23:18.5689254Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5689369Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5689373Z 
2025-04-11T04:23:18.5689449Z device = None
2025-04-11T04:23:18.5689453Z 
2025-04-11T04:23:18.5689572Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5689719Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5689791Z     
2025-04-11T04:23:18.5689866Z         Args:
2025-04-11T04:23:18.5690031Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5690205Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5690401Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5690480Z         """
2025-04-11T04:23:18.5690558Z         _lazy_init()
2025-04-11T04:23:18.5690657Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5690760Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5690867Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5691153Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5691288Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5691445Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5691453Z 
2025-04-11T04:23:18.5691693Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5691861Z _____________ test_context_attention[False-True-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.5691869Z 
2025-04-11T04:23:18.5692023Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5692167Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5692341Z use_new_kcache_layout = False
2025-04-11T04:23:18.5692346Z 
2025-04-11T04:23:18.5692546Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5692655Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5692770Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5692909Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5693028Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5693140Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5693279Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5693414Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5693571Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5693660Z     def test_context_attention(
2025-04-11T04:23:18.5693738Z         bsz: int,
2025-04-11T04:23:18.5693824Z         block_size: int,
2025-04-11T04:23:18.5693912Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5693998Z         num_attn_heads: int,
2025-04-11T04:23:18.5694082Z         kv_group_num: int,
2025-04-11T04:23:18.5694167Z         same_context_len: bool,
2025-04-11T04:23:18.5694254Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5694342Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5694418Z     ):
2025-04-11T04:23:18.5694527Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5694720Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5694903Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5695079Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5695247Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5695325Z             return
2025-04-11T04:23:18.5695401Z     
2025-04-11T04:23:18.5695488Z         torch.manual_seed(123)
2025-04-11T04:23:18.5695590Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5695683Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5695772Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5695776Z 
2025-04-11T04:23:18.5695947Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5696069Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5696074Z 
2025-04-11T04:23:18.5696155Z device = None
2025-04-11T04:23:18.5696159Z 
2025-04-11T04:23:18.5696277Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5696430Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5696594Z     
2025-04-11T04:23:18.5696671Z         Args:
2025-04-11T04:23:18.5696849Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5697016Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5697129Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5697205Z         """
2025-04-11T04:23:18.5697292Z         _lazy_init()
2025-04-11T04:23:18.5697391Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5697497Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5697612Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5697901Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5698044Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5698212Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5698216Z 
2025-04-11T04:23:18.5698573Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5698744Z _____________ test_context_attention[False-True-True-1-16-8-32-32] _____________
2025-04-11T04:23:18.5698749Z 
2025-04-11T04:23:18.5698902Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5699051Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5699139Z use_new_kcache_layout = False
2025-04-11T04:23:18.5699145Z 
2025-04-11T04:23:18.5699350Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5699456Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5699578Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5699717Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5699837Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5699954Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5700092Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5700231Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5700385Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5700476Z     def test_context_attention(
2025-04-11T04:23:18.5700553Z         bsz: int,
2025-04-11T04:23:18.5700636Z         block_size: int,
2025-04-11T04:23:18.5700729Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5700812Z         num_attn_heads: int,
2025-04-11T04:23:18.5700899Z         kv_group_num: int,
2025-04-11T04:23:18.5700985Z         same_context_len: bool,
2025-04-11T04:23:18.5701069Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5701162Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5701239Z     ):
2025-04-11T04:23:18.5701363Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5737797Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5738098Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5738271Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5738445Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5738520Z             return
2025-04-11T04:23:18.5738593Z     
2025-04-11T04:23:18.5738709Z         torch.manual_seed(123)
2025-04-11T04:23:18.5738817Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5738908Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5738999Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5739003Z 
2025-04-11T04:23:18.5739194Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5739423Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5739431Z 
2025-04-11T04:23:18.5739510Z device = None
2025-04-11T04:23:18.5739525Z 
2025-04-11T04:23:18.5739641Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5739800Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5739877Z     
2025-04-11T04:23:18.5739957Z         Args:
2025-04-11T04:23:18.5740149Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5740347Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5740459Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5740547Z         """
2025-04-11T04:23:18.5740625Z         _lazy_init()
2025-04-11T04:23:18.5740724Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5740827Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5740942Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5741346Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5741491Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5741655Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5741660Z 
2025-04-11T04:23:18.5741902Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5742074Z _____________ test_context_attention[False-True-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.5742078Z 
2025-04-11T04:23:18.5742239Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5742384Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5742482Z use_new_kcache_layout = False
2025-04-11T04:23:18.5742487Z 
2025-04-11T04:23:18.5742703Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5742809Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5742927Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5743071Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5743186Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5743304Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5743440Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5743580Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5743732Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5743820Z     def test_context_attention(
2025-04-11T04:23:18.5743908Z         bsz: int,
2025-04-11T04:23:18.5743994Z         block_size: int,
2025-04-11T04:23:18.5744095Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5744179Z         num_attn_heads: int,
2025-04-11T04:23:18.5744273Z         kv_group_num: int,
2025-04-11T04:23:18.5744357Z         same_context_len: bool,
2025-04-11T04:23:18.5744440Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5744531Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5744613Z     ):
2025-04-11T04:23:18.5744725Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5744927Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5745116Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5745304Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5745481Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5745661Z             return
2025-04-11T04:23:18.5745741Z     
2025-04-11T04:23:18.5745831Z         torch.manual_seed(123)
2025-04-11T04:23:18.5745929Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5746021Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5746117Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5746121Z 
2025-04-11T04:23:18.5746290Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5746405Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5746410Z 
2025-04-11T04:23:18.5746486Z device = None
2025-04-11T04:23:18.5746491Z 
2025-04-11T04:23:18.5746611Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5746759Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5746831Z     
2025-04-11T04:23:18.5746907Z         Args:
2025-04-11T04:23:18.5747074Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5747246Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5747433Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5747510Z         """
2025-04-11T04:23:18.5747587Z         _lazy_init()
2025-04-11T04:23:18.5747683Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5747788Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5747893Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5748181Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5748335Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5748563Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5748568Z 
2025-04-11T04:23:18.5748819Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5749003Z ____________ test_context_attention[False-True-True-1-16-16-16-32] _____________
2025-04-11T04:23:18.5749013Z 
2025-04-11T04:23:18.5749167Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5749309Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5749400Z use_new_kcache_layout = False
2025-04-11T04:23:18.5749405Z 
2025-04-11T04:23:18.5749602Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5749706Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5749824Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5749965Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5750081Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5750193Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5750338Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5750470Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5750628Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5750716Z     def test_context_attention(
2025-04-11T04:23:18.5750792Z         bsz: int,
2025-04-11T04:23:18.5750876Z         block_size: int,
2025-04-11T04:23:18.5750965Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5751052Z         num_attn_heads: int,
2025-04-11T04:23:18.5751134Z         kv_group_num: int,
2025-04-11T04:23:18.5751222Z         same_context_len: bool,
2025-04-11T04:23:18.5751305Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5751392Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5751468Z     ):
2025-04-11T04:23:18.5751578Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5751776Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5752053Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5752231Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5752405Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5752486Z             return
2025-04-11T04:23:18.5752564Z     
2025-04-11T04:23:18.5752649Z         torch.manual_seed(123)
2025-04-11T04:23:18.5752750Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5752838Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5752931Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5752935Z 
2025-04-11T04:23:18.5753106Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5753221Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5753225Z 
2025-04-11T04:23:18.5753310Z device = None
2025-04-11T04:23:18.5753314Z 
2025-04-11T04:23:18.5753439Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5753591Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5753759Z     
2025-04-11T04:23:18.5753841Z         Args:
2025-04-11T04:23:18.5754022Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5754188Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5754295Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5754368Z         """
2025-04-11T04:23:18.5754448Z         _lazy_init()
2025-04-11T04:23:18.5754542Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5754642Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5754750Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5755029Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5755169Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5755327Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5755332Z 
2025-04-11T04:23:18.5755572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5755738Z _____________ test_context_attention[False-True-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.5755743Z 
2025-04-11T04:23:18.5755897Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5756040Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5756128Z use_new_kcache_layout = False
2025-04-11T04:23:18.5756133Z 
2025-04-11T04:23:18.5756333Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5756434Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5756557Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5756693Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5756813Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5756924Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5757059Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5757196Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5757344Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5757434Z     def test_context_attention(
2025-04-11T04:23:18.5757511Z         bsz: int,
2025-04-11T04:23:18.5757595Z         block_size: int,
2025-04-11T04:23:18.5757682Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5757764Z         num_attn_heads: int,
2025-04-11T04:23:18.5757848Z         kv_group_num: int,
2025-04-11T04:23:18.5758007Z         same_context_len: bool,
2025-04-11T04:23:18.5758094Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5758181Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5758257Z     ):
2025-04-11T04:23:18.5758371Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5758563Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5758746Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5758916Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5759080Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5759154Z             return
2025-04-11T04:23:18.5759225Z     
2025-04-11T04:23:18.5759312Z         torch.manual_seed(123)
2025-04-11T04:23:18.5759409Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5759503Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5759591Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5759595Z 
2025-04-11T04:23:18.5759760Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5759962Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5759967Z 
2025-04-11T04:23:18.5760044Z device = None
2025-04-11T04:23:18.5760048Z 
2025-04-11T04:23:18.5760183Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5760339Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5760415Z     
2025-04-11T04:23:18.5760488Z         Args:
2025-04-11T04:23:18.5760658Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5760825Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5760928Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5761009Z         """
2025-04-11T04:23:18.5761088Z         _lazy_init()
2025-04-11T04:23:18.5761187Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5761291Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5761396Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5761680Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5761812Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5761974Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5761978Z 
2025-04-11T04:23:18.5762218Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5762391Z ____________ test_context_attention[False-True-True-1-16-16-32-32] _____________
2025-04-11T04:23:18.5762394Z 
2025-04-11T04:23:18.5762554Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5762698Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5762795Z use_new_kcache_layout = False
2025-04-11T04:23:18.5762799Z 
2025-04-11T04:23:18.5762997Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5763103Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5763216Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5763356Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5763470Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5763581Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5763719Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5763850Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5764003Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5764195Z     def test_context_attention(
2025-04-11T04:23:18.5764274Z         bsz: int,
2025-04-11T04:23:18.5764360Z         block_size: int,
2025-04-11T04:23:18.5764450Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5764539Z         num_attn_heads: int,
2025-04-11T04:23:18.5764621Z         kv_group_num: int,
2025-04-11T04:23:18.5764713Z         same_context_len: bool,
2025-04-11T04:23:18.5764804Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5764894Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5764990Z     ):
2025-04-11T04:23:18.5765101Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5765313Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5765503Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5765710Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5765886Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5766075Z             return
2025-04-11T04:23:18.5766152Z     
2025-04-11T04:23:18.5766238Z         torch.manual_seed(123)
2025-04-11T04:23:18.5766339Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5766428Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5766521Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5766524Z 
2025-04-11T04:23:18.5766691Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5766800Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5766808Z 
2025-04-11T04:23:18.5766885Z device = None
2025-04-11T04:23:18.5766888Z 
2025-04-11T04:23:18.5767002Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5767154Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5767229Z     
2025-04-11T04:23:18.5767306Z         Args:
2025-04-11T04:23:18.5767471Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5767638Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5767743Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5767816Z         """
2025-04-11T04:23:18.5767896Z         _lazy_init()
2025-04-11T04:23:18.5767990Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5768093Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5768195Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5768474Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5768611Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5768768Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5768776Z 
2025-04-11T04:23:18.5769019Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5769187Z _____________ test_context_attention[False-True-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.5769192Z 
2025-04-11T04:23:18.5769344Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5769501Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5769602Z use_new_kcache_layout = False
2025-04-11T04:23:18.5769607Z 
2025-04-11T04:23:18.5769818Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5769921Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5770050Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5770188Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5770400Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5770513Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5770657Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5770793Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5770946Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5771037Z     def test_context_attention(
2025-04-11T04:23:18.5771114Z         bsz: int,
2025-04-11T04:23:18.5771196Z         block_size: int,
2025-04-11T04:23:18.5771285Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5771367Z         num_attn_heads: int,
2025-04-11T04:23:18.5771453Z         kv_group_num: int,
2025-04-11T04:23:18.5771539Z         same_context_len: bool,
2025-04-11T04:23:18.5771625Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5771712Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5771789Z     ):
2025-04-11T04:23:18.5771903Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5772095Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5772366Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5772536Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5772704Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5772779Z             return
2025-04-11T04:23:18.5772855Z     
2025-04-11T04:23:18.5772941Z         torch.manual_seed(123)
2025-04-11T04:23:18.5773039Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5773132Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5773221Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5773226Z 
2025-04-11T04:23:18.5773397Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5773511Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5773515Z 
2025-04-11T04:23:18.5773595Z device = None
2025-04-11T04:23:18.5773605Z 
2025-04-11T04:23:18.5773722Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5773871Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5773946Z     
2025-04-11T04:23:18.5774019Z         Args:
2025-04-11T04:23:18.5774190Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5774359Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5774471Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5774561Z         """
2025-04-11T04:23:18.5774641Z         _lazy_init()
2025-04-11T04:23:18.5774747Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5774850Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5774981Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5775287Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5775428Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5775591Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5775596Z 
2025-04-11T04:23:18.5775833Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5776001Z _____________ test_context_attention[False-True-True-4-16-8-16-32] _____________
2025-04-11T04:23:18.5776006Z 
2025-04-11T04:23:18.5776154Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5776302Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5776389Z use_new_kcache_layout = False
2025-04-11T04:23:18.5776479Z 
2025-04-11T04:23:18.5776683Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5776788Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5776908Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5777050Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5777167Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5777283Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5777420Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5777558Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5777711Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5777800Z     def test_context_attention(
2025-04-11T04:23:18.5777880Z         bsz: int,
2025-04-11T04:23:18.5777962Z         block_size: int,
2025-04-11T04:23:18.5778056Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5778142Z         num_attn_heads: int,
2025-04-11T04:23:18.5778223Z         kv_group_num: int,
2025-04-11T04:23:18.5778312Z         same_context_len: bool,
2025-04-11T04:23:18.5778481Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5778573Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5778647Z     ):
2025-04-11T04:23:18.5778760Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5778954Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5779135Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5779312Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5779474Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5779553Z             return
2025-04-11T04:23:18.5779626Z     
2025-04-11T04:23:18.5779721Z         torch.manual_seed(123)
2025-04-11T04:23:18.5779820Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5779907Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5780028Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5780033Z 
2025-04-11T04:23:18.5780208Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5780323Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5780327Z 
2025-04-11T04:23:18.5780406Z device = None
2025-04-11T04:23:18.5780410Z 
2025-04-11T04:23:18.5780534Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5780684Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5780755Z     
2025-04-11T04:23:18.5780836Z         Args:
2025-04-11T04:23:18.5781009Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5781186Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5781295Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5781382Z         """
2025-04-11T04:23:18.5781460Z         _lazy_init()
2025-04-11T04:23:18.5781555Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5781658Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5781760Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5782047Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5782180Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5782338Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5782347Z 
2025-04-11T04:23:18.5782587Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5782838Z _____________ test_context_attention[False-True-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.5782842Z 
2025-04-11T04:23:18.5782997Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5783144Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5783235Z use_new_kcache_layout = False
2025-04-11T04:23:18.5783240Z 
2025-04-11T04:23:18.5783435Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5783542Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5783657Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5783795Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5783915Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5784026Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5784165Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5784301Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5784455Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5784624Z     def test_context_attention(
2025-04-11T04:23:18.5784702Z         bsz: int,
2025-04-11T04:23:18.5784793Z         block_size: int,
2025-04-11T04:23:18.5784889Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5784975Z         num_attn_heads: int,
2025-04-11T04:23:18.5785058Z         kv_group_num: int,
2025-04-11T04:23:18.5785146Z         same_context_len: bool,
2025-04-11T04:23:18.5785236Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5785323Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5785406Z     ):
2025-04-11T04:23:18.5785516Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5785709Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5785890Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5786062Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5786233Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5786314Z             return
2025-04-11T04:23:18.5786405Z     
2025-04-11T04:23:18.5786495Z         torch.manual_seed(123)
2025-04-11T04:23:18.5786595Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5786681Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5786769Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5786773Z 
2025-04-11T04:23:18.5786943Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5787053Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5787057Z 
2025-04-11T04:23:18.5787138Z device = None
2025-04-11T04:23:18.5787142Z 
2025-04-11T04:23:18.5787256Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5787413Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5787488Z     
2025-04-11T04:23:18.5787560Z         Args:
2025-04-11T04:23:18.5787727Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5787891Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5787999Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5788073Z         """
2025-04-11T04:23:18.5788153Z         _lazy_init()
2025-04-11T04:23:18.5788249Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5788346Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5788502Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5788793Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5789055Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5789214Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5789221Z 
2025-04-11T04:23:18.5789469Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5789637Z _____________ test_context_attention[False-True-True-4-16-8-32-32] _____________
2025-04-11T04:23:18.5789641Z 
2025-04-11T04:23:18.5789796Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5789946Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5790033Z use_new_kcache_layout = False
2025-04-11T04:23:18.5790038Z 
2025-04-11T04:23:18.5790255Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5790358Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5790475Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5790630Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5790749Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5790999Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5791136Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5791285Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5791435Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5791526Z     def test_context_attention(
2025-04-11T04:23:18.5791602Z         bsz: int,
2025-04-11T04:23:18.5791689Z         block_size: int,
2025-04-11T04:23:18.5791780Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5791864Z         num_attn_heads: int,
2025-04-11T04:23:18.5791949Z         kv_group_num: int,
2025-04-11T04:23:18.5792032Z         same_context_len: bool,
2025-04-11T04:23:18.5792123Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5792212Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5792285Z     ):
2025-04-11T04:23:18.5792399Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5792591Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5792776Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5792946Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5793113Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5793187Z             return
2025-04-11T04:23:18.5793259Z     
2025-04-11T04:23:18.5793348Z         torch.manual_seed(123)
2025-04-11T04:23:18.5793445Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5793534Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5793624Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5793632Z 
2025-04-11T04:23:18.5793797Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5793914Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5793919Z 
2025-04-11T04:23:18.5793996Z device = None
2025-04-11T04:23:18.5793999Z 
2025-04-11T04:23:18.5794119Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5794267Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5794340Z     
2025-04-11T04:23:18.5794413Z         Args:
2025-04-11T04:23:18.5794580Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5794749Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5794852Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5794941Z         """
2025-04-11T04:23:18.5795025Z         _lazy_init()
2025-04-11T04:23:18.5795227Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5795334Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5795444Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5795734Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5795872Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5796031Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5796036Z 
2025-04-11T04:23:18.5796268Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5796436Z _____________ test_context_attention[False-True-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.5796440Z 
2025-04-11T04:23:18.5796591Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5796739Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5796826Z use_new_kcache_layout = False
2025-04-11T04:23:18.5796830Z 
2025-04-11T04:23:18.5797110Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5797217Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5797334Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5797475Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5797591Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5797706Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5797841Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5797975Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5798129Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5798215Z     def test_context_attention(
2025-04-11T04:23:18.5798296Z         bsz: int,
2025-04-11T04:23:18.5798378Z         block_size: int,
2025-04-11T04:23:18.5798470Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5798555Z         num_attn_heads: int,
2025-04-11T04:23:18.5798638Z         kv_group_num: int,
2025-04-11T04:23:18.5798725Z         same_context_len: bool,
2025-04-11T04:23:18.5798808Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5798906Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5798986Z     ):
2025-04-11T04:23:18.5799106Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5799301Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5799480Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5799654Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5799816Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5799896Z             return
2025-04-11T04:23:18.5799968Z     
2025-04-11T04:23:18.5800057Z         torch.manual_seed(123)
2025-04-11T04:23:18.5800158Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5800245Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5800337Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5800341Z 
2025-04-11T04:23:18.5800508Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5800621Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5800626Z 
2025-04-11T04:23:18.5800702Z device = None
2025-04-11T04:23:18.5800706Z 
2025-04-11T04:23:18.5800823Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5800974Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5801046Z     
2025-04-11T04:23:18.5801129Z         Args:
2025-04-11T04:23:18.5801378Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5801554Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5801662Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5801756Z         """
2025-04-11T04:23:18.5801838Z         _lazy_init()
2025-04-11T04:23:18.5801932Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5802036Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5802139Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5802418Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5802555Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5802716Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5802724Z 
2025-04-11T04:23:18.5802962Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5803130Z ____________ test_context_attention[False-True-True-4-16-16-16-32] _____________
2025-04-11T04:23:18.5803215Z 
2025-04-11T04:23:18.5803371Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5803510Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5803601Z use_new_kcache_layout = False
2025-04-11T04:23:18.5803605Z 
2025-04-11T04:23:18.5803803Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5803909Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5804026Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5804161Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5804280Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5804398Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5804536Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5804673Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5804823Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5804913Z     def test_context_attention(
2025-04-11T04:23:18.5805002Z         bsz: int,
2025-04-11T04:23:18.5805101Z         block_size: int,
2025-04-11T04:23:18.5805190Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5805280Z         num_attn_heads: int,
2025-04-11T04:23:18.5805368Z         kv_group_num: int,
2025-04-11T04:23:18.5805454Z         same_context_len: bool,
2025-04-11T04:23:18.5805553Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5805639Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5805718Z     ):
2025-04-11T04:23:18.5805825Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5806020Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5806203Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5806378Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5806542Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5806616Z             return
2025-04-11T04:23:18.5806692Z     
2025-04-11T04:23:18.5806778Z         torch.manual_seed(123)
2025-04-11T04:23:18.5806878Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5806971Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5807061Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5807065Z 
2025-04-11T04:23:18.5807237Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5807347Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5807442Z 
2025-04-11T04:23:18.5807526Z device = None
2025-04-11T04:23:18.5807531Z 
2025-04-11T04:23:18.5807648Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5807800Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5807877Z     
2025-04-11T04:23:18.5807951Z         Args:
2025-04-11T04:23:18.5808120Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5808286Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5808396Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5808470Z         """
2025-04-11T04:23:18.5808549Z         _lazy_init()
2025-04-11T04:23:18.5808648Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5808749Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5808856Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5809142Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5809373Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5809539Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5809544Z 
2025-04-11T04:23:18.5809792Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5809961Z _____________ test_context_attention[False-True-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.5809966Z 
2025-04-11T04:23:18.5810117Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5810268Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5810360Z use_new_kcache_layout = False
2025-04-11T04:23:18.5810365Z 
2025-04-11T04:23:18.5810566Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5810674Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5810802Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5810943Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5811058Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5811182Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5811329Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5811477Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5811628Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5811719Z     def test_context_attention(
2025-04-11T04:23:18.5811795Z         bsz: int,
2025-04-11T04:23:18.5811876Z         block_size: int,
2025-04-11T04:23:18.5811968Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5812050Z         num_attn_heads: int,
2025-04-11T04:23:18.5812136Z         kv_group_num: int,
2025-04-11T04:23:18.5812220Z         same_context_len: bool,
2025-04-11T04:23:18.5812305Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5812402Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5812475Z     ):
2025-04-11T04:23:18.5812590Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5812781Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5812959Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5813131Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5813293Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5813373Z             return
2025-04-11T04:23:18.5813445Z     
2025-04-11T04:23:18.5813534Z         torch.manual_seed(123)
2025-04-11T04:23:18.5813838Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5813927Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5814024Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5814033Z 
2025-04-11T04:23:18.5814200Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5814315Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5814320Z 
2025-04-11T04:23:18.5814398Z device = None
2025-04-11T04:23:18.5814403Z 
2025-04-11T04:23:18.5814524Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5814673Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5814749Z     
2025-04-11T04:23:18.5814823Z         Args:
2025-04-11T04:23:18.5814990Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5815160Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5815268Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5815345Z         """
2025-04-11T04:23:18.5815424Z         _lazy_init()
2025-04-11T04:23:18.5815602Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5815707Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5815812Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5816119Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5816258Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5816419Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5816424Z 
2025-04-11T04:23:18.5816669Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5816841Z ____________ test_context_attention[False-True-True-4-16-16-32-32] _____________
2025-04-11T04:23:18.5816853Z 
2025-04-11T04:23:18.5817020Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5817166Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5817266Z use_new_kcache_layout = False
2025-04-11T04:23:18.5817271Z 
2025-04-11T04:23:18.5817479Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5817593Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5817710Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5817849Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5817965Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5818081Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5818221Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5818355Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5818512Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5818599Z     def test_context_attention(
2025-04-11T04:23:18.5818680Z         bsz: int,
2025-04-11T04:23:18.5818761Z         block_size: int,
2025-04-11T04:23:18.5818852Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5818938Z         num_attn_heads: int,
2025-04-11T04:23:18.5819020Z         kv_group_num: int,
2025-04-11T04:23:18.5819106Z         same_context_len: bool,
2025-04-11T04:23:18.5819190Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5819279Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5819357Z     ):
2025-04-11T04:23:18.5819466Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5819664Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5819846Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5820119Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5820291Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5820369Z             return
2025-04-11T04:23:18.5820451Z     
2025-04-11T04:23:18.5820541Z         torch.manual_seed(123)
2025-04-11T04:23:18.5820641Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5820729Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5820819Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5820827Z 
2025-04-11T04:23:18.5820995Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5821109Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5821113Z 
2025-04-11T04:23:18.5821194Z device = None
2025-04-11T04:23:18.5821199Z 
2025-04-11T04:23:18.5821314Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5821472Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5821543Z     
2025-04-11T04:23:18.5821622Z         Args:
2025-04-11T04:23:18.5821872Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5822038Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5822149Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5822223Z         """
2025-04-11T04:23:18.5822306Z         _lazy_init()
2025-04-11T04:23:18.5822400Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5822501Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5822611Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5822897Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5823039Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5823207Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5823211Z 
2025-04-11T04:23:18.5823470Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5823639Z _____________ test_context_attention[False-True-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.5823643Z 
2025-04-11T04:23:18.5823797Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5823944Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5824033Z use_new_kcache_layout = False
2025-04-11T04:23:18.5824041Z 
2025-04-11T04:23:18.5824242Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5824345Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5824468Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5824610Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5824731Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5824847Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5824982Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5825119Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5825269Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5825360Z     def test_context_attention(
2025-04-11T04:23:18.5825437Z         bsz: int,
2025-04-11T04:23:18.5825521Z         block_size: int,
2025-04-11T04:23:18.5825610Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5825693Z         num_attn_heads: int,
2025-04-11T04:23:18.5825778Z         kv_group_num: int,
2025-04-11T04:23:18.5825862Z         same_context_len: bool,
2025-04-11T04:23:18.5825949Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5826035Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5826212Z     ):
2025-04-11T04:23:18.5826328Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5826527Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5826715Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5826885Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5827051Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5827124Z             return
2025-04-11T04:23:18.5827196Z     
2025-04-11T04:23:18.5827289Z         torch.manual_seed(123)
2025-04-11T04:23:18.5827386Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5827480Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5827571Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5827576Z 
2025-04-11T04:23:18.5827747Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5827859Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5827946Z 
2025-04-11T04:23:18.5828026Z device = None
2025-04-11T04:23:18.5828030Z 
2025-04-11T04:23:18.5828164Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5828313Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5828401Z     
2025-04-11T04:23:18.5828516Z         Args:
2025-04-11T04:23:18.5828686Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5828848Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5828952Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5829033Z         """
2025-04-11T04:23:18.5829108Z         _lazy_init()
2025-04-11T04:23:18.5829207Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5829311Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5829419Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5829704Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5829843Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5830014Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5830018Z 
2025-04-11T04:23:18.5830260Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5830451Z ____________ test_context_attention[False-True-False-1-16-8-16-32] _____________
2025-04-11T04:23:18.5830455Z 
2025-04-11T04:23:18.5830612Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5830769Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5830869Z use_new_kcache_layout = False
2025-04-11T04:23:18.5830873Z 
2025-04-11T04:23:18.5831079Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5831185Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5831301Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5831442Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5831556Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5831673Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5831809Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5831945Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5832096Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5832184Z     def test_context_attention(
2025-04-11T04:23:18.5832263Z         bsz: int,
2025-04-11T04:23:18.5832439Z         block_size: int,
2025-04-11T04:23:18.5832534Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5832616Z         num_attn_heads: int,
2025-04-11T04:23:18.5832702Z         kv_group_num: int,
2025-04-11T04:23:18.5832793Z         same_context_len: bool,
2025-04-11T04:23:18.5832877Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5832968Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5833042Z     ):
2025-04-11T04:23:18.5833151Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5833344Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5833523Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5833698Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5833860Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5833941Z             return
2025-04-11T04:23:18.5834015Z     
2025-04-11T04:23:18.5834101Z         torch.manual_seed(123)
2025-04-11T04:23:18.5834316Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5834404Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5834502Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5834506Z 
2025-04-11T04:23:18.5834672Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5834793Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5834797Z 
2025-04-11T04:23:18.5834882Z device = None
2025-04-11T04:23:18.5834886Z 
2025-04-11T04:23:18.5835016Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5835170Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5835246Z     
2025-04-11T04:23:18.5835325Z         Args:
2025-04-11T04:23:18.5835490Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5835662Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5835771Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5835844Z         """
2025-04-11T04:23:18.5835924Z         _lazy_init()
2025-04-11T04:23:18.5836019Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5836124Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5836227Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5836516Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5836651Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5836812Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5836820Z 
2025-04-11T04:23:18.5837061Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5837233Z _____________ test_context_attention[False-True-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.5837240Z 
2025-04-11T04:23:18.5837393Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5837540Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5837645Z use_new_kcache_layout = False
2025-04-11T04:23:18.5837649Z 
2025-04-11T04:23:18.5837849Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5837956Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5838073Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5838217Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5838343Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5838455Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5838684Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5838819Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5838975Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5839063Z     def test_context_attention(
2025-04-11T04:23:18.5839138Z         bsz: int,
2025-04-11T04:23:18.5839226Z         block_size: int,
2025-04-11T04:23:18.5839315Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5839400Z         num_attn_heads: int,
2025-04-11T04:23:18.5839482Z         kv_group_num: int,
2025-04-11T04:23:18.5839564Z         same_context_len: bool,
2025-04-11T04:23:18.5839651Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5839741Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5839825Z     ):
2025-04-11T04:23:18.5839940Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5840131Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5840329Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5840608Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5840776Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5840855Z             return
2025-04-11T04:23:18.5840932Z     
2025-04-11T04:23:18.5841020Z         torch.manual_seed(123)
2025-04-11T04:23:18.5841124Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5841220Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5841310Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5841315Z 
2025-04-11T04:23:18.5841484Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5841596Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5841600Z 
2025-04-11T04:23:18.5841683Z device = None
2025-04-11T04:23:18.5841687Z 
2025-04-11T04:23:18.5841804Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5841956Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5842031Z     
2025-04-11T04:23:18.5842104Z         Args:
2025-04-11T04:23:18.5842272Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5842435Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5842555Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5842628Z         """
2025-04-11T04:23:18.5842706Z         _lazy_init()
2025-04-11T04:23:18.5842812Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5842914Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5843022Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5843307Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5843448Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5843609Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5843614Z 
2025-04-11T04:23:18.5843856Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5844024Z ____________ test_context_attention[False-True-False-1-16-8-32-32] _____________
2025-04-11T04:23:18.5844028Z 
2025-04-11T04:23:18.5844177Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5844330Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5844423Z use_new_kcache_layout = False
2025-04-11T04:23:18.5844428Z 
2025-04-11T04:23:18.5844630Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5844827Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5844952Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5845091Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5845218Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5845333Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5845470Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5845607Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5845758Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5845848Z     def test_context_attention(
2025-04-11T04:23:18.5845924Z         bsz: int,
2025-04-11T04:23:18.5846005Z         block_size: int,
2025-04-11T04:23:18.5846098Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5846182Z         num_attn_heads: int,
2025-04-11T04:23:18.5846270Z         kv_group_num: int,
2025-04-11T04:23:18.5846358Z         same_context_len: bool,
2025-04-11T04:23:18.5846441Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5846534Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5846691Z     ):
2025-04-11T04:23:18.5846807Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5847004Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5847189Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5847370Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5847542Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5847628Z             return
2025-04-11T04:23:18.5847700Z     
2025-04-11T04:23:18.5847788Z         torch.manual_seed(123)
2025-04-11T04:23:18.5847887Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5847982Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5848073Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5848077Z 
2025-04-11T04:23:18.5848242Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5848364Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5848368Z 
2025-04-11T04:23:18.5848448Z device = None
2025-04-11T04:23:18.5848452Z 
2025-04-11T04:23:18.5848574Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5848724Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5848798Z     
2025-04-11T04:23:18.5848873Z         Args:
2025-04-11T04:23:18.5849042Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5849214Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5849323Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5849416Z         """
2025-04-11T04:23:18.5849493Z         _lazy_init()
2025-04-11T04:23:18.5849592Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5849698Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5849805Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5850095Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5850230Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5850390Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5850394Z 
2025-04-11T04:23:18.5850634Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5850807Z ____________ test_context_attention[False-True-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.5850811Z 
2025-04-11T04:23:18.5851046Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5851192Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5851288Z use_new_kcache_layout = False
2025-04-11T04:23:18.5851293Z 
2025-04-11T04:23:18.5851492Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5851599Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5851714Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5851853Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5851967Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5852080Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5852225Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5852366Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5852518Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5852610Z     def test_context_attention(
2025-04-11T04:23:18.5852689Z         bsz: int,
2025-04-11T04:23:18.5852856Z         block_size: int,
2025-04-11T04:23:18.5852948Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5853036Z         num_attn_heads: int,
2025-04-11T04:23:18.5853118Z         kv_group_num: int,
2025-04-11T04:23:18.5853206Z         same_context_len: bool,
2025-04-11T04:23:18.5853289Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5853377Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5853455Z     ):
2025-04-11T04:23:18.5853564Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5853763Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5853942Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5854112Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5854280Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5854359Z             return
2025-04-11T04:23:18.5854438Z     
2025-04-11T04:23:18.5854525Z         torch.manual_seed(123)
2025-04-11T04:23:18.5854627Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5854715Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5854815Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5854819Z 
2025-04-11T04:23:18.5854989Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5855101Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5855116Z 
2025-04-11T04:23:18.5855193Z device = None
2025-04-11T04:23:18.5855198Z 
2025-04-11T04:23:18.5855314Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5855473Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5855548Z     
2025-04-11T04:23:18.5855627Z         Args:
2025-04-11T04:23:18.5855794Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5855961Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5856069Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5856141Z         """
2025-04-11T04:23:18.5856222Z         _lazy_init()
2025-04-11T04:23:18.5856316Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5856419Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5856525Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5856805Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5856943Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5857102Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5857190Z 
2025-04-11T04:23:18.5857438Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5857610Z ____________ test_context_attention[False-True-False-1-16-16-16-32] ____________
2025-04-11T04:23:18.5857615Z 
2025-04-11T04:23:18.5857768Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5857915Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5858005Z use_new_kcache_layout = False
2025-04-11T04:23:18.5858009Z 
2025-04-11T04:23:18.5858206Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5858310Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5858433Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5858570Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5858692Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5858808Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5859048Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5859183Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5859332Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5859424Z     def test_context_attention(
2025-04-11T04:23:18.5859498Z         bsz: int,
2025-04-11T04:23:18.5859582Z         block_size: int,
2025-04-11T04:23:18.5859672Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5859753Z         num_attn_heads: int,
2025-04-11T04:23:18.5859839Z         kv_group_num: int,
2025-04-11T04:23:18.5859925Z         same_context_len: bool,
2025-04-11T04:23:18.5860012Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5860099Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5860175Z     ):
2025-04-11T04:23:18.5860293Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5860489Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5860685Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5860853Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5861019Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5861093Z             return
2025-04-11T04:23:18.5861167Z     
2025-04-11T04:23:18.5861262Z         torch.manual_seed(123)
2025-04-11T04:23:18.5861360Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5861451Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5861540Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5861544Z 
2025-04-11T04:23:18.5861714Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5861828Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5861832Z 
2025-04-11T04:23:18.5861917Z device = None
2025-04-11T04:23:18.5861921Z 
2025-04-11T04:23:18.5862035Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5862181Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5862258Z     
2025-04-11T04:23:18.5862331Z         Args:
2025-04-11T04:23:18.5862501Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5862663Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5862767Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5862843Z         """
2025-04-11T04:23:18.5862921Z         _lazy_init()
2025-04-11T04:23:18.5863017Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5863117Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5863320Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5863614Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5863753Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5863915Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5863919Z 
2025-04-11T04:23:18.5864157Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5864330Z ____________ test_context_attention[False-True-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.5864335Z 
2025-04-11T04:23:18.5864482Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5864632Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5864720Z use_new_kcache_layout = False
2025-04-11T04:23:18.5864727Z 
2025-04-11T04:23:18.5864928Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5865119Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5865238Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5865380Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5865495Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5865614Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5865753Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5865895Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5866047Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5866140Z     def test_context_attention(
2025-04-11T04:23:18.5866221Z         bsz: int,
2025-04-11T04:23:18.5866303Z         block_size: int,
2025-04-11T04:23:18.5866400Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5866483Z         num_attn_heads: int,
2025-04-11T04:23:18.5866564Z         kv_group_num: int,
2025-04-11T04:23:18.5866656Z         same_context_len: bool,
2025-04-11T04:23:18.5866741Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5866833Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5866906Z     ):
2025-04-11T04:23:18.5867023Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5867215Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5867395Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5867568Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5867732Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5867811Z             return
2025-04-11T04:23:18.5867889Z     
2025-04-11T04:23:18.5867975Z         torch.manual_seed(123)
2025-04-11T04:23:18.5868073Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5868164Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5868258Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5868262Z 
2025-04-11T04:23:18.5868466Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5868582Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5868586Z 
2025-04-11T04:23:18.5868663Z device = None
2025-04-11T04:23:18.5868667Z 
2025-04-11T04:23:18.5868786Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5868936Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5869007Z     
2025-04-11T04:23:18.5869084Z         Args:
2025-04-11T04:23:18.5869249Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5869512Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5869618Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5869698Z         """
2025-04-11T04:23:18.5869778Z         _lazy_init()
2025-04-11T04:23:18.5869872Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5869976Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5870078Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5870362Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5870498Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5870662Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5870670Z 
2025-04-11T04:23:18.5870911Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5871091Z ____________ test_context_attention[False-True-False-1-16-16-32-32] ____________
2025-04-11T04:23:18.5871095Z 
2025-04-11T04:23:18.5871249Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5871493Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5871591Z use_new_kcache_layout = False
2025-04-11T04:23:18.5871595Z 
2025-04-11T04:23:18.5871792Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5871897Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5872018Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5872156Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5872275Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5872387Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5872526Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5872663Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5872812Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5872903Z     def test_context_attention(
2025-04-11T04:23:18.5872980Z         bsz: int,
2025-04-11T04:23:18.5873065Z         block_size: int,
2025-04-11T04:23:18.5873153Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5873238Z         num_attn_heads: int,
2025-04-11T04:23:18.5873319Z         kv_group_num: int,
2025-04-11T04:23:18.5873403Z         same_context_len: bool,
2025-04-11T04:23:18.5873489Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5873575Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5873652Z     ):
2025-04-11T04:23:18.5873761Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5873954Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5874139Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5874309Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5874478Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5874552Z             return
2025-04-11T04:23:18.5874627Z     
2025-04-11T04:23:18.5874713Z         torch.manual_seed(123)
2025-04-11T04:23:18.5874812Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5874903Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5874992Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5874996Z 
2025-04-11T04:23:18.5875165Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5875274Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5875277Z 
2025-04-11T04:23:18.5875360Z device = None
2025-04-11T04:23:18.5875364Z 
2025-04-11T04:23:18.5875565Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5875718Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5875794Z     
2025-04-11T04:23:18.5875870Z         Args:
2025-04-11T04:23:18.5876043Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5876213Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5876325Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5876401Z         """
2025-04-11T04:23:18.5876488Z         _lazy_init()
2025-04-11T04:23:18.5876587Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5876689Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5876795Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5877078Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5877221Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5877380Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5877467Z 
2025-04-11T04:23:18.5877714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5877881Z _____________ test_context_attention[False-True-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.5877886Z 
2025-04-11T04:23:18.5878035Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5878187Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5878275Z use_new_kcache_layout = False
2025-04-11T04:23:18.5878279Z 
2025-04-11T04:23:18.5878482Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5878585Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5878711Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5878848Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5878967Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5879083Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5879216Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5879353Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5879503Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5879594Z     def test_context_attention(
2025-04-11T04:23:18.5879671Z         bsz: int,
2025-04-11T04:23:18.5879752Z         block_size: int,
2025-04-11T04:23:18.5879845Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5879927Z         num_attn_heads: int,
2025-04-11T04:23:18.5880013Z         kv_group_num: int,
2025-04-11T04:23:18.5880098Z         same_context_len: bool,
2025-04-11T04:23:18.5880186Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5880277Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5880348Z     ):
2025-04-11T04:23:18.5880465Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5880656Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5880842Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5881011Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5881173Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5881251Z             return
2025-04-11T04:23:18.5881324Z     
2025-04-11T04:23:18.5881427Z         torch.manual_seed(123)
2025-04-11T04:23:18.5881527Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5881624Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5881825Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5881829Z 
2025-04-11T04:23:18.5881998Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5882115Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5882119Z 
2025-04-11T04:23:18.5882196Z device = None
2025-04-11T04:23:18.5882200Z 
2025-04-11T04:23:18.5882319Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5882466Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5882540Z     
2025-04-11T04:23:18.5882614Z         Args:
2025-04-11T04:23:18.5882778Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5882942Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5883047Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5883123Z         """
2025-04-11T04:23:18.5883204Z         _lazy_init()
2025-04-11T04:23:18.5883300Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5883401Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5883607Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5883892Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5884024Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5884185Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5884189Z 
2025-04-11T04:23:18.5884430Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5884601Z ____________ test_context_attention[False-True-False-4-16-8-16-32] _____________
2025-04-11T04:23:18.5884606Z 
2025-04-11T04:23:18.5884754Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5884904Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5884994Z use_new_kcache_layout = False
2025-04-11T04:23:18.5885002Z 
2025-04-11T04:23:18.5885196Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5885304Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5885420Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5885559Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5885674Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5885788Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5885922Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5886054Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5886207Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5886298Z     def test_context_attention(
2025-04-11T04:23:18.5886375Z         bsz: int,
2025-04-11T04:23:18.5886457Z         block_size: int,
2025-04-11T04:23:18.5886550Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5886643Z         num_attn_heads: int,
2025-04-11T04:23:18.5886727Z         kv_group_num: int,
2025-04-11T04:23:18.5886822Z         same_context_len: bool,
2025-04-11T04:23:18.5886910Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5886996Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5887073Z     ):
2025-04-11T04:23:18.5887183Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5887380Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5887562Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5887733Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5887992Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5888071Z             return
2025-04-11T04:23:18.5888146Z     
2025-04-11T04:23:18.5888232Z         torch.manual_seed(123)
2025-04-11T04:23:18.5888335Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5888424Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5888515Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5888519Z 
2025-04-11T04:23:18.5888687Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5888795Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5888804Z 
2025-04-11T04:23:18.5888881Z device = None
2025-04-11T04:23:18.5888885Z 
2025-04-11T04:23:18.5888999Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5889152Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5889225Z     
2025-04-11T04:23:18.5889305Z         Args:
2025-04-11T04:23:18.5889470Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5889633Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5889827Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5889901Z         """
2025-04-11T04:23:18.5889983Z         _lazy_init()
2025-04-11T04:23:18.5890077Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5890182Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5890284Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5890567Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5890704Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5890863Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5890871Z 
2025-04-11T04:23:18.5891115Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5891286Z _____________ test_context_attention[False-True-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.5891291Z 
2025-04-11T04:23:18.5891445Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5891591Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5891683Z use_new_kcache_layout = False
2025-04-11T04:23:18.5891688Z 
2025-04-11T04:23:18.5891892Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5892005Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5892137Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5892279Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5892395Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5892510Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5892648Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5892786Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5892936Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5893027Z     def test_context_attention(
2025-04-11T04:23:18.5893102Z         bsz: int,
2025-04-11T04:23:18.5893186Z         block_size: int,
2025-04-11T04:23:18.5893274Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5893355Z         num_attn_heads: int,
2025-04-11T04:23:18.5893440Z         kv_group_num: int,
2025-04-11T04:23:18.5893523Z         same_context_len: bool,
2025-04-11T04:23:18.5893610Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5893696Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5893772Z     ):
2025-04-11T04:23:18.5893879Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5894157Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5894342Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5894513Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5894678Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5894752Z             return
2025-04-11T04:23:18.5894826Z     
2025-04-11T04:23:18.5894911Z         torch.manual_seed(123)
2025-04-11T04:23:18.5895009Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5895101Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5895192Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5895196Z 
2025-04-11T04:23:18.5895366Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5895475Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5895482Z 
2025-04-11T04:23:18.5895562Z device = None
2025-04-11T04:23:18.5895566Z 
2025-04-11T04:23:18.5895768Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5895920Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5895995Z     
2025-04-11T04:23:18.5896071Z         Args:
2025-04-11T04:23:18.5896240Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5896405Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5896509Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5896585Z         """
2025-04-11T04:23:18.5896662Z         _lazy_init()
2025-04-11T04:23:18.5896760Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5896860Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5896966Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5897249Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5897386Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5897548Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5897553Z 
2025-04-11T04:23:18.5897788Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5897961Z ____________ test_context_attention[False-True-False-4-16-8-32-32] _____________
2025-04-11T04:23:18.5897965Z 
2025-04-11T04:23:18.5898114Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5898263Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5898351Z use_new_kcache_layout = False
2025-04-11T04:23:18.5898355Z 
2025-04-11T04:23:18.5898553Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5898658Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5898777Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5898917Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5899032Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5899147Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5899283Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5899421Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5899571Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5899657Z     def test_context_attention(
2025-04-11T04:23:18.5899736Z         bsz: int,
2025-04-11T04:23:18.5899818Z         block_size: int,
2025-04-11T04:23:18.5899911Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5900083Z         num_attn_heads: int,
2025-04-11T04:23:18.5900164Z         kv_group_num: int,
2025-04-11T04:23:18.5900254Z         same_context_len: bool,
2025-04-11T04:23:18.5900345Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5900437Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5900512Z     ):
2025-04-11T04:23:18.5900627Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5900821Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5901002Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5901178Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5901344Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5901422Z             return
2025-04-11T04:23:18.5901497Z     
2025-04-11T04:23:18.5901593Z         torch.manual_seed(123)
2025-04-11T04:23:18.5901700Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5901789Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5901979Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5901984Z 
2025-04-11T04:23:18.5902152Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5902267Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5902271Z 
2025-04-11T04:23:18.5902349Z device = None
2025-04-11T04:23:18.5902354Z 
2025-04-11T04:23:18.5902473Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5902622Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5902695Z     
2025-04-11T04:23:18.5902772Z         Args:
2025-04-11T04:23:18.5902937Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5903104Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5903214Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5903290Z         """
2025-04-11T04:23:18.5903371Z         _lazy_init()
2025-04-11T04:23:18.5903465Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5903569Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5903674Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5903957Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5904092Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5904252Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5904260Z 
2025-04-11T04:23:18.5904494Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5904663Z ____________ test_context_attention[False-True-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.5904669Z 
2025-04-11T04:23:18.5904823Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5904972Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5905062Z use_new_kcache_layout = False
2025-04-11T04:23:18.5905066Z 
2025-04-11T04:23:18.5905266Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5905371Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5905487Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5905624Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5905742Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5905854Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5905994Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5906127Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5906386Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5906477Z     def test_context_attention(
2025-04-11T04:23:18.5906553Z         bsz: int,
2025-04-11T04:23:18.5906638Z         block_size: int,
2025-04-11T04:23:18.5906727Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5906813Z         num_attn_heads: int,
2025-04-11T04:23:18.5906896Z         kv_group_num: int,
2025-04-11T04:23:18.5906980Z         same_context_len: bool,
2025-04-11T04:23:18.5907068Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5907155Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5907231Z     ):
2025-04-11T04:23:18.5907340Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5907536Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5907718Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5907891Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5908141Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5908216Z             return
2025-04-11T04:23:18.5908293Z     
2025-04-11T04:23:18.5908377Z         torch.manual_seed(123)
2025-04-11T04:23:18.5908516Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5908607Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5908698Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5908702Z 
2025-04-11T04:23:18.5908871Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5908983Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5908987Z 
2025-04-11T04:23:18.5909068Z device = None
2025-04-11T04:23:18.5909072Z 
2025-04-11T04:23:18.5909188Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5909343Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5909417Z     
2025-04-11T04:23:18.5909494Z         Args:
2025-04-11T04:23:18.5909664Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5909828Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5909937Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5910010Z         """
2025-04-11T04:23:18.5910089Z         _lazy_init()
2025-04-11T04:23:18.5910186Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5910285Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5910393Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5910674Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5910820Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5910982Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5910989Z 
2025-04-11T04:23:18.5911235Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5911406Z ____________ test_context_attention[False-True-False-4-16-16-16-32] ____________
2025-04-11T04:23:18.5911410Z 
2025-04-11T04:23:18.5911563Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5911718Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5911805Z use_new_kcache_layout = False
2025-04-11T04:23:18.5911809Z 
2025-04-11T04:23:18.5912010Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5912116Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5912234Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5912556Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5912670Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5912789Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5912924Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5913062Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5913212Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5913301Z     def test_context_attention(
2025-04-11T04:23:18.5913377Z         bsz: int,
2025-04-11T04:23:18.5913459Z         block_size: int,
2025-04-11T04:23:18.5913555Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5913637Z         num_attn_heads: int,
2025-04-11T04:23:18.5913723Z         kv_group_num: int,
2025-04-11T04:23:18.5913808Z         same_context_len: bool,
2025-04-11T04:23:18.5913891Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5913987Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5914060Z     ):
2025-04-11T04:23:18.5914181Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5914473Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5914669Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5914845Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5915012Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5915093Z             return
2025-04-11T04:23:18.5915171Z     
2025-04-11T04:23:18.5915270Z         torch.manual_seed(123)
2025-04-11T04:23:18.5915373Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5915468Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5915562Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5915569Z 
2025-04-11T04:23:18.5915738Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5915856Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5915864Z 
2025-04-11T04:23:18.5915945Z device = None
2025-04-11T04:23:18.5915949Z 
2025-04-11T04:23:18.5916071Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5916223Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5916301Z     
2025-04-11T04:23:18.5916377Z         Args:
2025-04-11T04:23:18.5916547Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5916731Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5916839Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5916917Z         """
2025-04-11T04:23:18.5916999Z         _lazy_init()
2025-04-11T04:23:18.5917103Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5917208Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5917315Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5917608Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5917746Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5917912Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5917916Z 
2025-04-11T04:23:18.5918156Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5918331Z ____________ test_context_attention[False-True-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.5918335Z 
2025-04-11T04:23:18.5918489Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5918644Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5918815Z use_new_kcache_layout = False
2025-04-11T04:23:18.5918820Z 
2025-04-11T04:23:18.5919019Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5919132Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5919249Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5919391Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5919506Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5919621Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5919756Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5919890Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5920044Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5920134Z     def test_context_attention(
2025-04-11T04:23:18.5920222Z         bsz: int,
2025-04-11T04:23:18.5920308Z         block_size: int,
2025-04-11T04:23:18.5920396Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5920570Z         num_attn_heads: int,
2025-04-11T04:23:18.5920656Z         kv_group_num: int,
2025-04-11T04:23:18.5920748Z         same_context_len: bool,
2025-04-11T04:23:18.5920832Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5920926Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5921004Z     ):
2025-04-11T04:23:18.5921114Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5921316Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5921499Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5921674Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5921837Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5921917Z             return
2025-04-11T04:23:18.5921990Z     
2025-04-11T04:23:18.5922075Z         torch.manual_seed(123)
2025-04-11T04:23:18.5922181Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5922271Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5922365Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5922369Z 
2025-04-11T04:23:18.5922538Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5922651Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5922658Z 
2025-04-11T04:23:18.5922734Z device = None
2025-04-11T04:23:18.5922738Z 
2025-04-11T04:23:18.5922854Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5923022Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5923093Z     
2025-04-11T04:23:18.5923169Z         Args:
2025-04-11T04:23:18.5923335Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5923505Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5923617Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5923691Z         """
2025-04-11T04:23:18.5923772Z         _lazy_init()
2025-04-11T04:23:18.5923867Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5923971Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5924075Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5924360Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5924498Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5924657Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5924661Z 
2025-04-11T04:23:18.5924907Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5925162Z ____________ test_context_attention[False-True-False-4-16-16-32-32] ____________
2025-04-11T04:23:18.5925170Z 
2025-04-11T04:23:18.5925327Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5925473Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5925564Z use_new_kcache_layout = False
2025-04-11T04:23:18.5925568Z 
2025-04-11T04:23:18.5925768Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5925869Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5925991Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5926130Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5926258Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5926378Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5926522Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5926663Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5926919Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5927019Z     def test_context_attention(
2025-04-11T04:23:18.5927094Z         bsz: int,
2025-04-11T04:23:18.5927185Z         block_size: int,
2025-04-11T04:23:18.5927274Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5927358Z         num_attn_heads: int,
2025-04-11T04:23:18.5927449Z         kv_group_num: int,
2025-04-11T04:23:18.5927532Z         same_context_len: bool,
2025-04-11T04:23:18.5927625Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5927713Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5927790Z     ):
2025-04-11T04:23:18.5927903Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5928097Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5928285Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5928458Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5928630Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5928706Z             return
2025-04-11T04:23:18.5928783Z     
2025-04-11T04:23:18.5928875Z         torch.manual_seed(123)
2025-04-11T04:23:18.5928973Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5929072Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5929164Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5929168Z 
2025-04-11T04:23:18.5929338Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5929455Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5929463Z 
2025-04-11T04:23:18.5929546Z device = None
2025-04-11T04:23:18.5929550Z 
2025-04-11T04:23:18.5929671Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5929825Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5929902Z     
2025-04-11T04:23:18.5929980Z         Args:
2025-04-11T04:23:18.5930150Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5930319Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5930424Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5930500Z         """
2025-04-11T04:23:18.5930576Z         _lazy_init()
2025-04-11T04:23:18.5930673Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5930774Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5930879Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5931241Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5931375Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5931543Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5931548Z 
2025-04-11T04:23:18.5931788Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5931961Z _____________ test_context_attention[False-False-True-1-16-8-16-7] _____________
2025-04-11T04:23:18.5931966Z 
2025-04-11T04:23:18.5932115Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5932268Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5932360Z use_new_kcache_layout = False
2025-04-11T04:23:18.5932365Z 
2025-04-11T04:23:18.5932570Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5932679Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5932798Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5933033Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5933153Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5933269Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5933404Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5933542Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5933693Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5933780Z     def test_context_attention(
2025-04-11T04:23:18.5933859Z         bsz: int,
2025-04-11T04:23:18.5933940Z         block_size: int,
2025-04-11T04:23:18.5934032Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5934115Z         num_attn_heads: int,
2025-04-11T04:23:18.5934202Z         kv_group_num: int,
2025-04-11T04:23:18.5934289Z         same_context_len: bool,
2025-04-11T04:23:18.5934372Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5934470Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5934544Z     ):
2025-04-11T04:23:18.5934656Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5934849Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5935028Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5935202Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5935365Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5935442Z             return
2025-04-11T04:23:18.5935515Z     
2025-04-11T04:23:18.5935604Z         torch.manual_seed(123)
2025-04-11T04:23:18.5935703Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5935794Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5935887Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5935891Z 
2025-04-11T04:23:18.5936061Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5936175Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5936179Z 
2025-04-11T04:23:18.5936256Z device = None
2025-04-11T04:23:18.5936261Z 
2025-04-11T04:23:18.5936380Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5936528Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5936598Z     
2025-04-11T04:23:18.5936677Z         Args:
2025-04-11T04:23:18.5936846Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5937012Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5937115Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5937279Z         """
2025-04-11T04:23:18.5937366Z         _lazy_init()
2025-04-11T04:23:18.5937461Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5937570Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5937676Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5937962Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5938096Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5938256Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5938267Z 
2025-04-11T04:23:18.5938507Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5938678Z ____________ test_context_attention[False-False-True-1-16-8-16-32] _____________
2025-04-11T04:23:18.5938685Z 
2025-04-11T04:23:18.5938839Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5938988Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5939164Z use_new_kcache_layout = False
2025-04-11T04:23:18.5939170Z 
2025-04-11T04:23:18.5939371Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5939478Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5939593Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5939730Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5939848Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5939961Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5940101Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5940238Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5940396Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5940482Z     def test_context_attention(
2025-04-11T04:23:18.5940563Z         bsz: int,
2025-04-11T04:23:18.5940649Z         block_size: int,
2025-04-11T04:23:18.5940739Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5940825Z         num_attn_heads: int,
2025-04-11T04:23:18.5940909Z         kv_group_num: int,
2025-04-11T04:23:18.5940994Z         same_context_len: bool,
2025-04-11T04:23:18.5941083Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5941172Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5941249Z     ):
2025-04-11T04:23:18.5941359Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5941554Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5941735Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5941908Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5942081Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5942160Z             return
2025-04-11T04:23:18.5942236Z     
2025-04-11T04:23:18.5942322Z         torch.manual_seed(123)
2025-04-11T04:23:18.5942423Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5942510Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5942600Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5942604Z 
2025-04-11T04:23:18.5942777Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5942888Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5942892Z 
2025-04-11T04:23:18.5942971Z device = None
2025-04-11T04:23:18.5942976Z 
2025-04-11T04:23:18.5943091Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5943245Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5943404Z     
2025-04-11T04:23:18.5943481Z         Args:
2025-04-11T04:23:18.5943662Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5943830Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5943943Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5944017Z         """
2025-04-11T04:23:18.5944099Z         _lazy_init()
2025-04-11T04:23:18.5944200Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5944299Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5944406Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5944687Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5944824Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5944987Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5944992Z 
2025-04-11T04:23:18.5945233Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5945509Z _____________ test_context_attention[False-False-True-1-16-8-32-7] _____________
2025-04-11T04:23:18.5945513Z 
2025-04-11T04:23:18.5945665Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5945816Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5945904Z use_new_kcache_layout = False
2025-04-11T04:23:18.5945908Z 
2025-04-11T04:23:18.5946116Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5946220Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5946340Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5946478Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5946601Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5946715Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5946855Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5946996Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5947147Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5947240Z     def test_context_attention(
2025-04-11T04:23:18.5947315Z         bsz: int,
2025-04-11T04:23:18.5947397Z         block_size: int,
2025-04-11T04:23:18.5947490Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5947573Z         num_attn_heads: int,
2025-04-11T04:23:18.5947661Z         kv_group_num: int,
2025-04-11T04:23:18.5947745Z         same_context_len: bool,
2025-04-11T04:23:18.5947829Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5947921Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5947998Z     ):
2025-04-11T04:23:18.5948111Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5948303Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5948528Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5948700Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5948862Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5948942Z             return
2025-04-11T04:23:18.5949016Z     
2025-04-11T04:23:18.5949107Z         torch.manual_seed(123)
2025-04-11T04:23:18.5949206Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5949298Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5949387Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5949391Z 
2025-04-11T04:23:18.5949556Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5949837Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5949845Z 
2025-04-11T04:23:18.5949924Z device = None
2025-04-11T04:23:18.5949928Z 
2025-04-11T04:23:18.5950047Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5950197Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5950273Z     
2025-04-11T04:23:18.5950347Z         Args:
2025-04-11T04:23:18.5950512Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5950682Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5950786Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5950862Z         """
2025-04-11T04:23:18.5950939Z         _lazy_init()
2025-04-11T04:23:18.5951037Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5951142Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5951243Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5951530Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5951774Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5951937Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5951942Z 
2025-04-11T04:23:18.5952181Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5952354Z ____________ test_context_attention[False-False-True-1-16-8-32-32] _____________
2025-04-11T04:23:18.5952358Z 
2025-04-11T04:23:18.5952507Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5952655Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5952742Z use_new_kcache_layout = False
2025-04-11T04:23:18.5952750Z 
2025-04-11T04:23:18.5952948Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5953059Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5953174Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5953316Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5953431Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5953549Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5953688Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5953824Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5953983Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5954069Z     def test_context_attention(
2025-04-11T04:23:18.5954155Z         bsz: int,
2025-04-11T04:23:18.5954236Z         block_size: int,
2025-04-11T04:23:18.5954329Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5954422Z         num_attn_heads: int,
2025-04-11T04:23:18.5954503Z         kv_group_num: int,
2025-04-11T04:23:18.5954594Z         same_context_len: bool,
2025-04-11T04:23:18.5954679Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5954770Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5954842Z     ):
2025-04-11T04:23:18.5954951Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5955146Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5955326Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5955496Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5955658Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5955736Z             return
2025-04-11T04:23:18.5955902Z     
2025-04-11T04:23:18.5955991Z         torch.manual_seed(123)
2025-04-11T04:23:18.5956095Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5956189Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5956282Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5956287Z 
2025-04-11T04:23:18.5956454Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5956564Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5956572Z 
2025-04-11T04:23:18.5956649Z device = None
2025-04-11T04:23:18.5956653Z 
2025-04-11T04:23:18.5956768Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5956921Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5956993Z     
2025-04-11T04:23:18.5957070Z         Args:
2025-04-11T04:23:18.5957236Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5957403Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5957512Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5957669Z         """
2025-04-11T04:23:18.5957754Z         _lazy_init()
2025-04-11T04:23:18.5957850Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5957957Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5958064Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5958347Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5958486Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5958644Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5958648Z 
2025-04-11T04:23:18.5958890Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5959061Z ____________ test_context_attention[False-False-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.5959065Z 
2025-04-11T04:23:18.5959224Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5959370Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5959462Z use_new_kcache_layout = False
2025-04-11T04:23:18.5959467Z 
2025-04-11T04:23:18.5959665Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5959768Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5959886Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5960023Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5960141Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5960253Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5960394Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5960533Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5960689Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5960783Z     def test_context_attention(
2025-04-11T04:23:18.5960859Z         bsz: int,
2025-04-11T04:23:18.5960944Z         block_size: int,
2025-04-11T04:23:18.5961032Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5961114Z         num_attn_heads: int,
2025-04-11T04:23:18.5961200Z         kv_group_num: int,
2025-04-11T04:23:18.5961286Z         same_context_len: bool,
2025-04-11T04:23:18.5961373Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5961460Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5961535Z     ):
2025-04-11T04:23:18.5961646Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5961840Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5962114Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5962286Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5962457Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5962531Z             return
2025-04-11T04:23:18.5962609Z     
2025-04-11T04:23:18.5962695Z         torch.manual_seed(123)
2025-04-11T04:23:18.5962794Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5962885Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5962975Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5962979Z 
2025-04-11T04:23:18.5963149Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5963259Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5963263Z 
2025-04-11T04:23:18.5963342Z device = None
2025-04-11T04:23:18.5963350Z 
2025-04-11T04:23:18.5963466Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5963620Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5963785Z     
2025-04-11T04:23:18.5963867Z         Args:
2025-04-11T04:23:18.5964045Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5964215Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5964333Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5964409Z         """
2025-04-11T04:23:18.5964492Z         _lazy_init()
2025-04-11T04:23:18.5964601Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5964705Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5964817Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5965105Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5965246Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5965414Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5965419Z 
2025-04-11T04:23:18.5965662Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5965839Z ____________ test_context_attention[False-False-True-1-16-16-16-32] ____________
2025-04-11T04:23:18.5965843Z 
2025-04-11T04:23:18.5965997Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5966151Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5966244Z use_new_kcache_layout = False
2025-04-11T04:23:18.5966248Z 
2025-04-11T04:23:18.5966452Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5966561Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5966684Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5966829Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5966953Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5967073Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5967212Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5967352Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5967507Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5967599Z     def test_context_attention(
2025-04-11T04:23:18.5967684Z         bsz: int,
2025-04-11T04:23:18.5967768Z         block_size: int,
2025-04-11T04:23:18.5967866Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5967954Z         num_attn_heads: int,
2025-04-11T04:23:18.5968042Z         kv_group_num: int,
2025-04-11T04:23:18.5968134Z         same_context_len: bool,
2025-04-11T04:23:18.5968311Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5968401Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5968478Z     ):
2025-04-11T04:23:18.5968591Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5968785Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5968966Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5969137Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5969299Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5969377Z             return
2025-04-11T04:23:18.5969448Z     
2025-04-11T04:23:18.5969537Z         torch.manual_seed(123)
2025-04-11T04:23:18.5969635Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5969724Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5969822Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5969826Z 
2025-04-11T04:23:18.5969991Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5970191Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5970195Z 
2025-04-11T04:23:18.5970273Z device = None
2025-04-11T04:23:18.5970278Z 
2025-04-11T04:23:18.5970396Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5970547Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5970619Z     
2025-04-11T04:23:18.5970696Z         Args:
2025-04-11T04:23:18.5970860Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5971027Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5971133Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5971213Z         """
2025-04-11T04:23:18.5971289Z         _lazy_init()
2025-04-11T04:23:18.5971384Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5971488Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5971597Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5971887Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5972031Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5972199Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5972210Z 
2025-04-11T04:23:18.5972453Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5972632Z ____________ test_context_attention[False-False-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.5972636Z 
2025-04-11T04:23:18.5972792Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5972940Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5973045Z use_new_kcache_layout = False
2025-04-11T04:23:18.5973053Z 
2025-04-11T04:23:18.5973252Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5973358Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5973476Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5973613Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5973732Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5973845Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5973989Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5974123Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5974276Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5974468Z     def test_context_attention(
2025-04-11T04:23:18.5974542Z         bsz: int,
2025-04-11T04:23:18.5974628Z         block_size: int,
2025-04-11T04:23:18.5974722Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5974808Z         num_attn_heads: int,
2025-04-11T04:23:18.5974890Z         kv_group_num: int,
2025-04-11T04:23:18.5974973Z         same_context_len: bool,
2025-04-11T04:23:18.5975059Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5975146Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5975222Z     ):
2025-04-11T04:23:18.5975331Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5975528Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5975710Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5975879Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5976051Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5976125Z             return
2025-04-11T04:23:18.5976289Z     
2025-04-11T04:23:18.5976377Z         torch.manual_seed(123)
2025-04-11T04:23:18.5976478Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5976566Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5976656Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5976661Z 
2025-04-11T04:23:18.5976831Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5976941Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5976945Z 
2025-04-11T04:23:18.5977027Z device = None
2025-04-11T04:23:18.5977032Z 
2025-04-11T04:23:18.5977149Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5977302Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5977378Z     
2025-04-11T04:23:18.5977452Z         Args:
2025-04-11T04:23:18.5977621Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5977788Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5977898Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5977971Z         """
2025-04-11T04:23:18.5978052Z         _lazy_init()
2025-04-11T04:23:18.5978147Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5978248Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5978355Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5978635Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5978771Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5978928Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5978936Z 
2025-04-11T04:23:18.5979179Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5979352Z ____________ test_context_attention[False-False-True-1-16-16-32-32] ____________
2025-04-11T04:23:18.5979356Z 
2025-04-11T04:23:18.5979507Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5979658Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5979745Z use_new_kcache_layout = False
2025-04-11T04:23:18.5979749Z 
2025-04-11T04:23:18.5979951Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5980055Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5980174Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5980311Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5980429Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5980629Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5980766Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5980909Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5981059Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5981148Z     def test_context_attention(
2025-04-11T04:23:18.5981224Z         bsz: int,
2025-04-11T04:23:18.5981305Z         block_size: int,
2025-04-11T04:23:18.5981398Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5981481Z         num_attn_heads: int,
2025-04-11T04:23:18.5981567Z         kv_group_num: int,
2025-04-11T04:23:18.5981651Z         same_context_len: bool,
2025-04-11T04:23:18.5981741Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5981830Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5981902Z     ):
2025-04-11T04:23:18.5982015Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5982213Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5982486Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5982655Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5982819Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5982907Z             return
2025-04-11T04:23:18.5982981Z     
2025-04-11T04:23:18.5983070Z         torch.manual_seed(123)
2025-04-11T04:23:18.5983169Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5983266Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5983357Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5983361Z 
2025-04-11T04:23:18.5983528Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5983651Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5983656Z 
2025-04-11T04:23:18.5983734Z device = None
2025-04-11T04:23:18.5983744Z 
2025-04-11T04:23:18.5983874Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5984025Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5984100Z     
2025-04-11T04:23:18.5984176Z         Args:
2025-04-11T04:23:18.5984341Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5984512Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5984616Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5984692Z         """
2025-04-11T04:23:18.5984769Z         _lazy_init()
2025-04-11T04:23:18.5984866Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5984967Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5985077Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5985366Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5985503Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5985666Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5985670Z 
2025-04-11T04:23:18.5985908Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5986078Z _____________ test_context_attention[False-False-True-4-16-8-16-7] _____________
2025-04-11T04:23:18.5986082Z 
2025-04-11T04:23:18.5986231Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5986381Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5986468Z use_new_kcache_layout = False
2025-04-11T04:23:18.5986472Z 
2025-04-11T04:23:18.5986759Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5986868Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5986987Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5987128Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5987245Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5987361Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5987496Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5987628Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5987781Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5987869Z     def test_context_attention(
2025-04-11T04:23:18.5987948Z         bsz: int,
2025-04-11T04:23:18.5988029Z         block_size: int,
2025-04-11T04:23:18.5988117Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5988206Z         num_attn_heads: int,
2025-04-11T04:23:18.5988288Z         kv_group_num: int,
2025-04-11T04:23:18.5988376Z         same_context_len: bool,
2025-04-11T04:23:18.5988578Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5988669Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5988742Z     ):
2025-04-11T04:23:18.5988851Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5989046Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5989228Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5989402Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5989565Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5989643Z             return
2025-04-11T04:23:18.5989715Z     
2025-04-11T04:23:18.5989805Z         torch.manual_seed(123)
2025-04-11T04:23:18.5989906Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5989994Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5990088Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5990092Z 
2025-04-11T04:23:18.5990259Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5990370Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5990378Z 
2025-04-11T04:23:18.5990454Z device = None
2025-04-11T04:23:18.5990459Z 
2025-04-11T04:23:18.5990572Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5990726Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5990798Z     
2025-04-11T04:23:18.5990873Z         Args:
2025-04-11T04:23:18.5991039Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5991204Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5991317Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5991389Z         """
2025-04-11T04:23:18.5991474Z         _lazy_init()
2025-04-11T04:23:18.5991570Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5991675Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5991780Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5992062Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5992202Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5992359Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5992363Z 
2025-04-11T04:23:18.5992607Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5992775Z ____________ test_context_attention[False-False-True-4-16-8-16-32] _____________
2025-04-11T04:23:18.5992874Z 
2025-04-11T04:23:18.5993029Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5993178Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5993268Z use_new_kcache_layout = False
2025-04-11T04:23:18.5993273Z 
2025-04-11T04:23:18.5993471Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5993575Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5993693Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5993830Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5993948Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5994061Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5994200Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5994337Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5994487Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5994671Z     def test_context_attention(
2025-04-11T04:23:18.5994746Z         bsz: int,
2025-04-11T04:23:18.5994832Z         block_size: int,
2025-04-11T04:23:18.5994922Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5995005Z         num_attn_heads: int,
2025-04-11T04:23:18.5995097Z         kv_group_num: int,
2025-04-11T04:23:18.5995182Z         same_context_len: bool,
2025-04-11T04:23:18.5995276Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5995368Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5995444Z     ):
2025-04-11T04:23:18.5995556Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5995753Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5995938Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5996112Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5996282Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5996356Z             return
2025-04-11T04:23:18.5996430Z     
2025-04-11T04:23:18.5996516Z         torch.manual_seed(123)
2025-04-11T04:23:18.5996612Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5996704Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5996794Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5996797Z 
2025-04-11T04:23:18.5996965Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5997076Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5997081Z 
2025-04-11T04:23:18.5997161Z device = None
2025-04-11T04:23:18.5997165Z 
2025-04-11T04:23:18.5997279Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5997431Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5997505Z     
2025-04-11T04:23:18.5997581Z         Args:
2025-04-11T04:23:18.5997750Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5997915Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5998022Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5998094Z         """
2025-04-11T04:23:18.5998170Z         _lazy_init()
2025-04-11T04:23:18.5998267Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5998366Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5998474Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5998756Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5998998Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5999160Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5999168Z 
2025-04-11T04:23:18.5999403Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5999573Z _____________ test_context_attention[False-False-True-4-16-8-32-7] _____________
2025-04-11T04:23:18.5999577Z 
2025-04-11T04:23:18.5999729Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5999879Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5999967Z use_new_kcache_layout = False
2025-04-11T04:23:18.5999971Z 
2025-04-11T04:23:18.6000171Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6000274Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6000389Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6000532Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6000646Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6000844Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6000981Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6001120Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6001271Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6001358Z     def test_context_attention(
2025-04-11T04:23:18.6001436Z         bsz: int,
2025-04-11T04:23:18.6001517Z         block_size: int,
2025-04-11T04:23:18.6001608Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6001691Z         num_attn_heads: int,
2025-04-11T04:23:18.6001774Z         kv_group_num: int,
2025-04-11T04:23:18.6001859Z         same_context_len: bool,
2025-04-11T04:23:18.6001942Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6002037Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6002110Z     ):
2025-04-11T04:23:18.6002221Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6002414Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6002596Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6002771Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6002935Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6003014Z             return
2025-04-11T04:23:18.6003086Z     
2025-04-11T04:23:18.6003174Z         torch.manual_seed(123)
2025-04-11T04:23:18.6003272Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6003359Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6003453Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6003460Z 
2025-04-11T04:23:18.6003626Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6003741Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6003749Z 
2025-04-11T04:23:18.6003827Z device = None
2025-04-11T04:23:18.6003831Z 
2025-04-11T04:23:18.6003949Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6004098Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6004169Z     
2025-04-11T04:23:18.6004246Z         Args:
2025-04-11T04:23:18.6004410Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6004580Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6004684Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6004759Z         """
2025-04-11T04:23:18.6004835Z         _lazy_init()
2025-04-11T04:23:18.6005037Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6005141Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6005248Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6005535Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6005671Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6005828Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6005836Z 
2025-04-11T04:23:18.6006078Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6006247Z ____________ test_context_attention[False-False-True-4-16-8-32-32] _____________
2025-04-11T04:23:18.6006251Z 
2025-04-11T04:23:18.6006405Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6006551Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6006647Z use_new_kcache_layout = False
2025-04-11T04:23:18.6006652Z 
2025-04-11T04:23:18.6007027Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6007135Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6007252Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6007397Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6007528Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6007641Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6007791Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6007927Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6008087Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6008175Z     def test_context_attention(
2025-04-11T04:23:18.6008254Z         bsz: int,
2025-04-11T04:23:18.6008339Z         block_size: int,
2025-04-11T04:23:18.6008427Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6008516Z         num_attn_heads: int,
2025-04-11T04:23:18.6008599Z         kv_group_num: int,
2025-04-11T04:23:18.6008686Z         same_context_len: bool,
2025-04-11T04:23:18.6008769Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6008856Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6008931Z     ):
2025-04-11T04:23:18.6009039Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6009234Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6009415Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6009583Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6009745Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6009822Z             return
2025-04-11T04:23:18.6009898Z     
2025-04-11T04:23:18.6009983Z         torch.manual_seed(123)
2025-04-11T04:23:18.6010087Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6010175Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6010265Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6010269Z 
2025-04-11T04:23:18.6010437Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6010546Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6010550Z 
2025-04-11T04:23:18.6010629Z device = None
2025-04-11T04:23:18.6010634Z 
2025-04-11T04:23:18.6010745Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6010897Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6010968Z     
2025-04-11T04:23:18.6011041Z         Args:
2025-04-11T04:23:18.6011299Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6011466Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6011580Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6011652Z         """
2025-04-11T04:23:18.6011733Z         _lazy_init()
2025-04-11T04:23:18.6011828Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6011930Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6012038Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6012320Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6012457Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6012615Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6012619Z 
2025-04-11T04:23:18.6012864Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6013032Z ____________ test_context_attention[False-False-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.6013119Z 
2025-04-11T04:23:18.6013273Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6013421Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6013507Z use_new_kcache_layout = False
2025-04-11T04:23:18.6013512Z 
2025-04-11T04:23:18.6013712Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6013815Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6013932Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6014068Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6014185Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6014301Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6014436Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6014576Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6014726Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6014817Z     def test_context_attention(
2025-04-11T04:23:18.6014893Z         bsz: int,
2025-04-11T04:23:18.6014974Z         block_size: int,
2025-04-11T04:23:18.6015065Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6015149Z         num_attn_heads: int,
2025-04-11T04:23:18.6015235Z         kv_group_num: int,
2025-04-11T04:23:18.6015319Z         same_context_len: bool,
2025-04-11T04:23:18.6015406Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6015493Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6015565Z     ):
2025-04-11T04:23:18.6015677Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6015869Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6016055Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6016226Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6016388Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6016467Z             return
2025-04-11T04:23:18.6016544Z     
2025-04-11T04:23:18.6016632Z         torch.manual_seed(123)
2025-04-11T04:23:18.6016732Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6016831Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6016919Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6016923Z 
2025-04-11T04:23:18.6017089Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6017209Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6017298Z 
2025-04-11T04:23:18.6017378Z device = None
2025-04-11T04:23:18.6017383Z 
2025-04-11T04:23:18.6017502Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6017656Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6017736Z     
2025-04-11T04:23:18.6017809Z         Args:
2025-04-11T04:23:18.6017977Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6018146Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6018251Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6018328Z         """
2025-04-11T04:23:18.6018406Z         _lazy_init()
2025-04-11T04:23:18.6018504Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6018607Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6018709Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6018997Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6019238Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6019402Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6019407Z 
2025-04-11T04:23:18.6019644Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6019816Z ____________ test_context_attention[False-False-True-4-16-16-16-32] ____________
2025-04-11T04:23:18.6019821Z 
2025-04-11T04:23:18.6019973Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6020120Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6020209Z use_new_kcache_layout = False
2025-04-11T04:23:18.6020213Z 
2025-04-11T04:23:18.6020411Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6020522Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6020639Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6020782Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6020898Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6021014Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6021152Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6021287Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6021444Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6021532Z     def test_context_attention(
2025-04-11T04:23:18.6021611Z         bsz: int,
2025-04-11T04:23:18.6021693Z         block_size: int,
2025-04-11T04:23:18.6021783Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6021869Z         num_attn_heads: int,
2025-04-11T04:23:18.6021960Z         kv_group_num: int,
2025-04-11T04:23:18.6022047Z         same_context_len: bool,
2025-04-11T04:23:18.6022129Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6022225Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6022298Z     ):
2025-04-11T04:23:18.6022407Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6022602Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6022782Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6022955Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6023119Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6023196Z             return
2025-04-11T04:23:18.6023269Z     
2025-04-11T04:23:18.6023354Z         torch.manual_seed(123)
2025-04-11T04:23:18.6023544Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6023634Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6023728Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6023738Z 
2025-04-11T04:23:18.6023904Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6024015Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6024023Z 
2025-04-11T04:23:18.6024101Z device = None
2025-04-11T04:23:18.6024106Z 
2025-04-11T04:23:18.6024221Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6024374Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6024444Z     
2025-04-11T04:23:18.6024521Z         Args:
2025-04-11T04:23:18.6024690Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6024856Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6024970Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6025045Z         """
2025-04-11T04:23:18.6025132Z         _lazy_init()
2025-04-11T04:23:18.6025311Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6025419Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6025527Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6025825Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6025963Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6026121Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6026125Z 
2025-04-11T04:23:18.6026364Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6026535Z ____________ test_context_attention[False-False-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.6026542Z 
2025-04-11T04:23:18.6026696Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6026841Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6026934Z use_new_kcache_layout = False
2025-04-11T04:23:18.6026938Z 
2025-04-11T04:23:18.6027135Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6027238Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6027356Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6027492Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6027611Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6027724Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6027862Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6027996Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6028151Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6028244Z     def test_context_attention(
2025-04-11T04:23:18.6028325Z         bsz: int,
2025-04-11T04:23:18.6028441Z         block_size: int,
2025-04-11T04:23:18.6028531Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6028614Z         num_attn_heads: int,
2025-04-11T04:23:18.6028708Z         kv_group_num: int,
2025-04-11T04:23:18.6028792Z         same_context_len: bool,
2025-04-11T04:23:18.6028879Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6028966Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6029042Z     ):
2025-04-11T04:23:18.6029152Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6029344Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6029529Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6029797Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6029966Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6030044Z             return
2025-04-11T04:23:18.6030119Z     
2025-04-11T04:23:18.6030205Z         torch.manual_seed(123)
2025-04-11T04:23:18.6030305Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6030399Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6030489Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6030493Z 
2025-04-11T04:23:18.6030663Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6030774Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6030778Z 
2025-04-11T04:23:18.6030858Z device = None
2025-04-11T04:23:18.6030862Z 
2025-04-11T04:23:18.6030978Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6031129Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6031207Z     
2025-04-11T04:23:18.6031278Z         Args:
2025-04-11T04:23:18.6031542Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6031712Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6031824Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6031899Z         """
2025-04-11T04:23:18.6031978Z         _lazy_init()
2025-04-11T04:23:18.6032081Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6032183Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6032288Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6032573Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6032711Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6032877Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6032881Z 
2025-04-11T04:23:18.6033122Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6033294Z ____________ test_context_attention[False-False-True-4-16-16-32-32] ____________
2025-04-11T04:23:18.6033298Z 
2025-04-11T04:23:18.6033448Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6033598Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6033685Z use_new_kcache_layout = False
2025-04-11T04:23:18.6033689Z 
2025-04-11T04:23:18.6033890Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6033995Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6034109Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6034250Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6034370Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6034487Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6034628Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6034765Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6034917Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6035003Z     def test_context_attention(
2025-04-11T04:23:18.6035082Z         bsz: int,
2025-04-11T04:23:18.6035163Z         block_size: int,
2025-04-11T04:23:18.6035255Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6035336Z         num_attn_heads: int,
2025-04-11T04:23:18.6035417Z         kv_group_num: int,
2025-04-11T04:23:18.6035506Z         same_context_len: bool,
2025-04-11T04:23:18.6035591Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6035682Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6035837Z     ):
2025-04-11T04:23:18.6035950Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6036142Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6036327Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6036496Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6036657Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6036735Z             return
2025-04-11T04:23:18.6036807Z     
2025-04-11T04:23:18.6036895Z         torch.manual_seed(123)
2025-04-11T04:23:18.6036993Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6037081Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6037175Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6037179Z 
2025-04-11T04:23:18.6037348Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6037462Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6037548Z 
2025-04-11T04:23:18.6037629Z device = None
2025-04-11T04:23:18.6037634Z 
2025-04-11T04:23:18.6037752Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6037902Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6037974Z     
2025-04-11T04:23:18.6038052Z         Args:
2025-04-11T04:23:18.6038216Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6038384Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6038487Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6038563Z         """
2025-04-11T04:23:18.6038641Z         _lazy_init()
2025-04-11T04:23:18.6038734Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6038848Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6038951Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6039246Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6039384Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6039553Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6039557Z 
2025-04-11T04:23:18.6039794Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6039960Z ____________ test_context_attention[False-False-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.6039967Z 
2025-04-11T04:23:18.6040117Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6040265Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6040360Z use_new_kcache_layout = False
2025-04-11T04:23:18.6040364Z 
2025-04-11T04:23:18.6040561Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6040672Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6040787Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6040923Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6041041Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6041153Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6041291Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6041422Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6041575Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6041660Z     def test_context_attention(
2025-04-11T04:23:18.6041734Z         bsz: int,
2025-04-11T04:23:18.6041906Z         block_size: int,
2025-04-11T04:23:18.6041998Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6042083Z         num_attn_heads: int,
2025-04-11T04:23:18.6042171Z         kv_group_num: int,
2025-04-11T04:23:18.6042257Z         same_context_len: bool,
2025-04-11T04:23:18.6042341Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6042428Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6042504Z     ):
2025-04-11T04:23:18.6042613Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6042812Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6042996Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6043167Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6043334Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6043412Z             return
2025-04-11T04:23:18.6043489Z     
2025-04-11T04:23:18.6043574Z         torch.manual_seed(123)
2025-04-11T04:23:18.6043675Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6043870Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6043960Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6043964Z 
2025-04-11T04:23:18.6044135Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6044246Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6044250Z 
2025-04-11T04:23:18.6044332Z device = None
2025-04-11T04:23:18.6044336Z 
2025-04-11T04:23:18.6044451Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6044601Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6044673Z     
2025-04-11T04:23:18.6044745Z         Args:
2025-04-11T04:23:18.6044914Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6045078Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6045188Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6045261Z         """
2025-04-11T04:23:18.6045341Z         _lazy_init()
2025-04-11T04:23:18.6045435Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6045536Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6045643Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6045925Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6046063Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6046223Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6046227Z 
2025-04-11T04:23:18.6046466Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6046638Z ____________ test_context_attention[False-False-False-1-16-8-16-32] ____________
2025-04-11T04:23:18.6046645Z 
2025-04-11T04:23:18.6046798Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6046945Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6047033Z use_new_kcache_layout = False
2025-04-11T04:23:18.6047037Z 
2025-04-11T04:23:18.6047238Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6047340Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6047457Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6047592Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6047709Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6047820Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6048045Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6048185Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6048335Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6048428Z     def test_context_attention(
2025-04-11T04:23:18.6048504Z         bsz: int,
2025-04-11T04:23:18.6048587Z         block_size: int,
2025-04-11T04:23:18.6048678Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6048762Z         num_attn_heads: int,
2025-04-11T04:23:18.6048848Z         kv_group_num: int,
2025-04-11T04:23:18.6048934Z         same_context_len: bool,
2025-04-11T04:23:18.6049021Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6049107Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6049178Z     ):
2025-04-11T04:23:18.6049290Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6049481Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6049667Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6049922Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6050084Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6050162Z             return
2025-04-11T04:23:18.6050234Z     
2025-04-11T04:23:18.6050325Z         torch.manual_seed(123)
2025-04-11T04:23:18.6050424Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6050514Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6050604Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6050608Z 
2025-04-11T04:23:18.6050774Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6050892Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6050896Z 
2025-04-11T04:23:18.6050976Z device = None
2025-04-11T04:23:18.6050980Z 
2025-04-11T04:23:18.6051106Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6051256Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6051334Z     
2025-04-11T04:23:18.6051407Z         Args:
2025-04-11T04:23:18.6051576Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6051757Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6051866Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6051954Z         """
2025-04-11T04:23:18.6052031Z         _lazy_init()
2025-04-11T04:23:18.6052129Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6052240Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6052344Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6052632Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6052772Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6052941Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6052946Z 
2025-04-11T04:23:18.6053184Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6053360Z ____________ test_context_attention[False-False-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.6053363Z 
2025-04-11T04:23:18.6053515Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6053664Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6053752Z use_new_kcache_layout = False
2025-04-11T04:23:18.6053757Z 
2025-04-11T04:23:18.6053957Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6054148Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6054264Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6054405Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6054522Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6054636Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6054769Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6054901Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6055055Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6055140Z     def test_context_attention(
2025-04-11T04:23:18.6055219Z         bsz: int,
2025-04-11T04:23:18.6055300Z         block_size: int,
2025-04-11T04:23:18.6055390Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6055475Z         num_attn_heads: int,
2025-04-11T04:23:18.6055556Z         kv_group_num: int,
2025-04-11T04:23:18.6055646Z         same_context_len: bool,
2025-04-11T04:23:18.6055729Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6055818Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6055974Z     ):
2025-04-11T04:23:18.6056085Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6056287Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6056469Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6056642Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6056805Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6056883Z             return
2025-04-11T04:23:18.6056954Z     
2025-04-11T04:23:18.6057040Z         torch.manual_seed(123)
2025-04-11T04:23:18.6057142Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6057234Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6057328Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6057333Z 
2025-04-11T04:23:18.6057502Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6057617Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6057624Z 
2025-04-11T04:23:18.6057702Z device = None
2025-04-11T04:23:18.6057706Z 
2025-04-11T04:23:18.6057822Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6057976Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6058047Z     
2025-04-11T04:23:18.6058123Z         Args:
2025-04-11T04:23:18.6058290Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6058453Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6058561Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6058637Z         """
2025-04-11T04:23:18.6058717Z         _lazy_init()
2025-04-11T04:23:18.6058812Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6058919Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6059025Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6059307Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6059446Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6059607Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6059611Z 
2025-04-11T04:23:18.6059854Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6060023Z ____________ test_context_attention[False-False-False-1-16-8-32-32] ____________
2025-04-11T04:23:18.6060026Z 
2025-04-11T04:23:18.6060178Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6060410Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6060507Z use_new_kcache_layout = False
2025-04-11T04:23:18.6060512Z 
2025-04-11T04:23:18.6060708Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6060812Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6060930Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6061066Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6061185Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6061296Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6061434Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6061567Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6061715Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6061811Z     def test_context_attention(
2025-04-11T04:23:18.6061888Z         bsz: int,
2025-04-11T04:23:18.6062073Z         block_size: int,
2025-04-11T04:23:18.6062166Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6062251Z         num_attn_heads: int,
2025-04-11T04:23:18.6062346Z         kv_group_num: int,
2025-04-11T04:23:18.6062432Z         same_context_len: bool,
2025-04-11T04:23:18.6062528Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6062617Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6062693Z     ):
2025-04-11T04:23:18.6062803Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6062998Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6063182Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6063355Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6063524Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6063601Z             return
2025-04-11T04:23:18.6063676Z     
2025-04-11T04:23:18.6063761Z         torch.manual_seed(123)
2025-04-11T04:23:18.6063859Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6063953Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6064044Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6064049Z 
2025-04-11T04:23:18.6064217Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6064329Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6064332Z 
2025-04-11T04:23:18.6064411Z device = None
2025-04-11T04:23:18.6064416Z 
2025-04-11T04:23:18.6064533Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6064685Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6064763Z     
2025-04-11T04:23:18.6064836Z         Args:
2025-04-11T04:23:18.6065007Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6065175Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6065282Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6065356Z         """
2025-04-11T04:23:18.6065433Z         _lazy_init()
2025-04-11T04:23:18.6065531Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6065631Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6065739Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6066023Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6066160Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6066323Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6066446Z 
2025-04-11T04:23:18.6066690Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6066865Z ____________ test_context_attention[False-False-False-1-16-16-16-7] ____________
2025-04-11T04:23:18.6066870Z 
2025-04-11T04:23:18.6067020Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6067169Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6067258Z use_new_kcache_layout = False
2025-04-11T04:23:18.6067262Z 
2025-04-11T04:23:18.6067465Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6067569Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6067683Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6067824Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6067943Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6068059Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6068278Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6068441Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6068589Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6068679Z     def test_context_attention(
2025-04-11T04:23:18.6068759Z         bsz: int,
2025-04-11T04:23:18.6068840Z         block_size: int,
2025-04-11T04:23:18.6068932Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6069013Z         num_attn_heads: int,
2025-04-11T04:23:18.6069099Z         kv_group_num: int,
2025-04-11T04:23:18.6069182Z         same_context_len: bool,
2025-04-11T04:23:18.6069265Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6069355Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6069426Z     ):
2025-04-11T04:23:18.6069542Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6069735Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6069917Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6070088Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6070247Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6070325Z             return
2025-04-11T04:23:18.6070396Z     
2025-04-11T04:23:18.6070484Z         torch.manual_seed(123)
2025-04-11T04:23:18.6070581Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6070668Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6070762Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6070766Z 
2025-04-11T04:23:18.6070931Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6071048Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6071051Z 
2025-04-11T04:23:18.6071130Z device = None
2025-04-11T04:23:18.6071137Z 
2025-04-11T04:23:18.6071255Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6071403Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6071474Z     
2025-04-11T04:23:18.6071551Z         Args:
2025-04-11T04:23:18.6071716Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6071881Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6071985Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6072061Z         """
2025-04-11T04:23:18.6072137Z         _lazy_init()
2025-04-11T04:23:18.6072232Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6072336Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6072537Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6072822Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6072960Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6073122Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6073126Z 
2025-04-11T04:23:18.6073363Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6073534Z ___________ test_context_attention[False-False-False-1-16-16-16-32] ____________
2025-04-11T04:23:18.6073542Z 
2025-04-11T04:23:18.6073693Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6073837Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6073929Z use_new_kcache_layout = False
2025-04-11T04:23:18.6073937Z 
2025-04-11T04:23:18.6074140Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6074343Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6074462Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6074603Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6074718Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6074830Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6074971Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6075106Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6075258Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6075344Z     def test_context_attention(
2025-04-11T04:23:18.6075419Z         bsz: int,
2025-04-11T04:23:18.6075503Z         block_size: int,
2025-04-11T04:23:18.6075595Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6075681Z         num_attn_heads: int,
2025-04-11T04:23:18.6075763Z         kv_group_num: int,
2025-04-11T04:23:18.6075852Z         same_context_len: bool,
2025-04-11T04:23:18.6075937Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6076025Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6076101Z     ):
2025-04-11T04:23:18.6076209Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6076405Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6076587Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6076758Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6076924Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6076998Z             return
2025-04-11T04:23:18.6077078Z     
2025-04-11T04:23:18.6077162Z         torch.manual_seed(123)
2025-04-11T04:23:18.6077262Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6077354Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6077443Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6077447Z 
2025-04-11T04:23:18.6077617Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6077726Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6077730Z 
2025-04-11T04:23:18.6077809Z device = None
2025-04-11T04:23:18.6077813Z 
2025-04-11T04:23:18.6077927Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6078081Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6078152Z     
2025-04-11T04:23:18.6078225Z         Args:
2025-04-11T04:23:18.6078392Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6078649Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6078759Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6078837Z         """
2025-04-11T04:23:18.6078917Z         _lazy_init()
2025-04-11T04:23:18.6079015Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6079117Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6079225Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6079507Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6079645Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6079802Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6079806Z 
2025-04-11T04:23:18.6080046Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6080217Z ____________ test_context_attention[False-False-False-1-16-16-32-7] ____________
2025-04-11T04:23:18.6080221Z 
2025-04-11T04:23:18.6080376Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6080610Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6080698Z use_new_kcache_layout = False
2025-04-11T04:23:18.6080703Z 
2025-04-11T04:23:18.6080905Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6081008Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6081127Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6081264Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6081381Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6081494Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6081631Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6081772Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6081922Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6082020Z     def test_context_attention(
2025-04-11T04:23:18.6082096Z         bsz: int,
2025-04-11T04:23:18.6082178Z         block_size: int,
2025-04-11T04:23:18.6082271Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6082355Z         num_attn_heads: int,
2025-04-11T04:23:18.6082442Z         kv_group_num: int,
2025-04-11T04:23:18.6082526Z         same_context_len: bool,
2025-04-11T04:23:18.6082613Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6082700Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6082772Z     ):
2025-04-11T04:23:18.6082886Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6083081Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6083264Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6083437Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6083606Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6083680Z             return
2025-04-11T04:23:18.6083752Z     
2025-04-11T04:23:18.6083841Z         torch.manual_seed(123)
2025-04-11T04:23:18.6083939Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6084030Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6084122Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6084126Z 
2025-04-11T04:23:18.6084292Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6084406Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6084410Z 
2025-04-11T04:23:18.6084487Z device = None
2025-04-11T04:23:18.6084490Z 
2025-04-11T04:23:18.6084699Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6084848Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6084927Z     
2025-04-11T04:23:18.6085010Z         Args:
2025-04-11T04:23:18.6085177Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6085352Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6085458Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6085541Z         """
2025-04-11T04:23:18.6085621Z         _lazy_init()
2025-04-11T04:23:18.6085722Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6085822Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6085928Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6086215Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6086354Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6086516Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6086604Z 
2025-04-11T04:23:18.6086843Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6087022Z ___________ test_context_attention[False-False-False-1-16-16-32-32] ____________
2025-04-11T04:23:18.6087026Z 
2025-04-11T04:23:18.6087178Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6087327Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6087416Z use_new_kcache_layout = False
2025-04-11T04:23:18.6087420Z 
2025-04-11T04:23:18.6087614Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6087722Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6087841Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6087982Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6088100Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6088215Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6088350Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6088483Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6088638Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6088726Z     def test_context_attention(
2025-04-11T04:23:18.6088804Z         bsz: int,
2025-04-11T04:23:18.6088885Z         block_size: int,
2025-04-11T04:23:18.6088973Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6089058Z         num_attn_heads: int,
2025-04-11T04:23:18.6089140Z         kv_group_num: int,
2025-04-11T04:23:18.6089228Z         same_context_len: bool,
2025-04-11T04:23:18.6089315Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6089405Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6089478Z     ):
2025-04-11T04:23:18.6089590Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6089786Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6089966Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6090139Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6090301Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6090378Z             return
2025-04-11T04:23:18.6090449Z     
2025-04-11T04:23:18.6090534Z         torch.manual_seed(123)
2025-04-11T04:23:18.6090637Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6090725Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6090918Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6090922Z 
2025-04-11T04:23:18.6091091Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6091205Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6091213Z 
2025-04-11T04:23:18.6091290Z device = None
2025-04-11T04:23:18.6091294Z 
2025-04-11T04:23:18.6091409Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6091564Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6091636Z     
2025-04-11T04:23:18.6091712Z         Args:
2025-04-11T04:23:18.6091879Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6092042Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6092151Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6092223Z         """
2025-04-11T04:23:18.6092306Z         _lazy_init()
2025-04-11T04:23:18.6092401Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6092505Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6092695Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6092975Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6093113Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6093271Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6093276Z 
2025-04-11T04:23:18.6093515Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6093682Z ____________ test_context_attention[False-False-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.6093686Z 
2025-04-11T04:23:18.6093839Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6093988Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6094081Z use_new_kcache_layout = False
2025-04-11T04:23:18.6094089Z 
2025-04-11T04:23:18.6094286Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6094389Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6094509Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6094646Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6094763Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6094873Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6095012Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6095144Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6095291Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6095386Z     def test_context_attention(
2025-04-11T04:23:18.6095463Z         bsz: int,
2025-04-11T04:23:18.6095548Z         block_size: int,
2025-04-11T04:23:18.6095641Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6095723Z         num_attn_heads: int,
2025-04-11T04:23:18.6095806Z         kv_group_num: int,
2025-04-11T04:23:18.6095889Z         same_context_len: bool,
2025-04-11T04:23:18.6095975Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6096062Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6096136Z     ):
2025-04-11T04:23:18.6096245Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6096437Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6096621Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6096788Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6097044Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6097123Z             return
2025-04-11T04:23:18.6097208Z     
2025-04-11T04:23:18.6097294Z         torch.manual_seed(123)
2025-04-11T04:23:18.6097396Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6097488Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6097578Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6097582Z 
2025-04-11T04:23:18.6097760Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6097871Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6097875Z 
2025-04-11T04:23:18.6097954Z device = None
2025-04-11T04:23:18.6097958Z 
2025-04-11T04:23:18.6098073Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6098223Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6098297Z     
2025-04-11T04:23:18.6098374Z         Args:
2025-04-11T04:23:18.6098543Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6098707Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6098900Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6098973Z         """
2025-04-11T04:23:18.6099051Z         _lazy_init()
2025-04-11T04:23:18.6099151Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6099252Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6099361Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6099648Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6099785Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6099948Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6099956Z 
2025-04-11T04:23:18.6100195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6100377Z ____________ test_context_attention[False-False-False-4-16-8-16-32] ____________
2025-04-11T04:23:18.6100381Z 
2025-04-11T04:23:18.6100533Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6100685Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6100773Z use_new_kcache_layout = False
2025-04-11T04:23:18.6100778Z 
2025-04-11T04:23:18.6100981Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6101084Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6101199Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6101339Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6101456Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6101578Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6101714Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6101855Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6102006Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6102093Z     def test_context_attention(
2025-04-11T04:23:18.6102173Z         bsz: int,
2025-04-11T04:23:18.6102257Z         block_size: int,
2025-04-11T04:23:18.6102350Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6102435Z         num_attn_heads: int,
2025-04-11T04:23:18.6102521Z         kv_group_num: int,
2025-04-11T04:23:18.6102604Z         same_context_len: bool,
2025-04-11T04:23:18.6102688Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6102780Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6102853Z     ):
2025-04-11T04:23:18.6102966Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6103253Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6103432Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6103608Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6103769Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6103850Z             return
2025-04-11T04:23:18.6103923Z     
2025-04-11T04:23:18.6104011Z         torch.manual_seed(123)
2025-04-11T04:23:18.6104110Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6104198Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6104291Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6104296Z 
2025-04-11T04:23:18.6104462Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6104578Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6104586Z 
2025-04-11T04:23:18.6104663Z device = None
2025-04-11T04:23:18.6104667Z 
2025-04-11T04:23:18.6104964Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6105113Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6105184Z     
2025-04-11T04:23:18.6105262Z         Args:
2025-04-11T04:23:18.6105430Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6105603Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6105706Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6105782Z         """
2025-04-11T04:23:18.6105860Z         _lazy_init()
2025-04-11T04:23:18.6105955Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6106064Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6106167Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6106458Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6106600Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6106763Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6106767Z 
2025-04-11T04:23:18.6107006Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6107175Z ____________ test_context_attention[False-False-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.6107183Z 
2025-04-11T04:23:18.6107332Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6107477Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6107569Z use_new_kcache_layout = False
2025-04-11T04:23:18.6107574Z 
2025-04-11T04:23:18.6107772Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6107883Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6108002Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6108145Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6108260Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6108375Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6108547Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6108682Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6108833Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6108920Z     def test_context_attention(
2025-04-11T04:23:18.6108994Z         bsz: int,
2025-04-11T04:23:18.6109080Z         block_size: int,
2025-04-11T04:23:18.6109170Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6109365Z         num_attn_heads: int,
2025-04-11T04:23:18.6109448Z         kv_group_num: int,
2025-04-11T04:23:18.6109536Z         same_context_len: bool,
2025-04-11T04:23:18.6109626Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6109718Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6109796Z     ):
2025-04-11T04:23:18.6109906Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6110103Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6110285Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6110460Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6110627Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6110701Z             return
2025-04-11T04:23:18.6110782Z     
2025-04-11T04:23:18.6110867Z         torch.manual_seed(123)
2025-04-11T04:23:18.6110970Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6111058Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6111260Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6111265Z 
2025-04-11T04:23:18.6111435Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6111545Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6111549Z 
2025-04-11T04:23:18.6111629Z device = None
2025-04-11T04:23:18.6111633Z 
2025-04-11T04:23:18.6111748Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6111901Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6111972Z     
2025-04-11T04:23:18.6112044Z         Args:
2025-04-11T04:23:18.6112209Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6112374Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6112484Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6112556Z         """
2025-04-11T04:23:18.6112638Z         _lazy_init()
2025-04-11T04:23:18.6112733Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6112833Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6112942Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6113223Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6113362Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6113520Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6113524Z 
2025-04-11T04:23:18.6113766Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6113938Z ____________ test_context_attention[False-False-False-4-16-8-32-32] ____________
2025-04-11T04:23:18.6113945Z 
2025-04-11T04:23:18.6114099Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6114249Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6114337Z use_new_kcache_layout = False
2025-04-11T04:23:18.6114341Z 
2025-04-11T04:23:18.6114541Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6114644Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6114764Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6114900Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6115018Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6115130Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6115264Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6115403Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6115639Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6115734Z     def test_context_attention(
2025-04-11T04:23:18.6115812Z         bsz: int,
2025-04-11T04:23:18.6115893Z         block_size: int,
2025-04-11T04:23:18.6115986Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6116068Z         num_attn_heads: int,
2025-04-11T04:23:18.6116156Z         kv_group_num: int,
2025-04-11T04:23:18.6116240Z         same_context_len: bool,
2025-04-11T04:23:18.6116328Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6116415Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6116487Z     ):
2025-04-11T04:23:18.6116601Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6116794Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6116983Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6117157Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6117416Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6117492Z             return
2025-04-11T04:23:18.6117569Z     
2025-04-11T04:23:18.6117666Z         torch.manual_seed(123)
2025-04-11T04:23:18.6117764Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6117855Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6117946Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6117950Z 
2025-04-11T04:23:18.6118115Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6118231Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6118235Z 
2025-04-11T04:23:18.6118313Z device = None
2025-04-11T04:23:18.6118317Z 
2025-04-11T04:23:18.6118436Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6118587Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6118662Z     
2025-04-11T04:23:18.6118739Z         Args:
2025-04-11T04:23:18.6118905Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6119072Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6119178Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6119264Z         """
2025-04-11T04:23:18.6119341Z         _lazy_init()
2025-04-11T04:23:18.6119443Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6119543Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6119647Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6119931Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6120064Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6120229Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6120236Z 
2025-04-11T04:23:18.6120476Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6120646Z ____________ test_context_attention[False-False-False-4-16-16-16-7] ____________
2025-04-11T04:23:18.6120650Z 
2025-04-11T04:23:18.6120800Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6120948Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6121036Z use_new_kcache_layout = False
2025-04-11T04:23:18.6121040Z 
2025-04-11T04:23:18.6121240Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6121346Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6121462Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6121689Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6121805Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6121924Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6122060Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6122195Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6122349Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6122437Z     def test_context_attention(
2025-04-11T04:23:18.6122518Z         bsz: int,
2025-04-11T04:23:18.6122599Z         block_size: int,
2025-04-11T04:23:18.6122689Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6122774Z         num_attn_heads: int,
2025-04-11T04:23:18.6122858Z         kv_group_num: int,
2025-04-11T04:23:18.6122946Z         same_context_len: bool,
2025-04-11T04:23:18.6123029Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6123122Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6123194Z     ):
2025-04-11T04:23:18.6123303Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6123603Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6123785Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6123960Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6124126Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6124204Z             return
2025-04-11T04:23:18.6124277Z     
2025-04-11T04:23:18.6124364Z         torch.manual_seed(123)
2025-04-11T04:23:18.6124467Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6124554Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6124648Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6124655Z 
2025-04-11T04:23:18.6124824Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6124936Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6124947Z 
2025-04-11T04:23:18.6125024Z device = None
2025-04-11T04:23:18.6125027Z 
2025-04-11T04:23:18.6125143Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6125301Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6125374Z     
2025-04-11T04:23:18.6125452Z         Args:
2025-04-11T04:23:18.6125620Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6125788Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6125894Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6125967Z         """
2025-04-11T04:23:18.6126047Z         _lazy_init()
2025-04-11T04:23:18.6126146Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6126250Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6126357Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6126645Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6126788Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6126949Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6126953Z 
2025-04-11T04:23:18.6127200Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6127376Z ___________ test_context_attention[False-False-False-4-16-16-16-32] ____________
2025-04-11T04:23:18.6127380Z 
2025-04-11T04:23:18.6127536Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6127685Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6127865Z use_new_kcache_layout = False
2025-04-11T04:23:18.6127870Z 
2025-04-11T04:23:18.6128070Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6128178Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6128300Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6128440Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6128560Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6128675Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6128816Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6128951Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6129100Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6129194Z     def test_context_attention(
2025-04-11T04:23:18.6129274Z         bsz: int,
2025-04-11T04:23:18.6129361Z         block_size: int,
2025-04-11T04:23:18.6129451Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6129627Z         num_attn_heads: int,
2025-04-11T04:23:18.6129710Z         kv_group_num: int,
2025-04-11T04:23:18.6129799Z         same_context_len: bool,
2025-04-11T04:23:18.6129892Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6129986Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6130063Z     ):
2025-04-11T04:23:18.6130182Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6130372Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6130566Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6130736Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6130903Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6130980Z             return
2025-04-11T04:23:18.6131054Z     
2025-04-11T04:23:18.6131140Z         torch.manual_seed(123)
2025-04-11T04:23:18.6131242Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6131337Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6131429Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6131433Z 
2025-04-11T04:23:18.6131600Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6131711Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6131716Z 
2025-04-11T04:23:18.6131800Z device = None
2025-04-11T04:23:18.6131804Z 
2025-04-11T04:23:18.6131922Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6132069Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6132145Z     
2025-04-11T04:23:18.6132219Z         Args:
2025-04-11T04:23:18.6132386Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6132559Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6132673Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6132746Z         """
2025-04-11T04:23:18.6132822Z         _lazy_init()
2025-04-11T04:23:18.6132922Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6133022Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6133131Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6133412Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6133544Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6133705Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6133709Z 
2025-04-11T04:23:18.6134034Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6134207Z ____________ test_context_attention[False-False-False-4-16-16-32-7] ____________
2025-04-11T04:23:18.6134215Z 
2025-04-11T04:23:18.6134366Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6134515Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6134603Z use_new_kcache_layout = False
2025-04-11T04:23:18.6134607Z 
2025-04-11T04:23:18.6134810Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6134913Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6135030Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6135170Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6135286Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6135404Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6135542Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6135682Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6135930Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6136017Z     def test_context_attention(
2025-04-11T04:23:18.6136098Z         bsz: int,
2025-04-11T04:23:18.6136181Z         block_size: int,
2025-04-11T04:23:18.6136271Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6136355Z         num_attn_heads: int,
2025-04-11T04:23:18.6136443Z         kv_group_num: int,
2025-04-11T04:23:18.6136526Z         same_context_len: bool,
2025-04-11T04:23:18.6136609Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6136700Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6136774Z     ):
2025-04-11T04:23:18.6136886Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6137078Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6137262Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6137441Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6137604Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6137683Z             return
2025-04-11T04:23:18.6137756Z     
2025-04-11T04:23:18.6137844Z         torch.manual_seed(123)
2025-04-11T04:23:18.6137943Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6138032Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6138126Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6138130Z 
2025-04-11T04:23:18.6138296Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6138412Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6138419Z 
2025-04-11T04:23:18.6138496Z device = None
2025-04-11T04:23:18.6138499Z 
2025-04-11T04:23:18.6138618Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6138770Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6138841Z     
2025-04-11T04:23:18.6138918Z         Args:
2025-04-11T04:23:18.6139084Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6139252Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6139354Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6139431Z         """
2025-04-11T04:23:18.6139509Z         _lazy_init()
2025-04-11T04:23:18.6139601Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6139707Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6139812Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6140201Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6140336Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6140501Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6140505Z 
2025-04-11T04:23:18.6140744Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6140920Z ___________ test_context_attention[False-False-False-4-16-16-32-32] ____________
2025-04-11T04:23:18.6140929Z 
2025-04-11T04:23:18.6141081Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6141227Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6141321Z use_new_kcache_layout = False
2025-04-11T04:23:18.6141325Z 
2025-04-11T04:23:18.6141525Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6141638Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6141753Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6141983Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6142109Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6142229Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6142384Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6142528Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6142688Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6142776Z     def test_context_attention(
2025-04-11T04:23:18.6142853Z         bsz: int,
2025-04-11T04:23:18.6142942Z         block_size: int,
2025-04-11T04:23:18.6143032Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6143117Z         num_attn_heads: int,
2025-04-11T04:23:18.6143205Z         kv_group_num: int,
2025-04-11T04:23:18.6143292Z         same_context_len: bool,
2025-04-11T04:23:18.6143376Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6143471Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6143548Z     ):
2025-04-11T04:23:18.6143659Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6143857Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6144035Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6144204Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6144371Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6144444Z             return
2025-04-11T04:23:18.6144522Z     
2025-04-11T04:23:18.6144607Z         torch.manual_seed(123)
2025-04-11T04:23:18.6144711Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6144805Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6144896Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6144906Z 
2025-04-11T04:23:18.6145080Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6145194Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6145198Z 
2025-04-11T04:23:18.6145279Z device = None
2025-04-11T04:23:18.6145284Z 
2025-04-11T04:23:18.6145401Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6145550Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6145622Z     
2025-04-11T04:23:18.6145694Z         Args:
2025-04-11T04:23:18.6145862Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6146024Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6146132Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6146298Z         """
2025-04-11T04:23:18.6146380Z         _lazy_init()
2025-04-11T04:23:18.6146475Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6146581Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6146692Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6146975Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6147115Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6147273Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6147277Z 
2025-04-11T04:23:18.6147514Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6147678Z ______________ test_flash_decoding[True-False-1-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6147686Z 
2025-04-11T04:23:18.6147838Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6148001Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6148175Z use_new_kcache_layout = True
2025-04-11T04:23:18.6148180Z 
2025-04-11T04:23:18.6148384Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6148520Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6148643Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6148783Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6148903Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6149015Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6149152Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6149260Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6149402Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6149556Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6149645Z     def test_flash_decoding(
2025-04-11T04:23:18.6149726Z         bsz: int,
2025-04-11T04:23:18.6149811Z         block_size: int,
2025-04-11T04:23:18.6149902Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6149990Z         num_attn_heads: int,
2025-04-11T04:23:18.6150073Z         kv_group_num: int,
2025-04-11T04:23:18.6150160Z         same_context_len: bool,
2025-04-11T04:23:18.6150237Z         q_len: int,
2025-04-11T04:23:18.6150321Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6150412Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6150484Z     ):
2025-04-11T04:23:18.6150596Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6150791Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6150978Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6151153Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6151322Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6151486Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6151559Z     
2025-04-11T04:23:18.6151648Z         torch.manual_seed(123)
2025-04-11T04:23:18.6151737Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6151828Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6151833Z 
2025-04-11T04:23:18.6151991Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6152103Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6152107Z 
2025-04-11T04:23:18.6152189Z device = None
2025-04-11T04:23:18.6152193Z 
2025-04-11T04:23:18.6152401Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6152556Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6152632Z     
2025-04-11T04:23:18.6152707Z         Args:
2025-04-11T04:23:18.6152878Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6153046Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6153153Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6153227Z         """
2025-04-11T04:23:18.6153317Z         _lazy_init()
2025-04-11T04:23:18.6153420Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6153527Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6153648Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6153932Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6154077Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6154238Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6154336Z 
2025-04-11T04:23:18.6154579Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6154745Z _____________ test_flash_decoding[True-False-1-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6154749Z 
2025-04-11T04:23:18.6154902Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6155064Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6155153Z use_new_kcache_layout = True
2025-04-11T04:23:18.6155157Z 
2025-04-11T04:23:18.6155356Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6155463Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6155588Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6155726Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6155848Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6155960Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6156096Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6156201Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6156335Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6156490Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6156575Z     def test_flash_decoding(
2025-04-11T04:23:18.6156655Z         bsz: int,
2025-04-11T04:23:18.6156737Z         block_size: int,
2025-04-11T04:23:18.6156828Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6156916Z         num_attn_heads: int,
2025-04-11T04:23:18.6156998Z         kv_group_num: int,
2025-04-11T04:23:18.6157091Z         same_context_len: bool,
2025-04-11T04:23:18.6157167Z         q_len: int,
2025-04-11T04:23:18.6157251Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6157345Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6157419Z     ):
2025-04-11T04:23:18.6157531Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6157721Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6157900Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6158072Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6158234Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6158394Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6158466Z     
2025-04-11T04:23:18.6158555Z         torch.manual_seed(123)
2025-04-11T04:23:18.6158743Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6158835Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6158843Z 
2025-04-11T04:23:18.6159003Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6159117Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6159120Z 
2025-04-11T04:23:18.6159201Z device = None
2025-04-11T04:23:18.6159206Z 
2025-04-11T04:23:18.6159321Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6159475Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6159546Z     
2025-04-11T04:23:18.6159623Z         Args:
2025-04-11T04:23:18.6159789Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6159954Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6160065Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6160140Z         """
2025-04-11T04:23:18.6160222Z         _lazy_init()
2025-04-11T04:23:18.6160315Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6160502Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6160614Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6160900Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6161043Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6161204Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6161209Z 
2025-04-11T04:23:18.6161452Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6161623Z ______________ test_flash_decoding[True-False-1-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6161627Z 
2025-04-11T04:23:18.6161787Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6161955Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6162051Z use_new_kcache_layout = True
2025-04-11T04:23:18.6162059Z 
2025-04-11T04:23:18.6162259Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6162367Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6162490Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6162631Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6162753Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6162871Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6163010Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6163121Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6163265Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6163422Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6163514Z     def test_flash_decoding(
2025-04-11T04:23:18.6163596Z         bsz: int,
2025-04-11T04:23:18.6163682Z         block_size: int,
2025-04-11T04:23:18.6163776Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6163869Z         num_attn_heads: int,
2025-04-11T04:23:18.6163954Z         kv_group_num: int,
2025-04-11T04:23:18.6164048Z         same_context_len: bool,
2025-04-11T04:23:18.6164133Z         q_len: int,
2025-04-11T04:23:18.6164228Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6164325Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6164406Z     ):
2025-04-11T04:23:18.6164523Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6164724Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6164914Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6165169Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6165340Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6165504Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6165576Z     
2025-04-11T04:23:18.6165664Z         torch.manual_seed(123)
2025-04-11T04:23:18.6165752Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6165846Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6165850Z 
2025-04-11T04:23:18.6166005Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6166118Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6166122Z 
2025-04-11T04:23:18.6166202Z device = None
2025-04-11T04:23:18.6166206Z 
2025-04-11T04:23:18.6166322Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6166479Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6166637Z     
2025-04-11T04:23:18.6166717Z         Args:
2025-04-11T04:23:18.6166888Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6167052Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6167163Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6167237Z         """
2025-04-11T04:23:18.6167317Z         _lazy_init()
2025-04-11T04:23:18.6167412Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6167513Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6167620Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6167900Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6168043Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6168198Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6168205Z 
2025-04-11T04:23:18.6168449Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6168613Z _____________ test_flash_decoding[True-False-1-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6168617Z 
2025-04-11T04:23:18.6168768Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6168932Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6169020Z use_new_kcache_layout = True
2025-04-11T04:23:18.6169028Z 
2025-04-11T04:23:18.6169227Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6169329Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6169453Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6169588Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6169711Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6169823Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6169959Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6170065Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6170198Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6170349Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6170434Z     def test_flash_decoding(
2025-04-11T04:23:18.6170512Z         bsz: int,
2025-04-11T04:23:18.6170594Z         block_size: int,
2025-04-11T04:23:18.6170683Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6170769Z         num_attn_heads: int,
2025-04-11T04:23:18.6170850Z         kv_group_num: int,
2025-04-11T04:23:18.6171028Z         same_context_len: bool,
2025-04-11T04:23:18.6171103Z         q_len: int,
2025-04-11T04:23:18.6171186Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6171281Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6171354Z     ):
2025-04-11T04:23:18.6171468Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6171662Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6171849Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6172019Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6172181Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6172344Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6172417Z     
2025-04-11T04:23:18.6172506Z         torch.manual_seed(123)
2025-04-11T04:23:18.6172599Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6172695Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6172699Z 
2025-04-11T04:23:18.6172934Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6173046Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6173054Z 
2025-04-11T04:23:18.6173132Z device = None
2025-04-11T04:23:18.6173136Z 
2025-04-11T04:23:18.6173252Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6173405Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6173478Z     
2025-04-11T04:23:18.6173555Z         Args:
2025-04-11T04:23:18.6173719Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6173884Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6173994Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6174069Z         """
2025-04-11T04:23:18.6174152Z         _lazy_init()
2025-04-11T04:23:18.6174250Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6174365Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6174476Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6174763Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6174903Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6175062Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6175067Z 
2025-04-11T04:23:18.6175309Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6175476Z _____________ test_flash_decoding[True-False-1-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6175480Z 
2025-04-11T04:23:18.6175638Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6175800Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6175894Z use_new_kcache_layout = True
2025-04-11T04:23:18.6175898Z 
2025-04-11T04:23:18.6176096Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6176200Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6176319Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6176457Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6176575Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6176686Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6176826Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6176928Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6177062Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6177302Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6177392Z     def test_flash_decoding(
2025-04-11T04:23:18.6177472Z         bsz: int,
2025-04-11T04:23:18.6177556Z         block_size: int,
2025-04-11T04:23:18.6177645Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6177733Z         num_attn_heads: int,
2025-04-11T04:23:18.6177814Z         kv_group_num: int,
2025-04-11T04:23:18.6177904Z         same_context_len: bool,
2025-04-11T04:23:18.6177979Z         q_len: int,
2025-04-11T04:23:18.6178067Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6178154Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6178227Z     ):
2025-04-11T04:23:18.6178339Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6178530Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6178714Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6178888Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6179139Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6179302Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6179375Z     
2025-04-11T04:23:18.6179466Z         torch.manual_seed(123)
2025-04-11T04:23:18.6179554Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6179648Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6179653Z 
2025-04-11T04:23:18.6179807Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6179919Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6179926Z 
2025-04-11T04:23:18.6180004Z device = None
2025-04-11T04:23:18.6180009Z 
2025-04-11T04:23:18.6180126Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6180284Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6180359Z     
2025-04-11T04:23:18.6180435Z         Args:
2025-04-11T04:23:18.6180602Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6180767Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6180874Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6180946Z         """
2025-04-11T04:23:18.6181029Z         _lazy_init()
2025-04-11T04:23:18.6181122Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6181224Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6181328Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6181615Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6181757Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6181914Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6181922Z 
2025-04-11T04:23:18.6182170Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6182336Z _____________ test_flash_decoding[True-False-1-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6182341Z 
2025-04-11T04:23:18.6182496Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6182661Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6182752Z use_new_kcache_layout = True
2025-04-11T04:23:18.6182757Z 
2025-04-11T04:23:18.6182957Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6183061Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6183180Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6183424Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6183541Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6183659Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6183799Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6183903Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6184038Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6184190Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6184277Z     def test_flash_decoding(
2025-04-11T04:23:18.6184357Z         bsz: int,
2025-04-11T04:23:18.6184439Z         block_size: int,
2025-04-11T04:23:18.6184528Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6184620Z         num_attn_heads: int,
2025-04-11T04:23:18.6184709Z         kv_group_num: int,
2025-04-11T04:23:18.6184808Z         same_context_len: bool,
2025-04-11T04:23:18.6184883Z         q_len: int,
2025-04-11T04:23:18.6184981Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6185174Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6185250Z     ):
2025-04-11T04:23:18.6185373Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6185566Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6185756Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6185929Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6186098Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6186256Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6186329Z     
2025-04-11T04:23:18.6186419Z         torch.manual_seed(123)
2025-04-11T04:23:18.6186513Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6186607Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6186611Z 
2025-04-11T04:23:18.6186771Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6186882Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6186890Z 
2025-04-11T04:23:18.6186968Z device = None
2025-04-11T04:23:18.6186973Z 
2025-04-11T04:23:18.6187091Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6187248Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6187320Z     
2025-04-11T04:23:18.6187398Z         Args:
2025-04-11T04:23:18.6187568Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6187736Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6187839Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6187917Z         """
2025-04-11T04:23:18.6188000Z         _lazy_init()
2025-04-11T04:23:18.6188094Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6188203Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6188307Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6188789Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6188930Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6189087Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6189091Z 
2025-04-11T04:23:18.6189336Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6189501Z _____________ test_flash_decoding[True-False-1-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6189505Z 
2025-04-11T04:23:18.6189758Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6189925Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6190020Z use_new_kcache_layout = True
2025-04-11T04:23:18.6190025Z 
2025-04-11T04:23:18.6190222Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6190326Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6190445Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6190582Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6190701Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6190815Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6190957Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6191060Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6191193Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6191349Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6191535Z     def test_flash_decoding(
2025-04-11T04:23:18.6191616Z         bsz: int,
2025-04-11T04:23:18.6191699Z         block_size: int,
2025-04-11T04:23:18.6191793Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6191876Z         num_attn_heads: int,
2025-04-11T04:23:18.6191958Z         kv_group_num: int,
2025-04-11T04:23:18.6192049Z         same_context_len: bool,
2025-04-11T04:23:18.6192125Z         q_len: int,
2025-04-11T04:23:18.6192213Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6192300Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6192371Z     ):
2025-04-11T04:23:18.6192486Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6192680Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6192866Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6193042Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6193212Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6193373Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6193445Z     
2025-04-11T04:23:18.6193535Z         torch.manual_seed(123)
2025-04-11T04:23:18.6193624Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6193717Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6193721Z 
2025-04-11T04:23:18.6193875Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6193988Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6193992Z 
2025-04-11T04:23:18.6194072Z device = None
2025-04-11T04:23:18.6194075Z 
2025-04-11T04:23:18.6194190Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6194347Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6194420Z     
2025-04-11T04:23:18.6194500Z         Args:
2025-04-11T04:23:18.6194664Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6194832Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6194936Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6195009Z         """
2025-04-11T04:23:18.6195093Z         _lazy_init()
2025-04-11T04:23:18.6195187Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6195292Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6195397Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6195680Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6195908Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6196064Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6196072Z 
2025-04-11T04:23:18.6196313Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6196478Z _____________ test_flash_decoding[True-False-1-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6196482Z 
2025-04-11T04:23:18.6196638Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6196800Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6196892Z use_new_kcache_layout = True
2025-04-11T04:23:18.6196896Z 
2025-04-11T04:23:18.6197092Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6197202Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6197322Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6197462Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6197583Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6197785Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6197925Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6198029Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6198168Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6198320Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6198404Z     def test_flash_decoding(
2025-04-11T04:23:18.6198484Z         bsz: int,
2025-04-11T04:23:18.6198567Z         block_size: int,
2025-04-11T04:23:18.6198665Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6198745Z         num_attn_heads: int,
2025-04-11T04:23:18.6198828Z         kv_group_num: int,
2025-04-11T04:23:18.6198921Z         same_context_len: bool,
2025-04-11T04:23:18.6198998Z         q_len: int,
2025-04-11T04:23:18.6199085Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6199172Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6199249Z     ):
2025-04-11T04:23:18.6199366Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6199559Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6199744Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6199913Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6200078Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6200234Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6200307Z     
2025-04-11T04:23:18.6200396Z         torch.manual_seed(123)
2025-04-11T04:23:18.6200487Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6200579Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6200582Z 
2025-04-11T04:23:18.6200739Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6200851Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6200855Z 
2025-04-11T04:23:18.6200933Z device = None
2025-04-11T04:23:18.6200937Z 
2025-04-11T04:23:18.6201052Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6201204Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6201275Z     
2025-04-11T04:23:18.6201353Z         Args:
2025-04-11T04:23:18.6201516Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6201683Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6201788Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6202030Z         """
2025-04-11T04:23:18.6202112Z         _lazy_init()
2025-04-11T04:23:18.6202207Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6202317Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6202421Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6202708Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6202843Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6202999Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6203003Z 
2025-04-11T04:23:18.6203245Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6203409Z ______________ test_flash_decoding[True-False-1-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6203413Z 
2025-04-11T04:23:18.6203570Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6203734Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6203921Z use_new_kcache_layout = True
2025-04-11T04:23:18.6203926Z 
2025-04-11T04:23:18.6204126Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6204234Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6204350Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6204488Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6204607Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6204720Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6204859Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6204961Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6205098Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6205252Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6205337Z     def test_flash_decoding(
2025-04-11T04:23:18.6205420Z         bsz: int,
2025-04-11T04:23:18.6205502Z         block_size: int,
2025-04-11T04:23:18.6205596Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6205677Z         num_attn_heads: int,
2025-04-11T04:23:18.6205760Z         kv_group_num: int,
2025-04-11T04:23:18.6205850Z         same_context_len: bool,
2025-04-11T04:23:18.6205925Z         q_len: int,
2025-04-11T04:23:18.6206012Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6206100Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6206173Z     ):
2025-04-11T04:23:18.6206287Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6206479Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6206662Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6206836Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6207006Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6207163Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6207236Z     
2025-04-11T04:23:18.6207326Z         torch.manual_seed(123)
2025-04-11T04:23:18.6207414Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6207507Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6207511Z 
2025-04-11T04:23:18.6207664Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6207780Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6207784Z 
2025-04-11T04:23:18.6207861Z device = None
2025-04-11T04:23:18.6207865Z 
2025-04-11T04:23:18.6207985Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6208216Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6208288Z     
2025-04-11T04:23:18.6208370Z         Args:
2025-04-11T04:23:18.6208536Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6208705Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6208812Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6208886Z         """
2025-04-11T04:23:18.6208968Z         _lazy_init()
2025-04-11T04:23:18.6209063Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6209168Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6209275Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6209566Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6209709Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6209873Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6209969Z 
2025-04-11T04:23:18.6210222Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6210389Z _____________ test_flash_decoding[True-False-1-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6210393Z 
2025-04-11T04:23:18.6210551Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6210715Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6210808Z use_new_kcache_layout = True
2025-04-11T04:23:18.6210812Z 
2025-04-11T04:23:18.6211015Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6211124Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6211239Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6211379Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6211504Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6211620Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6211760Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6211862Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6211999Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6212152Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6212239Z     def test_flash_decoding(
2025-04-11T04:23:18.6212323Z         bsz: int,
2025-04-11T04:23:18.6212406Z         block_size: int,
2025-04-11T04:23:18.6212499Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6212582Z         num_attn_heads: int,
2025-04-11T04:23:18.6212667Z         kv_group_num: int,
2025-04-11T04:23:18.6212757Z         same_context_len: bool,
2025-04-11T04:23:18.6212836Z         q_len: int,
2025-04-11T04:23:18.6212927Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6213015Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6213097Z     ):
2025-04-11T04:23:18.6213209Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6213403Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6213585Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6213755Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6213920Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6214081Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6214157Z     
2025-04-11T04:23:18.6214244Z         torch.manual_seed(123)
2025-04-11T04:23:18.6214420Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6214517Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6214521Z 
2025-04-11T04:23:18.6214676Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6214799Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6214804Z 
2025-04-11T04:23:18.6214881Z device = None
2025-04-11T04:23:18.6214885Z 
2025-04-11T04:23:18.6215003Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6215152Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6215223Z     
2025-04-11T04:23:18.6215303Z         Args:
2025-04-11T04:23:18.6215469Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6215639Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6215745Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6215825Z         """
2025-04-11T04:23:18.6215904Z         _lazy_init()
2025-04-11T04:23:18.6215999Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6216188Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6216295Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6216581Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6216715Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6216872Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6216879Z 
2025-04-11T04:23:18.6217117Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6217281Z ______________ test_flash_decoding[True-False-1-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6217285Z 
2025-04-11T04:23:18.6217438Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6217605Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6217700Z use_new_kcache_layout = True
2025-04-11T04:23:18.6217705Z 
2025-04-11T04:23:18.6217903Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6218010Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6218125Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6218262Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6218382Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6218496Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6218635Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6218735Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6218874Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6219025Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6219112Z     def test_flash_decoding(
2025-04-11T04:23:18.6219201Z         bsz: int,
2025-04-11T04:23:18.6219283Z         block_size: int,
2025-04-11T04:23:18.6219382Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6219468Z         num_attn_heads: int,
2025-04-11T04:23:18.6219550Z         kv_group_num: int,
2025-04-11T04:23:18.6219644Z         same_context_len: bool,
2025-04-11T04:23:18.6219719Z         q_len: int,
2025-04-11T04:23:18.6219808Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6219895Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6219972Z     ):
2025-04-11T04:23:18.6220080Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6220270Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6220457Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6220715Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6220886Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6221045Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6221123Z     
2025-04-11T04:23:18.6221210Z         torch.manual_seed(123)
2025-04-11T04:23:18.6221299Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6221396Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6221400Z 
2025-04-11T04:23:18.6221555Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6221669Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6221673Z 
2025-04-11T04:23:18.6221751Z device = None
2025-04-11T04:23:18.6221756Z 
2025-04-11T04:23:18.6221877Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6222031Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6222103Z     
2025-04-11T04:23:18.6222262Z         Args:
2025-04-11T04:23:18.6222429Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6222599Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6222703Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6222779Z         """
2025-04-11T04:23:18.6222858Z         _lazy_init()
2025-04-11T04:23:18.6222954Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6223058Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6223162Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6223448Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6223588Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6223749Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6223756Z 
2025-04-11T04:23:18.6223992Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6224156Z _____________ test_flash_decoding[True-False-1-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6224163Z 
2025-04-11T04:23:18.6224313Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6224475Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6224568Z use_new_kcache_layout = True
2025-04-11T04:23:18.6224572Z 
2025-04-11T04:23:18.6224770Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6224880Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6224994Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6225135Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6225255Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6225372Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6225514Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6225619Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6225762Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6225912Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6225999Z     def test_flash_decoding(
2025-04-11T04:23:18.6226080Z         bsz: int,
2025-04-11T04:23:18.6226165Z         block_size: int,
2025-04-11T04:23:18.6226262Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6226343Z         num_attn_heads: int,
2025-04-11T04:23:18.6226432Z         kv_group_num: int,
2025-04-11T04:23:18.6226519Z         same_context_len: bool,
2025-04-11T04:23:18.6226687Z         q_len: int,
2025-04-11T04:23:18.6226778Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6226869Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6226952Z     ):
2025-04-11T04:23:18.6227062Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6227253Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6227441Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6227611Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6227779Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6227935Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6228012Z     
2025-04-11T04:23:18.6228099Z         torch.manual_seed(123)
2025-04-11T04:23:18.6228191Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6228286Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6228291Z 
2025-04-11T04:23:18.6228471Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6228704Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6228709Z 
2025-04-11T04:23:18.6228786Z device = None
2025-04-11T04:23:18.6228791Z 
2025-04-11T04:23:18.6228913Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6229063Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6229135Z     
2025-04-11T04:23:18.6229225Z         Args:
2025-04-11T04:23:18.6229395Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6229572Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6229680Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6229767Z         """
2025-04-11T04:23:18.6229847Z         _lazy_init()
2025-04-11T04:23:18.6229947Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6230062Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6230166Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6230459Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6230594Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6230754Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6230758Z 
2025-04-11T04:23:18.6230996Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6231163Z _____________ test_flash_decoding[True-False-1-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6231171Z 
2025-04-11T04:23:18.6231322Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6231490Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6231584Z use_new_kcache_layout = True
2025-04-11T04:23:18.6231589Z 
2025-04-11T04:23:18.6231785Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6231893Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6232010Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6232150Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6232265Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6232376Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6232519Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6232622Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6232760Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6233004Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6233092Z     def test_flash_decoding(
2025-04-11T04:23:18.6233175Z         bsz: int,
2025-04-11T04:23:18.6233257Z         block_size: int,
2025-04-11T04:23:18.6233352Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6233435Z         num_attn_heads: int,
2025-04-11T04:23:18.6233519Z         kv_group_num: int,
2025-04-11T04:23:18.6233605Z         same_context_len: bool,
2025-04-11T04:23:18.6233682Z         q_len: int,
2025-04-11T04:23:18.6233770Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6233859Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6233937Z     ):
2025-04-11T04:23:18.6234047Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6234237Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6234423Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6234594Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6234847Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6235005Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6235082Z     
2025-04-11T04:23:18.6235168Z         torch.manual_seed(123)
2025-04-11T04:23:18.6235256Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6235350Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6235354Z 
2025-04-11T04:23:18.6235507Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6235621Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6235625Z 
2025-04-11T04:23:18.6235702Z device = None
2025-04-11T04:23:18.6235706Z 
2025-04-11T04:23:18.6235825Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6235977Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6236054Z     
2025-04-11T04:23:18.6236131Z         Args:
2025-04-11T04:23:18.6236296Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6236464Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6236568Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6236644Z         """
2025-04-11T04:23:18.6236722Z         _lazy_init()
2025-04-11T04:23:18.6236817Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6236922Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6237027Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6237314Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6237450Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6237611Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6237618Z 
2025-04-11T04:23:18.6237854Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6238020Z _____________ test_flash_decoding[True-False-1-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6238028Z 
2025-04-11T04:23:18.6238181Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6238345Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6238438Z use_new_kcache_layout = True
2025-04-11T04:23:18.6238443Z 
2025-04-11T04:23:18.6238639Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6238746Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6238861Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6239087Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6239205Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6239318Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6239460Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6239564Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6239704Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6239851Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6239940Z     def test_flash_decoding(
2025-04-11T04:23:18.6240018Z         bsz: int,
2025-04-11T04:23:18.6240101Z         block_size: int,
2025-04-11T04:23:18.6240194Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6240275Z         num_attn_heads: int,
2025-04-11T04:23:18.6240361Z         kv_group_num: int,
2025-04-11T04:23:18.6240448Z         same_context_len: bool,
2025-04-11T04:23:18.6240529Z         q_len: int,
2025-04-11T04:23:18.6240618Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6240705Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6240866Z     ):
2025-04-11T04:23:18.6240978Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6241173Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6241358Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6241528Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6241696Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6241853Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6241932Z     
2025-04-11T04:23:18.6242024Z         torch.manual_seed(123)
2025-04-11T04:23:18.6242117Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6242218Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6242222Z 
2025-04-11T04:23:18.6242377Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6242503Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6242507Z 
2025-04-11T04:23:18.6242585Z device = None
2025-04-11T04:23:18.6242589Z 
2025-04-11T04:23:18.6242717Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6242867Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6242941Z     
2025-04-11T04:23:18.6243016Z         Args:
2025-04-11T04:23:18.6243182Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6243352Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6243458Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6243537Z         """
2025-04-11T04:23:18.6243615Z         _lazy_init()
2025-04-11T04:23:18.6243711Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6243819Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6243925Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6244212Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6244346Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6244513Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6244517Z 
2025-04-11T04:23:18.6244756Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6244925Z _____________ test_flash_decoding[True-False-1-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6244929Z 
2025-04-11T04:23:18.6245079Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6245346Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6245443Z use_new_kcache_layout = True
2025-04-11T04:23:18.6245447Z 
2025-04-11T04:23:18.6245644Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6245753Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6245867Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6246009Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6246126Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6246238Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6246378Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6246480Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6246618Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6246771Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6246860Z     def test_flash_decoding(
2025-04-11T04:23:18.6247022Z         bsz: int,
2025-04-11T04:23:18.6247107Z         block_size: int,
2025-04-11T04:23:18.6247200Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6247284Z         num_attn_heads: int,
2025-04-11T04:23:18.6247370Z         kv_group_num: int,
2025-04-11T04:23:18.6247455Z         same_context_len: bool,
2025-04-11T04:23:18.6247531Z         q_len: int,
2025-04-11T04:23:18.6247621Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6247708Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6247784Z     ):
2025-04-11T04:23:18.6247894Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6248088Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6248271Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6248443Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6248613Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6248770Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6248847Z     
2025-04-11T04:23:18.6248934Z         torch.manual_seed(123)
2025-04-11T04:23:18.6249029Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6249118Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6249123Z 
2025-04-11T04:23:18.6249276Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6249391Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6249394Z 
2025-04-11T04:23:18.6249471Z device = None
2025-04-11T04:23:18.6249475Z 
2025-04-11T04:23:18.6249595Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6249746Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6249821Z     
2025-04-11T04:23:18.6249895Z         Args:
2025-04-11T04:23:18.6250063Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6250230Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6250335Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6250411Z         """
2025-04-11T04:23:18.6250488Z         _lazy_init()
2025-04-11T04:23:18.6250584Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6250690Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6250793Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6251080Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6251213Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6251473Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6251481Z 
2025-04-11T04:23:18.6251720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6251893Z _____________ test_flash_decoding[True-False-1-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6251897Z 
2025-04-11T04:23:18.6252047Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6252210Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6252303Z use_new_kcache_layout = True
2025-04-11T04:23:18.6252307Z 
2025-04-11T04:23:18.6252502Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6252609Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6252725Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6252869Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6252983Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6253182Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6253322Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6253423Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6253563Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6253712Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6253802Z     def test_flash_decoding(
2025-04-11T04:23:18.6253879Z         bsz: int,
2025-04-11T04:23:18.6253962Z         block_size: int,
2025-04-11T04:23:18.6254061Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6254147Z         num_attn_heads: int,
2025-04-11T04:23:18.6254238Z         kv_group_num: int,
2025-04-11T04:23:18.6254323Z         same_context_len: bool,
2025-04-11T04:23:18.6254405Z         q_len: int,
2025-04-11T04:23:18.6254497Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6254584Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6254663Z     ):
2025-04-11T04:23:18.6254772Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6254968Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6255147Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6255316Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6255482Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6255639Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6255718Z     
2025-04-11T04:23:18.6255803Z         torch.manual_seed(123)
2025-04-11T04:23:18.6255893Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6255988Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6255992Z 
2025-04-11T04:23:18.6256146Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6256264Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6256269Z 
2025-04-11T04:23:18.6256345Z device = None
2025-04-11T04:23:18.6256349Z 
2025-04-11T04:23:18.6256466Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6256613Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6256687Z     
2025-04-11T04:23:18.6256760Z         Args:
2025-04-11T04:23:18.6256935Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6257109Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6257212Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6257383Z         """
2025-04-11T04:23:18.6257460Z         _lazy_init()
2025-04-11T04:23:18.6257558Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6257665Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6257771Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6258061Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6258196Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6258358Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6258362Z 
2025-04-11T04:23:18.6258599Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6258770Z _____________ test_flash_decoding[True-False-1-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6258774Z 
2025-04-11T04:23:18.6258924Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6259094Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6259266Z use_new_kcache_layout = True
2025-04-11T04:23:18.6259271Z 
2025-04-11T04:23:18.6259473Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6259584Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6259703Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6259846Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6259961Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6260077Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6260213Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6260317Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6260458Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6260611Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6260701Z     def test_flash_decoding(
2025-04-11T04:23:18.6260781Z         bsz: int,
2025-04-11T04:23:18.6260863Z         block_size: int,
2025-04-11T04:23:18.6260955Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6261036Z         num_attn_heads: int,
2025-04-11T04:23:18.6261121Z         kv_group_num: int,
2025-04-11T04:23:18.6261206Z         same_context_len: bool,
2025-04-11T04:23:18.6261285Z         q_len: int,
2025-04-11T04:23:18.6261370Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6261457Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6261534Z     ):
2025-04-11T04:23:18.6261643Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6261839Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6262025Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6262201Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6262380Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6262542Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6262618Z     
2025-04-11T04:23:18.6262704Z         torch.manual_seed(123)
2025-04-11T04:23:18.6262796Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6262887Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6262891Z 
2025-04-11T04:23:18.6263047Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6263161Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6263165Z 
2025-04-11T04:23:18.6263242Z device = None
2025-04-11T04:23:18.6263246Z 
2025-04-11T04:23:18.6263368Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6263605Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6263681Z     
2025-04-11T04:23:18.6263756Z         Args:
2025-04-11T04:23:18.6263930Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6264097Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6264201Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6264278Z         """
2025-04-11T04:23:18.6264356Z         _lazy_init()
2025-04-11T04:23:18.6264454Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6264556Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6264661Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6264949Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6265085Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6265251Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6265338Z 
2025-04-11T04:23:18.6265581Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6265754Z _____________ test_flash_decoding[True-False-1-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.6265759Z 
2025-04-11T04:23:18.6265909Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6266079Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6266167Z use_new_kcache_layout = True
2025-04-11T04:23:18.6266171Z 
2025-04-11T04:23:18.6266367Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6266475Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6266592Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6266737Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6266852Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6266972Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6267109Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6267213Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6267356Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6267505Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6267593Z     def test_flash_decoding(
2025-04-11T04:23:18.6267669Z         bsz: int,
2025-04-11T04:23:18.6267752Z         block_size: int,
2025-04-11T04:23:18.6267846Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6267928Z         num_attn_heads: int,
2025-04-11T04:23:18.6268016Z         kv_group_num: int,
2025-04-11T04:23:18.6268103Z         same_context_len: bool,
2025-04-11T04:23:18.6268185Z         q_len: int,
2025-04-11T04:23:18.6268269Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6268357Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6268471Z     ):
2025-04-11T04:23:18.6268586Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6268784Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6268966Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6269136Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6269305Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6269463Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6269541Z     
2025-04-11T04:23:18.6269628Z         torch.manual_seed(123)
2025-04-11T04:23:18.6269723Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6269912Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6269916Z 
2025-04-11T04:23:18.6270073Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6270192Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6270196Z 
2025-04-11T04:23:18.6270275Z device = None
2025-04-11T04:23:18.6270280Z 
2025-04-11T04:23:18.6270401Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6270550Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6270626Z     
2025-04-11T04:23:18.6270705Z         Args:
2025-04-11T04:23:18.6270872Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6271048Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6271158Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6271245Z         """
2025-04-11T04:23:18.6271324Z         _lazy_init()
2025-04-11T04:23:18.6271421Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6271522Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6271723Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6272018Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6272153Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6272313Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6272318Z 
2025-04-11T04:23:18.6272557Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6272728Z _____________ test_flash_decoding[True-False-1-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.6272733Z 
2025-04-11T04:23:18.6272883Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6273056Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6273147Z use_new_kcache_layout = True
2025-04-11T04:23:18.6273151Z 
2025-04-11T04:23:18.6273351Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6273460Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6273576Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6273718Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6273834Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6273952Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6274087Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6274188Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6274326Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6274480Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6274570Z     def test_flash_decoding(
2025-04-11T04:23:18.6274650Z         bsz: int,
2025-04-11T04:23:18.6274735Z         block_size: int,
2025-04-11T04:23:18.6274825Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6274907Z         num_attn_heads: int,
2025-04-11T04:23:18.6274993Z         kv_group_num: int,
2025-04-11T04:23:18.6275078Z         same_context_len: bool,
2025-04-11T04:23:18.6275159Z         q_len: int,
2025-04-11T04:23:18.6275243Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6275331Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6275407Z     ):
2025-04-11T04:23:18.6275517Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6275710Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6275894Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6276172Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6276339Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6276498Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6276576Z     
2025-04-11T04:23:18.6276664Z         torch.manual_seed(123)
2025-04-11T04:23:18.6276758Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6276847Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6276851Z 
2025-04-11T04:23:18.6277005Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6277117Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6277122Z 
2025-04-11T04:23:18.6277199Z device = None
2025-04-11T04:23:18.6277203Z 
2025-04-11T04:23:18.6277321Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6277474Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6277550Z     
2025-04-11T04:23:18.6277624Z         Args:
2025-04-11T04:23:18.6277879Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6278044Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6278149Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6278227Z         """
2025-04-11T04:23:18.6278303Z         _lazy_init()
2025-04-11T04:23:18.6278400Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6278501Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6278604Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6278890Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6279027Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6279191Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6279199Z 
2025-04-11T04:23:18.6279436Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6279606Z _____________ test_flash_decoding[True-False-1-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.6279610Z 
2025-04-11T04:23:18.6279760Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6279928Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6280016Z use_new_kcache_layout = True
2025-04-11T04:23:18.6280020Z 
2025-04-11T04:23:18.6280224Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6280338Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6280464Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6280619Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6280742Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6280861Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6280994Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6281095Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6281234Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6281380Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6281471Z     def test_flash_decoding(
2025-04-11T04:23:18.6281547Z         bsz: int,
2025-04-11T04:23:18.6281631Z         block_size: int,
2025-04-11T04:23:18.6281719Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6281799Z         num_attn_heads: int,
2025-04-11T04:23:18.6281886Z         kv_group_num: int,
2025-04-11T04:23:18.6281972Z         same_context_len: bool,
2025-04-11T04:23:18.6282139Z         q_len: int,
2025-04-11T04:23:18.6282226Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6282315Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6282396Z     ):
2025-04-11T04:23:18.6282505Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6282704Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6282886Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6283060Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6283222Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6283380Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6283459Z     
2025-04-11T04:23:18.6283546Z         torch.manual_seed(123)
2025-04-11T04:23:18.6283638Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6283731Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6283735Z 
2025-04-11T04:23:18.6283891Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6284088Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6284093Z 
2025-04-11T04:23:18.6284171Z device = None
2025-04-11T04:23:18.6284178Z 
2025-04-11T04:23:18.6284295Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6284443Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6284521Z     
2025-04-11T04:23:18.6284595Z         Args:
2025-04-11T04:23:18.6284764Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6284929Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6285034Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6285115Z         """
2025-04-11T04:23:18.6285192Z         _lazy_init()
2025-04-11T04:23:18.6285291Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6285391Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6285498Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6285781Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6285916Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6286076Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6286081Z 
2025-04-11T04:23:18.6286316Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6286487Z _____________ test_flash_decoding[True-False-1-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.6286492Z 
2025-04-11T04:23:18.6286643Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6286815Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6286900Z use_new_kcache_layout = True
2025-04-11T04:23:18.6286908Z 
2025-04-11T04:23:18.6287111Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6287215Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6287331Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6287471Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6287587Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6287704Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6287840Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6287944Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6288079Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6288317Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6288406Z     def test_flash_decoding(
2025-04-11T04:23:18.6288487Z         bsz: int,
2025-04-11T04:23:18.6288575Z         block_size: int,
2025-04-11T04:23:18.6288666Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6288747Z         num_attn_heads: int,
2025-04-11T04:23:18.6288833Z         kv_group_num: int,
2025-04-11T04:23:18.6288918Z         same_context_len: bool,
2025-04-11T04:23:18.6288998Z         q_len: int,
2025-04-11T04:23:18.6289081Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6289169Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6289247Z     ):
2025-04-11T04:23:18.6289355Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6289551Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6289731Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6289907Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6290069Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6290311Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6290390Z     
2025-04-11T04:23:18.6290482Z         torch.manual_seed(123)
2025-04-11T04:23:18.6290582Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6290673Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6290678Z 
2025-04-11T04:23:18.6290842Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6290953Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6290957Z 
2025-04-11T04:23:18.6291035Z device = None
2025-04-11T04:23:18.6291043Z 
2025-04-11T04:23:18.6291159Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6291315Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6291393Z     
2025-04-11T04:23:18.6291465Z         Args:
2025-04-11T04:23:18.6291636Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6291801Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6291906Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6291982Z         """
2025-04-11T04:23:18.6292059Z         _lazy_init()
2025-04-11T04:23:18.6292158Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6292260Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6292368Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6292649Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6292781Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6292948Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6292952Z 
2025-04-11T04:23:18.6293193Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6293365Z ____________ test_flash_decoding[True-False-1-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.6293369Z 
2025-04-11T04:23:18.6293518Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6293686Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6293774Z use_new_kcache_layout = True
2025-04-11T04:23:18.6293778Z 
2025-04-11T04:23:18.6293981Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6294085Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6294201Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6294429Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6294544Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6294663Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6294799Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6294906Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6295040Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6295190Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6295280Z     def test_flash_decoding(
2025-04-11T04:23:18.6295359Z         bsz: int,
2025-04-11T04:23:18.6295443Z         block_size: int,
2025-04-11T04:23:18.6295533Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6295614Z         num_attn_heads: int,
2025-04-11T04:23:18.6295702Z         kv_group_num: int,
2025-04-11T04:23:18.6295787Z         same_context_len: bool,
2025-04-11T04:23:18.6295870Z         q_len: int,
2025-04-11T04:23:18.6295955Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6296045Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6296219Z     ):
2025-04-11T04:23:18.6296331Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6296527Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6296711Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6296884Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6297045Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6297205Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6297278Z     
2025-04-11T04:23:18.6297365Z         torch.manual_seed(123)
2025-04-11T04:23:18.6297458Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6297553Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6297557Z 
2025-04-11T04:23:18.6297716Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6297830Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6297835Z 
2025-04-11T04:23:18.6297916Z device = None
2025-04-11T04:23:18.6297920Z 
2025-04-11T04:23:18.6298036Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6298184Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6298259Z     
2025-04-11T04:23:18.6298333Z         Args:
2025-04-11T04:23:18.6298504Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6298670Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6298777Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6298851Z         """
2025-04-11T04:23:18.6298933Z         _lazy_init()
2025-04-11T04:23:18.6299033Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6299133Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6299245Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6299527Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6299662Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6299826Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6299830Z 
2025-04-11T04:23:18.6300069Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6300241Z _____________ test_flash_decoding[True-False-1-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.6300245Z 
2025-04-11T04:23:18.6300394Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6300743Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6300833Z use_new_kcache_layout = True
2025-04-11T04:23:18.6300841Z 
2025-04-11T04:23:18.6301048Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6301154Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6301275Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6301416Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6301530Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6301647Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6301791Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6301894Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6302029Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6302184Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6302276Z     def test_flash_decoding(
2025-04-11T04:23:18.6302437Z         bsz: int,
2025-04-11T04:23:18.6302526Z         block_size: int,
2025-04-11T04:23:18.6302615Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6302697Z         num_attn_heads: int,
2025-04-11T04:23:18.6302785Z         kv_group_num: int,
2025-04-11T04:23:18.6302871Z         same_context_len: bool,
2025-04-11T04:23:18.6302958Z         q_len: int,
2025-04-11T04:23:18.6303042Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6303133Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6303206Z     ):
2025-04-11T04:23:18.6303315Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6303511Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6303691Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6303868Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6304029Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6304194Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6304266Z     
2025-04-11T04:23:18.6304351Z         torch.manual_seed(123)
2025-04-11T04:23:18.6304442Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6304531Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6304535Z 
2025-04-11T04:23:18.6304692Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6304801Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6304805Z 
2025-04-11T04:23:18.6304888Z device = None
2025-04-11T04:23:18.6304892Z 
2025-04-11T04:23:18.6305014Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6305171Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6305255Z     
2025-04-11T04:23:18.6305336Z         Args:
2025-04-11T04:23:18.6305512Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6305677Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6305784Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6305856Z         """
2025-04-11T04:23:18.6305935Z         _lazy_init()
2025-04-11T04:23:18.6306032Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6306132Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6306237Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6306517Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6306649Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6306910Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6306914Z 
2025-04-11T04:23:18.6307163Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6307338Z ____________ test_flash_decoding[True-False-1-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.6307342Z 
2025-04-11T04:23:18.6307493Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6307663Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6307751Z use_new_kcache_layout = True
2025-04-11T04:23:18.6307755Z 
2025-04-11T04:23:18.6307956Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6308059Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6308176Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6308321Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6308461Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6308674Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6308814Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6308924Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6309064Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6309219Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6309395Z     def test_flash_decoding(
2025-04-11T04:23:18.6309501Z         bsz: int,
2025-04-11T04:23:18.6309665Z         block_size: int,
2025-04-11T04:23:18.6309791Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6309982Z         num_attn_heads: int,
2025-04-11T04:23:18.6310141Z         kv_group_num: int,
2025-04-11T04:23:18.6310259Z         same_context_len: bool,
2025-04-11T04:23:18.6310419Z         q_len: int,
2025-04-11T04:23:18.6310539Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6310649Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6310830Z     ):
2025-04-11T04:23:18.6310973Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6311238Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6311454Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6311673Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6311878Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6312076Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6312210Z     
2025-04-11T04:23:18.6312338Z         torch.manual_seed(123)
2025-04-11T04:23:18.6312572Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6312687Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6312692Z 
2025-04-11T04:23:18.6312927Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6313074Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6313079Z 
2025-04-11T04:23:18.6313201Z device = None
2025-04-11T04:23:18.6313239Z 
2025-04-11T04:23:18.6313388Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6313567Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6313698Z     
2025-04-11T04:23:18.6313817Z         Args:
2025-04-11T04:23:18.6314056Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6314250Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6314383Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6314507Z         """
2025-04-11T04:23:18.6314716Z         _lazy_init()
2025-04-11T04:23:18.6314893Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6315023Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6315194Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6315504Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6315652Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6315900Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6315905Z 
2025-04-11T04:23:18.6316181Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6316506Z _____________ test_flash_decoding[True-False-1-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.6316511Z 
2025-04-11T04:23:18.6316690Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6316913Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6317038Z use_new_kcache_layout = True
2025-04-11T04:23:18.6317133Z 
2025-04-11T04:23:18.6317410Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6317545Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6317725Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6317894Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6318036Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6318231Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6318397Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6318559Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6318724Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6318936Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6319062Z     def test_flash_decoding(
2025-04-11T04:23:18.6319184Z         bsz: int,
2025-04-11T04:23:18.6319333Z         block_size: int,
2025-04-11T04:23:18.6319455Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6319613Z         num_attn_heads: int,
2025-04-11T04:23:18.6319715Z         kv_group_num: int,
2025-04-11T04:23:18.6319839Z         same_context_len: bool,
2025-04-11T04:23:18.6319984Z         q_len: int,
2025-04-11T04:23:18.6320189Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6320349Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6320453Z     ):
2025-04-11T04:23:18.6320622Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6320857Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6321069Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6321304Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6321507Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6321722Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6321831Z     
2025-04-11T04:23:18.6321992Z         torch.manual_seed(123)
2025-04-11T04:23:18.6322112Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6322245Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6322249Z 
2025-04-11T04:23:18.6322470Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6322598Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6322603Z 
2025-04-11T04:23:18.6322756Z device = None
2025-04-11T04:23:18.6322761Z 
2025-04-11T04:23:18.6322911Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6323221Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6323323Z     
2025-04-11T04:23:18.6323445Z         Args:
2025-04-11T04:23:18.6323653Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6323943Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6324118Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6324219Z         """
2025-04-11T04:23:18.6324356Z         _lazy_init()
2025-04-11T04:23:18.6324466Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6324649Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6324796Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6325112Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6325313Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6325503Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6325507Z 
2025-04-11T04:23:18.6325884Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6326104Z _____________ test_flash_decoding[True-False-1-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.6326108Z 
2025-04-11T04:23:18.6326318Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6326510Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6326656Z use_new_kcache_layout = True
2025-04-11T04:23:18.6326661Z 
2025-04-11T04:23:18.6326875Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6327025Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6327213Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6327381Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6327559Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6327707Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6327984Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6328128Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6328289Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6328498Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6328611Z     def test_flash_decoding(
2025-04-11T04:23:18.6328749Z         bsz: int,
2025-04-11T04:23:18.6328870Z         block_size: int,
2025-04-11T04:23:18.6329036Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6329150Z         num_attn_heads: int,
2025-04-11T04:23:18.6329266Z         kv_group_num: int,
2025-04-11T04:23:18.6329425Z         same_context_len: bool,
2025-04-11T04:23:18.6329525Z         q_len: int,
2025-04-11T04:23:18.6329695Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6329817Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6329920Z     ):
2025-04-11T04:23:18.6330091Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6330327Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6330567Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6330776Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6330999Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6331187Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6331328Z     
2025-04-11T04:23:18.6331432Z         torch.manual_seed(123)
2025-04-11T04:23:18.6331647Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6331898Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6331903Z 
2025-04-11T04:23:18.6332087Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6332271Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6332277Z 
2025-04-11T04:23:18.6332387Z device = None
2025-04-11T04:23:18.6332391Z 
2025-04-11T04:23:18.6332584Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6332773Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6332877Z     
2025-04-11T04:23:18.6333024Z         Args:
2025-04-11T04:23:18.6333221Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6361713Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6361978Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6362088Z         """
2025-04-11T04:23:18.6362199Z         _lazy_init()
2025-04-11T04:23:18.6362332Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6362493Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6362846Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6363169Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6363324Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6363496Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6363503Z 
2025-04-11T04:23:18.6363772Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6363954Z _____________ test_flash_decoding[True-False-1-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.6363959Z 
2025-04-11T04:23:18.6364126Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6364303Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6364407Z use_new_kcache_layout = True
2025-04-11T04:23:18.6364415Z 
2025-04-11T04:23:18.6364639Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6364755Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6364888Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6365038Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6365163Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6365283Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6365430Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6365544Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6365690Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6365860Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6365965Z     def test_flash_decoding(
2025-04-11T04:23:18.6366053Z         bsz: int,
2025-04-11T04:23:18.6366146Z         block_size: int,
2025-04-11T04:23:18.6366246Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6366337Z         num_attn_heads: int,
2025-04-11T04:23:18.6366424Z         kv_group_num: int,
2025-04-11T04:23:18.6366520Z         same_context_len: bool,
2025-04-11T04:23:18.6366599Z         q_len: int,
2025-04-11T04:23:18.6366690Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6366780Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6366855Z     ):
2025-04-11T04:23:18.6366981Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6367181Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6367376Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6367664Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6367837Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6368007Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6368085Z     
2025-04-11T04:23:18.6368183Z         torch.manual_seed(123)
2025-04-11T04:23:18.6368278Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6368383Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6368388Z 
2025-04-11T04:23:18.6368555Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6368676Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6368685Z 
2025-04-11T04:23:18.6368765Z device = None
2025-04-11T04:23:18.6368771Z 
2025-04-11T04:23:18.6368899Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6369063Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6369141Z     
2025-04-11T04:23:18.6369222Z         Args:
2025-04-11T04:23:18.6369487Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6369663Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6369777Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6369852Z         """
2025-04-11T04:23:18.6369938Z         _lazy_init()
2025-04-11T04:23:18.6370036Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6370145Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6370253Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6370548Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6370693Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6370858Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6370863Z 
2025-04-11T04:23:18.6371121Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6371301Z _____________ test_flash_decoding[True-False-1-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.6371306Z 
2025-04-11T04:23:18.6371467Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6371635Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6371729Z use_new_kcache_layout = True
2025-04-11T04:23:18.6371733Z 
2025-04-11T04:23:18.6371936Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6372044Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6372170Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6372316Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6372441Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6372562Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6372709Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6372817Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6372958Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6373113Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6373203Z     def test_flash_decoding(
2025-04-11T04:23:18.6373287Z         bsz: int,
2025-04-11T04:23:18.6373372Z         block_size: int,
2025-04-11T04:23:18.6373471Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6373558Z         num_attn_heads: int,
2025-04-11T04:23:18.6373645Z         kv_group_num: int,
2025-04-11T04:23:18.6373741Z         same_context_len: bool,
2025-04-11T04:23:18.6373819Z         q_len: int,
2025-04-11T04:23:18.6374000Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6374093Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6374169Z     ):
2025-04-11T04:23:18.6374294Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6374493Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6374683Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6374858Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6375031Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6375189Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6375263Z     
2025-04-11T04:23:18.6395920Z         torch.manual_seed(123)
2025-04-11T04:23:18.6396043Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6396148Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6396152Z 
2025-04-11T04:23:18.6396316Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6396543Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6396549Z 
2025-04-11T04:23:18.6396629Z device = None
2025-04-11T04:23:18.6396635Z 
2025-04-11T04:23:18.6396770Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6396939Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6397016Z     
2025-04-11T04:23:18.6397098Z         Args:
2025-04-11T04:23:18.6397271Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6397450Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6397561Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6397637Z         """
2025-04-11T04:23:18.6397729Z         _lazy_init()
2025-04-11T04:23:18.6397828Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6397939Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6398053Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6398352Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6398501Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6398659Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6398664Z 
2025-04-11T04:23:18.6398915Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6399087Z _____________ test_flash_decoding[True-False-1-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.6399091Z 
2025-04-11T04:23:18.6399252Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6399426Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6399522Z use_new_kcache_layout = True
2025-04-11T04:23:18.6399530Z 
2025-04-11T04:23:18.6399733Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6399844Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6399965Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6400107Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6400228Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6400339Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6400482Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6400618Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6400753Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6401006Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6401100Z     def test_flash_decoding(
2025-04-11T04:23:18.6401180Z         bsz: int,
2025-04-11T04:23:18.6401265Z         block_size: int,
2025-04-11T04:23:18.6401362Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6401445Z         num_attn_heads: int,
2025-04-11T04:23:18.6401528Z         kv_group_num: int,
2025-04-11T04:23:18.6401617Z         same_context_len: bool,
2025-04-11T04:23:18.6401693Z         q_len: int,
2025-04-11T04:23:18.6401782Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6401869Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6401941Z     ):
2025-04-11T04:23:18.6402058Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6402250Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6402437Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6402614Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6402781Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6403039Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6403113Z     
2025-04-11T04:23:18.6403204Z         torch.manual_seed(123)
2025-04-11T04:23:18.6403292Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6403387Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6403391Z 
2025-04-11T04:23:18.6403547Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6403662Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6403666Z 
2025-04-11T04:23:18.6403743Z device = None
2025-04-11T04:23:18.6403747Z 
2025-04-11T04:23:18.6403867Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6404020Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6404095Z     
2025-04-11T04:23:18.6404171Z         Args:
2025-04-11T04:23:18.6404336Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6404509Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6404615Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6404687Z         """
2025-04-11T04:23:18.6404770Z         _lazy_init()
2025-04-11T04:23:18.6404867Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6404971Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6405074Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6405364Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6405502Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6405662Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6405667Z 
2025-04-11T04:23:18.6405917Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6406088Z ____________ test_flash_decoding[True-False-1-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.6406092Z 
2025-04-11T04:23:18.6406251Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6406417Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6406507Z use_new_kcache_layout = True
2025-04-11T04:23:18.6406511Z 
2025-04-11T04:23:18.6406709Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6406815Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6406930Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6407067Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6407295Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6407405Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6407548Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6407651Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6407790Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6407939Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6408024Z     def test_flash_decoding(
2025-04-11T04:23:18.6408101Z         bsz: int,
2025-04-11T04:23:18.6408200Z         block_size: int,
2025-04-11T04:23:18.6408301Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6408381Z         num_attn_heads: int,
2025-04-11T04:23:18.6408463Z         kv_group_num: int,
2025-04-11T04:23:18.6408551Z         same_context_len: bool,
2025-04-11T04:23:18.6408625Z         q_len: int,
2025-04-11T04:23:18.6429261Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6429356Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6429430Z     ):
2025-04-11T04:23:18.6429646Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6429844Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6430032Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6430202Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6430371Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6430527Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6430603Z     
2025-04-11T04:23:18.6430692Z         torch.manual_seed(123)
2025-04-11T04:23:18.6430779Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6430876Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6430881Z 
2025-04-11T04:23:18.6431034Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6431152Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6431157Z 
2025-04-11T04:23:18.6431234Z device = None
2025-04-11T04:23:18.6431238Z 
2025-04-11T04:23:18.6431356Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6431504Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6431575Z     
2025-04-11T04:23:18.6431651Z         Args:
2025-04-11T04:23:18.6431813Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6431978Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6432082Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6432157Z         """
2025-04-11T04:23:18.6432238Z         _lazy_init()
2025-04-11T04:23:18.6432332Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6432435Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6432541Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6432826Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6432959Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6433112Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6433122Z 
2025-04-11T04:23:18.6433360Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6433527Z _____________ test_flash_decoding[True-False-1-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.6433531Z 
2025-04-11T04:23:18.6433686Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6433946Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6434037Z use_new_kcache_layout = True
2025-04-11T04:23:18.6434044Z 
2025-04-11T04:23:18.6434241Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6434347Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6434460Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6434597Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6434717Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6434829Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6434971Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6435077Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6435214Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6435368Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6435452Z     def test_flash_decoding(
2025-04-11T04:23:18.6435530Z         bsz: int,
2025-04-11T04:23:18.6435699Z         block_size: int,
2025-04-11T04:23:18.6435791Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6435873Z         num_attn_heads: int,
2025-04-11T04:23:18.6435955Z         kv_group_num: int,
2025-04-11T04:23:18.6436044Z         same_context_len: bool,
2025-04-11T04:23:18.6436120Z         q_len: int,
2025-04-11T04:23:18.6436208Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6436296Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6436371Z     ):
2025-04-11T04:23:18.6436482Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6436678Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6436865Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6437049Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6437213Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6437378Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6437450Z     
2025-04-11T04:23:18.6437544Z         torch.manual_seed(123)
2025-04-11T04:23:18.6437633Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6437725Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6437733Z 
2025-04-11T04:23:18.6437886Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6437999Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6438003Z 
2025-04-11T04:23:18.6438082Z device = None
2025-04-11T04:23:18.6438087Z 
2025-04-11T04:23:18.6438202Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6438356Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6438430Z     
2025-04-11T04:23:18.6438507Z         Args:
2025-04-11T04:23:18.6438673Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6438840Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6438951Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6439023Z         """
2025-04-11T04:23:18.6439105Z         _lazy_init()
2025-04-11T04:23:18.6439199Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6439299Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6439407Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6439691Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6439829Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6440072Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6440077Z 
2025-04-11T04:23:18.6440320Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6440493Z ____________ test_flash_decoding[True-False-1-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.6440497Z 
2025-04-11T04:23:18.6440655Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6440816Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6440900Z use_new_kcache_layout = True
2025-04-11T04:23:18.6440905Z 
2025-04-11T04:23:18.6441104Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6441206Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6441323Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6441459Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6441583Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6441692Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6441935Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6442041Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6442177Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6442331Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6442418Z     def test_flash_decoding(
2025-04-11T04:23:18.6442501Z         bsz: int,
2025-04-11T04:23:18.6442581Z         block_size: int,
2025-04-11T04:23:18.6442672Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6442756Z         num_attn_heads: int,
2025-04-11T04:23:18.6442840Z         kv_group_num: int,
2025-04-11T04:23:18.6442928Z         same_context_len: bool,
2025-04-11T04:23:18.6443005Z         q_len: int,
2025-04-11T04:23:18.6443093Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6443185Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6443257Z     ):
2025-04-11T04:23:18.6443372Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6443565Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6443750Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6443920Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6444081Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6444239Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6444310Z     
2025-04-11T04:23:18.6444399Z         torch.manual_seed(123)
2025-04-11T04:23:18.6444485Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6444581Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6444590Z 
2025-04-11T04:23:18.6444743Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6444859Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6444863Z 
2025-04-11T04:23:18.6444946Z device = None
2025-04-11T04:23:18.6444951Z 
2025-04-11T04:23:18.6445068Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6445222Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6445293Z     
2025-04-11T04:23:18.6445369Z         Args:
2025-04-11T04:23:18.6445533Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6445696Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6445803Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6445875Z         """
2025-04-11T04:23:18.6446044Z         _lazy_init()
2025-04-11T04:23:18.6446140Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6446239Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6446351Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6446637Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6446776Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6446932Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6446937Z 
2025-04-11T04:23:18.6447183Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6447351Z ______________ test_flash_decoding[True-False-5-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6447356Z 
2025-04-11T04:23:18.6447509Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6447675Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6447760Z use_new_kcache_layout = True
2025-04-11T04:23:18.6447863Z 
2025-04-11T04:23:18.6448064Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6448165Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6448285Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6448421Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6448539Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6448649Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6448785Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6448890Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6449023Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6449175Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6449262Z     def test_flash_decoding(
2025-04-11T04:23:18.6449340Z         bsz: int,
2025-04-11T04:23:18.6449424Z         block_size: int,
2025-04-11T04:23:18.6449515Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6449600Z         num_attn_heads: int,
2025-04-11T04:23:18.6449682Z         kv_group_num: int,
2025-04-11T04:23:18.6449771Z         same_context_len: bool,
2025-04-11T04:23:18.6449848Z         q_len: int,
2025-04-11T04:23:18.6449932Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6450024Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6450097Z     ):
2025-04-11T04:23:18.6450210Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6450403Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6450587Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6450762Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6450928Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6451093Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6451166Z     
2025-04-11T04:23:18.6451254Z         torch.manual_seed(123)
2025-04-11T04:23:18.6451341Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6451433Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6451437Z 
2025-04-11T04:23:18.6451588Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6451698Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6451702Z 
2025-04-11T04:23:18.6451785Z device = None
2025-04-11T04:23:18.6451790Z 
2025-04-11T04:23:18.6451901Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6452053Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6452285Z     
2025-04-11T04:23:18.6452364Z         Args:
2025-04-11T04:23:18.6452529Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6452696Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6452804Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6452877Z         """
2025-04-11T04:23:18.6452957Z         _lazy_init()
2025-04-11T04:23:18.6453051Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6453156Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6453260Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6453540Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6453677Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6453835Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6453840Z 
2025-04-11T04:23:18.6454085Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6454339Z _____________ test_flash_decoding[True-False-5-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6454343Z 
2025-04-11T04:23:18.6454497Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6454660Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6454748Z use_new_kcache_layout = True
2025-04-11T04:23:18.6454752Z 
2025-04-11T04:23:18.6454952Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6455054Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6455174Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6455311Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6455433Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6455544Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6455686Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6455787Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6455921Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6456074Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6456158Z     def test_flash_decoding(
2025-04-11T04:23:18.6456235Z         bsz: int,
2025-04-11T04:23:18.6456316Z         block_size: int,
2025-04-11T04:23:18.6456406Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6456492Z         num_attn_heads: int,
2025-04-11T04:23:18.6456574Z         kv_group_num: int,
2025-04-11T04:23:18.6456662Z         same_context_len: bool,
2025-04-11T04:23:18.6456738Z         q_len: int,
2025-04-11T04:23:18.6456826Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6456916Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6456986Z     ):
2025-04-11T04:23:18.6457102Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6457293Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6457475Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6457645Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6457806Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6457964Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6458036Z     
2025-04-11T04:23:18.6458125Z         torch.manual_seed(123)
2025-04-11T04:23:18.6458212Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6458399Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6458403Z 
2025-04-11T04:23:18.6458555Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6458671Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6458679Z 
2025-04-11T04:23:18.6458757Z device = None
2025-04-11T04:23:18.6458761Z 
2025-04-11T04:23:18.6458877Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6459029Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6459101Z     
2025-04-11T04:23:18.6459176Z         Args:
2025-04-11T04:23:18.6459339Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6459502Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6459610Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6459682Z         """
2025-04-11T04:23:18.6459766Z         _lazy_init()
2025-04-11T04:23:18.6459861Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6459964Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6460152Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6460431Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6460569Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6460724Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6460729Z 
2025-04-11T04:23:18.6460969Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6461133Z ______________ test_flash_decoding[True-False-5-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6461138Z 
2025-04-11T04:23:18.6461291Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6461457Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6461543Z use_new_kcache_layout = True
2025-04-11T04:23:18.6461551Z 
2025-04-11T04:23:18.6461749Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6461850Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6461970Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6462107Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6462225Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6462338Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6462476Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6462578Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6462710Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6462862Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6462950Z     def test_flash_decoding(
2025-04-11T04:23:18.6463027Z         bsz: int,
2025-04-11T04:23:18.6463113Z         block_size: int,
2025-04-11T04:23:18.6463203Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6463287Z         num_attn_heads: int,
2025-04-11T04:23:18.6463367Z         kv_group_num: int,
2025-04-11T04:23:18.6463454Z         same_context_len: bool,
2025-04-11T04:23:18.6463529Z         q_len: int,
2025-04-11T04:23:18.6463618Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6463704Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6463774Z     ):
2025-04-11T04:23:18.6463889Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6464080Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6464263Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6464521Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6464682Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6464844Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6464916Z     
2025-04-11T04:23:18.6465005Z         torch.manual_seed(123)
2025-04-11T04:23:18.6465092Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6465186Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6465190Z 
2025-04-11T04:23:18.6465341Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6465451Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6465458Z 
2025-04-11T04:23:18.6465535Z device = None
2025-04-11T04:23:18.6465540Z 
2025-04-11T04:23:18.6465654Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6465809Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6465884Z     
2025-04-11T04:23:18.6465961Z         Args:
2025-04-11T04:23:18.6466126Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6466379Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6466487Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6466559Z         """
2025-04-11T04:23:18.6466641Z         _lazy_init()
2025-04-11T04:23:18.6466734Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6466840Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6466944Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6467225Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6467364Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6467522Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6467526Z 
2025-04-11T04:23:18.6467769Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6467938Z _____________ test_flash_decoding[True-False-5-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6467942Z 
2025-04-11T04:23:18.6468095Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6468255Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6468345Z use_new_kcache_layout = True
2025-04-11T04:23:18.6468349Z 
2025-04-11T04:23:18.6468608Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6468711Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6468830Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6468966Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6469088Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6469200Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6469341Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6469443Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6469574Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6469726Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6469809Z     def test_flash_decoding(
2025-04-11T04:23:18.6469887Z         bsz: int,
2025-04-11T04:23:18.6469967Z         block_size: int,
2025-04-11T04:23:18.6470061Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6470145Z         num_attn_heads: int,
2025-04-11T04:23:18.6470228Z         kv_group_num: int,
2025-04-11T04:23:18.6470315Z         same_context_len: bool,
2025-04-11T04:23:18.6470390Z         q_len: int,
2025-04-11T04:23:18.6470585Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6470672Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6470744Z     ):
2025-04-11T04:23:18.6470862Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6471055Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6471241Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6471412Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6471582Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6471738Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6471809Z     
2025-04-11T04:23:18.6471899Z         torch.manual_seed(123)
2025-04-11T04:23:18.6471984Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6472076Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6472084Z 
2025-04-11T04:23:18.6472238Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6472461Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6472467Z 
2025-04-11T04:23:18.6472543Z device = None
2025-04-11T04:23:18.6472548Z 
2025-04-11T04:23:18.6472662Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6472816Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6472888Z     
2025-04-11T04:23:18.6472964Z         Args:
2025-04-11T04:23:18.6473128Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6473293Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6473398Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6473470Z         """
2025-04-11T04:23:18.6473550Z         _lazy_init()
2025-04-11T04:23:18.6473647Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6473750Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6473856Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6474135Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6474275Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6474429Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6474434Z 
2025-04-11T04:23:18.6474674Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6474838Z _____________ test_flash_decoding[True-False-5-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6474843Z 
2025-04-11T04:23:18.6474996Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6475160Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6475248Z use_new_kcache_layout = True
2025-04-11T04:23:18.6475255Z 
2025-04-11T04:23:18.6475451Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6475557Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6475673Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6475808Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6475926Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6476037Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6476173Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6476276Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6476407Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6476559Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6476736Z     def test_flash_decoding(
2025-04-11T04:23:18.6476814Z         bsz: int,
2025-04-11T04:23:18.6476901Z         block_size: int,
2025-04-11T04:23:18.6476996Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6477078Z         num_attn_heads: int,
2025-04-11T04:23:18.6477161Z         kv_group_num: int,
2025-04-11T04:23:18.6477251Z         same_context_len: bool,
2025-04-11T04:23:18.6477326Z         q_len: int,
2025-04-11T04:23:18.6477413Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6477500Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6477572Z     ):
2025-04-11T04:23:18.6477688Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6477878Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6478060Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6478227Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6478397Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6478643Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6478715Z     
2025-04-11T04:23:18.6478807Z         torch.manual_seed(123)
2025-04-11T04:23:18.6478894Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6478990Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6478994Z 
2025-04-11T04:23:18.6479147Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6479262Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6479266Z 
2025-04-11T04:23:18.6479342Z device = None
2025-04-11T04:23:18.6479347Z 
2025-04-11T04:23:18.6479463Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6479616Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6479690Z     
2025-04-11T04:23:18.6479767Z         Args:
2025-04-11T04:23:18.6479935Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6480108Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6480214Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6480286Z         """
2025-04-11T04:23:18.6480371Z         _lazy_init()
2025-04-11T04:23:18.6480465Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6480571Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6480674Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6480958Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6481099Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6481258Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6481262Z 
2025-04-11T04:23:18.6481503Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6481674Z _____________ test_flash_decoding[True-False-5-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6481679Z 
2025-04-11T04:23:18.6481836Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6481998Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6482088Z use_new_kcache_layout = True
2025-04-11T04:23:18.6482092Z 
2025-04-11T04:23:18.6482289Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6482394Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6482509Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6482644Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6482861Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6482971Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6483113Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6483215Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6483351Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6483497Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6483581Z     def test_flash_decoding(
2025-04-11T04:23:18.6483660Z         bsz: int,
2025-04-11T04:23:18.6483743Z         block_size: int,
2025-04-11T04:23:18.6483838Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6483918Z         num_attn_heads: int,
2025-04-11T04:23:18.6484001Z         kv_group_num: int,
2025-04-11T04:23:18.6484089Z         same_context_len: bool,
2025-04-11T04:23:18.6484162Z         q_len: int,
2025-04-11T04:23:18.6484256Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6484343Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6484415Z     ):
2025-04-11T04:23:18.6484529Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6484814Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6484998Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6485167Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6485333Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6485487Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6485559Z     
2025-04-11T04:23:18.6485648Z         torch.manual_seed(123)
2025-04-11T04:23:18.6485733Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6485831Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6485841Z 
2025-04-11T04:23:18.6485994Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6486109Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6486117Z 
2025-04-11T04:23:18.6486196Z device = None
2025-04-11T04:23:18.6486200Z 
2025-04-11T04:23:18.6486319Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6486468Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6486537Z     
2025-04-11T04:23:18.6486616Z         Args:
2025-04-11T04:23:18.6486781Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6486951Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6487056Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6487127Z         """
2025-04-11T04:23:18.6487205Z         _lazy_init()
2025-04-11T04:23:18.6487302Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6487407Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6487513Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6487801Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6487936Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6488093Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6488101Z 
2025-04-11T04:23:18.6488340Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6488506Z _____________ test_flash_decoding[True-False-5-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6488510Z 
2025-04-11T04:23:18.6488662Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6488915Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6489003Z use_new_kcache_layout = True
2025-04-11T04:23:18.6489007Z 
2025-04-11T04:23:18.6489206Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6489312Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6489426Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6489563Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6489682Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6489793Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6489931Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6490032Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6490169Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6490321Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6490414Z     def test_flash_decoding(
2025-04-11T04:23:18.6490495Z         bsz: int,
2025-04-11T04:23:18.6490577Z         block_size: int,
2025-04-11T04:23:18.6490756Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6490837Z         num_attn_heads: int,
2025-04-11T04:23:18.6490918Z         kv_group_num: int,
2025-04-11T04:23:18.6491009Z         same_context_len: bool,
2025-04-11T04:23:18.6491085Z         q_len: int,
2025-04-11T04:23:18.6491172Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6491259Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6491332Z     ):
2025-04-11T04:23:18.6491446Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6491640Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6491821Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6491990Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6492159Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6492317Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6492391Z     
2025-04-11T04:23:18.6492477Z         torch.manual_seed(123)
2025-04-11T04:23:18.6492564Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6492657Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6492661Z 
2025-04-11T04:23:18.6492813Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6492927Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6492931Z 
2025-04-11T04:23:18.6493009Z device = None
2025-04-11T04:23:18.6493013Z 
2025-04-11T04:23:18.6493134Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6493283Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6493357Z     
2025-04-11T04:23:18.6493435Z         Args:
2025-04-11T04:23:18.6493599Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6493772Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6493876Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6493951Z         """
2025-04-11T04:23:18.6494030Z         _lazy_init()
2025-04-11T04:23:18.6494124Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6494228Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6494330Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6494616Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6494750Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6495007Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6495016Z 
2025-04-11T04:23:18.6495252Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6495422Z _____________ test_flash_decoding[True-False-5-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6495426Z 
2025-04-11T04:23:18.6495583Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6495746Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6495836Z use_new_kcache_layout = True
2025-04-11T04:23:18.6495840Z 
2025-04-11T04:23:18.6496038Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6496146Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6496262Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6496399Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6496523Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6496634Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6496857Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6496960Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6497098Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6497249Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6497336Z     def test_flash_decoding(
2025-04-11T04:23:18.6497417Z         bsz: int,
2025-04-11T04:23:18.6497499Z         block_size: int,
2025-04-11T04:23:18.6497592Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6497674Z         num_attn_heads: int,
2025-04-11T04:23:18.6497757Z         kv_group_num: int,
2025-04-11T04:23:18.6497846Z         same_context_len: bool,
2025-04-11T04:23:18.6497921Z         q_len: int,
2025-04-11T04:23:18.6498015Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6498102Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6498178Z     ):
2025-04-11T04:23:18.6498287Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6498482Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6498669Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6498835Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6499002Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6499157Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6499234Z     
2025-04-11T04:23:18.6499319Z         torch.manual_seed(123)
2025-04-11T04:23:18.6499406Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6499499Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6499507Z 
2025-04-11T04:23:18.6499660Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6499776Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6499784Z 
2025-04-11T04:23:18.6499863Z device = None
2025-04-11T04:23:18.6499867Z 
2025-04-11T04:23:18.6499986Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6500133Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6500207Z     
2025-04-11T04:23:18.6500288Z         Args:
2025-04-11T04:23:18.6500455Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6500621Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6500727Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6500802Z         """
2025-04-11T04:23:18.6500882Z         _lazy_init()
2025-04-11T04:23:18.6501064Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6501169Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6501277Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6501569Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6501708Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6501866Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6501874Z 
2025-04-11T04:23:18.6502115Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6502284Z ______________ test_flash_decoding[True-False-5-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6502288Z 
2025-04-11T04:23:18.6502443Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6502605Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6502697Z use_new_kcache_layout = True
2025-04-11T04:23:18.6502701Z 
2025-04-11T04:23:18.6503002Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6503109Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6503224Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6503362Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6503484Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6503595Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6503733Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6503834Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6503969Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6504117Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6504207Z     def test_flash_decoding(
2025-04-11T04:23:18.6504286Z         bsz: int,
2025-04-11T04:23:18.6504365Z         block_size: int,
2025-04-11T04:23:18.6504462Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6504543Z         num_attn_heads: int,
2025-04-11T04:23:18.6504627Z         kv_group_num: int,
2025-04-11T04:23:18.6504710Z         same_context_len: bool,
2025-04-11T04:23:18.6504785Z         q_len: int,
2025-04-11T04:23:18.6504874Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6504964Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6505041Z     ):
2025-04-11T04:23:18.6505152Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6505345Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6505529Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6505698Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6505869Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6506027Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6506101Z     
2025-04-11T04:23:18.6506186Z         torch.manual_seed(123)
2025-04-11T04:23:18.6506273Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6506367Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6506371Z 
2025-04-11T04:23:18.6506523Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6506637Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6506641Z 
2025-04-11T04:23:18.6506715Z device = None
2025-04-11T04:23:18.6506719Z 
2025-04-11T04:23:18.6506835Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6506982Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6507140Z     
2025-04-11T04:23:18.6507217Z         Args:
2025-04-11T04:23:18.6507384Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6507557Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6507661Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6507735Z         """
2025-04-11T04:23:18.6507812Z         _lazy_init()
2025-04-11T04:23:18.6507905Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6508011Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6508113Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6508397Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6508578Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6508738Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6508746Z 
2025-04-11T04:23:18.6508985Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6509248Z _____________ test_flash_decoding[True-False-5-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6509256Z 
2025-04-11T04:23:18.6509406Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6509568Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6509661Z use_new_kcache_layout = True
2025-04-11T04:23:18.6509665Z 
2025-04-11T04:23:18.6509858Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6509963Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6510076Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6510215Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6510331Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6510443Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6510585Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6510686Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6510821Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6510971Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6511055Z     def test_flash_decoding(
2025-04-11T04:23:18.6511134Z         bsz: int,
2025-04-11T04:23:18.6511216Z         block_size: int,
2025-04-11T04:23:18.6511307Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6511388Z         num_attn_heads: int,
2025-04-11T04:23:18.6511471Z         kv_group_num: int,
2025-04-11T04:23:18.6511555Z         same_context_len: bool,
2025-04-11T04:23:18.6511629Z         q_len: int,
2025-04-11T04:23:18.6511715Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6511806Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6511881Z     ):
2025-04-11T04:23:18.6511989Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6512183Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6512367Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6512535Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6512700Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6512854Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6512931Z     
2025-04-11T04:23:18.6513016Z         torch.manual_seed(123)
2025-04-11T04:23:18.6513105Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6513202Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6513300Z 
2025-04-11T04:23:18.6513456Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6513571Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6513579Z 
2025-04-11T04:23:18.6513657Z device = None
2025-04-11T04:23:18.6513661Z 
2025-04-11T04:23:18.6513781Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6513930Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6514004Z     
2025-04-11T04:23:18.6514077Z         Args:
2025-04-11T04:23:18.6514242Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6514408Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6514511Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6514587Z         """
2025-04-11T04:23:18.6514663Z         _lazy_init()
2025-04-11T04:23:18.6514760Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6514865Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6514968Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6515335Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6515470Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6515630Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6515635Z 
2025-04-11T04:23:18.6515874Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6516038Z ______________ test_flash_decoding[True-False-5-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6516047Z 
2025-04-11T04:23:18.6516197Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6516358Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6516453Z use_new_kcache_layout = True
2025-04-11T04:23:18.6516457Z 
2025-04-11T04:23:18.6516659Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6516765Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6516878Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6517018Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6517134Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6517245Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6517384Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6517483Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6517617Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6517765Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6517856Z     def test_flash_decoding(
2025-04-11T04:23:18.6517932Z         bsz: int,
2025-04-11T04:23:18.6518012Z         block_size: int,
2025-04-11T04:23:18.6518112Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6518193Z         num_attn_heads: int,
2025-04-11T04:23:18.6518277Z         kv_group_num: int,
2025-04-11T04:23:18.6518360Z         same_context_len: bool,
2025-04-11T04:23:18.6518437Z         q_len: int,
2025-04-11T04:23:18.6518524Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6518611Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6518686Z     ):
2025-04-11T04:23:18.6518796Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6518987Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6519166Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6519334Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6519610Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6519769Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6519844Z     
2025-04-11T04:23:18.6519932Z         torch.manual_seed(123)
2025-04-11T04:23:18.6520021Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6520115Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6520120Z 
2025-04-11T04:23:18.6520272Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6520386Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6520390Z 
2025-04-11T04:23:18.6520467Z device = None
2025-04-11T04:23:18.6520471Z 
2025-04-11T04:23:18.6520592Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6520741Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6520819Z     
2025-04-11T04:23:18.6520892Z         Args:
2025-04-11T04:23:18.6521056Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6521307Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6521414Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6521488Z         """
2025-04-11T04:23:18.6521567Z         _lazy_init()
2025-04-11T04:23:18.6521660Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6521764Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6521865Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6522153Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6522289Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6522451Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6522459Z 
2025-04-11T04:23:18.6522697Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6522871Z _____________ test_flash_decoding[True-False-5-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6522876Z 
2025-04-11T04:23:18.6523024Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6523186Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6523277Z use_new_kcache_layout = True
2025-04-11T04:23:18.6523281Z 
2025-04-11T04:23:18.6523478Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6523584Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6523697Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6523836Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6523954Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6524066Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6524208Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6524309Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6524449Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6524600Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6524688Z     def test_flash_decoding(
2025-04-11T04:23:18.6524763Z         bsz: int,
2025-04-11T04:23:18.6524846Z         block_size: int,
2025-04-11T04:23:18.6524940Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6525021Z         num_attn_heads: int,
2025-04-11T04:23:18.6525107Z         kv_group_num: int,
2025-04-11T04:23:18.6525190Z         same_context_len: bool,
2025-04-11T04:23:18.6525268Z         q_len: int,
2025-04-11T04:23:18.6525356Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6525527Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6525604Z     ):
2025-04-11T04:23:18.6525715Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6525909Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6526091Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6526260Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6526427Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6526581Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6526656Z     
2025-04-11T04:23:18.6526741Z         torch.manual_seed(123)
2025-04-11T04:23:18.6526827Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6526919Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6526928Z 
2025-04-11T04:23:18.6527079Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6527196Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6527285Z 
2025-04-11T04:23:18.6527365Z device = None
2025-04-11T04:23:18.6527369Z 
2025-04-11T04:23:18.6527489Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6527638Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6527711Z     
2025-04-11T04:23:18.6527787Z         Args:
2025-04-11T04:23:18.6527952Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6528117Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6528220Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6528294Z         """
2025-04-11T04:23:18.6528372Z         _lazy_init()
2025-04-11T04:23:18.6528468Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6528573Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6528675Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6528962Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6529096Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6529253Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6529258Z 
2025-04-11T04:23:18.6529494Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6529660Z _____________ test_flash_decoding[True-False-5-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6529664Z 
2025-04-11T04:23:18.6529815Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6529975Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6530067Z use_new_kcache_layout = True
2025-04-11T04:23:18.6530071Z 
2025-04-11T04:23:18.6530268Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6530378Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6530491Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6530630Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6530742Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6530852Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6530990Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6531093Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6531230Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6531378Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6531557Z     def test_flash_decoding(
2025-04-11T04:23:18.6531633Z         bsz: int,
2025-04-11T04:23:18.6531713Z         block_size: int,
2025-04-11T04:23:18.6531809Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6531890Z         num_attn_heads: int,
2025-04-11T04:23:18.6531975Z         kv_group_num: int,
2025-04-11T04:23:18.6532056Z         same_context_len: bool,
2025-04-11T04:23:18.6532131Z         q_len: int,
2025-04-11T04:23:18.6532218Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6532304Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6532380Z     ):
2025-04-11T04:23:18.6532489Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6532681Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6532859Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6533028Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6533199Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6533440Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6533516Z     
2025-04-11T04:23:18.6533600Z         torch.manual_seed(123)
2025-04-11T04:23:18.6533690Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6533779Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6533783Z 
2025-04-11T04:23:18.6533935Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6534049Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6534053Z 
2025-04-11T04:23:18.6534130Z device = None
2025-04-11T04:23:18.6534134Z 
2025-04-11T04:23:18.6534253Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6534401Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6534479Z     
2025-04-11T04:23:18.6534551Z         Args:
2025-04-11T04:23:18.6534713Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6534882Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6534987Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6535064Z         """
2025-04-11T04:23:18.6535139Z         _lazy_init()
2025-04-11T04:23:18.6535237Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6535337Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6535440Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6535723Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6535857Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6536015Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6536022Z 
2025-04-11T04:23:18.6536263Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6536433Z _____________ test_flash_decoding[True-False-5-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6536437Z 
2025-04-11T04:23:18.6536588Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6536749Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6536838Z use_new_kcache_layout = True
2025-04-11T04:23:18.6536843Z 
2025-04-11T04:23:18.6537039Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6537148Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6537263Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6537404Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6537603Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6537716Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6537853Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6537955Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6538091Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6538240Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6538327Z     def test_flash_decoding(
2025-04-11T04:23:18.6538402Z         bsz: int,
2025-04-11T04:23:18.6538480Z         block_size: int,
2025-04-11T04:23:18.6538572Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6538651Z         num_attn_heads: int,
2025-04-11T04:23:18.6538734Z         kv_group_num: int,
2025-04-11T04:23:18.6538816Z         same_context_len: bool,
2025-04-11T04:23:18.6538890Z         q_len: int,
2025-04-11T04:23:18.6538977Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6539067Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6539141Z     ):
2025-04-11T04:23:18.6539251Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6539542Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6539727Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6539900Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6540067Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6540224Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6540298Z     
2025-04-11T04:23:18.6540382Z         torch.manual_seed(123)
2025-04-11T04:23:18.6540472Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6540561Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6540569Z 
2025-04-11T04:23:18.6540721Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6540834Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6540842Z 
2025-04-11T04:23:18.6540919Z device = None
2025-04-11T04:23:18.6540923Z 
2025-04-11T04:23:18.6541040Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6541190Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6541263Z     
2025-04-11T04:23:18.6541335Z         Args:
2025-04-11T04:23:18.6541501Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6541669Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6541773Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6541849Z         """
2025-04-11T04:23:18.6541926Z         _lazy_init()
2025-04-11T04:23:18.6542026Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6542128Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6542230Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6542524Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6542659Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6542817Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6542821Z 
2025-04-11T04:23:18.6543061Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6543231Z _____________ test_flash_decoding[True-False-5-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6543235Z 
2025-04-11T04:23:18.6543385Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6543549Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6543724Z use_new_kcache_layout = True
2025-04-11T04:23:18.6543728Z 
2025-04-11T04:23:18.6543929Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6544039Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6544155Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6544296Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6544410Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6544526Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6544661Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6544762Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6544900Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6545048Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6545139Z     def test_flash_decoding(
2025-04-11T04:23:18.6545213Z         bsz: int,
2025-04-11T04:23:18.6545292Z         block_size: int,
2025-04-11T04:23:18.6545545Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6545626Z         num_attn_heads: int,
2025-04-11T04:23:18.6545712Z         kv_group_num: int,
2025-04-11T04:23:18.6545796Z         same_context_len: bool,
2025-04-11T04:23:18.6545874Z         q_len: int,
2025-04-11T04:23:18.6545957Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6546044Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6546120Z     ):
2025-04-11T04:23:18.6546227Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6546423Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6546604Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6546773Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6546942Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6547103Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6547180Z     
2025-04-11T04:23:18.6547265Z         torch.manual_seed(123)
2025-04-11T04:23:18.6547356Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6547448Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6547453Z 
2025-04-11T04:23:18.6547609Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6547723Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6547726Z 
2025-04-11T04:23:18.6547803Z device = None
2025-04-11T04:23:18.6547807Z 
2025-04-11T04:23:18.6547925Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6548075Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6548155Z     
2025-04-11T04:23:18.6548228Z         Args:
2025-04-11T04:23:18.6548390Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6548587Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6548693Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6548768Z         """
2025-04-11T04:23:18.6548843Z         _lazy_init()
2025-04-11T04:23:18.6548938Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6549041Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6549143Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6549428Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6549562Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6549721Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6549826Z 
2025-04-11T04:23:18.6550063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6550234Z _____________ test_flash_decoding[True-False-5-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6550239Z 
2025-04-11T04:23:18.6550391Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6550555Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6550641Z use_new_kcache_layout = True
2025-04-11T04:23:18.6550645Z 
2025-04-11T04:23:18.6550842Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6550948Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6551063Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6551204Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6551321Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6551439Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6551667Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6551768Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6551907Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6552059Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6552149Z     def test_flash_decoding(
2025-04-11T04:23:18.6552223Z         bsz: int,
2025-04-11T04:23:18.6552306Z         block_size: int,
2025-04-11T04:23:18.6552394Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6552473Z         num_attn_heads: int,
2025-04-11T04:23:18.6552557Z         kv_group_num: int,
2025-04-11T04:23:18.6552640Z         same_context_len: bool,
2025-04-11T04:23:18.6552718Z         q_len: int,
2025-04-11T04:23:18.6552801Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6552892Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6552967Z     ):
2025-04-11T04:23:18.6553076Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6553275Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6553453Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6553628Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6553793Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6553947Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6554021Z     
2025-04-11T04:23:18.6554104Z         torch.manual_seed(123)
2025-04-11T04:23:18.6554195Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6554284Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6554292Z 
2025-04-11T04:23:18.6554441Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6554554Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6554562Z 
2025-04-11T04:23:18.6554640Z device = None
2025-04-11T04:23:18.6554645Z 
2025-04-11T04:23:18.6554764Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6554912Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6554988Z     
2025-04-11T04:23:18.6555061Z         Args:
2025-04-11T04:23:18.6555228Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6555392Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6555495Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6555572Z         """
2025-04-11T04:23:18.6555648Z         _lazy_init()
2025-04-11T04:23:18.6555745Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6555933Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6556035Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6556322Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6556459Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6556620Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6556624Z 
2025-04-11T04:23:18.6556865Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6557032Z _____________ test_flash_decoding[True-False-5-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6557036Z 
2025-04-11T04:23:18.6557185Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6557349Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6557439Z use_new_kcache_layout = True
2025-04-11T04:23:18.6557443Z 
2025-04-11T04:23:18.6557642Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6557833Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6557952Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6558098Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6558216Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6558336Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6558476Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6558581Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6558722Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6558876Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6558969Z     def test_flash_decoding(
2025-04-11T04:23:18.6559047Z         bsz: int,
2025-04-11T04:23:18.6559136Z         block_size: int,
2025-04-11T04:23:18.6559236Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6559322Z         num_attn_heads: int,
2025-04-11T04:23:18.6559412Z         kv_group_num: int,
2025-04-11T04:23:18.6559500Z         same_context_len: bool,
2025-04-11T04:23:18.6559586Z         q_len: int,
2025-04-11T04:23:18.6559674Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6559765Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6559847Z     ):
2025-04-11T04:23:18.6559962Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6560162Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6560348Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6560524Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6560696Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6560859Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6560938Z     
2025-04-11T04:23:18.6561029Z         torch.manual_seed(123)
2025-04-11T04:23:18.6561125Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6561221Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6561226Z 
2025-04-11T04:23:18.6561386Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6561501Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6561505Z 
2025-04-11T04:23:18.6561586Z device = None
2025-04-11T04:23:18.6561591Z 
2025-04-11T04:23:18.6561713Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6561867Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6562046Z     
2025-04-11T04:23:18.6562120Z         Args:
2025-04-11T04:23:18.6562291Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6562460Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6562563Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6562640Z         """
2025-04-11T04:23:18.6562717Z         _lazy_init()
2025-04-11T04:23:18.6562815Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6562915Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6563019Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6563306Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6563442Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6563601Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6563609Z 
2025-04-11T04:23:18.6563854Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6564136Z _____________ test_flash_decoding[True-False-5-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.6564140Z 
2025-04-11T04:23:18.6564291Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6564457Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6564544Z use_new_kcache_layout = True
2025-04-11T04:23:18.6564548Z 
2025-04-11T04:23:18.6564750Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6564852Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6564968Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6565109Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6565230Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6565344Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6565482Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6565584Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6565721Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6565870Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6565959Z     def test_flash_decoding(
2025-04-11T04:23:18.6566034Z         bsz: int,
2025-04-11T04:23:18.6566115Z         block_size: int,
2025-04-11T04:23:18.6566206Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6566285Z         num_attn_heads: int,
2025-04-11T04:23:18.6566373Z         kv_group_num: int,
2025-04-11T04:23:18.6566457Z         same_context_len: bool,
2025-04-11T04:23:18.6566535Z         q_len: int,
2025-04-11T04:23:18.6566619Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6566711Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6566787Z     ):
2025-04-11T04:23:18.6566899Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6567096Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6567275Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6567446Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6567608Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6567764Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6567839Z     
2025-04-11T04:23:18.6567923Z         torch.manual_seed(123)
2025-04-11T04:23:18.6568015Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6568104Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6568196Z 
2025-04-11T04:23:18.6568354Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6568466Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6568474Z 
2025-04-11T04:23:18.6568552Z device = None
2025-04-11T04:23:18.6568560Z 
2025-04-11T04:23:18.6568673Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6568823Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6568897Z     
2025-04-11T04:23:18.6568971Z         Args:
2025-04-11T04:23:18.6569139Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6569304Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6569408Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6569485Z         """
2025-04-11T04:23:18.6569561Z         _lazy_init()
2025-04-11T04:23:18.6569658Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6569762Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6569871Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6570235Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6570369Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6570528Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6570533Z 
2025-04-11T04:23:18.6570770Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6570939Z _____________ test_flash_decoding[True-False-5-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.6570943Z 
2025-04-11T04:23:18.6571091Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6571256Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6571344Z use_new_kcache_layout = True
2025-04-11T04:23:18.6571348Z 
2025-04-11T04:23:18.6571551Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6571659Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6571771Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6571916Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6572028Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6572151Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6572287Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6572396Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6572531Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6572680Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6572771Z     def test_flash_decoding(
2025-04-11T04:23:18.6572847Z         bsz: int,
2025-04-11T04:23:18.6572932Z         block_size: int,
2025-04-11T04:23:18.6573023Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6573105Z         num_attn_heads: int,
2025-04-11T04:23:18.6573190Z         kv_group_num: int,
2025-04-11T04:23:18.6573274Z         same_context_len: bool,
2025-04-11T04:23:18.6573356Z         q_len: int,
2025-04-11T04:23:18.6573438Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6573526Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6573601Z     ):
2025-04-11T04:23:18.6573710Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6573904Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6574084Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6574256Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6574513Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6574673Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6574749Z     
2025-04-11T04:23:18.6574834Z         torch.manual_seed(123)
2025-04-11T04:23:18.6574926Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6575015Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6575019Z 
2025-04-11T04:23:18.6575174Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6575285Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6575289Z 
2025-04-11T04:23:18.6575369Z device = None
2025-04-11T04:23:18.6575373Z 
2025-04-11T04:23:18.6575487Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6575636Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6575714Z     
2025-04-11T04:23:18.6575787Z         Args:
2025-04-11T04:23:18.6575955Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6576214Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6576317Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6576393Z         """
2025-04-11T04:23:18.6576468Z         _lazy_init()
2025-04-11T04:23:18.6576565Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6576666Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6576772Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6577055Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6577187Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6577345Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6577353Z 
2025-04-11T04:23:18.6577592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6577766Z _____________ test_flash_decoding[True-False-5-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.6577772Z 
2025-04-11T04:23:18.6577921Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6578088Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6578175Z use_new_kcache_layout = True
2025-04-11T04:23:18.6578179Z 
2025-04-11T04:23:18.6578380Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6578484Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6578598Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6578739Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6578858Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6578972Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6579107Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6579213Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6579348Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6579496Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6579586Z     def test_flash_decoding(
2025-04-11T04:23:18.6579659Z         bsz: int,
2025-04-11T04:23:18.6579744Z         block_size: int,
2025-04-11T04:23:18.6579835Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6579916Z         num_attn_heads: int,
2025-04-11T04:23:18.6580002Z         kv_group_num: int,
2025-04-11T04:23:18.6580086Z         same_context_len: bool,
2025-04-11T04:23:18.6580164Z         q_len: int,
2025-04-11T04:23:18.6580247Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6580427Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6580502Z     ):
2025-04-11T04:23:18.6580612Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6580810Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6580993Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6581166Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6581327Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6581486Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6581559Z     
2025-04-11T04:23:18.6581647Z         torch.manual_seed(123)
2025-04-11T04:23:18.6581739Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6581829Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6581834Z 
2025-04-11T04:23:18.6581996Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6582107Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6582198Z 
2025-04-11T04:23:18.6582284Z device = None
2025-04-11T04:23:18.6582289Z 
2025-04-11T04:23:18.6582405Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6582556Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6582631Z     
2025-04-11T04:23:18.6582704Z         Args:
2025-04-11T04:23:18.6582873Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6583037Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6583145Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6583222Z         """
2025-04-11T04:23:18.6583299Z         _lazy_init()
2025-04-11T04:23:18.6583399Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6583503Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6583613Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6583898Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6584032Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6584196Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6584201Z 
2025-04-11T04:23:18.6584444Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6584617Z _____________ test_flash_decoding[True-False-5-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.6584621Z 
2025-04-11T04:23:18.6584772Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6584939Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6585028Z use_new_kcache_layout = True
2025-04-11T04:23:18.6585032Z 
2025-04-11T04:23:18.6585233Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6585342Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6585457Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6585600Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6585713Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6585828Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6585963Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6586070Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6586204Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6586355Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6586547Z     def test_flash_decoding(
2025-04-11T04:23:18.6586622Z         bsz: int,
2025-04-11T04:23:18.6586707Z         block_size: int,
2025-04-11T04:23:18.6586799Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6586886Z         num_attn_heads: int,
2025-04-11T04:23:18.6586968Z         kv_group_num: int,
2025-04-11T04:23:18.6587052Z         same_context_len: bool,
2025-04-11T04:23:18.6587131Z         q_len: int,
2025-04-11T04:23:18.6587215Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6587303Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6587375Z     ):
2025-04-11T04:23:18.6587485Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6587681Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6587863Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6588036Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6588203Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6588478Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6588552Z     
2025-04-11T04:23:18.6588641Z         torch.manual_seed(123)
2025-04-11T04:23:18.6588733Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6588824Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6588828Z 
2025-04-11T04:23:18.6588983Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6589094Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6589099Z 
2025-04-11T04:23:18.6589181Z device = None
2025-04-11T04:23:18.6589185Z 
2025-04-11T04:23:18.6589299Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6589447Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6589524Z     
2025-04-11T04:23:18.6589595Z         Args:
2025-04-11T04:23:18.6589764Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6589932Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6590037Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6590108Z         """
2025-04-11T04:23:18.6590188Z         _lazy_init()
2025-04-11T04:23:18.6590285Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6590386Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6590494Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6590776Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6590914Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6591069Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6591076Z 
2025-04-11T04:23:18.6591314Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6591493Z ____________ test_flash_decoding[True-False-5-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.6591498Z 
2025-04-11T04:23:18.6591650Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6591816Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6591901Z use_new_kcache_layout = True
2025-04-11T04:23:18.6591906Z 
2025-04-11T04:23:18.6592105Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6592209Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6592326Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6592464Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6592675Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6592790Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6592925Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6593036Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6593170Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6593318Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6593407Z     def test_flash_decoding(
2025-04-11T04:23:18.6593482Z         bsz: int,
2025-04-11T04:23:18.6593566Z         block_size: int,
2025-04-11T04:23:18.6593656Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6593739Z         num_attn_heads: int,
2025-04-11T04:23:18.6593821Z         kv_group_num: int,
2025-04-11T04:23:18.6593906Z         same_context_len: bool,
2025-04-11T04:23:18.6593987Z         q_len: int,
2025-04-11T04:23:18.6594070Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6594165Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6594237Z     ):
2025-04-11T04:23:18.6594344Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6594636Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6594818Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6594990Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6595153Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6595309Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6595381Z     
2025-04-11T04:23:18.6595466Z         torch.manual_seed(123)
2025-04-11T04:23:18.6595557Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6595647Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6595651Z 
2025-04-11T04:23:18.6595817Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6595928Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6595935Z 
2025-04-11T04:23:18.6596021Z device = None
2025-04-11T04:23:18.6596026Z 
2025-04-11T04:23:18.6596140Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6596291Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6596368Z     
2025-04-11T04:23:18.6596441Z         Args:
2025-04-11T04:23:18.6596610Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6596775Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6596881Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6596954Z         """
2025-04-11T04:23:18.6597030Z         _lazy_init()
2025-04-11T04:23:18.6597127Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6597231Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6597338Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6597623Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6597760Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6597915Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6597919Z 
2025-04-11T04:23:18.6598156Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6598327Z _____________ test_flash_decoding[True-False-5-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.6598331Z 
2025-04-11T04:23:18.6598483Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6598650Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6598827Z use_new_kcache_layout = True
2025-04-11T04:23:18.6598832Z 
2025-04-11T04:23:18.6599037Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6599146Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6599269Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6599406Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6599518Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6599634Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6599766Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6599868Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6600002Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6600153Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6600239Z     def test_flash_decoding(
2025-04-11T04:23:18.6600313Z         bsz: int,
2025-04-11T04:23:18.6600396Z         block_size: int,
2025-04-11T04:23:18.6600486Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6600654Z         num_attn_heads: int,
2025-04-11T04:23:18.6600737Z         kv_group_num: int,
2025-04-11T04:23:18.6600821Z         same_context_len: bool,
2025-04-11T04:23:18.6600900Z         q_len: int,
2025-04-11T04:23:18.6600984Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6601076Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6601148Z     ):
2025-04-11T04:23:18.6601257Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6601454Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6601633Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6601806Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6601972Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6602130Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6602205Z     
2025-04-11T04:23:18.6602291Z         torch.manual_seed(123)
2025-04-11T04:23:18.6602384Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6602473Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6602477Z 
2025-04-11T04:23:18.6602635Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6602746Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6602751Z 
2025-04-11T04:23:18.6602830Z device = None
2025-04-11T04:23:18.6602835Z 
2025-04-11T04:23:18.6602949Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6603102Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6603178Z     
2025-04-11T04:23:18.6603250Z         Args:
2025-04-11T04:23:18.6603420Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6603589Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6603699Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6603771Z         """
2025-04-11T04:23:18.6603846Z         _lazy_init()
2025-04-11T04:23:18.6603945Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6604044Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6604150Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6604432Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6604569Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6604726Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6604820Z 
2025-04-11T04:23:18.6605062Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6605242Z ____________ test_flash_decoding[True-False-5-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.6605246Z 
2025-04-11T04:23:18.6605399Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6605565Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6605652Z use_new_kcache_layout = True
2025-04-11T04:23:18.6605656Z 
2025-04-11T04:23:18.6605854Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6605956Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6606071Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6606206Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6606321Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6606439Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6606574Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6606762Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6606897Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6607050Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6607132Z     def test_flash_decoding(
2025-04-11T04:23:18.6607207Z         bsz: int,
2025-04-11T04:23:18.6607294Z         block_size: int,
2025-04-11T04:23:18.6607385Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6607472Z         num_attn_heads: int,
2025-04-11T04:23:18.6607554Z         kv_group_num: int,
2025-04-11T04:23:18.6607643Z         same_context_len: bool,
2025-04-11T04:23:18.6607723Z         q_len: int,
2025-04-11T04:23:18.6607806Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6607904Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6607977Z     ):
2025-04-11T04:23:18.6608087Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6608285Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6608466Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6608643Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6608804Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6608964Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6609036Z     
2025-04-11T04:23:18.6609124Z         torch.manual_seed(123)
2025-04-11T04:23:18.6609216Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6609305Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6609310Z 
2025-04-11T04:23:18.6609474Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6609585Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6609593Z 
2025-04-11T04:23:18.6609671Z device = None
2025-04-11T04:23:18.6609676Z 
2025-04-11T04:23:18.6609789Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6609941Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6610012Z     
2025-04-11T04:23:18.6610084Z         Args:
2025-04-11T04:23:18.6610253Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6610416Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6610522Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6610593Z         """
2025-04-11T04:23:18.6610667Z         _lazy_init()
2025-04-11T04:23:18.6610764Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6610969Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6611074Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6611357Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6611495Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6611649Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6611654Z 
2025-04-11T04:23:18.6611896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6612060Z _____________ test_flash_decoding[True-False-5-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.6612064Z 
2025-04-11T04:23:18.6612212Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6612377Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6612466Z use_new_kcache_layout = True
2025-04-11T04:23:18.6612470Z 
2025-04-11T04:23:18.6612672Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6612863Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6612981Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6613118Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6613235Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6613352Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6613488Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6613598Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6613735Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6613891Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6613981Z     def test_flash_decoding(
2025-04-11T04:23:18.6614060Z         bsz: int,
2025-04-11T04:23:18.6614149Z         block_size: int,
2025-04-11T04:23:18.6614242Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6614333Z         num_attn_heads: int,
2025-04-11T04:23:18.6614419Z         kv_group_num: int,
2025-04-11T04:23:18.6614506Z         same_context_len: bool,
2025-04-11T04:23:18.6614588Z         q_len: int,
2025-04-11T04:23:18.6614675Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6614768Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6614842Z     ):
2025-04-11T04:23:18.6614958Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6615152Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6615335Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6615513Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6615682Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6615846Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6615923Z     
2025-04-11T04:23:18.6616016Z         torch.manual_seed(123)
2025-04-11T04:23:18.6616107Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6616200Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6616204Z 
2025-04-11T04:23:18.6616364Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6616479Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6616483Z 
2025-04-11T04:23:18.6616566Z device = None
2025-04-11T04:23:18.6616570Z 
2025-04-11T04:23:18.6616689Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6616845Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6616920Z     
2025-04-11T04:23:18.6617083Z         Args:
2025-04-11T04:23:18.6617251Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6617416Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6617524Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6617596Z         """
2025-04-11T04:23:18.6617677Z         _lazy_init()
2025-04-11T04:23:18.6617774Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6617872Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6617979Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6618259Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6618398Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6618552Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6618560Z 
2025-04-11T04:23:18.6618800Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6619051Z _____________ test_flash_decoding[True-False-5-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.6619056Z 
2025-04-11T04:23:18.6619207Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6619372Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6619457Z use_new_kcache_layout = True
2025-04-11T04:23:18.6619461Z 
2025-04-11T04:23:18.6619663Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6619766Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6619884Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6620020Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6620132Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6620250Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6620384Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6620492Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6620626Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6620778Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6620861Z     def test_flash_decoding(
2025-04-11T04:23:18.6620935Z         bsz: int,
2025-04-11T04:23:18.6621019Z         block_size: int,
2025-04-11T04:23:18.6621108Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6621191Z         num_attn_heads: int,
2025-04-11T04:23:18.6621271Z         kv_group_num: int,
2025-04-11T04:23:18.6621353Z         same_context_len: bool,
2025-04-11T04:23:18.6621433Z         q_len: int,
2025-04-11T04:23:18.6621514Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6621605Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6621679Z     ):
2025-04-11T04:23:18.6621791Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6621986Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6622165Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6622337Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6622499Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6622657Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6622728Z     
2025-04-11T04:23:18.6622815Z         torch.manual_seed(123)
2025-04-11T04:23:18.6622902Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6622990Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6622994Z 
2025-04-11T04:23:18.6623258Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6623367Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6623375Z 
2025-04-11T04:23:18.6623456Z device = None
2025-04-11T04:23:18.6623460Z 
2025-04-11T04:23:18.6623573Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6623727Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6623799Z     
2025-04-11T04:23:18.6623873Z         Args:
2025-04-11T04:23:18.6624043Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6624205Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6624311Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6624383Z         """
2025-04-11T04:23:18.6624462Z         _lazy_init()
2025-04-11T04:23:18.6624553Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6624655Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6624763Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6625132Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6625268Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6625423Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6625427Z 
2025-04-11T04:23:18.6625666Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6625832Z _____________ test_flash_decoding[True-False-5-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.6625837Z 
2025-04-11T04:23:18.6625989Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6626152Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6626240Z use_new_kcache_layout = True
2025-04-11T04:23:18.6626244Z 
2025-04-11T04:23:18.6626449Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6626555Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6626672Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6626809Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6626924Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6627034Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6627167Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6627272Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6627403Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6627552Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6627637Z     def test_flash_decoding(
2025-04-11T04:23:18.6627716Z         bsz: int,
2025-04-11T04:23:18.6627800Z         block_size: int,
2025-04-11T04:23:18.6627889Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6627976Z         num_attn_heads: int,
2025-04-11T04:23:18.6628058Z         kv_group_num: int,
2025-04-11T04:23:18.6628145Z         same_context_len: bool,
2025-04-11T04:23:18.6628221Z         q_len: int,
2025-04-11T04:23:18.6628304Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6628395Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6628522Z     ):
2025-04-11T04:23:18.6628636Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6628828Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6629008Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6629183Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6629447Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6629608Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6629684Z     
2025-04-11T04:23:18.6629773Z         torch.manual_seed(123)
2025-04-11T04:23:18.6629860Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6629949Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6629953Z 
2025-04-11T04:23:18.6630110Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6630221Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6630225Z 
2025-04-11T04:23:18.6630306Z device = None
2025-04-11T04:23:18.6630311Z 
2025-04-11T04:23:18.6630426Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6630577Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6630651Z     
2025-04-11T04:23:18.6630727Z         Args:
2025-04-11T04:23:18.6630895Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6631167Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6631276Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6631348Z         """
2025-04-11T04:23:18.6631428Z         _lazy_init()
2025-04-11T04:23:18.6631522Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6631621Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6631729Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6632012Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6632150Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6632305Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6632313Z 
2025-04-11T04:23:18.6632552Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6632723Z _____________ test_flash_decoding[True-False-5-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.6632727Z 
2025-04-11T04:23:18.6632880Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6633042Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6633127Z use_new_kcache_layout = True
2025-04-11T04:23:18.6633131Z 
2025-04-11T04:23:18.6633331Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6633433Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6633550Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6633688Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6633805Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6633920Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6634053Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6634162Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6634295Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6634445Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6634529Z     def test_flash_decoding(
2025-04-11T04:23:18.6634606Z         bsz: int,
2025-04-11T04:23:18.6634685Z         block_size: int,
2025-04-11T04:23:18.6634774Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6634858Z         num_attn_heads: int,
2025-04-11T04:23:18.6634938Z         kv_group_num: int,
2025-04-11T04:23:18.6635024Z         same_context_len: bool,
2025-04-11T04:23:18.6635098Z         q_len: int,
2025-04-11T04:23:18.6635180Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6635270Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6635426Z     ):
2025-04-11T04:23:18.6635539Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6635731Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6635912Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6636084Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6636242Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6636403Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6636473Z     
2025-04-11T04:23:18.6636561Z         torch.manual_seed(123)
2025-04-11T04:23:18.6636647Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6636736Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6636745Z 
2025-04-11T04:23:18.6636897Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6637010Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6637098Z 
2025-04-11T04:23:18.6637181Z device = None
2025-04-11T04:23:18.6637186Z 
2025-04-11T04:23:18.6637300Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6637451Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6637523Z     
2025-04-11T04:23:18.6637598Z         Args:
2025-04-11T04:23:18.6637765Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6637929Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6638036Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6638109Z         """
2025-04-11T04:23:18.6638189Z         _lazy_init()
2025-04-11T04:23:18.6638282Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6638386Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6638493Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6638777Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6638915Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6639071Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6639076Z 
2025-04-11T04:23:18.6639316Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6639483Z _____________ test_flash_decoding[True-False-5-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.6639488Z 
2025-04-11T04:23:18.6639642Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6639808Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6639895Z use_new_kcache_layout = True
2025-04-11T04:23:18.6639899Z 
2025-04-11T04:23:18.6640102Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6640207Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6640328Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6640464Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6640580Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6640692Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6640827Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6640933Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6641069Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6641219Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6641303Z     def test_flash_decoding(
2025-04-11T04:23:18.6641466Z         bsz: int,
2025-04-11T04:23:18.6641548Z         block_size: int,
2025-04-11T04:23:18.6641637Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6641725Z         num_attn_heads: int,
2025-04-11T04:23:18.6641805Z         kv_group_num: int,
2025-04-11T04:23:18.6641893Z         same_context_len: bool,
2025-04-11T04:23:18.6641967Z         q_len: int,
2025-04-11T04:23:18.6642052Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6642148Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6642219Z     ):
2025-04-11T04:23:18.6642332Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6642525Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6642709Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6642880Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6643045Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6643206Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6643436Z     
2025-04-11T04:23:18.6643528Z         torch.manual_seed(123)
2025-04-11T04:23:18.6643617Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6643707Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6643715Z 
2025-04-11T04:23:18.6643869Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6643978Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6643982Z 
2025-04-11T04:23:18.6644063Z device = None
2025-04-11T04:23:18.6644067Z 
2025-04-11T04:23:18.6644183Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6644334Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6644407Z     
2025-04-11T04:23:18.6644490Z         Args:
2025-04-11T04:23:18.6644655Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6644817Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6644929Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6645002Z         """
2025-04-11T04:23:18.6645081Z         _lazy_init()
2025-04-11T04:23:18.6645176Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6645276Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6645380Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6645661Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6645796Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6645951Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6645958Z 
2025-04-11T04:23:18.6646207Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6646378Z ____________ test_flash_decoding[True-False-5-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.6646383Z 
2025-04-11T04:23:18.6646536Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6646699Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6646782Z use_new_kcache_layout = True
2025-04-11T04:23:18.6646789Z 
2025-04-11T04:23:18.6646986Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6647086Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6647204Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6647339Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6647455Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6647656Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6647789Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6647897Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6648030Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6648180Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6648262Z     def test_flash_decoding(
2025-04-11T04:23:18.6648341Z         bsz: int,
2025-04-11T04:23:18.6648420Z         block_size: int,
2025-04-11T04:23:18.6648510Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6648595Z         num_attn_heads: int,
2025-04-11T04:23:18.6648679Z         kv_group_num: int,
2025-04-11T04:23:18.6648768Z         same_context_len: bool,
2025-04-11T04:23:18.6648841Z         q_len: int,
2025-04-11T04:23:18.6648925Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6649015Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6649093Z     ):
2025-04-11T04:23:18.6649206Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6649398Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6649666Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6649835Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6649997Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6650156Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6650227Z     
2025-04-11T04:23:18.6650316Z         torch.manual_seed(123)
2025-04-11T04:23:18.6650404Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6650497Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6650501Z 
2025-04-11T04:23:18.6650653Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6650767Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6650774Z 
2025-04-11T04:23:18.6650856Z device = None
2025-04-11T04:23:18.6650860Z 
2025-04-11T04:23:18.6650975Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6651126Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6651196Z     
2025-04-11T04:23:18.6651272Z         Args:
2025-04-11T04:23:18.6651437Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6651601Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6651708Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6651780Z         """
2025-04-11T04:23:18.6651858Z         _lazy_init()
2025-04-11T04:23:18.6651955Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6652061Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6652164Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6652450Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6652587Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6652744Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6652749Z 
2025-04-11T04:23:18.6652993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6653160Z _____________ test_flash_decoding[True-False-5-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.6653163Z 
2025-04-11T04:23:18.6653316Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6653480Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6653653Z use_new_kcache_layout = True
2025-04-11T04:23:18.6653657Z 
2025-04-11T04:23:18.6653855Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6653962Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6654080Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6654217Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6654334Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6654442Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6654576Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6654677Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6654810Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6654961Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6655046Z     def test_flash_decoding(
2025-04-11T04:23:18.6655127Z         bsz: int,
2025-04-11T04:23:18.6655206Z         block_size: int,
2025-04-11T04:23:18.6655294Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6655482Z         num_attn_heads: int,
2025-04-11T04:23:18.6655567Z         kv_group_num: int,
2025-04-11T04:23:18.6655654Z         same_context_len: bool,
2025-04-11T04:23:18.6655729Z         q_len: int,
2025-04-11T04:23:18.6655812Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6655903Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6655976Z     ):
2025-04-11T04:23:18.6656088Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6656278Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6656457Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6656628Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6656793Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6656951Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6657026Z     
2025-04-11T04:23:18.6657115Z         torch.manual_seed(123)
2025-04-11T04:23:18.6657201Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6657294Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6657298Z 
2025-04-11T04:23:18.6657450Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6657558Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6657566Z 
2025-04-11T04:23:18.6657642Z device = None
2025-04-11T04:23:18.6657646Z 
2025-04-11T04:23:18.6657758Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6657910Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6657981Z     
2025-04-11T04:23:18.6658061Z         Args:
2025-04-11T04:23:18.6658225Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6658387Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6658497Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6658569Z         """
2025-04-11T04:23:18.6658648Z         _lazy_init()
2025-04-11T04:23:18.6658740Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6658843Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6658946Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6659229Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6659369Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6659523Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6659612Z 
2025-04-11T04:23:18.6659856Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6660031Z ____________ test_flash_decoding[True-False-5-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.6660035Z 
2025-04-11T04:23:18.6660189Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6660352Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6660441Z use_new_kcache_layout = True
2025-04-11T04:23:18.6660445Z 
2025-04-11T04:23:18.6660642Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6660743Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6660862Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6660996Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6661113Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6661226Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6661362Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6661550Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6661684Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6661840Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6661927Z     def test_flash_decoding(
2025-04-11T04:23:18.6662006Z         bsz: int,
2025-04-11T04:23:18.6662087Z         block_size: int,
2025-04-11T04:23:18.6662177Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6662263Z         num_attn_heads: int,
2025-04-11T04:23:18.6662344Z         kv_group_num: int,
2025-04-11T04:23:18.6662432Z         same_context_len: bool,
2025-04-11T04:23:18.6662506Z         q_len: int,
2025-04-11T04:23:18.6662592Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6662678Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6662755Z     ):
2025-04-11T04:23:18.6662867Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6663060Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6663247Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6663419Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6663579Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6663738Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6663809Z     
2025-04-11T04:23:18.6663897Z         torch.manual_seed(123)
2025-04-11T04:23:18.6663984Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6664083Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6664087Z 
2025-04-11T04:23:18.6664242Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6664356Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6664367Z 
2025-04-11T04:23:18.6664442Z device = None
2025-04-11T04:23:18.6664447Z 
2025-04-11T04:23:18.6664559Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6664710Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6664780Z     
2025-04-11T04:23:18.6664855Z         Args:
2025-04-11T04:23:18.6665018Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6665182Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6665288Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6665360Z         """
2025-04-11T04:23:18.6665441Z         _lazy_init()
2025-04-11T04:23:18.6665534Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6665729Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6665832Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6666115Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6666254Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6666410Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6666415Z 
2025-04-11T04:23:18.6666653Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6666819Z ______________ test_flash_decoding[False-True-1-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6666823Z 
2025-04-11T04:23:18.6666975Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6667135Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6667233Z use_new_kcache_layout = False
2025-04-11T04:23:18.6667238Z 
2025-04-11T04:23:18.6667435Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6667627Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6667747Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6667887Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6668004Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6668115Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6668252Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6668354Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6668519Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6668672Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6668756Z     def test_flash_decoding(
2025-04-11T04:23:18.6668837Z         bsz: int,
2025-04-11T04:23:18.6668917Z         block_size: int,
2025-04-11T04:23:18.6669007Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6669092Z         num_attn_heads: int,
2025-04-11T04:23:18.6669173Z         kv_group_num: int,
2025-04-11T04:23:18.6669259Z         same_context_len: bool,
2025-04-11T04:23:18.6669333Z         q_len: int,
2025-04-11T04:23:18.6669419Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6669506Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6669579Z     ):
2025-04-11T04:23:18.6669690Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6669883Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6670065Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6670236Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6670402Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6670557Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6670634Z     
2025-04-11T04:23:18.6670723Z         torch.manual_seed(123)
2025-04-11T04:23:18.6670811Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6670908Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6670913Z 
2025-04-11T04:23:18.6671068Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6671180Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6671184Z 
2025-04-11T04:23:18.6671260Z device = None
2025-04-11T04:23:18.6671265Z 
2025-04-11T04:23:18.6671376Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6671526Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6671596Z     
2025-04-11T04:23:18.6671771Z         Args:
2025-04-11T04:23:18.6671936Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6672100Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6672208Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6672280Z         """
2025-04-11T04:23:18.6672360Z         _lazy_init()
2025-04-11T04:23:18.6672454Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6672557Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6672659Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6672938Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6673076Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6673230Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6673238Z 
2025-04-11T04:23:18.6673481Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6673746Z _____________ test_flash_decoding[False-True-1-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6673750Z 
2025-04-11T04:23:18.6673907Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6674068Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6674159Z use_new_kcache_layout = False
2025-04-11T04:23:18.6674163Z 
2025-04-11T04:23:18.6674362Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6674468Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6674584Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6674719Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6674837Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6674950Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6675085Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6675193Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6675325Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6675479Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6675563Z     def test_flash_decoding(
2025-04-11T04:23:18.6675644Z         bsz: int,
2025-04-11T04:23:18.6675724Z         block_size: int,
2025-04-11T04:23:18.6675815Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6675898Z         num_attn_heads: int,
2025-04-11T04:23:18.6675979Z         kv_group_num: int,
2025-04-11T04:23:18.6676065Z         same_context_len: bool,
2025-04-11T04:23:18.6676139Z         q_len: int,
2025-04-11T04:23:18.6676227Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6676312Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6676386Z     ):
2025-04-11T04:23:18.6676499Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6676689Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6676875Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6677044Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6677213Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6677370Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6677440Z     
2025-04-11T04:23:18.6677528Z         torch.manual_seed(123)
2025-04-11T04:23:18.6677614Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6677706Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6677710Z 
2025-04-11T04:23:18.6677863Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6678077Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6678085Z 
2025-04-11T04:23:18.6678162Z device = None
2025-04-11T04:23:18.6678167Z 
2025-04-11T04:23:18.6678281Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6678433Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6678504Z     
2025-04-11T04:23:18.6678578Z         Args:
2025-04-11T04:23:18.6678742Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6678910Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6679014Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6679086Z         """
2025-04-11T04:23:18.6679167Z         _lazy_init()
2025-04-11T04:23:18.6679258Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6679366Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6679468Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6679750Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6679986Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6680141Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6680146Z 
2025-04-11T04:23:18.6680387Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6680551Z ______________ test_flash_decoding[False-True-1-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6680555Z 
2025-04-11T04:23:18.6680706Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6680865Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6680958Z use_new_kcache_layout = False
2025-04-11T04:23:18.6680963Z 
2025-04-11T04:23:18.6681161Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6681270Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6681384Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6681521Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6681638Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6681749Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6681885Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6681987Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6682122Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6682273Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6682357Z     def test_flash_decoding(
2025-04-11T04:23:18.6682440Z         bsz: int,
2025-04-11T04:23:18.6682519Z         block_size: int,
2025-04-11T04:23:18.6682611Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6682696Z         num_attn_heads: int,
2025-04-11T04:23:18.6682775Z         kv_group_num: int,
2025-04-11T04:23:18.6682864Z         same_context_len: bool,
2025-04-11T04:23:18.6682938Z         q_len: int,
2025-04-11T04:23:18.6683024Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6683111Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6683184Z     ):
2025-04-11T04:23:18.6683296Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6683488Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6683671Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6683842Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6684098Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6684256Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6684332Z     
2025-04-11T04:23:18.6684422Z         torch.manual_seed(123)
2025-04-11T04:23:18.6684509Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6684603Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6684608Z 
2025-04-11T04:23:18.6684763Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6684876Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6684880Z 
2025-04-11T04:23:18.6684956Z device = None
2025-04-11T04:23:18.6684961Z 
2025-04-11T04:23:18.6685078Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6685227Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6685298Z     
2025-04-11T04:23:18.6685374Z         Args:
2025-04-11T04:23:18.6685548Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6685717Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6685905Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6685981Z         """
2025-04-11T04:23:18.6686065Z         _lazy_init()
2025-04-11T04:23:18.6686164Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6686271Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6686380Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6686674Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6686814Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6686974Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6686982Z 
2025-04-11T04:23:18.6687230Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6687401Z _____________ test_flash_decoding[False-True-1-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6687409Z 
2025-04-11T04:23:18.6687569Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6687734Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6687829Z use_new_kcache_layout = False
2025-04-11T04:23:18.6687833Z 
2025-04-11T04:23:18.6688037Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6688147Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6688264Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6688405Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6688527Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6688647Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6688789Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6688897Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6689039Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6689191Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6689279Z     def test_flash_decoding(
2025-04-11T04:23:18.6689362Z         bsz: int,
2025-04-11T04:23:18.6689445Z         block_size: int,
2025-04-11T04:23:18.6689542Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6689627Z         num_attn_heads: int,
2025-04-11T04:23:18.6689714Z         kv_group_num: int,
2025-04-11T04:23:18.6689805Z         same_context_len: bool,
2025-04-11T04:23:18.6689883Z         q_len: int,
2025-04-11T04:23:18.6689976Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6690066Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6690223Z     ):
2025-04-11T04:23:18.6690339Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6690532Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6690719Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6690891Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6691056Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6691214Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6691287Z     
2025-04-11T04:23:18.6691373Z         torch.manual_seed(123)
2025-04-11T04:23:18.6691460Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6691553Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6691557Z 
2025-04-11T04:23:18.6691711Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6691828Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6691915Z 
2025-04-11T04:23:18.6691994Z device = None
2025-04-11T04:23:18.6691999Z 
2025-04-11T04:23:18.6692117Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6692266Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6692337Z     
2025-04-11T04:23:18.6692414Z         Args:
2025-04-11T04:23:18.6692576Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6692742Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6692844Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6692918Z         """
2025-04-11T04:23:18.6692993Z         _lazy_init()
2025-04-11T04:23:18.6693085Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6693191Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6693294Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6693575Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6693712Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6693865Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6693873Z 
2025-04-11T04:23:18.6694107Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6694270Z _____________ test_flash_decoding[False-True-1-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6694275Z 
2025-04-11T04:23:18.6694430Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6694588Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6694684Z use_new_kcache_layout = False
2025-04-11T04:23:18.6694689Z 
2025-04-11T04:23:18.6694885Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6694993Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6695107Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6695242Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6695359Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6695471Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6695607Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6695706Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6695841Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6695988Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6696073Z     def test_flash_decoding(
2025-04-11T04:23:18.6696240Z         bsz: int,
2025-04-11T04:23:18.6696321Z         block_size: int,
2025-04-11T04:23:18.6696412Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6696497Z         num_attn_heads: int,
2025-04-11T04:23:18.6696577Z         kv_group_num: int,
2025-04-11T04:23:18.6696664Z         same_context_len: bool,
2025-04-11T04:23:18.6696738Z         q_len: int,
2025-04-11T04:23:18.6696825Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6696911Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6696985Z     ):
2025-04-11T04:23:18.6697093Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6697286Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6697471Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6697640Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6697810Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6697966Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6698128Z     
2025-04-11T04:23:18.6698216Z         torch.manual_seed(123)
2025-04-11T04:23:18.6698305Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6698401Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6698405Z 
2025-04-11T04:23:18.6698557Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6698669Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6698674Z 
2025-04-11T04:23:18.6698752Z device = None
2025-04-11T04:23:18.6698756Z 
2025-04-11T04:23:18.6698872Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6699019Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6699090Z     
2025-04-11T04:23:18.6699167Z         Args:
2025-04-11T04:23:18.6699335Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6699498Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6699605Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6699680Z         """
2025-04-11T04:23:18.6699756Z         _lazy_init()
2025-04-11T04:23:18.6699848Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6699952Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6700055Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6700336Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6700470Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6700624Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6700636Z 
2025-04-11T04:23:18.6700870Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6701036Z _____________ test_flash_decoding[False-True-1-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6701043Z 
2025-04-11T04:23:18.6701198Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6701358Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6701450Z use_new_kcache_layout = False
2025-04-11T04:23:18.6701455Z 
2025-04-11T04:23:18.6701652Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6701756Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6701871Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6702005Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6702120Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6702334Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6702470Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6702573Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6702708Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6702855Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6702941Z     def test_flash_decoding(
2025-04-11T04:23:18.6703020Z         bsz: int,
2025-04-11T04:23:18.6703100Z         block_size: int,
2025-04-11T04:23:18.6703192Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6703272Z         num_attn_heads: int,
2025-04-11T04:23:18.6703356Z         kv_group_num: int,
2025-04-11T04:23:18.6703441Z         same_context_len: bool,
2025-04-11T04:23:18.6703515Z         q_len: int,
2025-04-11T04:23:18.6703602Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6703689Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6703766Z     ):
2025-04-11T04:23:18.6703874Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6704062Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6704340Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6704506Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6704670Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6704825Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6704899Z     
2025-04-11T04:23:18.6704983Z         torch.manual_seed(123)
2025-04-11T04:23:18.6705070Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6705164Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6705168Z 
2025-04-11T04:23:18.6705320Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6705436Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6705440Z 
2025-04-11T04:23:18.6705519Z device = None
2025-04-11T04:23:18.6705524Z 
2025-04-11T04:23:18.6705639Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6705785Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6705855Z     
2025-04-11T04:23:18.6705931Z         Args:
2025-04-11T04:23:18.6706093Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6706258Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6706360Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6706436Z         """
2025-04-11T04:23:18.6706513Z         _lazy_init()
2025-04-11T04:23:18.6706605Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6706707Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6706814Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6707096Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6707234Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6707392Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6707397Z 
2025-04-11T04:23:18.6707634Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6707798Z _____________ test_flash_decoding[False-True-1-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6707806Z 
2025-04-11T04:23:18.6707956Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6708116Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6708206Z use_new_kcache_layout = False
2025-04-11T04:23:18.6708298Z 
2025-04-11T04:23:18.6708530Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6708641Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6708755Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6708897Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6709010Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6709122Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6709260Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6709360Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6709497Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6709650Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6709737Z     def test_flash_decoding(
2025-04-11T04:23:18.6709816Z         bsz: int,
2025-04-11T04:23:18.6709896Z         block_size: int,
2025-04-11T04:23:18.6709987Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6710160Z         num_attn_heads: int,
2025-04-11T04:23:18.6710244Z         kv_group_num: int,
2025-04-11T04:23:18.6710327Z         same_context_len: bool,
2025-04-11T04:23:18.6710402Z         q_len: int,
2025-04-11T04:23:18.6710490Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6710577Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6710651Z     ):
2025-04-11T04:23:18.6710758Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6710949Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6711130Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6711298Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6711462Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6711621Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6711698Z     
2025-04-11T04:23:18.6711782Z         torch.manual_seed(123)
2025-04-11T04:23:18.6711869Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6711962Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6711966Z 
2025-04-11T04:23:18.6712118Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6712230Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6712234Z 
2025-04-11T04:23:18.6712310Z device = None
2025-04-11T04:23:18.6712314Z 
2025-04-11T04:23:18.6712429Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6712575Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6712648Z     
2025-04-11T04:23:18.6712720Z         Args:
2025-04-11T04:23:18.6712887Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6713052Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6713159Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6713233Z         """
2025-04-11T04:23:18.6713309Z         _lazy_init()
2025-04-11T04:23:18.6713400Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6713503Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6713603Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6713885Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6714018Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6714174Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6714269Z 
2025-04-11T04:23:18.6714508Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6714673Z _____________ test_flash_decoding[False-True-1-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6714684Z 
2025-04-11T04:23:18.6714835Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6714995Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6715084Z use_new_kcache_layout = False
2025-04-11T04:23:18.6715089Z 
2025-04-11T04:23:18.6715286Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6715390Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6715503Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6715641Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6715753Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6715865Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6716004Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6716186Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6716322Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6716471Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6716558Z     def test_flash_decoding(
2025-04-11T04:23:18.6716632Z         bsz: int,
2025-04-11T04:23:18.6716712Z         block_size: int,
2025-04-11T04:23:18.6716803Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6716883Z         num_attn_heads: int,
2025-04-11T04:23:18.6716966Z         kv_group_num: int,
2025-04-11T04:23:18.6717050Z         same_context_len: bool,
2025-04-11T04:23:18.6717123Z         q_len: int,
2025-04-11T04:23:18.6717210Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6717296Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6717374Z     ):
2025-04-11T04:23:18.6717483Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6717671Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6717859Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6718026Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6718191Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6718345Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6718419Z     
2025-04-11T04:23:18.6718505Z         torch.manual_seed(123)
2025-04-11T04:23:18.6718591Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6718685Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6718689Z 
2025-04-11T04:23:18.6718839Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6718954Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6718958Z 
2025-04-11T04:23:18.6719037Z device = None
2025-04-11T04:23:18.6719042Z 
2025-04-11T04:23:18.6719158Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6719304Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6719377Z     
2025-04-11T04:23:18.6719449Z         Args:
2025-04-11T04:23:18.6719612Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6719778Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6719879Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6719955Z         """
2025-04-11T04:23:18.6720030Z         _lazy_init()
2025-04-11T04:23:18.6720124Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6720227Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6720423Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6720711Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6720853Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6721016Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6721020Z 
2025-04-11T04:23:18.6721259Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6721427Z ______________ test_flash_decoding[False-True-1-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6721431Z 
2025-04-11T04:23:18.6721581Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6721743Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6721834Z use_new_kcache_layout = False
2025-04-11T04:23:18.6721842Z 
2025-04-11T04:23:18.6722042Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6722245Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6722359Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6722499Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6722611Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6722721Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6722858Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6722956Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6723094Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6723242Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6723328Z     def test_flash_decoding(
2025-04-11T04:23:18.6723410Z         bsz: int,
2025-04-11T04:23:18.6723491Z         block_size: int,
2025-04-11T04:23:18.6723584Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6723665Z         num_attn_heads: int,
2025-04-11T04:23:18.6723756Z         kv_group_num: int,
2025-04-11T04:23:18.6723841Z         same_context_len: bool,
2025-04-11T04:23:18.6723915Z         q_len: int,
2025-04-11T04:23:18.6724003Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6724090Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6724165Z     ):
2025-04-11T04:23:18.6724273Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6724466Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6724645Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6724812Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6724978Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6725135Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6725213Z     
2025-04-11T04:23:18.6725299Z         torch.manual_seed(123)
2025-04-11T04:23:18.6725389Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6725479Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6725484Z 
2025-04-11T04:23:18.6725634Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6725747Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6725752Z 
2025-04-11T04:23:18.6725829Z device = None
2025-04-11T04:23:18.6725834Z 
2025-04-11T04:23:18.6725950Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6726095Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6726168Z     
2025-04-11T04:23:18.6726241Z         Args:
2025-04-11T04:23:18.6726492Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6726659Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6726768Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6726842Z         """
2025-04-11T04:23:18.6726919Z         _lazy_init()
2025-04-11T04:23:18.6727012Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6727114Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6727217Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6727500Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6727633Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6727791Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6727795Z 
2025-04-11T04:23:18.6728036Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6728203Z _____________ test_flash_decoding[False-True-1-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6728293Z 
2025-04-11T04:23:18.6728445Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6728604Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6728694Z use_new_kcache_layout = False
2025-04-11T04:23:18.6728699Z 
2025-04-11T04:23:18.6728901Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6729007Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6729123Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6729263Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6729377Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6729492Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6729628Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6729733Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6729873Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6730022Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6730109Z     def test_flash_decoding(
2025-04-11T04:23:18.6730184Z         bsz: int,
2025-04-11T04:23:18.6730264Z         block_size: int,
2025-04-11T04:23:18.6730357Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6730437Z         num_attn_heads: int,
2025-04-11T04:23:18.6730521Z         kv_group_num: int,
2025-04-11T04:23:18.6730607Z         same_context_len: bool,
2025-04-11T04:23:18.6730681Z         q_len: int,
2025-04-11T04:23:18.6730768Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6730855Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6730935Z     ):
2025-04-11T04:23:18.6731045Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6731241Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6731423Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6731595Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6731763Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6731920Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6731995Z     
2025-04-11T04:23:18.6732082Z         torch.manual_seed(123)
2025-04-11T04:23:18.6732171Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6732261Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6732266Z 
2025-04-11T04:23:18.6732416Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6732616Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6732620Z 
2025-04-11T04:23:18.6732701Z device = None
2025-04-11T04:23:18.6732705Z 
2025-04-11T04:23:18.6732827Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6732976Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6733050Z     
2025-04-11T04:23:18.6733121Z         Args:
2025-04-11T04:23:18.6733289Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6733457Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6733560Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6733634Z         """
2025-04-11T04:23:18.6733709Z         _lazy_init()
2025-04-11T04:23:18.6733805Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6733905Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6734010Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6734295Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6734514Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6734673Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6734678Z 
2025-04-11T04:23:18.6734913Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6735080Z ______________ test_flash_decoding[False-True-1-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6735084Z 
2025-04-11T04:23:18.6735231Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6735394Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6735482Z use_new_kcache_layout = False
2025-04-11T04:23:18.6735489Z 
2025-04-11T04:23:18.6735685Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6735793Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6735907Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6736044Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6736156Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6736270Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6736404Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6736504Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6736639Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6736784Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6736871Z     def test_flash_decoding(
2025-04-11T04:23:18.6736945Z         bsz: int,
2025-04-11T04:23:18.6737029Z         block_size: int,
2025-04-11T04:23:18.6737120Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6737201Z         num_attn_heads: int,
2025-04-11T04:23:18.6737293Z         kv_group_num: int,
2025-04-11T04:23:18.6737375Z         same_context_len: bool,
2025-04-11T04:23:18.6737452Z         q_len: int,
2025-04-11T04:23:18.6737535Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6737619Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6737695Z     ):
2025-04-11T04:23:18.6737803Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6737996Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6738176Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6738342Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6738508Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6738815Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6738893Z     
2025-04-11T04:23:18.6738980Z         torch.manual_seed(123)
2025-04-11T04:23:18.6739070Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6739160Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6739164Z 
2025-04-11T04:23:18.6739316Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6739428Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6739432Z 
2025-04-11T04:23:18.6739509Z device = None
2025-04-11T04:23:18.6739513Z 
2025-04-11T04:23:18.6739632Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6739778Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6739852Z     
2025-04-11T04:23:18.6739924Z         Args:
2025-04-11T04:23:18.6740091Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6740257Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6740468Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6740545Z         """
2025-04-11T04:23:18.6740623Z         _lazy_init()
2025-04-11T04:23:18.6740721Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6740820Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6740924Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6741211Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6741347Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6741507Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6741511Z 
2025-04-11T04:23:18.6741752Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6741921Z _____________ test_flash_decoding[False-True-1-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6741928Z 
2025-04-11T04:23:18.6742079Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6742242Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6742328Z use_new_kcache_layout = False
2025-04-11T04:23:18.6742333Z 
2025-04-11T04:23:18.6742531Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6742640Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6742754Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6742896Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6743010Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6743128Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6743262Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6743365Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6743504Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6743653Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6743739Z     def test_flash_decoding(
2025-04-11T04:23:18.6743813Z         bsz: int,
2025-04-11T04:23:18.6743896Z         block_size: int,
2025-04-11T04:23:18.6743983Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6744062Z         num_attn_heads: int,
2025-04-11T04:23:18.6744150Z         kv_group_num: int,
2025-04-11T04:23:18.6744234Z         same_context_len: bool,
2025-04-11T04:23:18.6744311Z         q_len: int,
2025-04-11T04:23:18.6744393Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6744479Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6744553Z     ):
2025-04-11T04:23:18.6744755Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6744951Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6745136Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6745305Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6745469Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6745624Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6745699Z     
2025-04-11T04:23:18.6745784Z         torch.manual_seed(123)
2025-04-11T04:23:18.6745873Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6745964Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6745968Z 
2025-04-11T04:23:18.6746118Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6746234Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6746238Z 
2025-04-11T04:23:18.6746314Z device = None
2025-04-11T04:23:18.6746406Z 
2025-04-11T04:23:18.6746526Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6746676Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6746752Z     
2025-04-11T04:23:18.6746824Z         Args:
2025-04-11T04:23:18.6746987Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6747154Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6747258Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6747333Z         """
2025-04-11T04:23:18.6747410Z         _lazy_init()
2025-04-11T04:23:18.6747507Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6747606Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6747712Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6747993Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6748131Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6748291Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6748295Z 
2025-04-11T04:23:18.6748581Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6748752Z _____________ test_flash_decoding[False-True-1-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6748756Z 
2025-04-11T04:23:18.6748905Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6749064Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6749151Z use_new_kcache_layout = False
2025-04-11T04:23:18.6749159Z 
2025-04-11T04:23:18.6749356Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6749465Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6749578Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6749719Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6749832Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6749946Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6750079Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6750178Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6750313Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6750461Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6750549Z     def test_flash_decoding(
2025-04-11T04:23:18.6750624Z         bsz: int,
2025-04-11T04:23:18.6750804Z         block_size: int,
2025-04-11T04:23:18.6750893Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6750975Z         num_attn_heads: int,
2025-04-11T04:23:18.6751064Z         kv_group_num: int,
2025-04-11T04:23:18.6751151Z         same_context_len: bool,
2025-04-11T04:23:18.6751228Z         q_len: int,
2025-04-11T04:23:18.6751310Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6751395Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6751470Z     ):
2025-04-11T04:23:18.6751578Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6751772Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6751953Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6752126Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6752290Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6752448Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6752617Z     
2025-04-11T04:23:18.6752705Z         torch.manual_seed(123)
2025-04-11T04:23:18.6752799Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6752890Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6752894Z 
2025-04-11T04:23:18.6753048Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6753157Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6753162Z 
2025-04-11T04:23:18.6753238Z device = None
2025-04-11T04:23:18.6753242Z 
2025-04-11T04:23:18.6753361Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6753507Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6753581Z     
2025-04-11T04:23:18.6753654Z         Args:
2025-04-11T04:23:18.6753821Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6753987Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6754095Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6754172Z         """
2025-04-11T04:23:18.6754248Z         _lazy_init()
2025-04-11T04:23:18.6754350Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6754450Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6754553Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6754835Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6754969Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6755127Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6755132Z 
2025-04-11T04:23:18.6755370Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6755539Z _____________ test_flash_decoding[False-True-1-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6755547Z 
2025-04-11T04:23:18.6755699Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6755859Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6755945Z use_new_kcache_layout = False
2025-04-11T04:23:18.6755949Z 
2025-04-11T04:23:18.6756149Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6756252Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6756365Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6756502Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6756617Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6756729Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6756949Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6757050Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6757192Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6757343Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6757431Z     def test_flash_decoding(
2025-04-11T04:23:18.6757505Z         bsz: int,
2025-04-11T04:23:18.6757588Z         block_size: int,
2025-04-11T04:23:18.6757675Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6757756Z         num_attn_heads: int,
2025-04-11T04:23:18.6757841Z         kv_group_num: int,
2025-04-11T04:23:18.6757925Z         same_context_len: bool,
2025-04-11T04:23:18.6758002Z         q_len: int,
2025-04-11T04:23:18.6758084Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6758170Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6758245Z     ):
2025-04-11T04:23:18.6758357Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6758552Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6758820Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6758995Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6759160Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6759317Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6759393Z     
2025-04-11T04:23:18.6759482Z         torch.manual_seed(123)
2025-04-11T04:23:18.6759576Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6759672Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6759676Z 
2025-04-11T04:23:18.6759835Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6759953Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6759958Z 
2025-04-11T04:23:18.6760036Z device = None
2025-04-11T04:23:18.6760047Z 
2025-04-11T04:23:18.6760165Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6760317Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6760395Z     
2025-04-11T04:23:18.6760471Z         Args:
2025-04-11T04:23:18.6760642Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6760809Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6760915Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6760996Z         """
2025-04-11T04:23:18.6761076Z         _lazy_init()
2025-04-11T04:23:18.6761178Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6761280Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6761394Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6761682Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6761826Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6761989Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6761994Z 
2025-04-11T04:23:18.6762242Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6762417Z _____________ test_flash_decoding[False-True-1-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6762421Z 
2025-04-11T04:23:18.6762575Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6762743Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6762834Z use_new_kcache_layout = False
2025-04-11T04:23:18.6762925Z 
2025-04-11T04:23:18.6763128Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6763233Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6763346Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6763488Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6763601Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6763715Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6763849Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6763952Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6764085Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6764233Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6764322Z     def test_flash_decoding(
2025-04-11T04:23:18.6764397Z         bsz: int,
2025-04-11T04:23:18.6764485Z         block_size: int,
2025-04-11T04:23:18.6764576Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6764654Z         num_attn_heads: int,
2025-04-11T04:23:18.6764827Z         kv_group_num: int,
2025-04-11T04:23:18.6764911Z         same_context_len: bool,
2025-04-11T04:23:18.6764990Z         q_len: int,
2025-04-11T04:23:18.6765073Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6765161Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6765236Z     ):
2025-04-11T04:23:18.6765346Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6765546Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6765725Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6765901Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6766064Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6766227Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6766306Z     
2025-04-11T04:23:18.6766392Z         torch.manual_seed(123)
2025-04-11T04:23:18.6766481Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6766570Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6766574Z 
2025-04-11T04:23:18.6766733Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6766842Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6766847Z 
2025-04-11T04:23:18.6766928Z device = None
2025-04-11T04:23:18.6766933Z 
2025-04-11T04:23:18.6767047Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6767193Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6767266Z     
2025-04-11T04:23:18.6767339Z         Args:
2025-04-11T04:23:18.6767510Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6767676Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6767782Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6767858Z         """
2025-04-11T04:23:18.6767935Z         _lazy_init()
2025-04-11T04:23:18.6768031Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6768130Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6768236Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6768521Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6768656Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6768814Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6768819Z 
2025-04-11T04:23:18.6769063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6769338Z _____________ test_flash_decoding[False-True-1-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6769346Z 
2025-04-11T04:23:18.6769497Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6769661Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6769749Z use_new_kcache_layout = False
2025-04-11T04:23:18.6769754Z 
2025-04-11T04:23:18.6769954Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6770054Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6770167Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6770305Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6770420Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6770537Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6770675Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6770779Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6770999Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6771151Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6771241Z     def test_flash_decoding(
2025-04-11T04:23:18.6771315Z         bsz: int,
2025-04-11T04:23:18.6771401Z         block_size: int,
2025-04-11T04:23:18.6771489Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6771571Z         num_attn_heads: int,
2025-04-11T04:23:18.6771654Z         kv_group_num: int,
2025-04-11T04:23:18.6771738Z         same_context_len: bool,
2025-04-11T04:23:18.6771815Z         q_len: int,
2025-04-11T04:23:18.6771900Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6771991Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6772064Z     ):
2025-04-11T04:23:18.6772177Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6772371Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6772553Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6772725Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6772888Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6773047Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6773118Z     
2025-04-11T04:23:18.6773203Z         torch.manual_seed(123)
2025-04-11T04:23:18.6773293Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6773384Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6773388Z 
2025-04-11T04:23:18.6773542Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6773655Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6773658Z 
2025-04-11T04:23:18.6773739Z device = None
2025-04-11T04:23:18.6773746Z 
2025-04-11T04:23:18.6773860Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6774005Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6774079Z     
2025-04-11T04:23:18.6774151Z         Args:
2025-04-11T04:23:18.6774319Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6774484Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6774590Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6774662Z         """
2025-04-11T04:23:18.6774739Z         _lazy_init()
2025-04-11T04:23:18.6774837Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6774936Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6775133Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6775416Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6775556Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6775717Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6775721Z 
2025-04-11T04:23:18.6775958Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6776128Z _____________ test_flash_decoding[False-True-1-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6776132Z 
2025-04-11T04:23:18.6776280Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6776445Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6776532Z use_new_kcache_layout = False
2025-04-11T04:23:18.6776540Z 
2025-04-11T04:23:18.6776739Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6776842Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6777044Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6777186Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6777301Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6777418Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6777552Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6777656Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6777790Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6777939Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6778026Z     def test_flash_decoding(
2025-04-11T04:23:18.6778100Z         bsz: int,
2025-04-11T04:23:18.6778184Z         block_size: int,
2025-04-11T04:23:18.6778271Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6778353Z         num_attn_heads: int,
2025-04-11T04:23:18.6778439Z         kv_group_num: int,
2025-04-11T04:23:18.6778522Z         same_context_len: bool,
2025-04-11T04:23:18.6778601Z         q_len: int,
2025-04-11T04:23:18.6778684Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6778772Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6778843Z     ):
2025-04-11T04:23:18.6778951Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6779144Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6779324Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6779497Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6779659Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6779822Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6779895Z     
2025-04-11T04:23:18.6779979Z         torch.manual_seed(123)
2025-04-11T04:23:18.6780070Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6780159Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6780163Z 
2025-04-11T04:23:18.6780318Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6780427Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6780431Z 
2025-04-11T04:23:18.6780511Z device = None
2025-04-11T04:23:18.6780515Z 
2025-04-11T04:23:18.6780630Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6780778Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6780851Z     
2025-04-11T04:23:18.6780925Z         Args:
2025-04-11T04:23:18.6781092Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6781346Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6781457Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6781530Z         """
2025-04-11T04:23:18.6781606Z         _lazy_init()
2025-04-11T04:23:18.6781703Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6781803Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6781908Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6782188Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6782325Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6782481Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6782485Z 
2025-04-11T04:23:18.6782720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6782893Z _____________ test_flash_decoding[False-True-1-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.6782980Z 
2025-04-11T04:23:18.6783133Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6783301Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6783388Z use_new_kcache_layout = False
2025-04-11T04:23:18.6783393Z 
2025-04-11T04:23:18.6783591Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6783693Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6783812Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6783947Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6784062Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6784177Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6784315Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6784418Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6784555Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6784703Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6784789Z     def test_flash_decoding(
2025-04-11T04:23:18.6784864Z         bsz: int,
2025-04-11T04:23:18.6784946Z         block_size: int,
2025-04-11T04:23:18.6785034Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6785117Z         num_attn_heads: int,
2025-04-11T04:23:18.6785199Z         kv_group_num: int,
2025-04-11T04:23:18.6785283Z         same_context_len: bool,
2025-04-11T04:23:18.6785360Z         q_len: int,
2025-04-11T04:23:18.6785444Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6785532Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6785603Z     ):
2025-04-11T04:23:18.6785714Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6785906Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6786088Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6786258Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6786420Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6786578Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6786649Z     
2025-04-11T04:23:18.6786734Z         torch.manual_seed(123)
2025-04-11T04:23:18.6786824Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6786913Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6786917Z 
2025-04-11T04:23:18.6787073Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6787268Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6787272Z 
2025-04-11T04:23:18.6787352Z device = None
2025-04-11T04:23:18.6787360Z 
2025-04-11T04:23:18.6787473Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6787621Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6787696Z     
2025-04-11T04:23:18.6787768Z         Args:
2025-04-11T04:23:18.6787936Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6788098Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6788205Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6788277Z         """
2025-04-11T04:23:18.6788354Z         _lazy_init()
2025-04-11T04:23:18.6788481Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6788583Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6788693Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6788974Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6789210Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6789367Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6789371Z 
2025-04-11T04:23:18.6789607Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6789774Z _____________ test_flash_decoding[False-True-1-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.6789778Z 
2025-04-11T04:23:18.6789924Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6790089Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6790174Z use_new_kcache_layout = False
2025-04-11T04:23:18.6790182Z 
2025-04-11T04:23:18.6790382Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6790485Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6790607Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6790741Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6790854Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6790968Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6791102Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6791205Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6791336Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6791485Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6791569Z     def test_flash_decoding(
2025-04-11T04:23:18.6791643Z         bsz: int,
2025-04-11T04:23:18.6791732Z         block_size: int,
2025-04-11T04:23:18.6791820Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6791903Z         num_attn_heads: int,
2025-04-11T04:23:18.6791986Z         kv_group_num: int,
2025-04-11T04:23:18.6792069Z         same_context_len: bool,
2025-04-11T04:23:18.6792147Z         q_len: int,
2025-04-11T04:23:18.6792230Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6792321Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6792392Z     ):
2025-04-11T04:23:18.6792499Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6792689Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6792865Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6793036Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6793196Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6793460Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6793535Z     
2025-04-11T04:23:18.6793620Z         torch.manual_seed(123)
2025-04-11T04:23:18.6793711Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6793801Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6793805Z 
2025-04-11T04:23:18.6793962Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6794072Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6794076Z 
2025-04-11T04:23:18.6794157Z device = None
2025-04-11T04:23:18.6794161Z 
2025-04-11T04:23:18.6794273Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6794423Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6794494Z     
2025-04-11T04:23:18.6794567Z         Args:
2025-04-11T04:23:18.6794735Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6794902Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6795089Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6795162Z         """
2025-04-11T04:23:18.6795239Z         _lazy_init()
2025-04-11T04:23:18.6795338Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6795438Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6795543Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6795827Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6795968Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6796124Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6796129Z 
2025-04-11T04:23:18.6796369Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6796541Z _____________ test_flash_decoding[False-True-1-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.6796549Z 
2025-04-11T04:23:18.6796700Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6796865Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6796952Z use_new_kcache_layout = False
2025-04-11T04:23:18.6796956Z 
2025-04-11T04:23:18.6797158Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6797261Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6797380Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6797516Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6797630Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6797745Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6797884Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6797990Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6798125Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6798276Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6798361Z     def test_flash_decoding(
2025-04-11T04:23:18.6798435Z         bsz: int,
2025-04-11T04:23:18.6798519Z         block_size: int,
2025-04-11T04:23:18.6798607Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6798689Z         num_attn_heads: int,
2025-04-11T04:23:18.6798771Z         kv_group_num: int,
2025-04-11T04:23:18.6798859Z         same_context_len: bool,
2025-04-11T04:23:18.6798936Z         q_len: int,
2025-04-11T04:23:18.6799019Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6799108Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6799179Z     ):
2025-04-11T04:23:18.6799387Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6799585Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6799768Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6799943Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6800104Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6800261Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6800332Z     
2025-04-11T04:23:18.6800420Z         torch.manual_seed(123)
2025-04-11T04:23:18.6800507Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6800598Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6800601Z 
2025-04-11T04:23:18.6800759Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6800872Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6800876Z 
2025-04-11T04:23:18.6800957Z device = None
2025-04-11T04:23:18.6801042Z 
2025-04-11T04:23:18.6801158Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6801310Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6801380Z     
2025-04-11T04:23:18.6801452Z         Args:
2025-04-11T04:23:18.6801620Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6801784Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6801890Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6801961Z         """
2025-04-11T04:23:18.6802038Z         _lazy_init()
2025-04-11T04:23:18.6802133Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6802232Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6802343Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6802630Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6802771Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6802929Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6802933Z 
2025-04-11T04:23:18.6803176Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6803343Z _____________ test_flash_decoding[False-True-1-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.6803347Z 
2025-04-11T04:23:18.6803498Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6803664Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6803751Z use_new_kcache_layout = False
2025-04-11T04:23:18.6803759Z 
2025-04-11T04:23:18.6803963Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6804066Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6804187Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6804325Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6804438Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6804552Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6804689Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6804794Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6804929Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6805084Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6805169Z     def test_flash_decoding(
2025-04-11T04:23:18.6805243Z         bsz: int,
2025-04-11T04:23:18.6805418Z         block_size: int,
2025-04-11T04:23:18.6805508Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6805592Z         num_attn_heads: int,
2025-04-11T04:23:18.6805680Z         kv_group_num: int,
2025-04-11T04:23:18.6805764Z         same_context_len: bool,
2025-04-11T04:23:18.6805844Z         q_len: int,
2025-04-11T04:23:18.6805928Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6806018Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6806091Z     ):
2025-04-11T04:23:18.6806203Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6806394Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6806571Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6806742Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6806904Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6807064Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6807137Z     
2025-04-11T04:23:18.6807314Z         torch.manual_seed(123)
2025-04-11T04:23:18.6807404Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6807496Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6807500Z 
2025-04-11T04:23:18.6807658Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6807769Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6807774Z 
2025-04-11T04:23:18.6807854Z device = None
2025-04-11T04:23:18.6807858Z 
2025-04-11T04:23:18.6807974Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6808127Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6808198Z     
2025-04-11T04:23:18.6808271Z         Args:
2025-04-11T04:23:18.6808443Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6808612Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6808722Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6808795Z         """
2025-04-11T04:23:18.6808878Z         _lazy_init()
2025-04-11T04:23:18.6808971Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6809071Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6809181Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6809468Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6809606Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6809762Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6809766Z 
2025-04-11T04:23:18.6810011Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6810185Z ____________ test_flash_decoding[False-True-1-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.6810192Z 
2025-04-11T04:23:18.6810344Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6810511Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6810597Z use_new_kcache_layout = False
2025-04-11T04:23:18.6810602Z 
2025-04-11T04:23:18.6810804Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6810906Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6811022Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6811159Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6811275Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6811386Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6811606Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6811710Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6811851Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6812005Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6812090Z     def test_flash_decoding(
2025-04-11T04:23:18.6812164Z         bsz: int,
2025-04-11T04:23:18.6812248Z         block_size: int,
2025-04-11T04:23:18.6812335Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6812417Z         num_attn_heads: int,
2025-04-11T04:23:18.6812498Z         kv_group_num: int,
2025-04-11T04:23:18.6812585Z         same_context_len: bool,
2025-04-11T04:23:18.6812660Z         q_len: int,
2025-04-11T04:23:18.6812743Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6812834Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6812907Z     ):
2025-04-11T04:23:18.6813024Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6813214Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6813493Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6813664Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6813825Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6813984Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6814056Z     
2025-04-11T04:23:18.6814147Z         torch.manual_seed(123)
2025-04-11T04:23:18.6814233Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6814322Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6814326Z 
2025-04-11T04:23:18.6814483Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6814596Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6814600Z 
2025-04-11T04:23:18.6814683Z device = None
2025-04-11T04:23:18.6814691Z 
2025-04-11T04:23:18.6814804Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6814954Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6815025Z     
2025-04-11T04:23:18.6815097Z         Args:
2025-04-11T04:23:18.6815265Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6815428Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6815534Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6815606Z         """
2025-04-11T04:23:18.6815685Z         _lazy_init()
2025-04-11T04:23:18.6815779Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6815878Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6815990Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6816273Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6816418Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6816572Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6816577Z 
2025-04-11T04:23:18.6816815Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6816979Z _____________ test_flash_decoding[False-True-1-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.6816984Z 
2025-04-11T04:23:18.6817135Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6817295Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6817381Z use_new_kcache_layout = False
2025-04-11T04:23:18.6817386Z 
2025-04-11T04:23:18.6817674Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6817777Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6817900Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6818038Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6818155Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6818266Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6818399Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6818507Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6818639Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6818790Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6818875Z     def test_flash_decoding(
2025-04-11T04:23:18.6818953Z         bsz: int,
2025-04-11T04:23:18.6819037Z         block_size: int,
2025-04-11T04:23:18.6819126Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6819211Z         num_attn_heads: int,
2025-04-11T04:23:18.6819379Z         kv_group_num: int,
2025-04-11T04:23:18.6819466Z         same_context_len: bool,
2025-04-11T04:23:18.6819542Z         q_len: int,
2025-04-11T04:23:18.6819624Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6819713Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6819785Z     ):
2025-04-11T04:23:18.6819898Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6820087Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6820271Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6820446Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6820608Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6820770Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6820842Z     
2025-04-11T04:23:18.6820934Z         torch.manual_seed(123)
2025-04-11T04:23:18.6821021Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6821111Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6821115Z 
2025-04-11T04:23:18.6821272Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6821382Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6821386Z 
2025-04-11T04:23:18.6821465Z device = None
2025-04-11T04:23:18.6821469Z 
2025-04-11T04:23:18.6821584Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6821734Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6821804Z     
2025-04-11T04:23:18.6821879Z         Args:
2025-04-11T04:23:18.6822047Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6822217Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6822327Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6822400Z         """
2025-04-11T04:23:18.6822481Z         _lazy_init()
2025-04-11T04:23:18.6822575Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6822674Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6822780Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6823063Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6823201Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6823357Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6823362Z 
2025-04-11T04:23:18.6823605Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6823862Z ____________ test_flash_decoding[False-True-1-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.6823869Z 
2025-04-11T04:23:18.6824025Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6824185Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6824271Z use_new_kcache_layout = False
2025-04-11T04:23:18.6824276Z 
2025-04-11T04:23:18.6824477Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6824580Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6824698Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6824834Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6824951Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6825060Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6825196Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6825300Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6825517Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6825672Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6825756Z     def test_flash_decoding(
2025-04-11T04:23:18.6825834Z         bsz: int,
2025-04-11T04:23:18.6825914Z         block_size: int,
2025-04-11T04:23:18.6826001Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6826086Z         num_attn_heads: int,
2025-04-11T04:23:18.6826168Z         kv_group_num: int,
2025-04-11T04:23:18.6826255Z         same_context_len: bool,
2025-04-11T04:23:18.6826330Z         q_len: int,
2025-04-11T04:23:18.6826414Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6826506Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6826578Z     ):
2025-04-11T04:23:18.6826687Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6826884Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6827064Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6827233Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6827392Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6827552Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6827624Z     
2025-04-11T04:23:18.6827712Z         torch.manual_seed(123)
2025-04-11T04:23:18.6827799Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6827887Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6827895Z 
2025-04-11T04:23:18.6828047Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6828156Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6828163Z 
2025-04-11T04:23:18.6828241Z device = None
2025-04-11T04:23:18.6828249Z 
2025-04-11T04:23:18.6828365Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6828555Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6828627Z     
2025-04-11T04:23:18.6828703Z         Args:
2025-04-11T04:23:18.6828866Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6829026Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6829133Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6829205Z         """
2025-04-11T04:23:18.6829285Z         _lazy_init()
2025-04-11T04:23:18.6829378Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6829477Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6829683Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6829968Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6830112Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6830269Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6830274Z 
2025-04-11T04:23:18.6830513Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6830681Z _____________ test_flash_decoding[False-True-1-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.6830685Z 
2025-04-11T04:23:18.6830839Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6831001Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6831090Z use_new_kcache_layout = False
2025-04-11T04:23:18.6831098Z 
2025-04-11T04:23:18.6831299Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6831400Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6831619Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6831757Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6831875Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6831986Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6832120Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6832224Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6832355Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6832506Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6832591Z     def test_flash_decoding(
2025-04-11T04:23:18.6832669Z         bsz: int,
2025-04-11T04:23:18.6832749Z         block_size: int,
2025-04-11T04:23:18.6832841Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6832926Z         num_attn_heads: int,
2025-04-11T04:23:18.6833011Z         kv_group_num: int,
2025-04-11T04:23:18.6833100Z         same_context_len: bool,
2025-04-11T04:23:18.6833176Z         q_len: int,
2025-04-11T04:23:18.6833259Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6833349Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6833425Z     ):
2025-04-11T04:23:18.6833538Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6833730Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6833917Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6834086Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6834246Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6834410Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6834484Z     
2025-04-11T04:23:18.6834576Z         torch.manual_seed(123)
2025-04-11T04:23:18.6834663Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6834758Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6834762Z 
2025-04-11T04:23:18.6834915Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6835026Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6835030Z 
2025-04-11T04:23:18.6835110Z device = None
2025-04-11T04:23:18.6835114Z 
2025-04-11T04:23:18.6835229Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6835381Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6835452Z     
2025-04-11T04:23:18.6835526Z         Args:
2025-04-11T04:23:18.6835690Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6836035Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6836147Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6836220Z         """
2025-04-11T04:23:18.6836303Z         _lazy_init()
2025-04-11T04:23:18.6836398Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6836498Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6836603Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6836890Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6837030Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6837186Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6837189Z 
2025-04-11T04:23:18.6837437Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6837605Z _____________ test_flash_decoding[False-True-1-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.6837707Z 
2025-04-11T04:23:18.6837864Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6838027Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6838113Z use_new_kcache_layout = False
2025-04-11T04:23:18.6838121Z 
2025-04-11T04:23:18.6838319Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6838423Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6838543Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6838680Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6838797Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6838905Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6839042Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6839145Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6839282Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6839435Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6839518Z     def test_flash_decoding(
2025-04-11T04:23:18.6839598Z         bsz: int,
2025-04-11T04:23:18.6839678Z         block_size: int,
2025-04-11T04:23:18.6839767Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6839853Z         num_attn_heads: int,
2025-04-11T04:23:18.6839935Z         kv_group_num: int,
2025-04-11T04:23:18.6840022Z         same_context_len: bool,
2025-04-11T04:23:18.6840097Z         q_len: int,
2025-04-11T04:23:18.6840180Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6840271Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6840342Z     ):
2025-04-11T04:23:18.6840453Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6840647Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6840831Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6840997Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6841158Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6841316Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6841386Z     
2025-04-11T04:23:18.6841473Z         torch.manual_seed(123)
2025-04-11T04:23:18.6841559Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6841652Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6841655Z 
2025-04-11T04:23:18.6841807Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6841917Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6842009Z 
2025-04-11T04:23:18.6842089Z device = None
2025-04-11T04:23:18.6842093Z 
2025-04-11T04:23:18.6842211Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6842362Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6842433Z     
2025-04-11T04:23:18.6842508Z         Args:
2025-04-11T04:23:18.6842670Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6842830Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6842937Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6843010Z         """
2025-04-11T04:23:18.6843090Z         _lazy_init()
2025-04-11T04:23:18.6843182Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6843285Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6843390Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6843668Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6843892Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6844049Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6844054Z 
2025-04-11T04:23:18.6844292Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6844457Z _____________ test_flash_decoding[False-True-1-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.6844462Z 
2025-04-11T04:23:18.6844611Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6844772Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6844862Z use_new_kcache_layout = False
2025-04-11T04:23:18.6844867Z 
2025-04-11T04:23:18.6845068Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6845169Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6845299Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6845438Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6845556Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6845666Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6845801Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6845900Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6846033Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6846186Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6846271Z     def test_flash_decoding(
2025-04-11T04:23:18.6846349Z         bsz: int,
2025-04-11T04:23:18.6846430Z         block_size: int,
2025-04-11T04:23:18.6846524Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6846611Z         num_attn_heads: int,
2025-04-11T04:23:18.6846692Z         kv_group_num: int,
2025-04-11T04:23:18.6846785Z         same_context_len: bool,
2025-04-11T04:23:18.6846858Z         q_len: int,
2025-04-11T04:23:18.6846948Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6847036Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6847109Z     ):
2025-04-11T04:23:18.6847225Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6847420Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6847604Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6847776Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6847938Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6848189Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6848263Z     
2025-04-11T04:23:18.6848360Z         torch.manual_seed(123)
2025-04-11T04:23:18.6848449Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6848543Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6848547Z 
2025-04-11T04:23:18.6848702Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6848813Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6848820Z 
2025-04-11T04:23:18.6848896Z device = None
2025-04-11T04:23:18.6848901Z 
2025-04-11T04:23:18.6849017Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6849167Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6849237Z     
2025-04-11T04:23:18.6849313Z         Args:
2025-04-11T04:23:18.6849478Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6849643Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6849838Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6849916Z         """
2025-04-11T04:23:18.6850003Z         _lazy_init()
2025-04-11T04:23:18.6850101Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6850205Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6850306Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6850593Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6850733Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6850889Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6850894Z 
2025-04-11T04:23:18.6851140Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6851310Z _____________ test_flash_decoding[False-True-1-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.6851317Z 
2025-04-11T04:23:18.6851470Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6851636Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6851725Z use_new_kcache_layout = False
2025-04-11T04:23:18.6851729Z 
2025-04-11T04:23:18.6851929Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6852036Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6852156Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6852294Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6852413Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6852524Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6852668Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6852771Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6852911Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6853066Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6853151Z     def test_flash_decoding(
2025-04-11T04:23:18.6853232Z         bsz: int,
2025-04-11T04:23:18.6853315Z         block_size: int,
2025-04-11T04:23:18.6853405Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6853494Z         num_attn_heads: int,
2025-04-11T04:23:18.6853574Z         kv_group_num: int,
2025-04-11T04:23:18.6853663Z         same_context_len: bool,
2025-04-11T04:23:18.6853738Z         q_len: int,
2025-04-11T04:23:18.6853825Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6853913Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6853984Z     ):
2025-04-11T04:23:18.6854099Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6854377Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6854565Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6854732Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6854898Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6855053Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6855127Z     
2025-04-11T04:23:18.6855219Z         torch.manual_seed(123)
2025-04-11T04:23:18.6855306Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6855399Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6855403Z 
2025-04-11T04:23:18.6855560Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6855671Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6855680Z 
2025-04-11T04:23:18.6855757Z device = None
2025-04-11T04:23:18.6855761Z 
2025-04-11T04:23:18.6855963Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6856117Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6856188Z     
2025-04-11T04:23:18.6856264Z         Args:
2025-04-11T04:23:18.6856431Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6856602Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6856706Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6856777Z         """
2025-04-11T04:23:18.6856858Z         _lazy_init()
2025-04-11T04:23:18.6856951Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6857053Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6857155Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6857443Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6857584Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6857742Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6857746Z 
2025-04-11T04:23:18.6857989Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6858158Z _____________ test_flash_decoding[False-True-1-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.6858162Z 
2025-04-11T04:23:18.6858316Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6858478Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6858567Z use_new_kcache_layout = False
2025-04-11T04:23:18.6858572Z 
2025-04-11T04:23:18.6858774Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6858877Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6859000Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6859139Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6859257Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6859370Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6859508Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6859612Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6859745Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6859899Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6859985Z     def test_flash_decoding(
2025-04-11T04:23:18.6860063Z         bsz: int,
2025-04-11T04:23:18.6860142Z         block_size: int,
2025-04-11T04:23:18.6860355Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6860436Z         num_attn_heads: int,
2025-04-11T04:23:18.6860516Z         kv_group_num: int,
2025-04-11T04:23:18.6860609Z         same_context_len: bool,
2025-04-11T04:23:18.6860686Z         q_len: int,
2025-04-11T04:23:18.6860774Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6860861Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6860932Z     ):
2025-04-11T04:23:18.6861044Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6861237Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6861422Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6861591Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6861758Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6861923Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6861997Z     
2025-04-11T04:23:18.6862182Z         torch.manual_seed(123)
2025-04-11T04:23:18.6862276Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6862371Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6862375Z 
2025-04-11T04:23:18.6862527Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6862643Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6862647Z 
2025-04-11T04:23:18.6862724Z device = None
2025-04-11T04:23:18.6862728Z 
2025-04-11T04:23:18.6862842Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6862991Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6863063Z     
2025-04-11T04:23:18.6863142Z         Args:
2025-04-11T04:23:18.6863306Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6863479Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6863586Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6863658Z         """
2025-04-11T04:23:18.6863738Z         _lazy_init()
2025-04-11T04:23:18.6863832Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6863940Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6864046Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6864328Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6864472Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6864627Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6864631Z 
2025-04-11T04:23:18.6864873Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6865046Z ____________ test_flash_decoding[False-True-1-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.6865053Z 
2025-04-11T04:23:18.6865209Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6865370Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6865461Z use_new_kcache_layout = False
2025-04-11T04:23:18.6865465Z 
2025-04-11T04:23:18.6865661Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6865766Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6865883Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6866020Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6866137Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6866246Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6866473Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6866573Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6866708Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6866864Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6866947Z     def test_flash_decoding(
2025-04-11T04:23:18.6867027Z         bsz: int,
2025-04-11T04:23:18.6867108Z         block_size: int,
2025-04-11T04:23:18.6867199Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6867280Z         num_attn_heads: int,
2025-04-11T04:23:18.6867360Z         kv_group_num: int,
2025-04-11T04:23:18.6867447Z         same_context_len: bool,
2025-04-11T04:23:18.6867520Z         q_len: int,
2025-04-11T04:23:18.6867607Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6867693Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6867764Z     ):
2025-04-11T04:23:18.6867883Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6868080Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6868350Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6868546Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6868716Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6868871Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6868942Z     
2025-04-11T04:23:18.6869030Z         torch.manual_seed(123)
2025-04-11T04:23:18.6869117Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6869214Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6869218Z 
2025-04-11T04:23:18.6869370Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6869488Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6869494Z 
2025-04-11T04:23:18.6869570Z device = None
2025-04-11T04:23:18.6869574Z 
2025-04-11T04:23:18.6869689Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6869841Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6869912Z     
2025-04-11T04:23:18.6869988Z         Args:
2025-04-11T04:23:18.6870151Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6870319Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6870421Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6870493Z         """
2025-04-11T04:23:18.6870571Z         _lazy_init()
2025-04-11T04:23:18.6870666Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6870770Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6870872Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6871164Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6871301Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6871458Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6871462Z 
2025-04-11T04:23:18.6871705Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6871870Z _____________ test_flash_decoding[False-True-1-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.6871874Z 
2025-04-11T04:23:18.6872028Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6872190Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6872280Z use_new_kcache_layout = False
2025-04-11T04:23:18.6872284Z 
2025-04-11T04:23:18.6872584Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6872689Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6872807Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6872944Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6873063Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6873174Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6873311Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6873411Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6873545Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6873694Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6873778Z     def test_flash_decoding(
2025-04-11T04:23:18.6873856Z         bsz: int,
2025-04-11T04:23:18.6873937Z         block_size: int,
2025-04-11T04:23:18.6874032Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6874111Z         num_attn_heads: int,
2025-04-11T04:23:18.6874192Z         kv_group_num: int,
2025-04-11T04:23:18.6874373Z         same_context_len: bool,
2025-04-11T04:23:18.6874448Z         q_len: int,
2025-04-11T04:23:18.6874535Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6874621Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6874694Z     ):
2025-04-11T04:23:18.6874811Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6875001Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6875185Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6875355Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6875519Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6875680Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6875753Z     
2025-04-11T04:23:18.6875844Z         torch.manual_seed(123)
2025-04-11T04:23:18.6875932Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6876023Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6876028Z 
2025-04-11T04:23:18.6876181Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6876295Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6876299Z 
2025-04-11T04:23:18.6876375Z device = None
2025-04-11T04:23:18.6876380Z 
2025-04-11T04:23:18.6876497Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6876645Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6876715Z     
2025-04-11T04:23:18.6876794Z         Args:
2025-04-11T04:23:18.6876958Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6877129Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6877233Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6877308Z         """
2025-04-11T04:23:18.6877389Z         _lazy_init()
2025-04-11T04:23:18.6877481Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6877584Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6877686Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6877973Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6878105Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6878260Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6878269Z 
2025-04-11T04:23:18.6878504Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6878768Z ____________ test_flash_decoding[False-True-1-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.6878776Z 
2025-04-11T04:23:18.6878935Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6879096Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6879186Z use_new_kcache_layout = False
2025-04-11T04:23:18.6879191Z 
2025-04-11T04:23:18.6879387Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6879496Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6879610Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6879745Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6879862Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6879971Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6880114Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6880214Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6880435Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6880590Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6880673Z     def test_flash_decoding(
2025-04-11T04:23:18.6880755Z         bsz: int,
2025-04-11T04:23:18.6880834Z         block_size: int,
2025-04-11T04:23:18.6880928Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6881010Z         num_attn_heads: int,
2025-04-11T04:23:18.6881090Z         kv_group_num: int,
2025-04-11T04:23:18.6881177Z         same_context_len: bool,
2025-04-11T04:23:18.6881250Z         q_len: int,
2025-04-11T04:23:18.6881336Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6881425Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6881501Z     ):
2025-04-11T04:23:18.6881610Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6881805Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6881991Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6882158Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6882324Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6882478Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6882552Z     
2025-04-11T04:23:18.6882637Z         torch.manual_seed(123)
2025-04-11T04:23:18.6882725Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6882819Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6882823Z 
2025-04-11T04:23:18.6882974Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6883088Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6883095Z 
2025-04-11T04:23:18.6883171Z device = None
2025-04-11T04:23:18.6883175Z 
2025-04-11T04:23:18.6883293Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6883442Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6883513Z     
2025-04-11T04:23:18.6883587Z         Args:
2025-04-11T04:23:18.6883752Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6883917Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6884019Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6884094Z         """
2025-04-11T04:23:18.6884170Z         _lazy_init()
2025-04-11T04:23:18.6884262Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6884363Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6884466Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6884856Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6884994Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6885152Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6885162Z 
2025-04-11T04:23:18.6885401Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6885568Z ______________ test_flash_decoding[False-True-5-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6885572Z 
2025-04-11T04:23:18.6885727Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6885886Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6885978Z use_new_kcache_layout = False
2025-04-11T04:23:18.6885982Z 
2025-04-11T04:23:18.6886185Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6886292Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6886493Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6886631Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6886753Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6886866Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6887004Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6887105Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6887242Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6887392Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6887476Z     def test_flash_decoding(
2025-04-11T04:23:18.6887554Z         bsz: int,
2025-04-11T04:23:18.6887636Z         block_size: int,
2025-04-11T04:23:18.6887734Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6887815Z         num_attn_heads: int,
2025-04-11T04:23:18.6887901Z         kv_group_num: int,
2025-04-11T04:23:18.6887991Z         same_context_len: bool,
2025-04-11T04:23:18.6888067Z         q_len: int,
2025-04-11T04:23:18.6888155Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6888242Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6888318Z     ):
2025-04-11T04:23:18.6888429Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6888619Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6888800Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6888966Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6889130Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6889289Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6889365Z     
2025-04-11T04:23:18.6889451Z         torch.manual_seed(123)
2025-04-11T04:23:18.6889542Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6889638Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6889642Z 
2025-04-11T04:23:18.6889794Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6889906Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6889910Z 
2025-04-11T04:23:18.6889986Z device = None
2025-04-11T04:23:18.6889991Z 
2025-04-11T04:23:18.6890112Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6890267Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6890339Z     
2025-04-11T04:23:18.6890421Z         Args:
2025-04-11T04:23:18.6890586Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6890849Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6890958Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6891038Z         """
2025-04-11T04:23:18.6891115Z         _lazy_init()
2025-04-11T04:23:18.6891208Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6891312Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6891415Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6891703Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6891840Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6892002Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6892006Z 
2025-04-11T04:23:18.6892244Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6892415Z _____________ test_flash_decoding[False-True-5-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6892505Z 
2025-04-11T04:23:18.6892656Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6892816Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6892907Z use_new_kcache_layout = False
2025-04-11T04:23:18.6892911Z 
2025-04-11T04:23:18.6893107Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6893215Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6893330Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6893468Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6893582Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6893691Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6893832Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6893931Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6894072Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6894220Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6894302Z     def test_flash_decoding(
2025-04-11T04:23:18.6894382Z         bsz: int,
2025-04-11T04:23:18.6894464Z         block_size: int,
2025-04-11T04:23:18.6894557Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6894637Z         num_attn_heads: int,
2025-04-11T04:23:18.6894721Z         kv_group_num: int,
2025-04-11T04:23:18.6894807Z         same_context_len: bool,
2025-04-11T04:23:18.6894883Z         q_len: int,
2025-04-11T04:23:18.6894977Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6895069Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6895148Z     ):
2025-04-11T04:23:18.6895259Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6895453Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6895633Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6895805Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6895972Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6896130Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6896204Z     
2025-04-11T04:23:18.6896290Z         torch.manual_seed(123)
2025-04-11T04:23:18.6896381Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6896481Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6896485Z 
2025-04-11T04:23:18.6896636Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6896754Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6896847Z 
2025-04-11T04:23:18.6896931Z device = None
2025-04-11T04:23:18.6896935Z 
2025-04-11T04:23:18.6897061Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6897214Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6897291Z     
2025-04-11T04:23:18.6897362Z         Args:
2025-04-11T04:23:18.6897528Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6897693Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6897798Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6897872Z         """
2025-04-11T04:23:18.6897949Z         _lazy_init()
2025-04-11T04:23:18.6898040Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6898143Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6898245Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6898536Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6898763Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6898925Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6898930Z 
2025-04-11T04:23:18.6899169Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6899332Z ______________ test_flash_decoding[False-True-5-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6899342Z 
2025-04-11T04:23:18.6899490Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6899652Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6899746Z use_new_kcache_layout = False
2025-04-11T04:23:18.6899750Z 
2025-04-11T04:23:18.6899948Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6900057Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6900173Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6900313Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6900428Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6900538Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6900674Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6900773Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6900909Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6901058Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6901146Z     def test_flash_decoding(
2025-04-11T04:23:18.6901220Z         bsz: int,
2025-04-11T04:23:18.6901301Z         block_size: int,
2025-04-11T04:23:18.6901399Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6901479Z         num_attn_heads: int,
2025-04-11T04:23:18.6901563Z         kv_group_num: int,
2025-04-11T04:23:18.6901650Z         same_context_len: bool,
2025-04-11T04:23:18.6901724Z         q_len: int,
2025-04-11T04:23:18.6901812Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6901897Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6901971Z     ):
2025-04-11T04:23:18.6902080Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6902273Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6902461Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6902629Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6902796Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6903047Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6903123Z     
2025-04-11T04:23:18.6903208Z         torch.manual_seed(123)
2025-04-11T04:23:18.6903302Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6903401Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6903405Z 
2025-04-11T04:23:18.6903563Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6903680Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6903684Z 
2025-04-11T04:23:18.6903763Z device = None
2025-04-11T04:23:18.6903768Z 
2025-04-11T04:23:18.6903890Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6904036Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6904111Z     
2025-04-11T04:23:18.6904183Z         Args:
2025-04-11T04:23:18.6904346Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6904519Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6904622Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6904799Z         """
2025-04-11T04:23:18.6904880Z         _lazy_init()
2025-04-11T04:23:18.6904973Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6905077Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6905179Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6905464Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6905596Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6905754Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6905759Z 
2025-04-11T04:23:18.6905995Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6906168Z _____________ test_flash_decoding[False-True-5-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6906172Z 
2025-04-11T04:23:18.6906325Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6906485Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6906576Z use_new_kcache_layout = False
2025-04-11T04:23:18.6906581Z 
2025-04-11T04:23:18.6906778Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6906887Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6907004Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6907149Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6907264Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6907377Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6907522Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6907623Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6907763Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6907914Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6908000Z     def test_flash_decoding(
2025-04-11T04:23:18.6908076Z         bsz: int,
2025-04-11T04:23:18.6908156Z         block_size: int,
2025-04-11T04:23:18.6908247Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6908327Z         num_attn_heads: int,
2025-04-11T04:23:18.6908446Z         kv_group_num: int,
2025-04-11T04:23:18.6908535Z         same_context_len: bool,
2025-04-11T04:23:18.6908608Z         q_len: int,
2025-04-11T04:23:18.6908697Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6908784Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6908860Z     ):
2025-04-11T04:23:18.6908970Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6909259Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6909441Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6909613Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6909782Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6909940Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6910019Z     
2025-04-11T04:23:18.6910108Z         torch.manual_seed(123)
2025-04-11T04:23:18.6910195Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6910293Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6910297Z 
2025-04-11T04:23:18.6910453Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6910568Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6910575Z 
2025-04-11T04:23:18.6910651Z device = None
2025-04-11T04:23:18.6910656Z 
2025-04-11T04:23:18.6910773Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6911014Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6911091Z     
2025-04-11T04:23:18.6911165Z         Args:
2025-04-11T04:23:18.6911326Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6911493Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6911602Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6911676Z         """
2025-04-11T04:23:18.6911754Z         _lazy_init()
2025-04-11T04:23:18.6911848Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6911951Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6912053Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6912347Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6912484Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6912643Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6912647Z 
2025-04-11T04:23:18.6912884Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6913050Z _____________ test_flash_decoding[False-True-5-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6913055Z 
2025-04-11T04:23:18.6913202Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6913363Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6913451Z use_new_kcache_layout = False
2025-04-11T04:23:18.6913456Z 
2025-04-11T04:23:18.6913651Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6913763Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6913880Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6914019Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6914134Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6914246Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6914382Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6914482Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6914621Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6914769Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6914856Z     def test_flash_decoding(
2025-04-11T04:23:18.6914932Z         bsz: int,
2025-04-11T04:23:18.6915012Z         block_size: int,
2025-04-11T04:23:18.6915196Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6915278Z         num_attn_heads: int,
2025-04-11T04:23:18.6915368Z         kv_group_num: int,
2025-04-11T04:23:18.6915460Z         same_context_len: bool,
2025-04-11T04:23:18.6915534Z         q_len: int,
2025-04-11T04:23:18.6915624Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6915713Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6915790Z     ):
2025-04-11T04:23:18.6915897Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6916096Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6916276Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6916443Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6916613Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6916772Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6916846Z     
2025-04-11T04:23:18.6916931Z         torch.manual_seed(123)
2025-04-11T04:23:18.6917120Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6917212Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6917216Z 
2025-04-11T04:23:18.6917371Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6917491Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6917495Z 
2025-04-11T04:23:18.6917570Z device = None
2025-04-11T04:23:18.6917575Z 
2025-04-11T04:23:18.6917693Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6917847Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6917927Z     
2025-04-11T04:23:18.6918001Z         Args:
2025-04-11T04:23:18.6918171Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6918342Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6918447Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6918527Z         """
2025-04-11T04:23:18.6918603Z         _lazy_init()
2025-04-11T04:23:18.6918702Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6918801Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6918905Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6919189Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6919322Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6919483Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6919487Z 
2025-04-11T04:23:18.6919724Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6919896Z _____________ test_flash_decoding[False-True-5-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6919900Z 
2025-04-11T04:23:18.6920052Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6920211Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6920302Z use_new_kcache_layout = False
2025-04-11T04:23:18.6920306Z 
2025-04-11T04:23:18.6920501Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6920608Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6920721Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6920859Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6920974Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6921088Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6921314Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6921413Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6921551Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6921706Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6921792Z     def test_flash_decoding(
2025-04-11T04:23:18.6921867Z         bsz: int,
2025-04-11T04:23:18.6921947Z         block_size: int,
2025-04-11T04:23:18.6922043Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6922125Z         num_attn_heads: int,
2025-04-11T04:23:18.6922209Z         kv_group_num: int,
2025-04-11T04:23:18.6922294Z         same_context_len: bool,
2025-04-11T04:23:18.6922371Z         q_len: int,
2025-04-11T04:23:18.6922456Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6922543Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6922621Z     ):
2025-04-11T04:23:18.6922732Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6922933Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6923111Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6923369Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6923537Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6923693Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6923770Z     
2025-04-11T04:23:18.6923856Z         torch.manual_seed(123)
2025-04-11T04:23:18.6923944Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6924034Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6924038Z 
2025-04-11T04:23:18.6924190Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6924304Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6924311Z 
2025-04-11T04:23:18.6924387Z device = None
2025-04-11T04:23:18.6924392Z 
2025-04-11T04:23:18.6924510Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6924662Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6924735Z     
2025-04-11T04:23:18.6924807Z         Args:
2025-04-11T04:23:18.6924971Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6925142Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6925249Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6925328Z         """
2025-04-11T04:23:18.6925404Z         _lazy_init()
2025-04-11T04:23:18.6925502Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6925601Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6925704Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6925991Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6926132Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6926294Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6926298Z 
2025-04-11T04:23:18.6926536Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6926704Z _____________ test_flash_decoding[False-True-5-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6926709Z 
2025-04-11T04:23:18.6926857Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6927020Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6927107Z use_new_kcache_layout = False
2025-04-11T04:23:18.6927111Z 
2025-04-11T04:23:18.6927309Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6927504Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6927618Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6927763Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6927874Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6927988Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6928125Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6928226Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6928368Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6928515Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6928609Z     def test_flash_decoding(
2025-04-11T04:23:18.6928684Z         bsz: int,
2025-04-11T04:23:18.6928763Z         block_size: int,
2025-04-11T04:23:18.6928863Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6928945Z         num_attn_heads: int,
2025-04-11T04:23:18.6929033Z         kv_group_num: int,
2025-04-11T04:23:18.6929312Z         same_context_len: bool,
2025-04-11T04:23:18.6929389Z         q_len: int,
2025-04-11T04:23:18.6929472Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6929559Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6929634Z     ):
2025-04-11T04:23:18.6929744Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6929943Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6930124Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6930293Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6930465Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6930628Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6930704Z     
2025-04-11T04:23:18.6930791Z         torch.manual_seed(123)
2025-04-11T04:23:18.6930891Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6930985Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6930990Z 
2025-04-11T04:23:18.6931145Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6931259Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6931263Z 
2025-04-11T04:23:18.6931339Z device = None
2025-04-11T04:23:18.6931344Z 
2025-04-11T04:23:18.6931466Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6931614Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6931687Z     
2025-04-11T04:23:18.6931759Z         Args:
2025-04-11T04:23:18.6931925Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6932099Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6932202Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6932282Z         """
2025-04-11T04:23:18.6932360Z         _lazy_init()
2025-04-11T04:23:18.6932457Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6932560Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6932665Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6932957Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6933091Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6933255Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6933259Z 
2025-04-11T04:23:18.6933497Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6933756Z _____________ test_flash_decoding[False-True-5-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6933760Z 
2025-04-11T04:23:18.6933915Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6934078Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6934167Z use_new_kcache_layout = False
2025-04-11T04:23:18.6934172Z 
2025-04-11T04:23:18.6934370Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6934475Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6934589Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6934729Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6934843Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6934959Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6935093Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6935196Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6935333Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6935570Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6935658Z     def test_flash_decoding(
2025-04-11T04:23:18.6935734Z         bsz: int,
2025-04-11T04:23:18.6935817Z         block_size: int,
2025-04-11T04:23:18.6935906Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6935991Z         num_attn_heads: int,
2025-04-11T04:23:18.6936081Z         kv_group_num: int,
2025-04-11T04:23:18.6936163Z         same_context_len: bool,
2025-04-11T04:23:18.6936242Z         q_len: int,
2025-04-11T04:23:18.6936326Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6936412Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6936489Z     ):
2025-04-11T04:23:18.6936596Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6936793Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6936972Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6937147Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6937310Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6937464Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6937539Z     
2025-04-11T04:23:18.6937624Z         torch.manual_seed(123)
2025-04-11T04:23:18.6937715Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6937804Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6937808Z 
2025-04-11T04:23:18.6937961Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6938070Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6938077Z 
2025-04-11T04:23:18.6938154Z device = None
2025-04-11T04:23:18.6938157Z 
2025-04-11T04:23:18.6938278Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6938427Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6938507Z     
2025-04-11T04:23:18.6938580Z         Args:
2025-04-11T04:23:18.6938747Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6938912Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6939013Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6939094Z         """
2025-04-11T04:23:18.6939171Z         _lazy_init()
2025-04-11T04:23:18.6939269Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6939369Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6939470Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6939854Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6939992Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6940150Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6940155Z 
2025-04-11T04:23:18.6940397Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6940565Z ______________ test_flash_decoding[False-True-5-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6940570Z 
2025-04-11T04:23:18.6940721Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6940884Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6940969Z use_new_kcache_layout = False
2025-04-11T04:23:18.6940974Z 
2025-04-11T04:23:18.6941170Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6941279Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6941393Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6941617Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6941730Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6941843Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6941976Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6942074Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6942210Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6942356Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6942444Z     def test_flash_decoding(
2025-04-11T04:23:18.6942520Z         bsz: int,
2025-04-11T04:23:18.6942605Z         block_size: int,
2025-04-11T04:23:18.6942691Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6942774Z         num_attn_heads: int,
2025-04-11T04:23:18.6942860Z         kv_group_num: int,
2025-04-11T04:23:18.6942944Z         same_context_len: bool,
2025-04-11T04:23:18.6943027Z         q_len: int,
2025-04-11T04:23:18.6943114Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6943201Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6943279Z     ):
2025-04-11T04:23:18.6943385Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6943580Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6943757Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6943933Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6944093Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6944246Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6944326Z     
2025-04-11T04:23:18.6944411Z         torch.manual_seed(123)
2025-04-11T04:23:18.6944505Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6944595Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6944599Z 
2025-04-11T04:23:18.6944754Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6944865Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6944870Z 
2025-04-11T04:23:18.6944945Z device = None
2025-04-11T04:23:18.6944952Z 
2025-04-11T04:23:18.6945066Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6945213Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6945289Z     
2025-04-11T04:23:18.6945361Z         Args:
2025-04-11T04:23:18.6945527Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6945779Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6945883Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6945964Z         """
2025-04-11T04:23:18.6946042Z         _lazy_init()
2025-04-11T04:23:18.6946138Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6946237Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6946343Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6946624Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6946759Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6946918Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6946922Z 
2025-04-11T04:23:18.6947157Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6947330Z _____________ test_flash_decoding[False-True-5-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6947334Z 
2025-04-11T04:23:18.6947484Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6947733Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6947818Z use_new_kcache_layout = False
2025-04-11T04:23:18.6947823Z 
2025-04-11T04:23:18.6948022Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6948125Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6948240Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6948379Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6948524Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6948639Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6948775Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6948884Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6949019Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6949173Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6949263Z     def test_flash_decoding(
2025-04-11T04:23:18.6949337Z         bsz: int,
2025-04-11T04:23:18.6949422Z         block_size: int,
2025-04-11T04:23:18.6949509Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6949590Z         num_attn_heads: int,
2025-04-11T04:23:18.6949678Z         kv_group_num: int,
2025-04-11T04:23:18.6949762Z         same_context_len: bool,
2025-04-11T04:23:18.6949837Z         q_len: int,
2025-04-11T04:23:18.6949920Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6950005Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6950079Z     ):
2025-04-11T04:23:18.6950188Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6950388Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6950567Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6950743Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6950906Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6951063Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6951138Z     
2025-04-11T04:23:18.6951224Z         torch.manual_seed(123)
2025-04-11T04:23:18.6951313Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6951403Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6951407Z 
2025-04-11T04:23:18.6951561Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6951670Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6951788Z 
2025-04-11T04:23:18.6951867Z device = None
2025-04-11T04:23:18.6951875Z 
2025-04-11T04:23:18.6951989Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6952139Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6952216Z     
2025-04-11T04:23:18.6952289Z         Args:
2025-04-11T04:23:18.6952457Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6952622Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6952725Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6952801Z         """
2025-04-11T04:23:18.6952877Z         _lazy_init()
2025-04-11T04:23:18.6952976Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6953075Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6953182Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6953464Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6953693Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6953854Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6953859Z 
2025-04-11T04:23:18.6954096Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6954262Z ______________ test_flash_decoding[False-True-5-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6954266Z 
2025-04-11T04:23:18.6954414Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6954581Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6954666Z use_new_kcache_layout = False
2025-04-11T04:23:18.6954670Z 
2025-04-11T04:23:18.6954872Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6954980Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6955091Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6955234Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6955347Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6955462Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6955594Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6955698Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6955831Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6955978Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6956070Z     def test_flash_decoding(
2025-04-11T04:23:18.6956146Z         bsz: int,
2025-04-11T04:23:18.6956232Z         block_size: int,
2025-04-11T04:23:18.6956319Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6956402Z         num_attn_heads: int,
2025-04-11T04:23:18.6956488Z         kv_group_num: int,
2025-04-11T04:23:18.6956570Z         same_context_len: bool,
2025-04-11T04:23:18.6956650Z         q_len: int,
2025-04-11T04:23:18.6956734Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6956824Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6956896Z     ):
2025-04-11T04:23:18.6957003Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6957199Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6957378Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6957553Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6957718Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6957875Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6958037Z     
2025-04-11T04:23:18.6958123Z         torch.manual_seed(123)
2025-04-11T04:23:18.6958221Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6958312Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6958317Z 
2025-04-11T04:23:18.6958475Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6958586Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6958591Z 
2025-04-11T04:23:18.6958670Z device = None
2025-04-11T04:23:18.6958674Z 
2025-04-11T04:23:18.6958789Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6958935Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6959011Z     
2025-04-11T04:23:18.6959082Z         Args:
2025-04-11T04:23:18.6959250Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6959419Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6959526Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6959682Z         """
2025-04-11T04:23:18.6959760Z         _lazy_init()
2025-04-11T04:23:18.6959860Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6959959Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6960064Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6960345Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6960479Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6960639Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6960643Z 
2025-04-11T04:23:18.6960880Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6961055Z _____________ test_flash_decoding[False-True-5-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6961059Z 
2025-04-11T04:23:18.6961209Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6961379Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6961467Z use_new_kcache_layout = False
2025-04-11T04:23:18.6961471Z 
2025-04-11T04:23:18.6961672Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6961774Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6961887Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6962027Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6962143Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6962258Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6962390Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6962500Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6962633Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6962787Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6962873Z     def test_flash_decoding(
2025-04-11T04:23:18.6962947Z         bsz: int,
2025-04-11T04:23:18.6963032Z         block_size: int,
2025-04-11T04:23:18.6963120Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6963202Z         num_attn_heads: int,
2025-04-11T04:23:18.6963286Z         kv_group_num: int,
2025-04-11T04:23:18.6963369Z         same_context_len: bool,
2025-04-11T04:23:18.6963445Z         q_len: int,
2025-04-11T04:23:18.6963529Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6963619Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6963692Z     ):
2025-04-11T04:23:18.6963800Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6964092Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6964273Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6964453Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6964617Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6964777Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6964849Z     
2025-04-11T04:23:18.6964933Z         torch.manual_seed(123)
2025-04-11T04:23:18.6965029Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6965120Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6965124Z 
2025-04-11T04:23:18.6965279Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6965389Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6965396Z 
2025-04-11T04:23:18.6965476Z device = None
2025-04-11T04:23:18.6965480Z 
2025-04-11T04:23:18.6965594Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6965826Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6965903Z     
2025-04-11T04:23:18.6965977Z         Args:
2025-04-11T04:23:18.6966143Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6966307Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6966413Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6966487Z         """
2025-04-11T04:23:18.6966562Z         _lazy_init()
2025-04-11T04:23:18.6966660Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6966758Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6966864Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6967147Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6967286Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6967446Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6967451Z 
2025-04-11T04:23:18.6967688Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6967856Z _____________ test_flash_decoding[False-True-5-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6967860Z 
2025-04-11T04:23:18.6968009Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6968173Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6968259Z use_new_kcache_layout = False
2025-04-11T04:23:18.6968264Z 
2025-04-11T04:23:18.6968463Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6968571Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6968689Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6968829Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6968944Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6969059Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6969193Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6969297Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6969432Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6969580Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6969670Z     def test_flash_decoding(
2025-04-11T04:23:18.6969745Z         bsz: int,
2025-04-11T04:23:18.6969827Z         block_size: int,
2025-04-11T04:23:18.6969916Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6970089Z         num_attn_heads: int,
2025-04-11T04:23:18.6970171Z         kv_group_num: int,
2025-04-11T04:23:18.6970255Z         same_context_len: bool,
2025-04-11T04:23:18.6970340Z         q_len: int,
2025-04-11T04:23:18.6970426Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6970518Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6970590Z     ):
2025-04-11T04:23:18.6970702Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6970901Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6971083Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6971262Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6971429Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6971590Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6971666Z     
2025-04-11T04:23:18.6971752Z         torch.manual_seed(123)
2025-04-11T04:23:18.6971928Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6972018Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6972023Z 
2025-04-11T04:23:18.6972180Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6972292Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6972296Z 
2025-04-11T04:23:18.6972375Z device = None
2025-04-11T04:23:18.6972379Z 
2025-04-11T04:23:18.6972493Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6972640Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6972715Z     
2025-04-11T04:23:18.6972788Z         Args:
2025-04-11T04:23:18.6972957Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6973123Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6973230Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6973305Z         """
2025-04-11T04:23:18.6973381Z         _lazy_init()
2025-04-11T04:23:18.6973478Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6973577Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6973681Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6973964Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6974101Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6974257Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6974262Z 
2025-04-11T04:23:18.6974501Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6974678Z _____________ test_flash_decoding[False-True-5-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6974682Z 
2025-04-11T04:23:18.6974835Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6975007Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6975096Z use_new_kcache_layout = False
2025-04-11T04:23:18.6975100Z 
2025-04-11T04:23:18.6975302Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6975404Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6975519Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6975655Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6975769Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6975887Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6976023Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6976227Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6976361Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6976519Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6976601Z     def test_flash_decoding(
2025-04-11T04:23:18.6976677Z         bsz: int,
2025-04-11T04:23:18.6976760Z         block_size: int,
2025-04-11T04:23:18.6976849Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6976932Z         num_attn_heads: int,
2025-04-11T04:23:18.6977013Z         kv_group_num: int,
2025-04-11T04:23:18.6977096Z         same_context_len: bool,
2025-04-11T04:23:18.6977178Z         q_len: int,
2025-04-11T04:23:18.6977260Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6977351Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6977423Z     ):
2025-04-11T04:23:18.6977531Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6977724Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6977908Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6978189Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6978352Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6978513Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6978585Z     
2025-04-11T04:23:18.6978670Z         torch.manual_seed(123)
2025-04-11T04:23:18.6978762Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6978852Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6978856Z 
2025-04-11T04:23:18.6979014Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6979124Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6979132Z 
2025-04-11T04:23:18.6979213Z device = None
2025-04-11T04:23:18.6979217Z 
2025-04-11T04:23:18.6979331Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6979485Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6979556Z     
2025-04-11T04:23:18.6979628Z         Args:
2025-04-11T04:23:18.6979795Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6979960Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6980067Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6980139Z         """
2025-04-11T04:23:18.6980214Z         _lazy_init()
2025-04-11T04:23:18.6980312Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6980411Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6980517Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6980802Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6980937Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6981102Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6981106Z 
2025-04-11T04:23:18.6981342Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6981510Z _____________ test_flash_decoding[False-True-5-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6981514Z 
2025-04-11T04:23:18.6981664Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6981831Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6981916Z use_new_kcache_layout = False
2025-04-11T04:23:18.6981920Z 
2025-04-11T04:23:18.6982121Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6982309Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6982426Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6982564Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6982678Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6982792Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6982927Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6983031Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6983164Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6983317Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6983402Z     def test_flash_decoding(
2025-04-11T04:23:18.6983479Z         bsz: int,
2025-04-11T04:23:18.6983569Z         block_size: int,
2025-04-11T04:23:18.6983657Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6983745Z         num_attn_heads: int,
2025-04-11T04:23:18.6983825Z         kv_group_num: int,
2025-04-11T04:23:18.6983909Z         same_context_len: bool,
2025-04-11T04:23:18.6984073Z         q_len: int,
2025-04-11T04:23:18.6984158Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6984248Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6984320Z     ):
2025-04-11T04:23:18.6984427Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6984618Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6984798Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6984969Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6985131Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6985290Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6985364Z     
2025-04-11T04:23:18.6985449Z         torch.manual_seed(123)
2025-04-11T04:23:18.6985541Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6985636Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6985640Z 
2025-04-11T04:23:18.6985795Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6985904Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6985908Z 
2025-04-11T04:23:18.6985987Z device = None
2025-04-11T04:23:18.6985991Z 
2025-04-11T04:23:18.6986104Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6986255Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6986326Z     
2025-04-11T04:23:18.6986398Z         Args:
2025-04-11T04:23:18.6986577Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6986739Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6986854Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6986949Z         """
2025-04-11T04:23:18.6987027Z         _lazy_init()
2025-04-11T04:23:18.6987126Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6987228Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6987337Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6987622Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6987762Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6987920Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6987924Z 
2025-04-11T04:23:18.6988168Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6988457Z _____________ test_flash_decoding[False-True-5-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6988461Z 
2025-04-11T04:23:18.6988612Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6988782Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6988871Z use_new_kcache_layout = False
2025-04-11T04:23:18.6988875Z 
2025-04-11T04:23:18.6989078Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6989182Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6989304Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6989444Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6989560Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6989675Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6989810Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6989920Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6990054Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6990301Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6990388Z     def test_flash_decoding(
2025-04-11T04:23:18.6990467Z         bsz: int,
2025-04-11T04:23:18.6990561Z         block_size: int,
2025-04-11T04:23:18.6990654Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6990746Z         num_attn_heads: int,
2025-04-11T04:23:18.6990834Z         kv_group_num: int,
2025-04-11T04:23:18.6990925Z         same_context_len: bool,
2025-04-11T04:23:18.6991009Z         q_len: int,
2025-04-11T04:23:18.6991098Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6991193Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6991272Z     ):
2025-04-11T04:23:18.6991389Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6991585Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6991772Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6991956Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6992122Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6992286Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6992363Z     
2025-04-11T04:23:18.6992458Z         torch.manual_seed(123)
2025-04-11T04:23:18.6992552Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6992648Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6992652Z 
2025-04-11T04:23:18.6992815Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6992931Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6992939Z 
2025-04-11T04:23:18.6993025Z device = None
2025-04-11T04:23:18.6993029Z 
2025-04-11T04:23:18.6993150Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6993308Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6993383Z     
2025-04-11T04:23:18.6993459Z         Args:
2025-04-11T04:23:18.6993632Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6993800Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6993912Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6993988Z         """
2025-04-11T04:23:18.6994074Z         _lazy_init()
2025-04-11T04:23:18.6994175Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6994279Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6994392Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6994772Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6994912Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6995075Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6995080Z 
2025-04-11T04:23:18.6995324Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6995490Z _____________ test_flash_decoding[False-True-5-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6995494Z 
2025-04-11T04:23:18.6995642Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6995813Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6995901Z use_new_kcache_layout = False
2025-04-11T04:23:18.6995906Z 
2025-04-11T04:23:18.6996110Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6996218Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6996338Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6996574Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6996689Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6996809Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6996945Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6997054Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6997189Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6997344Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6997428Z     def test_flash_decoding(
2025-04-11T04:23:18.6997505Z         bsz: int,
2025-04-11T04:23:18.6997590Z         block_size: int,
2025-04-11T04:23:18.6997679Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6997771Z         num_attn_heads: int,
2025-04-11T04:23:18.6997853Z         kv_group_num: int,
2025-04-11T04:23:18.6997945Z         same_context_len: bool,
2025-04-11T04:23:18.6998026Z         q_len: int,
2025-04-11T04:23:18.6998112Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6998209Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6998283Z     ):
2025-04-11T04:23:18.6998401Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6998596Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6998780Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6998954Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6999120Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6999286Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6999363Z     
2025-04-11T04:23:18.6999454Z         torch.manual_seed(123)
2025-04-11T04:23:18.6999544Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6999639Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6999643Z 
2025-04-11T04:23:18.6999805Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7046257Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7046282Z 
2025-04-11T04:23:18.7046416Z device = None
2025-04-11T04:23:18.7046421Z 
2025-04-11T04:23:18.7046582Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7046762Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7046832Z     
2025-04-11T04:23:18.7046910Z         Args:
2025-04-11T04:23:18.7047104Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7047300Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7047608Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7047690Z         """
2025-04-11T04:23:18.7047773Z         _lazy_init()
2025-04-11T04:23:18.7047872Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7047980Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7048089Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7048388Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7048538Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7048697Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7048702Z 
2025-04-11T04:23:18.7048962Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7049140Z _____________ test_flash_decoding[False-True-5-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.7049145Z 
2025-04-11T04:23:18.7049304Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7049587Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7049685Z use_new_kcache_layout = False
2025-04-11T04:23:18.7049690Z 
2025-04-11T04:23:18.7049898Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7050005Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7050133Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7050274Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7050393Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7050507Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7050647Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7050754Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7050891Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7051054Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7051145Z     def test_flash_decoding(
2025-04-11T04:23:18.7051223Z         bsz: int,
2025-04-11T04:23:18.7051306Z         block_size: int,
2025-04-11T04:23:18.7051398Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7051485Z         num_attn_heads: int,
2025-04-11T04:23:18.7051568Z         kv_group_num: int,
2025-04-11T04:23:18.7051662Z         same_context_len: bool,
2025-04-11T04:23:18.7051737Z         q_len: int,
2025-04-11T04:23:18.7051825Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7051913Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7051982Z     ):
2025-04-11T04:23:18.7052102Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7052305Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7052496Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7052674Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7052837Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7052998Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7053068Z     
2025-04-11T04:23:18.7053160Z         torch.manual_seed(123)
2025-04-11T04:23:18.7053249Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7053342Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7053346Z 
2025-04-11T04:23:18.7053506Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7053620Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7053717Z 
2025-04-11T04:23:18.7053796Z device = None
2025-04-11T04:23:18.7053802Z 
2025-04-11T04:23:18.7053926Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7054087Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7054155Z     
2025-04-11T04:23:18.7054231Z         Args:
2025-04-11T04:23:18.7054405Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7054572Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7054682Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7054755Z         """
2025-04-11T04:23:18.7054836Z         _lazy_init()
2025-04-11T04:23:18.7054931Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7055036Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7055141Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7055430Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7055576Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7055820Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7055826Z 
2025-04-11T04:23:18.7056074Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7056244Z _____________ test_flash_decoding[False-True-5-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.7056248Z 
2025-04-11T04:23:18.7056403Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7056568Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7056660Z use_new_kcache_layout = False
2025-04-11T04:23:18.7056665Z 
2025-04-11T04:23:18.7056863Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7056973Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7057095Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7057235Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7057354Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7057467Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7057605Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7057707Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7057842Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7057999Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7058086Z     def test_flash_decoding(
2025-04-11T04:23:18.7058164Z         bsz: int,
2025-04-11T04:23:18.7058246Z         block_size: int,
2025-04-11T04:23:18.7058336Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7058425Z         num_attn_heads: int,
2025-04-11T04:23:18.7058507Z         kv_group_num: int,
2025-04-11T04:23:18.7058597Z         same_context_len: bool,
2025-04-11T04:23:18.7058674Z         q_len: int,
2025-04-11T04:23:18.7058762Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7058849Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7058919Z     ):
2025-04-11T04:23:18.7059034Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7059231Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7059416Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7059587Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7059754Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7059910Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7060160Z     
2025-04-11T04:23:18.7060247Z         torch.manual_seed(123)
2025-04-11T04:23:18.7060336Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7060438Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7060442Z 
2025-04-11T04:23:18.7060599Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7060714Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7060718Z 
2025-04-11T04:23:18.7060793Z device = None
2025-04-11T04:23:18.7060798Z 
2025-04-11T04:23:18.7060919Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7061068Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7061137Z     
2025-04-11T04:23:18.7061211Z         Args:
2025-04-11T04:23:18.7061380Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7061550Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7061662Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7061827Z         """
2025-04-11T04:23:18.7061904Z         _lazy_init()
2025-04-11T04:23:18.7061998Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7062103Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7062207Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7062503Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7062641Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7062794Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7062803Z 
2025-04-11T04:23:18.7063043Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7063208Z _____________ test_flash_decoding[False-True-5-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.7063217Z 
2025-04-11T04:23:18.7063369Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7063534Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7063625Z use_new_kcache_layout = False
2025-04-11T04:23:18.7063629Z 
2025-04-11T04:23:18.7063827Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7063935Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7064050Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7064186Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7064304Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7064416Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7064555Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7064661Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7064799Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7064955Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7065042Z     def test_flash_decoding(
2025-04-11T04:23:18.7065118Z         bsz: int,
2025-04-11T04:23:18.7065198Z         block_size: int,
2025-04-11T04:23:18.7065288Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7065369Z         num_attn_heads: int,
2025-04-11T04:23:18.7065454Z         kv_group_num: int,
2025-04-11T04:23:18.7065538Z         same_context_len: bool,
2025-04-11T04:23:18.7065613Z         q_len: int,
2025-04-11T04:23:18.7065700Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7065786Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7065858Z     ):
2025-04-11T04:23:18.7065967Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7066158Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7066435Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7066610Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7066776Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7066931Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7067003Z     
2025-04-11T04:23:18.7067088Z         torch.manual_seed(123)
2025-04-11T04:23:18.7067175Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7067268Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7067272Z 
2025-04-11T04:23:18.7067427Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7067540Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7067548Z 
2025-04-11T04:23:18.7067622Z device = None
2025-04-11T04:23:18.7067627Z 
2025-04-11T04:23:18.7067746Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7068002Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7068070Z     
2025-04-11T04:23:18.7068144Z         Args:
2025-04-11T04:23:18.7068312Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7068529Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7068637Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7068711Z         """
2025-04-11T04:23:18.7068787Z         _lazy_init()
2025-04-11T04:23:18.7068880Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7068985Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7069089Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7069375Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7069513Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7069673Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7069678Z 
2025-04-11T04:23:18.7069913Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7070081Z _____________ test_flash_decoding[False-True-5-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.7070090Z 
2025-04-11T04:23:18.7070241Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7070405Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7070496Z use_new_kcache_layout = False
2025-04-11T04:23:18.7070500Z 
2025-04-11T04:23:18.7070699Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7070809Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7070925Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7071068Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7071183Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7071296Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7071435Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7071536Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7071671Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7071823Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7071908Z     def test_flash_decoding(
2025-04-11T04:23:18.7071984Z         bsz: int,
2025-04-11T04:23:18.7072063Z         block_size: int,
2025-04-11T04:23:18.7072154Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7072334Z         num_attn_heads: int,
2025-04-11T04:23:18.7072421Z         kv_group_num: int,
2025-04-11T04:23:18.7072508Z         same_context_len: bool,
2025-04-11T04:23:18.7072588Z         q_len: int,
2025-04-11T04:23:18.7072679Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7072766Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7072840Z     ):
2025-04-11T04:23:18.7072950Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7073140Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7073329Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7073501Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7073668Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7073823Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7073900Z     
2025-04-11T04:23:18.7073987Z         torch.manual_seed(123)
2025-04-11T04:23:18.7074074Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7074262Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7074266Z 
2025-04-11T04:23:18.7074421Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7074535Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7074539Z 
2025-04-11T04:23:18.7074613Z device = None
2025-04-11T04:23:18.7074618Z 
2025-04-11T04:23:18.7074740Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7074888Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7074960Z     
2025-04-11T04:23:18.7075030Z         Args:
2025-04-11T04:23:18.7075196Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7075362Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7075472Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7075548Z         """
2025-04-11T04:23:18.7075624Z         _lazy_init()
2025-04-11T04:23:18.7075717Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7075821Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7075925Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7076208Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7076342Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7076498Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7076502Z 
2025-04-11T04:23:18.7076739Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7076906Z ____________ test_flash_decoding[False-True-5-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.7076916Z 
2025-04-11T04:23:18.7077067Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7077231Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7077324Z use_new_kcache_layout = False
2025-04-11T04:23:18.7077328Z 
2025-04-11T04:23:18.7077525Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7077630Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7077745Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7077882Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7077996Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7078106Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7078242Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7078433Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7078572Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7078726Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7078813Z     def test_flash_decoding(
2025-04-11T04:23:18.7078884Z         bsz: int,
2025-04-11T04:23:18.7078963Z         block_size: int,
2025-04-11T04:23:18.7079055Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7079135Z         num_attn_heads: int,
2025-04-11T04:23:18.7079220Z         kv_group_num: int,
2025-04-11T04:23:18.7079303Z         same_context_len: bool,
2025-04-11T04:23:18.7079378Z         q_len: int,
2025-04-11T04:23:18.7079465Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7079551Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7079623Z     ):
2025-04-11T04:23:18.7079732Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7079922Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7080108Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7080364Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7080530Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7080687Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7080762Z     
2025-04-11T04:23:18.7080851Z         torch.manual_seed(123)
2025-04-11T04:23:18.7080943Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7081040Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7081044Z 
2025-04-11T04:23:18.7081200Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7081319Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7081323Z 
2025-04-11T04:23:18.7081405Z device = None
2025-04-11T04:23:18.7081409Z 
2025-04-11T04:23:18.7081531Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7081687Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7081761Z     
2025-04-11T04:23:18.7081835Z         Args:
2025-04-11T04:23:18.7082004Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7082175Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7082284Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7082361Z         """
2025-04-11T04:23:18.7082441Z         _lazy_init()
2025-04-11T04:23:18.7082537Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7082645Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7082754Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7083044Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7083186Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7083351Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7083355Z 
2025-04-11T04:23:18.7083597Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7083770Z _____________ test_flash_decoding[False-True-5-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.7083774Z 
2025-04-11T04:23:18.7083928Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7084094Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7084189Z use_new_kcache_layout = False
2025-04-11T04:23:18.7084194Z 
2025-04-11T04:23:18.7084395Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7084607Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7084724Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7084869Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7084985Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7085097Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7085236Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7085336Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7085475Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7085623Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7085712Z     def test_flash_decoding(
2025-04-11T04:23:18.7085786Z         bsz: int,
2025-04-11T04:23:18.7085865Z         block_size: int,
2025-04-11T04:23:18.7085956Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7086040Z         num_attn_heads: int,
2025-04-11T04:23:18.7086127Z         kv_group_num: int,
2025-04-11T04:23:18.7086211Z         same_context_len: bool,
2025-04-11T04:23:18.7086371Z         q_len: int,
2025-04-11T04:23:18.7086459Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7086546Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7086618Z     ):
2025-04-11T04:23:18.7086729Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7086924Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7087111Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7087281Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7087448Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7087607Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7087682Z     
2025-04-11T04:23:18.7087766Z         torch.manual_seed(123)
2025-04-11T04:23:18.7087854Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7087945Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7087950Z 
2025-04-11T04:23:18.7088104Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7088218Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7088222Z 
2025-04-11T04:23:18.7088297Z device = None
2025-04-11T04:23:18.7088301Z 
2025-04-11T04:23:18.7088423Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7088573Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7088644Z     
2025-04-11T04:23:18.7088715Z         Args:
2025-04-11T04:23:18.7088881Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7089048Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7089157Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7089231Z         """
2025-04-11T04:23:18.7089313Z         _lazy_init()
2025-04-11T04:23:18.7089408Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7089514Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7089617Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7089904Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7090040Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7090198Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7090203Z 
2025-04-11T04:23:18.7090444Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7090618Z ____________ test_flash_decoding[False-True-5-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.7090721Z 
2025-04-11T04:23:18.7090874Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7091039Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7091129Z use_new_kcache_layout = False
2025-04-11T04:23:18.7091134Z 
2025-04-11T04:23:18.7091329Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7091436Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7091551Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7091691Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7091805Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7091916Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7092054Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7092161Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7092300Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7092534Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7092628Z     def test_flash_decoding(
2025-04-11T04:23:18.7092702Z         bsz: int,
2025-04-11T04:23:18.7092787Z         block_size: int,
2025-04-11T04:23:18.7092876Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7092956Z         num_attn_heads: int,
2025-04-11T04:23:18.7093043Z         kv_group_num: int,
2025-04-11T04:23:18.7093128Z         same_context_len: bool,
2025-04-11T04:23:18.7093207Z         q_len: int,
2025-04-11T04:23:18.7093290Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7093377Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7093450Z     ):
2025-04-11T04:23:18.7093560Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7093751Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7093934Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7094108Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7094269Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7094421Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7094493Z     
2025-04-11T04:23:18.7094578Z         torch.manual_seed(123)
2025-04-11T04:23:18.7094669Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7094759Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7094763Z 
2025-04-11T04:23:18.7094919Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7095031Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7095035Z 
2025-04-11T04:23:18.7095114Z device = None
2025-04-11T04:23:18.7095123Z 
2025-04-11T04:23:18.7095238Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7095388Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7095460Z     
2025-04-11T04:23:18.7095534Z         Args:
2025-04-11T04:23:18.7095701Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7095863Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7095966Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7096041Z         """
2025-04-11T04:23:18.7096118Z         _lazy_init()
2025-04-11T04:23:18.7096213Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7096314Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7096420Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7096701Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7096925Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7097086Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7097090Z 
2025-04-11T04:23:18.7097327Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7097498Z _____________ test_flash_decoding[False-True-5-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.7097502Z 
2025-04-11T04:23:18.7097651Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7097815Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7097902Z use_new_kcache_layout = False
2025-04-11T04:23:18.7097907Z 
2025-04-11T04:23:18.7098104Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7098210Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7098326Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7098548Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7098663Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7098777Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7098911Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7099013Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7099147Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7099294Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7099382Z     def test_flash_decoding(
2025-04-11T04:23:18.7099456Z         bsz: int,
2025-04-11T04:23:18.7099539Z         block_size: int,
2025-04-11T04:23:18.7099627Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7099712Z         num_attn_heads: int,
2025-04-11T04:23:18.7099797Z         kv_group_num: int,
2025-04-11T04:23:18.7099881Z         same_context_len: bool,
2025-04-11T04:23:18.7099961Z         q_len: int,
2025-04-11T04:23:18.7100043Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7100129Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7100202Z     ):
2025-04-11T04:23:18.7100307Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7100499Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7100676Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7100847Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7101009Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7101162Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7101237Z     
2025-04-11T04:23:18.7101322Z         torch.manual_seed(123)
2025-04-11T04:23:18.7101410Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7101501Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7101505Z 
2025-04-11T04:23:18.7101661Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7101771Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7101776Z 
2025-04-11T04:23:18.7101850Z device = None
2025-04-11T04:23:18.7101858Z 
2025-04-11T04:23:18.7101973Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7102119Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7102190Z     
2025-04-11T04:23:18.7102261Z         Args:
2025-04-11T04:23:18.7102427Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7102589Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7102785Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7102863Z         """
2025-04-11T04:23:18.7102942Z         _lazy_init()
2025-04-11T04:23:18.7103039Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7103141Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7103246Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7103526Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7103662Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7103818Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7103822Z 
2025-04-11T04:23:18.7104059Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7104229Z _____________ test_flash_decoding[False-True-5-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.7104236Z 
2025-04-11T04:23:18.7104386Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7104636Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7104723Z use_new_kcache_layout = False
2025-04-11T04:23:18.7104728Z 
2025-04-11T04:23:18.7104927Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7105029Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7105144Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7105283Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7105396Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7105511Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7105644Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7105752Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7105885Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7106036Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7106126Z     def test_flash_decoding(
2025-04-11T04:23:18.7106199Z         bsz: int,
2025-04-11T04:23:18.7106281Z         block_size: int,
2025-04-11T04:23:18.7106367Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7106448Z         num_attn_heads: int,
2025-04-11T04:23:18.7106532Z         kv_group_num: int,
2025-04-11T04:23:18.7106616Z         same_context_len: bool,
2025-04-11T04:23:18.7106693Z         q_len: int,
2025-04-11T04:23:18.7106776Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7106864Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7106932Z     ):
2025-04-11T04:23:18.7107040Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7107233Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7107413Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7107587Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7107748Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7107904Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7107972Z     
2025-04-11T04:23:18.7108054Z         torch.manual_seed(123)
2025-04-11T04:23:18.7108141Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7108227Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7108231Z 
2025-04-11T04:23:18.7108384Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7108541Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7108546Z 
2025-04-11T04:23:18.7108725Z device = None
2025-04-11T04:23:18.7108730Z 
2025-04-11T04:23:18.7108849Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7108997Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7109073Z     
2025-04-11T04:23:18.7109143Z         Args:
2025-04-11T04:23:18.7109317Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7109481Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7109588Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7109660Z         """
2025-04-11T04:23:18.7109733Z         _lazy_init()
2025-04-11T04:23:18.7109828Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7109929Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7110035Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7110318Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7110458Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7110708Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7110713Z 
2025-04-11T04:23:18.7110949Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7111118Z _____________ test_flash_decoding[False-True-5-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.7111122Z 
2025-04-11T04:23:18.7111270Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7111436Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7111523Z use_new_kcache_layout = False
2025-04-11T04:23:18.7111527Z 
2025-04-11T04:23:18.7111727Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7111833Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7111950Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7112094Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7112212Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7112330Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7112468Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7112573Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7112711Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7112860Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7112948Z     def test_flash_decoding(
2025-04-11T04:23:18.7113021Z         bsz: int,
2025-04-11T04:23:18.7113104Z         block_size: int,
2025-04-11T04:23:18.7113192Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7113276Z         num_attn_heads: int,
2025-04-11T04:23:18.7113362Z         kv_group_num: int,
2025-04-11T04:23:18.7113445Z         same_context_len: bool,
2025-04-11T04:23:18.7113523Z         q_len: int,
2025-04-11T04:23:18.7113612Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7113701Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7113771Z     ):
2025-04-11T04:23:18.7113881Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7114078Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7114257Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7114430Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7114592Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7114751Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7114931Z     
2025-04-11T04:23:18.7115016Z         torch.manual_seed(123)
2025-04-11T04:23:18.7115109Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7115204Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7115208Z 
2025-04-11T04:23:18.7115365Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7115476Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7115480Z 
2025-04-11T04:23:18.7115556Z device = None
2025-04-11T04:23:18.7115561Z 
2025-04-11T04:23:18.7115677Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7115823Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7115895Z     
2025-04-11T04:23:18.7115966Z         Args:
2025-04-11T04:23:18.7116135Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7116297Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7116409Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7116480Z         """
2025-04-11T04:23:18.7116648Z         _lazy_init()
2025-04-11T04:23:18.7116744Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7116844Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7116949Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7117229Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7117364Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7117522Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7117526Z 
2025-04-11T04:23:18.7117760Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7117927Z _____________ test_flash_decoding[False-True-5-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.7117934Z 
2025-04-11T04:23:18.7118082Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7118250Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7118338Z use_new_kcache_layout = False
2025-04-11T04:23:18.7118342Z 
2025-04-11T04:23:18.7118539Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7118642Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7118758Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7118895Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7119009Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7119125Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7119259Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7119366Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7119499Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7119651Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7119739Z     def test_flash_decoding(
2025-04-11T04:23:18.7119813Z         bsz: int,
2025-04-11T04:23:18.7119896Z         block_size: int,
2025-04-11T04:23:18.7119984Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7120070Z         num_attn_heads: int,
2025-04-11T04:23:18.7120151Z         kv_group_num: int,
2025-04-11T04:23:18.7120233Z         same_context_len: bool,
2025-04-11T04:23:18.7120312Z         q_len: int,
2025-04-11T04:23:18.7120398Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7120489Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7120557Z     ):
2025-04-11T04:23:18.7120665Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7120860Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7121127Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7121304Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7121466Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7121624Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7121692Z     
2025-04-11T04:23:18.7121774Z         torch.manual_seed(123)
2025-04-11T04:23:18.7121864Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7121951Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7121955Z 
2025-04-11T04:23:18.7122111Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7122221Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7122226Z 
2025-04-11T04:23:18.7122307Z device = None
2025-04-11T04:23:18.7122311Z 
2025-04-11T04:23:18.7122425Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7122572Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7122729Z     
2025-04-11T04:23:18.7122800Z         Args:
2025-04-11T04:23:18.7122971Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7123134Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7123243Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7123313Z         """
2025-04-11T04:23:18.7123387Z         _lazy_init()
2025-04-11T04:23:18.7123483Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7123583Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7123690Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7123977Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7124118Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7124277Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7124282Z 
2025-04-11T04:23:18.7124515Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7124685Z _____________ test_flash_decoding[False-True-5-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.7124689Z 
2025-04-11T04:23:18.7124837Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7125005Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7125094Z use_new_kcache_layout = False
2025-04-11T04:23:18.7125098Z 
2025-04-11T04:23:18.7125299Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7125405Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7125525Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7125665Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7125780Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7125896Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7126030Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7126135Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7126268Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7126421Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7126505Z     def test_flash_decoding(
2025-04-11T04:23:18.7126577Z         bsz: int,
2025-04-11T04:23:18.7126660Z         block_size: int,
2025-04-11T04:23:18.7126748Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7126831Z         num_attn_heads: int,
2025-04-11T04:23:18.7127006Z         kv_group_num: int,
2025-04-11T04:23:18.7127090Z         same_context_len: bool,
2025-04-11T04:23:18.7127170Z         q_len: int,
2025-04-11T04:23:18.7127257Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7127347Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7127417Z     ):
2025-04-11T04:23:18.7127523Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7127718Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7127899Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7128074Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7128237Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7128395Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7128467Z     
2025-04-11T04:23:18.7128550Z         torch.manual_seed(123)
2025-04-11T04:23:18.7128640Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7128810Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7128815Z 
2025-04-11T04:23:18.7128972Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7129082Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7129087Z 
2025-04-11T04:23:18.7129163Z device = None
2025-04-11T04:23:18.7129167Z 
2025-04-11T04:23:18.7129282Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7129433Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7129501Z     
2025-04-11T04:23:18.7129571Z         Args:
2025-04-11T04:23:18.7129739Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7129903Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7130015Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7130086Z         """
2025-04-11T04:23:18.7130164Z         _lazy_init()
2025-04-11T04:23:18.7130259Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7130359Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7130467Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7130753Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7130890Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7131045Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7131050Z 
2025-04-11T04:23:18.7131283Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7131457Z ____________ test_flash_decoding[False-True-5-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.7131465Z 
2025-04-11T04:23:18.7131614Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7131783Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7131871Z use_new_kcache_layout = False
2025-04-11T04:23:18.7131875Z 
2025-04-11T04:23:18.7132076Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7132179Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7132298Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7132433Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7132546Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7132664Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7132797Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7132990Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7133125Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7133280Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7133364Z     def test_flash_decoding(
2025-04-11T04:23:18.7133436Z         bsz: int,
2025-04-11T04:23:18.7133521Z         block_size: int,
2025-04-11T04:23:18.7133608Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7133693Z         num_attn_heads: int,
2025-04-11T04:23:18.7133774Z         kv_group_num: int,
2025-04-11T04:23:18.7133859Z         same_context_len: bool,
2025-04-11T04:23:18.7133938Z         q_len: int,
2025-04-11T04:23:18.7134020Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7134109Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7134178Z     ):
2025-04-11T04:23:18.7134285Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7134478Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7134662Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7134938Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7135098Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7135255Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7135324Z     
2025-04-11T04:23:18.7135407Z         torch.manual_seed(123)
2025-04-11T04:23:18.7135495Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7135582Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7135586Z 
2025-04-11T04:23:18.7135742Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7135850Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7135855Z 
2025-04-11T04:23:18.7135935Z device = None
2025-04-11T04:23:18.7135939Z 
2025-04-11T04:23:18.7136053Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7136203Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7136273Z     
2025-04-11T04:23:18.7136344Z         Args:
2025-04-11T04:23:18.7136510Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7136673Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7136779Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7136850Z         """
2025-04-11T04:23:18.7136925Z         _lazy_init()
2025-04-11T04:23:18.7137020Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7137127Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7137234Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7137517Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7137660Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7137816Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7137820Z 
2025-04-11T04:23:18.7138058Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7138223Z _____________ test_flash_decoding[False-True-5-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.7138227Z 
2025-04-11T04:23:18.7138376Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7138542Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7138628Z use_new_kcache_layout = False
2025-04-11T04:23:18.7138632Z 
2025-04-11T04:23:18.7138831Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7139018Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7139138Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7139276Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7139394Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7139510Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7139644Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7139749Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7139883Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7140039Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7140126Z     def test_flash_decoding(
2025-04-11T04:23:18.7140201Z         bsz: int,
2025-04-11T04:23:18.7140286Z         block_size: int,
2025-04-11T04:23:18.7140373Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7140456Z         num_attn_heads: int,
2025-04-11T04:23:18.7140540Z         kv_group_num: int,
2025-04-11T04:23:18.7140624Z         same_context_len: bool,
2025-04-11T04:23:18.7140702Z         q_len: int,
2025-04-11T04:23:18.7140868Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7140959Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7141030Z     ):
2025-04-11T04:23:18.7141144Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7141336Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7141513Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7141687Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7141850Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7142009Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7142082Z     
2025-04-11T04:23:18.7142169Z         torch.manual_seed(123)
2025-04-11T04:23:18.7142254Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7142346Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7142349Z 
2025-04-11T04:23:18.7142506Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7142617Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7142621Z 
2025-04-11T04:23:18.7142701Z device = None
2025-04-11T04:23:18.7142705Z 
2025-04-11T04:23:18.7142821Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7142973Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7143042Z     
2025-04-11T04:23:18.7143113Z         Args:
2025-04-11T04:23:18.7143283Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7143444Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7143556Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7143627Z         """
2025-04-11T04:23:18.7143710Z         _lazy_init()
2025-04-11T04:23:18.7143802Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7143902Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7144010Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7144287Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7144428Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7144582Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7144586Z 
2025-04-11T04:23:18.7144825Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7144995Z ____________ test_flash_decoding[False-True-5-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.7145107Z 
2025-04-11T04:23:18.7145258Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7145426Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7145514Z use_new_kcache_layout = False
2025-04-11T04:23:18.7145519Z 
2025-04-11T04:23:18.7145719Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7145822Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7145941Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7146079Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7146197Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7146311Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7146445Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7146554Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7146686Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7146838Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7147006Z     def test_flash_decoding(
2025-04-11T04:23:18.7147079Z         bsz: int,
2025-04-11T04:23:18.7147162Z         block_size: int,
2025-04-11T04:23:18.7147251Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7147335Z         num_attn_heads: int,
2025-04-11T04:23:18.7147414Z         kv_group_num: int,
2025-04-11T04:23:18.7147501Z         same_context_len: bool,
2025-04-11T04:23:18.7147575Z         q_len: int,
2025-04-11T04:23:18.7147657Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7147747Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7147816Z     ):
2025-04-11T04:23:18.7147928Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7148118Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7148298Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7148517Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7148681Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7148840Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7148909Z     
2025-04-11T04:23:18.7148994Z         torch.manual_seed(123)
2025-04-11T04:23:18.7149080Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7149168Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7149172Z 
2025-04-11T04:23:18.7149327Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7149438Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7149443Z 
2025-04-11T04:23:18.7149522Z device = None
2025-04-11T04:23:18.7149529Z 
2025-04-11T04:23:18.7149644Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7149797Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7149869Z     
2025-04-11T04:23:18.7149939Z         Args:
2025-04-11T04:23:18.7150109Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7150274Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7150382Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7150455Z         """
2025-04-11T04:23:18.7150534Z         _lazy_init()
2025-04-11T04:23:18.7150625Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7150723Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7150831Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7151114Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7151349Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7151508Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7151513Z 
2025-04-11T04:23:18.7151749Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7151917Z _____________ test_flash_decoding[False-False-1-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.7151921Z 
2025-04-11T04:23:18.7152073Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7152236Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7152323Z use_new_kcache_layout = False
2025-04-11T04:23:18.7152328Z 
2025-04-11T04:23:18.7152528Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7152634Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7152752Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7152886Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7153096Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7153208Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7153345Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7153448Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7153586Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7153737Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7153822Z     def test_flash_decoding(
2025-04-11T04:23:18.7153896Z         bsz: int,
2025-04-11T04:23:18.7153982Z         block_size: int,
2025-04-11T04:23:18.7154070Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7154154Z         num_attn_heads: int,
2025-04-11T04:23:18.7154240Z         kv_group_num: int,
2025-04-11T04:23:18.7154328Z         same_context_len: bool,
2025-04-11T04:23:18.7154401Z         q_len: int,
2025-04-11T04:23:18.7154490Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7154581Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7154651Z     ):
2025-04-11T04:23:18.7154762Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7154956Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7155137Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7155313Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7155476Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7155634Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7155705Z     
2025-04-11T04:23:18.7155792Z         torch.manual_seed(123)
2025-04-11T04:23:18.7155876Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7155972Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7155976Z 
2025-04-11T04:23:18.7156133Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7156242Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7156246Z 
2025-04-11T04:23:18.7156325Z device = None
2025-04-11T04:23:18.7156330Z 
2025-04-11T04:23:18.7156445Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7156596Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7156663Z     
2025-04-11T04:23:18.7156734Z         Args:
2025-04-11T04:23:18.7156906Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7157070Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7157411Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7157483Z         """
2025-04-11T04:23:18.7157567Z         _lazy_init()
2025-04-11T04:23:18.7157659Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7157759Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7157866Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7158149Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7158286Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7158439Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7158443Z 
2025-04-11T04:23:18.7158686Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7158852Z _____________ test_flash_decoding[False-False-1-True-1-16-8-16-16] _____________
2025-04-11T04:23:18.7158859Z 
2025-04-11T04:23:18.7159012Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7159277Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7159365Z use_new_kcache_layout = False
2025-04-11T04:23:18.7159369Z 
2025-04-11T04:23:18.7159571Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7159676Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7159795Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7159932Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7160051Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7160163Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7160297Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7160401Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7160543Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7160696Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7160784Z     def test_flash_decoding(
2025-04-11T04:23:18.7160860Z         bsz: int,
2025-04-11T04:23:18.7160940Z         block_size: int,
2025-04-11T04:23:18.7161029Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7161115Z         num_attn_heads: int,
2025-04-11T04:23:18.7161195Z         kv_group_num: int,
2025-04-11T04:23:18.7161284Z         same_context_len: bool,
2025-04-11T04:23:18.7161359Z         q_len: int,
2025-04-11T04:23:18.7161443Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7161533Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7161601Z     ):
2025-04-11T04:23:18.7161711Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7161902Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7162083Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7162261Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7162423Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7162583Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7162651Z     
2025-04-11T04:23:18.7162740Z         torch.manual_seed(123)
2025-04-11T04:23:18.7162829Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7162918Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7162927Z 
2025-04-11T04:23:18.7163078Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7163188Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7163192Z 
2025-04-11T04:23:18.7163273Z device = None
2025-04-11T04:23:18.7163361Z 
2025-04-11T04:23:18.7163477Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7163630Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7163702Z     
2025-04-11T04:23:18.7163777Z         Args:
2025-04-11T04:23:18.7163943Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7164109Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7164219Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7164291Z         """
2025-04-11T04:23:18.7164367Z         _lazy_init()
2025-04-11T04:23:18.7164460Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7164561Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7164669Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7164953Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7165095Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7165334Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7165340Z 
2025-04-11T04:23:18.7165581Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7165751Z _____________ test_flash_decoding[False-False-1-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.7165755Z 
2025-04-11T04:23:18.7165908Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7166071Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7166159Z use_new_kcache_layout = False
2025-04-11T04:23:18.7166168Z 
2025-04-11T04:23:18.7166366Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7166471Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7166591Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7166729Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7166851Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7166964Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7167097Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7167201Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7167336Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7167489Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7167577Z     def test_flash_decoding(
2025-04-11T04:23:18.7167653Z         bsz: int,
2025-04-11T04:23:18.7167732Z         block_size: int,
2025-04-11T04:23:18.7167821Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7167907Z         num_attn_heads: int,
2025-04-11T04:23:18.7167992Z         kv_group_num: int,
2025-04-11T04:23:18.7168080Z         same_context_len: bool,
2025-04-11T04:23:18.7168155Z         q_len: int,
2025-04-11T04:23:18.7168244Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7168335Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7168404Z     ):
2025-04-11T04:23:18.7168517Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7168709Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7168891Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7169063Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7169226Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7169385Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7169543Z     
2025-04-11T04:23:18.7169634Z         torch.manual_seed(123)
2025-04-11T04:23:18.7169721Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7169816Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7169823Z 
2025-04-11T04:23:18.7169976Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7170086Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7170090Z 
2025-04-11T04:23:18.7170173Z device = None
2025-04-11T04:23:18.7170177Z 
2025-04-11T04:23:18.7170293Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7170446Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7170514Z     
2025-04-11T04:23:18.7170588Z         Args:
2025-04-11T04:23:18.7170753Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7170918Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7171031Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7171102Z         """
2025-04-11T04:23:18.7171270Z         _lazy_init()
2025-04-11T04:23:18.7171363Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7171464Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7171569Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7171851Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7171990Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7172144Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7172148Z 
2025-04-11T04:23:18.7172384Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7172549Z _____________ test_flash_decoding[False-False-1-True-1-16-8-32-16] _____________
2025-04-11T04:23:18.7172556Z 
2025-04-11T04:23:18.7172708Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7172874Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7172960Z use_new_kcache_layout = False
2025-04-11T04:23:18.7172968Z 
2025-04-11T04:23:18.7173164Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7173267Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7173388Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7173523Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7173643Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7173756Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7173892Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7173995Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7174131Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7174287Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7174377Z     def test_flash_decoding(
2025-04-11T04:23:18.7174453Z         bsz: int,
2025-04-11T04:23:18.7174532Z         block_size: int,
2025-04-11T04:23:18.7174618Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7174703Z         num_attn_heads: int,
2025-04-11T04:23:18.7174784Z         kv_group_num: int,
2025-04-11T04:23:18.7174873Z         same_context_len: bool,
2025-04-11T04:23:18.7174949Z         q_len: int,
2025-04-11T04:23:18.7175031Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7175127Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7175195Z     ):
2025-04-11T04:23:18.7175309Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7175496Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7175765Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7175940Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7176106Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7176264Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7176332Z     
2025-04-11T04:23:18.7176419Z         torch.manual_seed(123)
2025-04-11T04:23:18.7176504Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7176595Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7176599Z 
2025-04-11T04:23:18.7176752Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7176863Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7176872Z 
2025-04-11T04:23:18.7176946Z device = None
2025-04-11T04:23:18.7176954Z 
2025-04-11T04:23:18.7177071Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7177227Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7177384Z     
2025-04-11T04:23:18.7177459Z         Args:
2025-04-11T04:23:18.7177625Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7177790Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7177898Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7177969Z         """
2025-04-11T04:23:18.7178047Z         _lazy_init()
2025-04-11T04:23:18.7178140Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7178243Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7178344Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7178626Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7178767Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7178927Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7178931Z 
2025-04-11T04:23:18.7179171Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7179337Z _____________ test_flash_decoding[False-False-1-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.7179341Z 
2025-04-11T04:23:18.7179496Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7179659Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7179752Z use_new_kcache_layout = False
2025-04-11T04:23:18.7179757Z 
2025-04-11T04:23:18.7179956Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7180057Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7180184Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7180321Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7180446Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7180559Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7180697Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7180797Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7180932Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7181088Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7181174Z     def test_flash_decoding(
2025-04-11T04:23:18.7181252Z         bsz: int,
2025-04-11T04:23:18.7181332Z         block_size: int,
2025-04-11T04:23:18.7181420Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7181506Z         num_attn_heads: int,
2025-04-11T04:23:18.7181684Z         kv_group_num: int,
2025-04-11T04:23:18.7181771Z         same_context_len: bool,
2025-04-11T04:23:18.7181846Z         q_len: int,
2025-04-11T04:23:18.7181937Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7182023Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7182093Z     ):
2025-04-11T04:23:18.7182204Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7182392Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7182579Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7182751Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7182911Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7183071Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7183144Z     
2025-04-11T04:23:18.7183232Z         torch.manual_seed(123)
2025-04-11T04:23:18.7183319Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7183411Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7183498Z 
2025-04-11T04:23:18.7183655Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7183766Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7183774Z 
2025-04-11T04:23:18.7183848Z device = None
2025-04-11T04:23:18.7183853Z 
2025-04-11T04:23:18.7183968Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7184123Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7184190Z     
2025-04-11T04:23:18.7184263Z         Args:
2025-04-11T04:23:18.7184428Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7184590Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7184704Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7184776Z         """
2025-04-11T04:23:18.7184855Z         _lazy_init()
2025-04-11T04:23:18.7184952Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7185057Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7185158Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7185442Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7185578Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7185732Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7185737Z 
2025-04-11T04:23:18.7185975Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7186144Z ____________ test_flash_decoding[False-False-1-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.7186152Z 
2025-04-11T04:23:18.7186306Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7186473Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7186563Z use_new_kcache_layout = False
2025-04-11T04:23:18.7186567Z 
2025-04-11T04:23:18.7186765Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7186867Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7186987Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7187125Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7187243Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7187352Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7187489Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7187589Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7187816Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7187970Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7188057Z     def test_flash_decoding(
2025-04-11T04:23:18.7188132Z         bsz: int,
2025-04-11T04:23:18.7188212Z         block_size: int,
2025-04-11T04:23:18.7188297Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7188381Z         num_attn_heads: int,
2025-04-11T04:23:18.7188494Z         kv_group_num: int,
2025-04-11T04:23:18.7188583Z         same_context_len: bool,
2025-04-11T04:23:18.7188657Z         q_len: int,
2025-04-11T04:23:18.7188741Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7188826Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7188895Z     ):
2025-04-11T04:23:18.7189006Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7189195Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7189379Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7189643Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7189809Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7189965Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7190034Z     
2025-04-11T04:23:18.7190123Z         torch.manual_seed(123)
2025-04-11T04:23:18.7190209Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7190301Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7190304Z 
2025-04-11T04:23:18.7190459Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7190569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7190577Z 
2025-04-11T04:23:18.7190652Z device = None
2025-04-11T04:23:18.7190659Z 
2025-04-11T04:23:18.7190776Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7190928Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7191001Z     
2025-04-11T04:23:18.7191073Z         Args:
2025-04-11T04:23:18.7191241Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7191408Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7191513Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7191584Z         """
2025-04-11T04:23:18.7191665Z         _lazy_init()
2025-04-11T04:23:18.7191757Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7191861Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7191963Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7192245Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7192385Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7192542Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7192546Z 
2025-04-11T04:23:18.7192789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7192954Z _____________ test_flash_decoding[False-False-1-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.7192959Z 
2025-04-11T04:23:18.7193111Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7193274Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7193366Z use_new_kcache_layout = False
2025-04-11T04:23:18.7193370Z 
2025-04-11T04:23:18.7193567Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7193667Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7193878Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7194014Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7194138Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7194250Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7194387Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7194490Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7194628Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7194783Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7194866Z     def test_flash_decoding(
2025-04-11T04:23:18.7194943Z         bsz: int,
2025-04-11T04:23:18.7195023Z         block_size: int,
2025-04-11T04:23:18.7195115Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7195195Z         num_attn_heads: int,
2025-04-11T04:23:18.7195283Z         kv_group_num: int,
2025-04-11T04:23:18.7195372Z         same_context_len: bool,
2025-04-11T04:23:18.7195446Z         q_len: int,
2025-04-11T04:23:18.7195619Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7195706Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7195774Z     ):
2025-04-11T04:23:18.7195888Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7196078Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7196262Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7196433Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7196602Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7196757Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7196829Z     
2025-04-11T04:23:18.7196916Z         torch.manual_seed(123)
2025-04-11T04:23:18.7197002Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7197091Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7197100Z 
2025-04-11T04:23:18.7197255Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7197368Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7197372Z 
2025-04-11T04:23:18.7197448Z device = None
2025-04-11T04:23:18.7197453Z 
2025-04-11T04:23:18.7197568Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7197721Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7197789Z     
2025-04-11T04:23:18.7197866Z         Args:
2025-04-11T04:23:18.7198033Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7198201Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7198309Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7198378Z         """
2025-04-11T04:23:18.7198458Z         _lazy_init()
2025-04-11T04:23:18.7198554Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7198656Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7198760Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7199045Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7199184Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7199337Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7199341Z 
2025-04-11T04:23:18.7199586Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7199756Z ____________ test_flash_decoding[False-False-1-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.7199844Z 
2025-04-11T04:23:18.7200004Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7200170Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7200263Z use_new_kcache_layout = False
2025-04-11T04:23:18.7200268Z 
2025-04-11T04:23:18.7200465Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7200572Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7200689Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7200825Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7200944Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7201055Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7201200Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7201302Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7201439Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7201593Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7201779Z     def test_flash_decoding(
2025-04-11T04:23:18.7201861Z         bsz: int,
2025-04-11T04:23:18.7201942Z         block_size: int,
2025-04-11T04:23:18.7202035Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7202118Z         num_attn_heads: int,
2025-04-11T04:23:18.7202201Z         kv_group_num: int,
2025-04-11T04:23:18.7202289Z         same_context_len: bool,
2025-04-11T04:23:18.7202368Z         q_len: int,
2025-04-11T04:23:18.7202454Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7202541Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7202610Z     ):
2025-04-11T04:23:18.7202723Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7202914Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7203103Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7203273Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7203443Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7203598Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7203667Z     
2025-04-11T04:23:18.7203754Z         torch.manual_seed(123)
2025-04-11T04:23:18.7203840Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7203932Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7203935Z 
2025-04-11T04:23:18.7204087Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7204199Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7204203Z 
2025-04-11T04:23:18.7204278Z device = None
2025-04-11T04:23:18.7204286Z 
2025-04-11T04:23:18.7204400Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7204554Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7204627Z     
2025-04-11T04:23:18.7204699Z         Args:
2025-04-11T04:23:18.7204863Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7205030Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7205137Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7205207Z         """
2025-04-11T04:23:18.7205286Z         _lazy_init()
2025-04-11T04:23:18.7205381Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7205485Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7205587Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7205874Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7206125Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7206283Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7206288Z 
2025-04-11T04:23:18.7206531Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7206699Z _____________ test_flash_decoding[False-False-1-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.7206703Z 
2025-04-11T04:23:18.7206854Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7207018Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7207108Z use_new_kcache_layout = False
2025-04-11T04:23:18.7207113Z 
2025-04-11T04:23:18.7207311Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7207414Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7207532Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7207672Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7207876Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7207987Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7208127Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7208227Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7208364Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7208513Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7208596Z     def test_flash_decoding(
2025-04-11T04:23:18.7208672Z         bsz: int,
2025-04-11T04:23:18.7208752Z         block_size: int,
2025-04-11T04:23:18.7208842Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7208923Z         num_attn_heads: int,
2025-04-11T04:23:18.7209007Z         kv_group_num: int,
2025-04-11T04:23:18.7209094Z         same_context_len: bool,
2025-04-11T04:23:18.7209168Z         q_len: int,
2025-04-11T04:23:18.7209258Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7209346Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7209415Z     ):
2025-04-11T04:23:18.7209526Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7209717Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7209901Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7210070Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7210235Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7210392Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7210468Z     
2025-04-11T04:23:18.7210551Z         torch.manual_seed(123)
2025-04-11T04:23:18.7210635Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7210728Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7210735Z 
2025-04-11T04:23:18.7210885Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7210996Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7211000Z 
2025-04-11T04:23:18.7211074Z device = None
2025-04-11T04:23:18.7211079Z 
2025-04-11T04:23:18.7211198Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7211345Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7211413Z     
2025-04-11T04:23:18.7211487Z         Args:
2025-04-11T04:23:18.7211652Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7211816Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7212012Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7212083Z         """
2025-04-11T04:23:18.7212167Z         _lazy_init()
2025-04-11T04:23:18.7212263Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7212368Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7212472Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7212758Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7212894Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7213047Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7213055Z 
2025-04-11T04:23:18.7213289Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7213456Z _____________ test_flash_decoding[False-False-1-True-4-16-8-16-16] _____________
2025-04-11T04:23:18.7213463Z 
2025-04-11T04:23:18.7213616Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7213866Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7213959Z use_new_kcache_layout = False
2025-04-11T04:23:18.7213964Z 
2025-04-11T04:23:18.7214163Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7214269Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7214386Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7214522Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7214641Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7214757Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7214894Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7214995Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7215138Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7215290Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7215378Z     def test_flash_decoding(
2025-04-11T04:23:18.7215454Z         bsz: int,
2025-04-11T04:23:18.7215534Z         block_size: int,
2025-04-11T04:23:18.7215625Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7215708Z         num_attn_heads: int,
2025-04-11T04:23:18.7215789Z         kv_group_num: int,
2025-04-11T04:23:18.7215877Z         same_context_len: bool,
2025-04-11T04:23:18.7215950Z         q_len: int,
2025-04-11T04:23:18.7216037Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7216124Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7216197Z     ):
2025-04-11T04:23:18.7216307Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7216497Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7216689Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7216856Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7217023Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7217178Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7217251Z     
2025-04-11T04:23:18.7217335Z         torch.manual_seed(123)
2025-04-11T04:23:18.7217422Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7217514Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7217518Z 
2025-04-11T04:23:18.7217671Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7217781Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7217786Z 
2025-04-11T04:23:18.7217862Z device = None
2025-04-11T04:23:18.7217952Z 
2025-04-11T04:23:18.7218071Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7218221Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7218294Z     
2025-04-11T04:23:18.7218370Z         Args:
2025-04-11T04:23:18.7218538Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7218706Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7218812Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7218886Z         """
2025-04-11T04:23:18.7218963Z         _lazy_init()
2025-04-11T04:23:18.7219055Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7219158Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7219260Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7219550Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7219690Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7219936Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7219944Z 
2025-04-11T04:23:18.7220177Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7220341Z _____________ test_flash_decoding[False-False-1-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.7220346Z 
2025-04-11T04:23:18.7220498Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7220659Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7220749Z use_new_kcache_layout = False
2025-04-11T04:23:18.7220753Z 
2025-04-11T04:23:18.7220951Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7221055Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7221174Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7221312Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7221433Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7221543Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7221680Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7221779Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7221915Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7222062Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7222146Z     def test_flash_decoding(
2025-04-11T04:23:18.7222226Z         bsz: int,
2025-04-11T04:23:18.7222303Z         block_size: int,
2025-04-11T04:23:18.7222392Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7222472Z         num_attn_heads: int,
2025-04-11T04:23:18.7222557Z         kv_group_num: int,
2025-04-11T04:23:18.7222645Z         same_context_len: bool,
2025-04-11T04:23:18.7222721Z         q_len: int,
2025-04-11T04:23:18.7222807Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7222896Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7222968Z     ):
2025-04-11T04:23:18.7223076Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7223267Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7223449Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7223623Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7223792Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7223946Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7224018Z     
2025-04-11T04:23:18.7224188Z         torch.manual_seed(123)
2025-04-11T04:23:18.7224274Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7224366Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7224374Z 
2025-04-11T04:23:18.7224527Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7224639Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7224643Z 
2025-04-11T04:23:18.7224717Z device = None
2025-04-11T04:23:18.7224721Z 
2025-04-11T04:23:18.7224839Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7224988Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7225056Z     
2025-04-11T04:23:18.7225131Z         Args:
2025-04-11T04:23:18.7225294Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7225459Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7225568Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7225641Z         """
2025-04-11T04:23:18.7225717Z         _lazy_init()
2025-04-11T04:23:18.7225915Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7226018Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7226120Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7226404Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7226538Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7226695Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7226699Z 
2025-04-11T04:23:18.7226932Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7227096Z _____________ test_flash_decoding[False-False-1-True-4-16-8-32-16] _____________
2025-04-11T04:23:18.7227107Z 
2025-04-11T04:23:18.7227256Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7227416Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7227510Z use_new_kcache_layout = False
2025-04-11T04:23:18.7227515Z 
2025-04-11T04:23:18.7227712Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7227823Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7227940Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7228083Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7228196Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7228308Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7228480Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7228584Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7228726Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7228874Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7228961Z     def test_flash_decoding(
2025-04-11T04:23:18.7229037Z         bsz: int,
2025-04-11T04:23:18.7229116Z         block_size: int,
2025-04-11T04:23:18.7229206Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7229286Z         num_attn_heads: int,
2025-04-11T04:23:18.7229369Z         kv_group_num: int,
2025-04-11T04:23:18.7229452Z         same_context_len: bool,
2025-04-11T04:23:18.7229526Z         q_len: int,
2025-04-11T04:23:18.7229612Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7229698Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7229770Z     ):
2025-04-11T04:23:18.7229876Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7230064Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7230342Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7230513Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7230681Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7230834Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7230904Z     
2025-04-11T04:23:18.7230989Z         torch.manual_seed(123)
2025-04-11T04:23:18.7231074Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7231165Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7231169Z 
2025-04-11T04:23:18.7231320Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7231433Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7231437Z 
2025-04-11T04:23:18.7231511Z device = None
2025-04-11T04:23:18.7231519Z 
2025-04-11T04:23:18.7231635Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7231782Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7231941Z     
2025-04-11T04:23:18.7232017Z         Args:
2025-04-11T04:23:18.7232183Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7232351Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7232456Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7232530Z         """
2025-04-11T04:23:18.7232606Z         _lazy_init()
2025-04-11T04:23:18.7232697Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7232800Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7232901Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7233183Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7233321Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7233481Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7233489Z 
2025-04-11T04:23:18.7233723Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7233888Z _____________ test_flash_decoding[False-False-1-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.7233896Z 
2025-04-11T04:23:18.7234044Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7234207Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7234298Z use_new_kcache_layout = False
2025-04-11T04:23:18.7234303Z 
2025-04-11T04:23:18.7234501Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7234607Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7234726Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7234867Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7234986Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7235099Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7235240Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7235340Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7235477Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7235628Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7235715Z     def test_flash_decoding(
2025-04-11T04:23:18.7235789Z         bsz: int,
2025-04-11T04:23:18.7235868Z         block_size: int,
2025-04-11T04:23:18.7235958Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7236038Z         num_attn_heads: int,
2025-04-11T04:23:18.7236210Z         kv_group_num: int,
2025-04-11T04:23:18.7236296Z         same_context_len: bool,
2025-04-11T04:23:18.7236370Z         q_len: int,
2025-04-11T04:23:18.7236458Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7236547Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7236620Z     ):
2025-04-11T04:23:18.7236729Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7236923Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7237109Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7237279Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7237446Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7237604Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7237676Z     
2025-04-11T04:23:18.7237763Z         torch.manual_seed(123)
2025-04-11T04:23:18.7237850Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7237942Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7238032Z 
2025-04-11T04:23:18.7238186Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7238299Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7238303Z 
2025-04-11T04:23:18.7238378Z device = None
2025-04-11T04:23:18.7238382Z 
2025-04-11T04:23:18.7238501Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7238649Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7238719Z     
2025-04-11T04:23:18.7238790Z         Args:
2025-04-11T04:23:18.7238954Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7239121Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7239230Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7239303Z         """
2025-04-11T04:23:18.7239378Z         _lazy_init()
2025-04-11T04:23:18.7239474Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7239577Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7239679Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7239967Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7240101Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7240259Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7240263Z 
2025-04-11T04:23:18.7240500Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7240670Z ____________ test_flash_decoding[False-False-1-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.7240677Z 
2025-04-11T04:23:18.7240825Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7240987Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7241081Z use_new_kcache_layout = False
2025-04-11T04:23:18.7241085Z 
2025-04-11T04:23:18.7241283Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7241388Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7241504Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7241641Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7241755Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7241867Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7242002Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7242103Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7242331Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7242483Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7242574Z     def test_flash_decoding(
2025-04-11T04:23:18.7242646Z         bsz: int,
2025-04-11T04:23:18.7242726Z         block_size: int,
2025-04-11T04:23:18.7242817Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7242899Z         num_attn_heads: int,
2025-04-11T04:23:18.7242982Z         kv_group_num: int,
2025-04-11T04:23:18.7243064Z         same_context_len: bool,
2025-04-11T04:23:18.7243138Z         q_len: int,
2025-04-11T04:23:18.7243223Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7243309Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7243379Z     ):
2025-04-11T04:23:18.7243488Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7243678Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7243863Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7244032Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7244282Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7244438Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7244507Z     
2025-04-11T04:23:18.7244592Z         torch.manual_seed(123)
2025-04-11T04:23:18.7244679Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7244770Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7244774Z 
2025-04-11T04:23:18.7244926Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7245040Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7245044Z 
2025-04-11T04:23:18.7245119Z device = None
2025-04-11T04:23:18.7245123Z 
2025-04-11T04:23:18.7245243Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7245391Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7245465Z     
2025-04-11T04:23:18.7245535Z         Args:
2025-04-11T04:23:18.7245699Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7245866Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7245969Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7246043Z         """
2025-04-11T04:23:18.7246120Z         _lazy_init()
2025-04-11T04:23:18.7246210Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7246313Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7246417Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7246701Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7246839Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7246995Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7247003Z 
2025-04-11T04:23:18.7247237Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7247404Z _____________ test_flash_decoding[False-False-1-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.7247408Z 
2025-04-11T04:23:18.7247558Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7247719Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7247808Z use_new_kcache_layout = False
2025-04-11T04:23:18.7247812Z 
2025-04-11T04:23:18.7248009Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7248114Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7248314Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7248454Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7248576Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7248695Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7248837Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7248943Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7249084Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7249239Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7249331Z     def test_flash_decoding(
2025-04-11T04:23:18.7249407Z         bsz: int,
2025-04-11T04:23:18.7249491Z         block_size: int,
2025-04-11T04:23:18.7249586Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7249671Z         num_attn_heads: int,
2025-04-11T04:23:18.7249758Z         kv_group_num: int,
2025-04-11T04:23:18.7249849Z         same_context_len: bool,
2025-04-11T04:23:18.7249925Z         q_len: int,
2025-04-11T04:23:18.7250016Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7250292Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7250367Z     ):
2025-04-11T04:23:18.7250476Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7250671Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7250853Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7251022Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7251189Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7251346Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7251420Z     
2025-04-11T04:23:18.7251507Z         torch.manual_seed(123)
2025-04-11T04:23:18.7251596Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7251684Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7251693Z 
2025-04-11T04:23:18.7251846Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7251961Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7251965Z 
2025-04-11T04:23:18.7252039Z device = None
2025-04-11T04:23:18.7252043Z 
2025-04-11T04:23:18.7252163Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7252313Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7252386Z     
2025-04-11T04:23:18.7252458Z         Args:
2025-04-11T04:23:18.7252622Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7252787Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7252895Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7252969Z         """
2025-04-11T04:23:18.7253045Z         _lazy_init()
2025-04-11T04:23:18.7253145Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7253243Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7253347Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7253631Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7253763Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7253918Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7253923Z 
2025-04-11T04:23:18.7254156Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7254327Z ____________ test_flash_decoding[False-False-1-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.7254417Z 
2025-04-11T04:23:18.7254570Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7254733Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7254827Z use_new_kcache_layout = False
2025-04-11T04:23:18.7254832Z 
2025-04-11T04:23:18.7255031Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7255136Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7255250Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7255390Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7255504Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7255619Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7255753Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7255855Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7256000Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7256150Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7256325Z     def test_flash_decoding(
2025-04-11T04:23:18.7256399Z         bsz: int,
2025-04-11T04:23:18.7256478Z         block_size: int,
2025-04-11T04:23:18.7256570Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7256652Z         num_attn_heads: int,
2025-04-11T04:23:18.7256738Z         kv_group_num: int,
2025-04-11T04:23:18.7256820Z         same_context_len: bool,
2025-04-11T04:23:18.7256898Z         q_len: int,
2025-04-11T04:23:18.7256981Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7257068Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7257141Z     ):
2025-04-11T04:23:18.7257251Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7257443Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7257624Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7257794Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7257963Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7258118Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7258189Z     
2025-04-11T04:23:18.7258274Z         torch.manual_seed(123)
2025-04-11T04:23:18.7258363Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7258452Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7258456Z 
2025-04-11T04:23:18.7258608Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7258722Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7258727Z 
2025-04-11T04:23:18.7258802Z device = None
2025-04-11T04:23:18.7258806Z 
2025-04-11T04:23:18.7258932Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7259078Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7259154Z     
2025-04-11T04:23:18.7259225Z         Args:
2025-04-11T04:23:18.7259390Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7259558Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7259662Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7259735Z         """
2025-04-11T04:23:18.7259811Z         _lazy_init()
2025-04-11T04:23:18.7259904Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7260005Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7260107Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7260390Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7260612Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7260769Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7260777Z 
2025-04-11T04:23:18.7261011Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7261180Z _____________ test_flash_decoding[False-False-1-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.7261184Z 
2025-04-11T04:23:18.7261334Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7261500Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7261587Z use_new_kcache_layout = False
2025-04-11T04:23:18.7261591Z 
2025-04-11T04:23:18.7261786Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7261895Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7262013Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7262154Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7262373Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7262486Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7262621Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7262721Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7262860Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7263008Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7263097Z     def test_flash_decoding(
2025-04-11T04:23:18.7263169Z         bsz: int,
2025-04-11T04:23:18.7263248Z         block_size: int,
2025-04-11T04:23:18.7263339Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7263419Z         num_attn_heads: int,
2025-04-11T04:23:18.7263505Z         kv_group_num: int,
2025-04-11T04:23:18.7263593Z         same_context_len: bool,
2025-04-11T04:23:18.7263669Z         q_len: int,
2025-04-11T04:23:18.7263753Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7263841Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7263916Z     ):
2025-04-11T04:23:18.7264023Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7264219Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7264400Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7264568Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7264732Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7264886Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7264957Z     
2025-04-11T04:23:18.7265045Z         torch.manual_seed(123)
2025-04-11T04:23:18.7265134Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7265221Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7265228Z 
2025-04-11T04:23:18.7265380Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7265491Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7265495Z 
2025-04-11T04:23:18.7265569Z device = None
2025-04-11T04:23:18.7265572Z 
2025-04-11T04:23:18.7265690Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7265837Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7265908Z     
2025-04-11T04:23:18.7265978Z         Args:
2025-04-11T04:23:18.7266140Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7266304Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7266498Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7266572Z         """
2025-04-11T04:23:18.7266649Z         _lazy_init()
2025-04-11T04:23:18.7266748Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7266849Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7266951Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7267237Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7267370Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7267531Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7267535Z 
2025-04-11T04:23:18.7267769Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7267941Z ____________ test_flash_decoding[False-False-1-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.7267949Z 
2025-04-11T04:23:18.7268097Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7268262Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7268494Z use_new_kcache_layout = False
2025-04-11T04:23:18.7268498Z 
2025-04-11T04:23:18.7268700Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7268807Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7268928Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7269067Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7269184Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7269301Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7269437Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7269537Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7269679Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7269829Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7269923Z     def test_flash_decoding(
2025-04-11T04:23:18.7269998Z         bsz: int,
2025-04-11T04:23:18.7270081Z         block_size: int,
2025-04-11T04:23:18.7270168Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7270251Z         num_attn_heads: int,
2025-04-11T04:23:18.7270337Z         kv_group_num: int,
2025-04-11T04:23:18.7270422Z         same_context_len: bool,
2025-04-11T04:23:18.7270499Z         q_len: int,
2025-04-11T04:23:18.7270583Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7270670Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7270744Z     ):
2025-04-11T04:23:18.7270852Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7271047Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7271231Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7271403Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7271570Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7271726Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7271799Z     
2025-04-11T04:23:18.7271894Z         torch.manual_seed(123)
2025-04-11T04:23:18.7271990Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7272080Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7272084Z 
2025-04-11T04:23:18.7272244Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7272358Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7272362Z 
2025-04-11T04:23:18.7272439Z device = None
2025-04-11T04:23:18.7272442Z 
2025-04-11T04:23:18.7272677Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7272828Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7272910Z     
2025-04-11T04:23:18.7272983Z         Args:
2025-04-11T04:23:18.7273152Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7273317Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7273424Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7273499Z         """
2025-04-11T04:23:18.7273577Z         _lazy_init()
2025-04-11T04:23:18.7273675Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7273777Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7273883Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7274171Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7274308Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7274470Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7274567Z 
2025-04-11T04:23:18.7274811Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7274983Z _____________ test_flash_decoding[False-False-1-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.7274987Z 
2025-04-11T04:23:18.7275136Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7275304Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7275392Z use_new_kcache_layout = False
2025-04-11T04:23:18.7275397Z 
2025-04-11T04:23:18.7275593Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7275704Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7275824Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7275965Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7276084Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7276198Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7276331Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7276433Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7276569Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7276718Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7276811Z     def test_flash_decoding(
2025-04-11T04:23:18.7276886Z         bsz: int,
2025-04-11T04:23:18.7276969Z         block_size: int,
2025-04-11T04:23:18.7277058Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7277141Z         num_attn_heads: int,
2025-04-11T04:23:18.7277227Z         kv_group_num: int,
2025-04-11T04:23:18.7277315Z         same_context_len: bool,
2025-04-11T04:23:18.7277393Z         q_len: int,
2025-04-11T04:23:18.7277475Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7277566Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7277638Z     ):
2025-04-11T04:23:18.7277746Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7277941Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7278119Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7278290Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7278450Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7278605Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7278678Z     
2025-04-11T04:23:18.7278853Z         torch.manual_seed(123)
2025-04-11T04:23:18.7278944Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7279034Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7279041Z 
2025-04-11T04:23:18.7279200Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7279310Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7279314Z 
2025-04-11T04:23:18.7279388Z device = None
2025-04-11T04:23:18.7279393Z 
2025-04-11T04:23:18.7279515Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7279662Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7279736Z     
2025-04-11T04:23:18.7279808Z         Args:
2025-04-11T04:23:18.7279976Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7280138Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7280249Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7280323Z         """
2025-04-11T04:23:18.7280401Z         _lazy_init()
2025-04-11T04:23:18.7280588Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7280689Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7280792Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7281079Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7281213Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7281371Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7281375Z 
2025-04-11T04:23:18.7281616Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7281788Z ____________ test_flash_decoding[False-False-1-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.7281796Z 
2025-04-11T04:23:18.7281946Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7282113Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7282204Z use_new_kcache_layout = False
2025-04-11T04:23:18.7282208Z 
2025-04-11T04:23:18.7282413Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7282518Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7282635Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7282776Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7282889Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7283004Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7283136Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7283242Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7283379Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7283529Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7283623Z     def test_flash_decoding(
2025-04-11T04:23:18.7283697Z         bsz: int,
2025-04-11T04:23:18.7283780Z         block_size: int,
2025-04-11T04:23:18.7283868Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7283948Z         num_attn_heads: int,
2025-04-11T04:23:18.7284034Z         kv_group_num: int,
2025-04-11T04:23:18.7284118Z         same_context_len: bool,
2025-04-11T04:23:18.7284196Z         q_len: int,
2025-04-11T04:23:18.7284280Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7284365Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7284439Z     ):
2025-04-11T04:23:18.7284549Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7284747Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7285012Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7285184Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7285346Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7285500Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7285574Z     
2025-04-11T04:23:18.7285660Z         torch.manual_seed(123)
2025-04-11T04:23:18.7285750Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7285838Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7285842Z 
2025-04-11T04:23:18.7285997Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7286107Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7286111Z 
2025-04-11T04:23:18.7286184Z device = None
2025-04-11T04:23:18.7286191Z 
2025-04-11T04:23:18.7286309Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7286455Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7286611Z     
2025-04-11T04:23:18.7286683Z         Args:
2025-04-11T04:23:18.7286853Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7287017Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7287123Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7287197Z         """
2025-04-11T04:23:18.7287273Z         _lazy_init()
2025-04-11T04:23:18.7287373Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7287474Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7287581Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7287863Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7288003Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7288163Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7288171Z 
2025-04-11T04:23:18.7288409Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7288582Z ____________ test_flash_decoding[False-False-1-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.7288586Z 
2025-04-11T04:23:18.7288738Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7288905Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7288994Z use_new_kcache_layout = False
2025-04-11T04:23:18.7288999Z 
2025-04-11T04:23:18.7289201Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7289306Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7289425Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7289565Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7289683Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7289799Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7289933Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7290039Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7290173Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7290320Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7290410Z     def test_flash_decoding(
2025-04-11T04:23:18.7290483Z         bsz: int,
2025-04-11T04:23:18.7290566Z         block_size: int,
2025-04-11T04:23:18.7290655Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7290737Z         num_attn_heads: int,
2025-04-11T04:23:18.7290821Z         kv_group_num: int,
2025-04-11T04:23:18.7290997Z         same_context_len: bool,
2025-04-11T04:23:18.7291075Z         q_len: int,
2025-04-11T04:23:18.7291159Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7291253Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7291321Z     ):
2025-04-11T04:23:18.7291431Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7291626Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7291808Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7291981Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7292144Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7292301Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7292371Z     
2025-04-11T04:23:18.7292462Z         torch.manual_seed(123)
2025-04-11T04:23:18.7292552Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7292643Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7292730Z 
2025-04-11T04:23:18.7292890Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7293001Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7293005Z 
2025-04-11T04:23:18.7293085Z device = None
2025-04-11T04:23:18.7293089Z 
2025-04-11T04:23:18.7293205Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7293353Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7293428Z     
2025-04-11T04:23:18.7293498Z         Args:
2025-04-11T04:23:18.7293666Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7293831Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7293944Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7294014Z         """
2025-04-11T04:23:18.7294091Z         _lazy_init()
2025-04-11T04:23:18.7294192Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7294293Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7294398Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7294679Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7294813Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7294973Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7294977Z 
2025-04-11T04:23:18.7295210Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7295381Z ____________ test_flash_decoding[False-False-1-False-1-16-16-16-16] ____________
2025-04-11T04:23:18.7295388Z 
2025-04-11T04:23:18.7295536Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7295702Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7295792Z use_new_kcache_layout = False
2025-04-11T04:23:18.7295796Z 
2025-04-11T04:23:18.7295998Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7296100Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7296217Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7296358Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7296471Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7296586Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7296719Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7296823Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7297062Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7297211Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7297303Z     def test_flash_decoding(
2025-04-11T04:23:18.7297376Z         bsz: int,
2025-04-11T04:23:18.7297459Z         block_size: int,
2025-04-11T04:23:18.7297547Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7297627Z         num_attn_heads: int,
2025-04-11T04:23:18.7297712Z         kv_group_num: int,
2025-04-11T04:23:18.7297794Z         same_context_len: bool,
2025-04-11T04:23:18.7297872Z         q_len: int,
2025-04-11T04:23:18.7297954Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7298044Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7298113Z     ):
2025-04-11T04:23:18.7298224Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7298419Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7298601Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7298771Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7299023Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7299182Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7299255Z     
2025-04-11T04:23:18.7299341Z         torch.manual_seed(123)
2025-04-11T04:23:18.7299432Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7299522Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7299526Z 
2025-04-11T04:23:18.7299685Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7299798Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7299802Z 
2025-04-11T04:23:18.7299879Z device = None
2025-04-11T04:23:18.7299883Z 
2025-04-11T04:23:18.7300007Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7300154Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7300229Z     
2025-04-11T04:23:18.7300300Z         Args:
2025-04-11T04:23:18.7300467Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7300630Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7300737Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7300809Z         """
2025-04-11T04:23:18.7300886Z         _lazy_init()
2025-04-11T04:23:18.7300982Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7301082Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7301188Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7301469Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7301605Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7301764Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7301771Z 
2025-04-11T04:23:18.7302006Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7302177Z ____________ test_flash_decoding[False-False-1-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.7302181Z 
2025-04-11T04:23:18.7302330Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7302495Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7302582Z use_new_kcache_layout = False
2025-04-11T04:23:18.7302587Z 
2025-04-11T04:23:18.7302787Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7302889Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7303095Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7303236Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7303354Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7303469Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7303604Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7303709Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7303844Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7303993Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7304083Z     def test_flash_decoding(
2025-04-11T04:23:18.7304155Z         bsz: int,
2025-04-11T04:23:18.7304239Z         block_size: int,
2025-04-11T04:23:18.7304325Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7304411Z         num_attn_heads: int,
2025-04-11T04:23:18.7304492Z         kv_group_num: int,
2025-04-11T04:23:18.7304578Z         same_context_len: bool,
2025-04-11T04:23:18.7304656Z         q_len: int,
2025-04-11T04:23:18.7304739Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7304912Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7304982Z     ):
2025-04-11T04:23:18.7305090Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7305281Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7305460Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7305631Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7305792Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7305948Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7306017Z     
2025-04-11T04:23:18.7306105Z         torch.manual_seed(123)
2025-04-11T04:23:18.7306195Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7306284Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7306290Z 
2025-04-11T04:23:18.7306447Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7306556Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7306561Z 
2025-04-11T04:23:18.7306637Z device = None
2025-04-11T04:23:18.7306641Z 
2025-04-11T04:23:18.7306756Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7306906Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7306979Z     
2025-04-11T04:23:18.7307051Z         Args:
2025-04-11T04:23:18.7307219Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7307382Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7307488Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7307563Z         """
2025-04-11T04:23:18.7307639Z         _lazy_init()
2025-04-11T04:23:18.7307734Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7307837Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7307943Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7308222Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7308359Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7308552Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7308557Z 
2025-04-11T04:23:18.7308793Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7308968Z ____________ test_flash_decoding[False-False-1-False-1-16-16-32-16] ____________
2025-04-11T04:23:18.7309067Z 
2025-04-11T04:23:18.7309219Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7309387Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7309478Z use_new_kcache_layout = False
2025-04-11T04:23:18.7309482Z 
2025-04-11T04:23:18.7309682Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7309785Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7309902Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7310042Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7310155Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7310269Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7310405Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7310510Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7310650Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7310799Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7310983Z     def test_flash_decoding(
2025-04-11T04:23:18.7311057Z         bsz: int,
2025-04-11T04:23:18.7311142Z         block_size: int,
2025-04-11T04:23:18.7311232Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7311317Z         num_attn_heads: int,
2025-04-11T04:23:18.7311399Z         kv_group_num: int,
2025-04-11T04:23:18.7311483Z         same_context_len: bool,
2025-04-11T04:23:18.7311560Z         q_len: int,
2025-04-11T04:23:18.7311643Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7311732Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7311801Z     ):
2025-04-11T04:23:18.7311910Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7312102Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7312282Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7312454Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7312618Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7312775Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7312844Z     
2025-04-11T04:23:18.7312929Z         torch.manual_seed(123)
2025-04-11T04:23:18.7313018Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7313106Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7313109Z 
2025-04-11T04:23:18.7313265Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7313373Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7313377Z 
2025-04-11T04:23:18.7313453Z device = None
2025-04-11T04:23:18.7313457Z 
2025-04-11T04:23:18.7313573Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7313721Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7313796Z     
2025-04-11T04:23:18.7313867Z         Args:
2025-04-11T04:23:18.7314037Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7314201Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7314308Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7314380Z         """
2025-04-11T04:23:18.7314455Z         _lazy_init()
2025-04-11T04:23:18.7314551Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7314651Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7314757Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7315037Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7315264Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7315418Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7315426Z 
2025-04-11T04:23:18.7315662Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7315830Z _____________ test_flash_decoding[False-False-1-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.7315834Z 
2025-04-11T04:23:18.7315983Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7316149Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7316236Z use_new_kcache_layout = False
2025-04-11T04:23:18.7316241Z 
2025-04-11T04:23:18.7316441Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7316545Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7316664Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7316801Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7317021Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7317137Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7317273Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7317377Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7317512Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7317666Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7317751Z     def test_flash_decoding(
2025-04-11T04:23:18.7317824Z         bsz: int,
2025-04-11T04:23:18.7317909Z         block_size: int,
2025-04-11T04:23:18.7317997Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7318081Z         num_attn_heads: int,
2025-04-11T04:23:18.7318161Z         kv_group_num: int,
2025-04-11T04:23:18.7318252Z         same_context_len: bool,
2025-04-11T04:23:18.7318331Z         q_len: int,
2025-04-11T04:23:18.7318414Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7318506Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7318576Z     ):
2025-04-11T04:23:18.7318682Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7318875Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7319054Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7319227Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7319387Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7319548Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7319617Z     
2025-04-11T04:23:18.7319700Z         torch.manual_seed(123)
2025-04-11T04:23:18.7319793Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7319883Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7319890Z 
2025-04-11T04:23:18.7320048Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7320155Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7320159Z 
2025-04-11T04:23:18.7320238Z device = None
2025-04-11T04:23:18.7320242Z 
2025-04-11T04:23:18.7320355Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7320505Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7320572Z     
2025-04-11T04:23:18.7320643Z         Args:
2025-04-11T04:23:18.7320809Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7320972Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7321079Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7321250Z         """
2025-04-11T04:23:18.7321327Z         _lazy_init()
2025-04-11T04:23:18.7321425Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7321528Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7321634Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7321919Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7322059Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7322216Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7322220Z 
2025-04-11T04:23:18.7322460Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7322629Z ____________ test_flash_decoding[False-False-1-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.7322636Z 
2025-04-11T04:23:18.7322787Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7322955Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7323125Z use_new_kcache_layout = False
2025-04-11T04:23:18.7323129Z 
2025-04-11T04:23:18.7323332Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7323435Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7323553Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7323692Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7323807Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7323923Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7324059Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7324163Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7324299Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7324451Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7324538Z     def test_flash_decoding(
2025-04-11T04:23:18.7324611Z         bsz: int,
2025-04-11T04:23:18.7324695Z         block_size: int,
2025-04-11T04:23:18.7324781Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7324865Z         num_attn_heads: int,
2025-04-11T04:23:18.7324946Z         kv_group_num: int,
2025-04-11T04:23:18.7325031Z         same_context_len: bool,
2025-04-11T04:23:18.7325108Z         q_len: int,
2025-04-11T04:23:18.7325190Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7325279Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7325347Z     ):
2025-04-11T04:23:18.7325454Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7325650Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7325833Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7326009Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7326176Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7326335Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7326404Z     
2025-04-11T04:23:18.7326492Z         torch.manual_seed(123)
2025-04-11T04:23:18.7326581Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7326673Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7326677Z 
2025-04-11T04:23:18.7326834Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7326944Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7326948Z 
2025-04-11T04:23:18.7327027Z device = None
2025-04-11T04:23:18.7327031Z 
2025-04-11T04:23:18.7327235Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7327390Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7327466Z     
2025-04-11T04:23:18.7327542Z         Args:
2025-04-11T04:23:18.7327712Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7327877Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7327989Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7328063Z         """
2025-04-11T04:23:18.7328145Z         _lazy_init()
2025-04-11T04:23:18.7328242Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7328346Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7328456Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7328738Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7328884Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7329041Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7329127Z 
2025-04-11T04:23:18.7329374Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7329542Z _____________ test_flash_decoding[False-False-1-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.7329547Z 
2025-04-11T04:23:18.7329696Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7329864Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7329952Z use_new_kcache_layout = False
2025-04-11T04:23:18.7329956Z 
2025-04-11T04:23:18.7330160Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7330264Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7330386Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7330523Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7330643Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7330757Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7330893Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7330998Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7331131Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7331282Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7331365Z     def test_flash_decoding(
2025-04-11T04:23:18.7331438Z         bsz: int,
2025-04-11T04:23:18.7331521Z         block_size: int,
2025-04-11T04:23:18.7331608Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7331691Z         num_attn_heads: int,
2025-04-11T04:23:18.7331771Z         kv_group_num: int,
2025-04-11T04:23:18.7331857Z         same_context_len: bool,
2025-04-11T04:23:18.7331935Z         q_len: int,
2025-04-11T04:23:18.7332020Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7332116Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7332185Z     ):
2025-04-11T04:23:18.7332297Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7332488Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7332666Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7332840Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7333003Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7333161Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7333231Z     
2025-04-11T04:23:18.7333318Z         torch.manual_seed(123)
2025-04-11T04:23:18.7333490Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7333578Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7333588Z 
2025-04-11T04:23:18.7333747Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7333857Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7333861Z 
2025-04-11T04:23:18.7333939Z device = None
2025-04-11T04:23:18.7333943Z 
2025-04-11T04:23:18.7334058Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7334208Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7334277Z     
2025-04-11T04:23:18.7334349Z         Args:
2025-04-11T04:23:18.7334516Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7334680Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7334790Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7334863Z         """
2025-04-11T04:23:18.7334941Z         _lazy_init()
2025-04-11T04:23:18.7335033Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7335222Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7335334Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7335618Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7335762Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7335922Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7335927Z 
2025-04-11T04:23:18.7336170Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7336342Z ____________ test_flash_decoding[False-False-1-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.7336346Z 
2025-04-11T04:23:18.7336503Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7336674Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7336769Z use_new_kcache_layout = False
2025-04-11T04:23:18.7336773Z 
2025-04-11T04:23:18.7336981Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7337088Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7337211Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7337354Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7337476Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7337591Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7337729Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7337840Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7337986Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7338144Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7338236Z     def test_flash_decoding(
2025-04-11T04:23:18.7338313Z         bsz: int,
2025-04-11T04:23:18.7338401Z         block_size: int,
2025-04-11T04:23:18.7338494Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7338583Z         num_attn_heads: int,
2025-04-11T04:23:18.7338670Z         kv_group_num: int,
2025-04-11T04:23:18.7338762Z         same_context_len: bool,
2025-04-11T04:23:18.7338839Z         q_len: int,
2025-04-11T04:23:18.7338927Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7339022Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7339094Z     ):
2025-04-11T04:23:18.7339211Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7339408Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7339592Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7339850Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7340014Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7340174Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7340243Z     
2025-04-11T04:23:18.7340332Z         torch.manual_seed(123)
2025-04-11T04:23:18.7340420Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7340507Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7340512Z 
2025-04-11T04:23:18.7340670Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7340782Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7340786Z 
2025-04-11T04:23:18.7340864Z device = None
2025-04-11T04:23:18.7340868Z 
2025-04-11T04:23:18.7340985Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7341139Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7341307Z     
2025-04-11T04:23:18.7341381Z         Args:
2025-04-11T04:23:18.7341551Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7341717Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7341827Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7341898Z         """
2025-04-11T04:23:18.7341981Z         _lazy_init()
2025-04-11T04:23:18.7342073Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7342176Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7342286Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7342570Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7342713Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7342870Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7342878Z 
2025-04-11T04:23:18.7343119Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7343285Z ____________ test_flash_decoding[False-False-1-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.7343289Z 
2025-04-11T04:23:18.7343445Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7343609Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7343697Z use_new_kcache_layout = False
2025-04-11T04:23:18.7343702Z 
2025-04-11T04:23:18.7343904Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7344008Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7344132Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7344270Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7344393Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7344506Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7344640Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7344745Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7344879Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7345031Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7345115Z     def test_flash_decoding(
2025-04-11T04:23:18.7345191Z         bsz: int,
2025-04-11T04:23:18.7345271Z         block_size: int,
2025-04-11T04:23:18.7345359Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7345442Z         num_attn_heads: int,
2025-04-11T04:23:18.7345523Z         kv_group_num: int,
2025-04-11T04:23:18.7345705Z         same_context_len: bool,
2025-04-11T04:23:18.7345779Z         q_len: int,
2025-04-11T04:23:18.7345863Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7345957Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7346026Z     ):
2025-04-11T04:23:18.7346137Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7346328Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7346505Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7346676Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7346838Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7346995Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7347065Z     
2025-04-11T04:23:18.7347152Z         torch.manual_seed(123)
2025-04-11T04:23:18.7347241Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7347328Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7347336Z 
2025-04-11T04:23:18.7347664Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7347775Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7347779Z 
2025-04-11T04:23:18.7347857Z device = None
2025-04-11T04:23:18.7347861Z 
2025-04-11T04:23:18.7347977Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7348128Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7348197Z     
2025-04-11T04:23:18.7348271Z         Args:
2025-04-11T04:23:18.7348475Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7348642Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7348752Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7348828Z         """
2025-04-11T04:23:18.7348908Z         _lazy_init()
2025-04-11T04:23:18.7349000Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7349105Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7349213Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7349493Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7349631Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7349788Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7349792Z 
2025-04-11T04:23:18.7350033Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7350203Z ____________ test_flash_decoding[False-False-1-False-4-16-16-16-16] ____________
2025-04-11T04:23:18.7350208Z 
2025-04-11T04:23:18.7350366Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7350530Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7350620Z use_new_kcache_layout = False
2025-04-11T04:23:18.7350624Z 
2025-04-11T04:23:18.7350826Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7350929Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7351046Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7351182Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7351299Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7351411Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7351544Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7351648Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7351881Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7352034Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7352121Z     def test_flash_decoding(
2025-04-11T04:23:18.7352197Z         bsz: int,
2025-04-11T04:23:18.7352277Z         block_size: int,
2025-04-11T04:23:18.7352364Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7352450Z         num_attn_heads: int,
2025-04-11T04:23:18.7352532Z         kv_group_num: int,
2025-04-11T04:23:18.7352617Z         same_context_len: bool,
2025-04-11T04:23:18.7352691Z         q_len: int,
2025-04-11T04:23:18.7352775Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7352865Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7352934Z     ):
2025-04-11T04:23:18.7353048Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7353240Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7353422Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7353597Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7353849Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7354010Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7354081Z     
2025-04-11T04:23:18.7354168Z         torch.manual_seed(123)
2025-04-11T04:23:18.7354254Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7354343Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7354350Z 
2025-04-11T04:23:18.7354501Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7354610Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7354614Z 
2025-04-11T04:23:18.7354691Z device = None
2025-04-11T04:23:18.7354695Z 
2025-04-11T04:23:18.7354809Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7354966Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7355037Z     
2025-04-11T04:23:18.7355112Z         Args:
2025-04-11T04:23:18.7355272Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7355434Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7355540Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7355610Z         """
2025-04-11T04:23:18.7355689Z         _lazy_init()
2025-04-11T04:23:18.7355779Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7355878Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7355983Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7356264Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7356406Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7356562Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7356569Z 
2025-04-11T04:23:18.7356807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7356972Z ____________ test_flash_decoding[False-False-1-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.7356977Z 
2025-04-11T04:23:18.7357125Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7357286Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7357374Z use_new_kcache_layout = False
2025-04-11T04:23:18.7357381Z 
2025-04-11T04:23:18.7357577Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7357678Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7357887Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7358025Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7358147Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7358260Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7358394Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7358500Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7358633Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7358787Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7358873Z     def test_flash_decoding(
2025-04-11T04:23:18.7358949Z         bsz: int,
2025-04-11T04:23:18.7359029Z         block_size: int,
2025-04-11T04:23:18.7359117Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7359202Z         num_attn_heads: int,
2025-04-11T04:23:18.7359283Z         kv_group_num: int,
2025-04-11T04:23:18.7359373Z         same_context_len: bool,
2025-04-11T04:23:18.7359446Z         q_len: int,
2025-04-11T04:23:18.7359530Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7359711Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7359783Z     ):
2025-04-11T04:23:18.7359898Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7360093Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7360277Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7360446Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7360610Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7360768Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7360838Z     
2025-04-11T04:23:18.7360925Z         torch.manual_seed(123)
2025-04-11T04:23:18.7361016Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7361108Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7361112Z 
2025-04-11T04:23:18.7361270Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7361382Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7361386Z 
2025-04-11T04:23:18.7361464Z device = None
2025-04-11T04:23:18.7361468Z 
2025-04-11T04:23:18.7361583Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7361740Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7361808Z     
2025-04-11T04:23:18.7361882Z         Args:
2025-04-11T04:23:18.7362049Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7362213Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7362320Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7362395Z         """
2025-04-11T04:23:18.7362473Z         _lazy_init()
2025-04-11T04:23:18.7362564Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7362669Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7362773Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7363055Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7363193Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7363348Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7363352Z 
2025-04-11T04:23:18.7363599Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7363768Z ____________ test_flash_decoding[False-False-1-False-4-16-16-32-16] ____________
2025-04-11T04:23:18.7363772Z 
2025-04-11T04:23:18.7364028Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7364191Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7364285Z use_new_kcache_layout = False
2025-04-11T04:23:18.7364289Z 
2025-04-11T04:23:18.7364489Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7364591Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7364712Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7364848Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7364962Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7365073Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7365210Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7365311Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7365449Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7365598Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7365768Z     def test_flash_decoding(
2025-04-11T04:23:18.7365845Z         bsz: int,
2025-04-11T04:23:18.7365926Z         block_size: int,
2025-04-11T04:23:18.7366013Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7366097Z         num_attn_heads: int,
2025-04-11T04:23:18.7366177Z         kv_group_num: int,
2025-04-11T04:23:18.7366263Z         same_context_len: bool,
2025-04-11T04:23:18.7366336Z         q_len: int,
2025-04-11T04:23:18.7366420Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7366511Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7366580Z     ):
2025-04-11T04:23:18.7366695Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7366890Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7367075Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7367250Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7367417Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7367576Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7367645Z     
2025-04-11T04:23:18.7367735Z         torch.manual_seed(123)
2025-04-11T04:23:18.7367821Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7367911Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7367915Z 
2025-04-11T04:23:18.7368068Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7368177Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7368185Z 
2025-04-11T04:23:18.7368259Z device = None
2025-04-11T04:23:18.7368263Z 
2025-04-11T04:23:18.7368378Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7368532Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7368604Z     
2025-04-11T04:23:18.7368679Z         Args:
2025-04-11T04:23:18.7368842Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7369004Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7369112Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7369183Z         """
2025-04-11T04:23:18.7369262Z         _lazy_init()
2025-04-11T04:23:18.7369356Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7369459Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7369562Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7369843Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7370071Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7370225Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7370233Z 
2025-04-11T04:23:18.7370475Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7370643Z _____________ test_flash_decoding[False-False-5-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.7370648Z 
2025-04-11T04:23:18.7370799Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7370962Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7371054Z use_new_kcache_layout = False
2025-04-11T04:23:18.7371059Z 
2025-04-11T04:23:18.7371258Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7371360Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7371482Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7371618Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7371820Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7371931Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7372070Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7372173Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7372306Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7372460Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7372545Z     def test_flash_decoding(
2025-04-11T04:23:18.7372622Z         bsz: int,
2025-04-11T04:23:18.7372703Z         block_size: int,
2025-04-11T04:23:18.7372790Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7372875Z         num_attn_heads: int,
2025-04-11T04:23:18.7372957Z         kv_group_num: int,
2025-04-11T04:23:18.7373050Z         same_context_len: bool,
2025-04-11T04:23:18.7373124Z         q_len: int,
2025-04-11T04:23:18.7373212Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7373302Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7373372Z     ):
2025-04-11T04:23:18.7373486Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7373677Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7373861Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7374029Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7374195Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7374350Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7374421Z     
2025-04-11T04:23:18.7374511Z         torch.manual_seed(123)
2025-04-11T04:23:18.7374601Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7374693Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7374697Z 
2025-04-11T04:23:18.7374857Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7374965Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7374973Z 
2025-04-11T04:23:18.7375046Z device = None
2025-04-11T04:23:18.7375050Z 
2025-04-11T04:23:18.7375165Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7375315Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7375384Z     
2025-04-11T04:23:18.7375459Z         Args:
2025-04-11T04:23:18.7375624Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7375787Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7375892Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7376057Z         """
2025-04-11T04:23:18.7376140Z         _lazy_init()
2025-04-11T04:23:18.7376234Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7376341Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7376445Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7376728Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7376868Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7377025Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7377030Z 
2025-04-11T04:23:18.7377277Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7377443Z _____________ test_flash_decoding[False-False-5-True-1-16-8-16-16] _____________
2025-04-11T04:23:18.7377447Z 
2025-04-11T04:23:18.7377602Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7377765Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7377946Z use_new_kcache_layout = False
2025-04-11T04:23:18.7377951Z 
2025-04-11T04:23:18.7378152Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7378254Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7378372Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7378509Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7378626Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7378737Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7378874Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7378974Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7379108Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7379263Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7379352Z     def test_flash_decoding(
2025-04-11T04:23:18.7379430Z         bsz: int,
2025-04-11T04:23:18.7379510Z         block_size: int,
2025-04-11T04:23:18.7379601Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7379680Z         num_attn_heads: int,
2025-04-11T04:23:18.7379760Z         kv_group_num: int,
2025-04-11T04:23:18.7379846Z         same_context_len: bool,
2025-04-11T04:23:18.7379919Z         q_len: int,
2025-04-11T04:23:18.7380005Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7380090Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7380159Z     ):
2025-04-11T04:23:18.7380273Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7380463Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7380646Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7380817Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7380983Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7381137Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7381206Z     
2025-04-11T04:23:18.7381293Z         torch.manual_seed(123)
2025-04-11T04:23:18.7381380Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7381470Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7381474Z 
2025-04-11T04:23:18.7381628Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7381742Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7381746Z 
2025-04-11T04:23:18.7381820Z device = None
2025-04-11T04:23:18.7381824Z 
2025-04-11T04:23:18.7381940Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7382195Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7382268Z     
2025-04-11T04:23:18.7382344Z         Args:
2025-04-11T04:23:18.7382510Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7382680Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7382785Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7382857Z         """
2025-04-11T04:23:18.7382939Z         _lazy_init()
2025-04-11T04:23:18.7383033Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7383136Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7383240Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7383521Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7383661Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7383816Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7383902Z 
2025-04-11T04:23:18.7384145Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7384310Z _____________ test_flash_decoding[False-False-5-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.7384315Z 
2025-04-11T04:23:18.7384467Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7384629Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7384719Z use_new_kcache_layout = False
2025-04-11T04:23:18.7384724Z 
2025-04-11T04:23:18.7384921Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7385024Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7385140Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7385281Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7385401Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7385518Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7385655Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7385757Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7385890Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7386042Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7386127Z     def test_flash_decoding(
2025-04-11T04:23:18.7386205Z         bsz: int,
2025-04-11T04:23:18.7386285Z         block_size: int,
2025-04-11T04:23:18.7386379Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7386461Z         num_attn_heads: int,
2025-04-11T04:23:18.7386543Z         kv_group_num: int,
2025-04-11T04:23:18.7386637Z         same_context_len: bool,
2025-04-11T04:23:18.7386711Z         q_len: int,
2025-04-11T04:23:18.7386800Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7386891Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7386960Z     ):
2025-04-11T04:23:18.7387072Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7387263Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7387446Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7387613Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7387779Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7387935Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7388003Z     
2025-04-11T04:23:18.7388093Z         torch.manual_seed(123)
2025-04-11T04:23:18.7388285Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7388377Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7388381Z 
2025-04-11T04:23:18.7388594Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7388708Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7388712Z 
2025-04-11T04:23:18.7388786Z device = None
2025-04-11T04:23:18.7388789Z 
2025-04-11T04:23:18.7388906Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7389059Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7389127Z     
2025-04-11T04:23:18.7389203Z         Args:
2025-04-11T04:23:18.7389366Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7389533Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7389637Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7389711Z         """
2025-04-11T04:23:18.7389790Z         _lazy_init()
2025-04-11T04:23:18.7389882Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7390079Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7390181Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7390465Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7390601Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7390758Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7390762Z 
2025-04-11T04:23:18.7391010Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7391177Z _____________ test_flash_decoding[False-False-5-True-1-16-8-32-16] _____________
2025-04-11T04:23:18.7391181Z 
2025-04-11T04:23:18.7391338Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7391503Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7391596Z use_new_kcache_layout = False
2025-04-11T04:23:18.7391600Z 
2025-04-11T04:23:18.7391801Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7391907Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7392022Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7392158Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7392275Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7392388Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7392526Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7392628Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7392766Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7392919Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7393008Z     def test_flash_decoding(
2025-04-11T04:23:18.7393087Z         bsz: int,
2025-04-11T04:23:18.7393167Z         block_size: int,
2025-04-11T04:23:18.7393259Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7393340Z         num_attn_heads: int,
2025-04-11T04:23:18.7393421Z         kv_group_num: int,
2025-04-11T04:23:18.7393508Z         same_context_len: bool,
2025-04-11T04:23:18.7393581Z         q_len: int,
2025-04-11T04:23:18.7393667Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7393753Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7393821Z     ):
2025-04-11T04:23:18.7393933Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7394126Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7394312Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7394575Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7394746Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7394902Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7394970Z     
2025-04-11T04:23:18.7395060Z         torch.manual_seed(123)
2025-04-11T04:23:18.7395147Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7395239Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7395243Z 
2025-04-11T04:23:18.7395395Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7395509Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7395513Z 
2025-04-11T04:23:18.7395587Z device = None
2025-04-11T04:23:18.7395591Z 
2025-04-11T04:23:18.7395710Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7395861Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7395929Z     
2025-04-11T04:23:18.7396089Z         Args:
2025-04-11T04:23:18.7396254Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7396422Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7396526Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7396597Z         """
2025-04-11T04:23:18.7396677Z         _lazy_init()
2025-04-11T04:23:18.7396771Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7396875Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7396978Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7397264Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7397402Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7397558Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7397569Z 
2025-04-11T04:23:18.7397807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7397971Z _____________ test_flash_decoding[False-False-5-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.7397975Z 
2025-04-11T04:23:18.7398127Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7398288Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7398381Z use_new_kcache_layout = False
2025-04-11T04:23:18.7398385Z 
2025-04-11T04:23:18.7398585Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7398690Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7398804Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7398942Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7399059Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7399173Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7399311Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7399412Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7399548Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7399697Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7399784Z     def test_flash_decoding(
2025-04-11T04:23:18.7399860Z         bsz: int,
2025-04-11T04:23:18.7399941Z         block_size: int,
2025-04-11T04:23:18.7400032Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7400113Z         num_attn_heads: int,
2025-04-11T04:23:18.7400194Z         kv_group_num: int,
2025-04-11T04:23:18.7400378Z         same_context_len: bool,
2025-04-11T04:23:18.7400453Z         q_len: int,
2025-04-11T04:23:18.7400541Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7400627Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7400702Z     ):
2025-04-11T04:23:18.7400813Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7401005Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7401187Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7401355Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7401520Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7401673Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7401745Z     
2025-04-11T04:23:18.7401831Z         torch.manual_seed(123)
2025-04-11T04:23:18.7401922Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7402016Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7402020Z 
2025-04-11T04:23:18.7402262Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7402375Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7402379Z 
2025-04-11T04:23:18.7402454Z device = None
2025-04-11T04:23:18.7402458Z 
2025-04-11T04:23:18.7402577Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7402728Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7402796Z     
2025-04-11T04:23:18.7402872Z         Args:
2025-04-11T04:23:18.7403039Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7403206Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7403310Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7403387Z         """
2025-04-11T04:23:18.7403463Z         _lazy_init()
2025-04-11T04:23:18.7403557Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7403663Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7403766Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7404052Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7404188Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7404344Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7404353Z 
2025-04-11T04:23:18.7404592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7404759Z ____________ test_flash_decoding[False-False-5-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.7404763Z 
2025-04-11T04:23:18.7404922Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7405085Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7405182Z use_new_kcache_layout = False
2025-04-11T04:23:18.7405186Z 
2025-04-11T04:23:18.7405386Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7405494Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7405611Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7405749Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7405867Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7405977Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7406115Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7406218Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7406356Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7406591Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7406676Z     def test_flash_decoding(
2025-04-11T04:23:18.7406758Z         bsz: int,
2025-04-11T04:23:18.7406838Z         block_size: int,
2025-04-11T04:23:18.7406929Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7407011Z         num_attn_heads: int,
2025-04-11T04:23:18.7407093Z         kv_group_num: int,
2025-04-11T04:23:18.7407180Z         same_context_len: bool,
2025-04-11T04:23:18.7407252Z         q_len: int,
2025-04-11T04:23:18.7407340Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7407426Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7407499Z     ):
2025-04-11T04:23:18.7407606Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7407799Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7407983Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7408155Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7408419Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7408573Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7408646Z     
2025-04-11T04:23:18.7408732Z         torch.manual_seed(123)
2025-04-11T04:23:18.7408824Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7408915Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7408919Z 
2025-04-11T04:23:18.7409073Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7409188Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7409192Z 
2025-04-11T04:23:18.7409268Z device = None
2025-04-11T04:23:18.7409272Z 
2025-04-11T04:23:18.7409395Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7409550Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7409619Z     
2025-04-11T04:23:18.7409698Z         Args:
2025-04-11T04:23:18.7409861Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7410028Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7410132Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7410204Z         """
2025-04-11T04:23:18.7410280Z         _lazy_init()
2025-04-11T04:23:18.7410372Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7410475Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7410578Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7410863Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7411002Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7411160Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7411168Z 
2025-04-11T04:23:18.7411404Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7411570Z _____________ test_flash_decoding[False-False-5-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.7411577Z 
2025-04-11T04:23:18.7411726Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7411887Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7411978Z use_new_kcache_layout = False
2025-04-11T04:23:18.7411982Z 
2025-04-11T04:23:18.7412180Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7412286Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7412400Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7412632Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7412747Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7412860Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7412998Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7413100Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7413236Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7413384Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7413469Z     def test_flash_decoding(
2025-04-11T04:23:18.7413545Z         bsz: int,
2025-04-11T04:23:18.7413626Z         block_size: int,
2025-04-11T04:23:18.7413718Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7413800Z         num_attn_heads: int,
2025-04-11T04:23:18.7413885Z         kv_group_num: int,
2025-04-11T04:23:18.7413969Z         same_context_len: bool,
2025-04-11T04:23:18.7414047Z         q_len: int,
2025-04-11T04:23:18.7414134Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7414219Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7414378Z     ):
2025-04-11T04:23:18.7414494Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7414687Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7414873Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7415043Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7415210Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7415366Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7415437Z     
2025-04-11T04:23:18.7415524Z         torch.manual_seed(123)
2025-04-11T04:23:18.7415614Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7415708Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7415712Z 
2025-04-11T04:23:18.7415873Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7415989Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7415993Z 
2025-04-11T04:23:18.7416068Z device = None
2025-04-11T04:23:18.7416072Z 
2025-04-11T04:23:18.7416194Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7416344Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7416412Z     
2025-04-11T04:23:18.7416488Z         Args:
2025-04-11T04:23:18.7416657Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7416825Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7416930Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7417007Z         """
2025-04-11T04:23:18.7417083Z         _lazy_init()
2025-04-11T04:23:18.7417177Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7417286Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7417392Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7417678Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7417813Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7417974Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7417978Z 
2025-04-11T04:23:18.7418222Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7418391Z ____________ test_flash_decoding[False-False-5-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.7418398Z 
2025-04-11T04:23:18.7418552Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7418800Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7418893Z use_new_kcache_layout = False
2025-04-11T04:23:18.7418898Z 
2025-04-11T04:23:18.7419096Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7419204Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7419320Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7419458Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7419571Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7419684Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7419822Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7419924Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7420062Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7420214Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7420306Z     def test_flash_decoding(
2025-04-11T04:23:18.7420470Z         bsz: int,
2025-04-11T04:23:18.7420551Z         block_size: int,
2025-04-11T04:23:18.7420649Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7420732Z         num_attn_heads: int,
2025-04-11T04:23:18.7420820Z         kv_group_num: int,
2025-04-11T04:23:18.7420904Z         same_context_len: bool,
2025-04-11T04:23:18.7420979Z         q_len: int,
2025-04-11T04:23:18.7421069Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7421156Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7421229Z     ):
2025-04-11T04:23:18.7421339Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7421529Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7421712Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7421886Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7422055Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7422210Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7422282Z     
2025-04-11T04:23:18.7422367Z         torch.manual_seed(123)
2025-04-11T04:23:18.7422454Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7422546Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7422550Z 
2025-04-11T04:23:18.7422702Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7422815Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7422819Z 
2025-04-11T04:23:18.7422893Z device = None
2025-04-11T04:23:18.7422898Z 
2025-04-11T04:23:18.7423016Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7423168Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7423240Z     
2025-04-11T04:23:18.7423316Z         Args:
2025-04-11T04:23:18.7423483Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7423648Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7423751Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7423824Z         """
2025-04-11T04:23:18.7423899Z         _lazy_init()
2025-04-11T04:23:18.7423991Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7424094Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7424195Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7424481Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7424702Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7424860Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7424868Z 
2025-04-11T04:23:18.7425108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7425274Z _____________ test_flash_decoding[False-False-5-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.7425281Z 
2025-04-11T04:23:18.7425430Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7425590Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7425682Z use_new_kcache_layout = False
2025-04-11T04:23:18.7425686Z 
2025-04-11T04:23:18.7425884Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7425990Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7426106Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7426250Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7426364Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7426567Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7426705Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7426806Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7426942Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7427088Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7427175Z     def test_flash_decoding(
2025-04-11T04:23:18.7427247Z         bsz: int,
2025-04-11T04:23:18.7427328Z         block_size: int,
2025-04-11T04:23:18.7427420Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7427500Z         num_attn_heads: int,
2025-04-11T04:23:18.7427583Z         kv_group_num: int,
2025-04-11T04:23:18.7427666Z         same_context_len: bool,
2025-04-11T04:23:18.7427743Z         q_len: int,
2025-04-11T04:23:18.7427831Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7427917Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7427993Z     ):
2025-04-11T04:23:18.7428101Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7428296Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7428510Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7428683Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7428851Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7429007Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7429081Z     
2025-04-11T04:23:18.7429165Z         torch.manual_seed(123)
2025-04-11T04:23:18.7429255Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7429346Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7429350Z 
2025-04-11T04:23:18.7429502Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7429621Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7429625Z 
2025-04-11T04:23:18.7429700Z device = None
2025-04-11T04:23:18.7429704Z 
2025-04-11T04:23:18.7429824Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7429974Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7430045Z     
2025-04-11T04:23:18.7430116Z         Args:
2025-04-11T04:23:18.7430280Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7430449Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7430553Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7430720Z         """
2025-04-11T04:23:18.7430797Z         _lazy_init()
2025-04-11T04:23:18.7430892Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7431000Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7431104Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7431389Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7431522Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7431682Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7431686Z 
2025-04-11T04:23:18.7431924Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7432090Z _____________ test_flash_decoding[False-False-5-True-4-16-8-16-16] _____________
2025-04-11T04:23:18.7432094Z 
2025-04-11T04:23:18.7432242Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7432406Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7432601Z use_new_kcache_layout = False
2025-04-11T04:23:18.7432605Z 
2025-04-11T04:23:18.7432805Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7432913Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7433026Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7433167Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7434969Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7435083Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7435225Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7435330Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7435471Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7435626Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7435711Z     def test_flash_decoding(
2025-04-11T04:23:18.7435792Z         bsz: int,
2025-04-11T04:23:18.7435875Z         block_size: int,
2025-04-11T04:23:18.7435967Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7436049Z         num_attn_heads: int,
2025-04-11T04:23:18.7436131Z         kv_group_num: int,
2025-04-11T04:23:18.7436220Z         same_context_len: bool,
2025-04-11T04:23:18.7436294Z         q_len: int,
2025-04-11T04:23:18.7436382Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7436498Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7436568Z     ):
2025-04-11T04:23:18.7436683Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7436874Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7437055Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7437226Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7437392Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7437547Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7437616Z     
2025-04-11T04:23:18.7437705Z         torch.manual_seed(123)
2025-04-11T04:23:18.7437792Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7437886Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7437890Z 
2025-04-11T04:23:18.7438046Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7438159Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7438163Z 
2025-04-11T04:23:18.7438239Z device = None
2025-04-11T04:23:18.7438242Z 
2025-04-11T04:23:18.7438357Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7438602Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7438670Z     
2025-04-11T04:23:18.7438747Z         Args:
2025-04-11T04:23:18.7438912Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7439080Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7439185Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7439256Z         """
2025-04-11T04:23:18.7439335Z         _lazy_init()
2025-04-11T04:23:18.7439427Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7439531Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7439636Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7439922Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7440063Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7440221Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7440292Z 
2025-04-11T04:23:18.7440534Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7440701Z _____________ test_flash_decoding[False-False-5-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.7440705Z 
2025-04-11T04:23:18.7440856Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7441019Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7441192Z use_new_kcache_layout = False
2025-04-11T04:23:18.7441197Z 
2025-04-11T04:23:18.7441396Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7441501Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7441616Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7441757Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7441875Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7441990Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7442131Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7442232Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7442366Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7442518Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7442604Z     def test_flash_decoding(
2025-04-11T04:23:18.7442682Z         bsz: int,
2025-04-11T04:23:18.7442762Z         block_size: int,
2025-04-11T04:23:18.7442851Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7442933Z         num_attn_heads: int,
2025-04-11T04:23:18.7443015Z         kv_group_num: int,
2025-04-11T04:23:18.7443102Z         same_context_len: bool,
2025-04-11T04:23:18.7443178Z         q_len: int,
2025-04-11T04:23:18.7443264Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7443349Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7443422Z     ):
2025-04-11T04:23:18.7443535Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7443728Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7443913Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7444082Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7444249Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7444406Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7444475Z     
2025-04-11T04:23:18.7444563Z         torch.manual_seed(123)
2025-04-11T04:23:18.7444828Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7444922Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7444926Z 
2025-04-11T04:23:18.7445082Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7445202Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7445207Z 
2025-04-11T04:23:18.7445285Z device = None
2025-04-11T04:23:18.7445289Z 
2025-04-11T04:23:18.7445412Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7445562Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7445631Z     
2025-04-11T04:23:18.7445709Z         Args:
2025-04-11T04:23:18.7445874Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7446042Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7446145Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7446219Z         """
2025-04-11T04:23:18.7446299Z         _lazy_init()
2025-04-11T04:23:18.7446392Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7446546Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7446650Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7446936Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7447072Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7447227Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7447290Z 
2025-04-11T04:23:18.7447533Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7447701Z _____________ test_flash_decoding[False-False-5-True-4-16-8-32-16] _____________
2025-04-11T04:23:18.7447705Z 
2025-04-11T04:23:18.7447861Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7448026Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7448119Z use_new_kcache_layout = False
2025-04-11T04:23:18.7448124Z 
2025-04-11T04:23:18.7448323Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7448427Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7448542Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7448679Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7448796Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7448910Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7449047Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7449151Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7449290Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7449443Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7449529Z     def test_flash_decoding(
2025-04-11T04:23:18.7449609Z         bsz: int,
2025-04-11T04:23:18.7449688Z         block_size: int,
2025-04-11T04:23:18.7449779Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7449861Z         num_attn_heads: int,
2025-04-11T04:23:18.7449943Z         kv_group_num: int,
2025-04-11T04:23:18.7450031Z         same_context_len: bool,
2025-04-11T04:23:18.7450105Z         q_len: int,
2025-04-11T04:23:18.7450191Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7450280Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7450350Z     ):
2025-04-11T04:23:18.7450461Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7450651Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7450837Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7451090Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7451257Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7451413Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7451488Z     
2025-04-11T04:23:18.7451573Z         torch.manual_seed(123)
2025-04-11T04:23:18.7451662Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7451756Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7451760Z 
2025-04-11T04:23:18.7451916Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7452030Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7452034Z 
2025-04-11T04:23:18.7452108Z device = None
2025-04-11T04:23:18.7452112Z 
2025-04-11T04:23:18.7452230Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7452380Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7452449Z     
2025-04-11T04:23:18.7452577Z         Args:
2025-04-11T04:23:18.7452743Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7452912Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7453018Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7453091Z         """
2025-04-11T04:23:18.7453170Z         _lazy_init()
2025-04-11T04:23:18.7453263Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7453418Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7453521Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7453813Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7453947Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7454106Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7454115Z 
2025-04-11T04:23:18.7454353Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7454518Z _____________ test_flash_decoding[False-False-5-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.7454522Z 
2025-04-11T04:23:18.7454675Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7454838Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7454930Z use_new_kcache_layout = False
2025-04-11T04:23:18.7454934Z 
2025-04-11T04:23:18.7455132Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7455238Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7455353Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7455494Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7455613Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7455729Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7455873Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7455975Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7456115Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7456262Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7456349Z     def test_flash_decoding(
2025-04-11T04:23:18.7456426Z         bsz: int,
2025-04-11T04:23:18.7456507Z         block_size: int,
2025-04-11T04:23:18.7456598Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7456679Z         num_attn_heads: int,
2025-04-11T04:23:18.7456760Z         kv_group_num: int,
2025-04-11T04:23:18.7456847Z         same_context_len: bool,
2025-04-11T04:23:18.7457027Z         q_len: int,
2025-04-11T04:23:18.7457115Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7457202Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7457275Z     ):
2025-04-11T04:23:18.7457386Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7457576Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7457757Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7457923Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7458088Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7458244Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7458317Z     
2025-04-11T04:23:18.7458403Z         torch.manual_seed(123)
2025-04-11T04:23:18.7458494Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7458587Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7458591Z 
2025-04-11T04:23:18.7458743Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7458907Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7458911Z 
2025-04-11T04:23:18.7458987Z device = None
2025-04-11T04:23:18.7458991Z 
2025-04-11T04:23:18.7459114Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7459260Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7459329Z     
2025-04-11T04:23:18.7459443Z         Args:
2025-04-11T04:23:18.7459608Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7459773Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7459876Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7459952Z         """
2025-04-11T04:23:18.7460028Z         _lazy_init()
2025-04-11T04:23:18.7460120Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7460226Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7460330Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7460612Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7460745Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7460899Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7460908Z 
2025-04-11T04:23:18.7461146Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7461315Z ____________ test_flash_decoding[False-False-5-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.7461319Z 
2025-04-11T04:23:18.7461473Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7461639Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7461732Z use_new_kcache_layout = False
2025-04-11T04:23:18.7461736Z 
2025-04-11T04:23:18.7461934Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7462039Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7462157Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7462293Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7462409Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7462521Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7462658Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7462759Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7462899Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7463134Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7463220Z     def test_flash_decoding(
2025-04-11T04:23:18.7463302Z         bsz: int,
2025-04-11T04:23:18.7463382Z         block_size: int,
2025-04-11T04:23:18.7463473Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7463554Z         num_attn_heads: int,
2025-04-11T04:23:18.7463640Z         kv_group_num: int,
2025-04-11T04:23:18.7463724Z         same_context_len: bool,
2025-04-11T04:23:18.7463798Z         q_len: int,
2025-04-11T04:23:18.7463885Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7463970Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7464046Z     ):
2025-04-11T04:23:18.7464155Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7464344Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7464531Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7464702Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7464919Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7465074Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7465148Z     
2025-04-11T04:23:18.7465234Z         torch.manual_seed(123)
2025-04-11T04:23:18.7465322Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7465413Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7465418Z 
2025-04-11T04:23:18.7465624Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7465736Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7465741Z 
2025-04-11T04:23:18.7465817Z device = None
2025-04-11T04:23:18.7465821Z 
2025-04-11T04:23:18.7465940Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7466092Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7466160Z     
2025-04-11T04:23:18.7466237Z         Args:
2025-04-11T04:23:18.7466402Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7466566Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7466672Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7466748Z         """
2025-04-11T04:23:18.7466824Z         _lazy_init()
2025-04-11T04:23:18.7466916Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7467022Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7467124Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7467409Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7467542Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7467702Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7467708Z 
2025-04-11T04:23:18.7467944Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7468109Z _____________ test_flash_decoding[False-False-5-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.7468117Z 
2025-04-11T04:23:18.7468266Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7468459Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7468555Z use_new_kcache_layout = False
2025-04-11T04:23:18.7468560Z 
2025-04-11T04:23:18.7468758Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7468865Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7468984Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7469224Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7469340Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7469454Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7469594Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7469697Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7469836Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7469986Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7470074Z     def test_flash_decoding(
2025-04-11T04:23:18.7470152Z         bsz: int,
2025-04-11T04:23:18.7470233Z         block_size: int,
2025-04-11T04:23:18.7470323Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7470405Z         num_attn_heads: int,
2025-04-11T04:23:18.7470489Z         kv_group_num: int,
2025-04-11T04:23:18.7470573Z         same_context_len: bool,
2025-04-11T04:23:18.7470650Z         q_len: int,
2025-04-11T04:23:18.7470738Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7470824Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7470950Z     ):
2025-04-11T04:23:18.7471060Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7471255Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7471441Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7471614Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7471836Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7471991Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7472064Z     
2025-04-11T04:23:18.7472150Z         torch.manual_seed(123)
2025-04-11T04:23:18.7472238Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7472335Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7472339Z 
2025-04-11T04:23:18.7472493Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7472612Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7472616Z 
2025-04-11T04:23:18.7472690Z device = None
2025-04-11T04:23:18.7472694Z 
2025-04-11T04:23:18.7472812Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7472962Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7473032Z     
2025-04-11T04:23:18.7473105Z         Args:
2025-04-11T04:23:18.7473269Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7473438Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7473542Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7473618Z         """
2025-04-11T04:23:18.7473694Z         _lazy_init()
2025-04-11T04:23:18.7473786Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7473891Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7473996Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7474279Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7474413Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7474571Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7474577Z 
2025-04-11T04:23:18.7474813Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7474979Z ____________ test_flash_decoding[False-False-5-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.7474987Z 
2025-04-11T04:23:18.7475138Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7475386Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7475479Z use_new_kcache_layout = False
2025-04-11T04:23:18.7475483Z 
2025-04-11T04:23:18.7475681Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7475788Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7475903Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7476045Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7476159Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7476273Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7476414Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7476516Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7476652Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7476803Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7476890Z     def test_flash_decoding(
2025-04-11T04:23:18.7477015Z         bsz: int,
2025-04-11T04:23:18.7477096Z         block_size: int,
2025-04-11T04:23:18.7477188Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7477270Z         num_attn_heads: int,
2025-04-11T04:23:18.7477355Z         kv_group_num: int,
2025-04-11T04:23:18.7477437Z         same_context_len: bool,
2025-04-11T04:23:18.7477511Z         q_len: int,
2025-04-11T04:23:18.7477598Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7477683Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7477828Z     ):
2025-04-11T04:23:18.7477940Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7478136Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7478323Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7478497Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7478669Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7478824Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7478898Z     
2025-04-11T04:23:18.7478983Z         torch.manual_seed(123)
2025-04-11T04:23:18.7479069Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7479159Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7479164Z 
2025-04-11T04:23:18.7479315Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7479432Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7479436Z 
2025-04-11T04:23:18.7479511Z device = None
2025-04-11T04:23:18.7479515Z 
2025-04-11T04:23:18.7479632Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7479783Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7479853Z     
2025-04-11T04:23:18.7479924Z         Args:
2025-04-11T04:23:18.7480090Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7480257Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7480360Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7480436Z         """
2025-04-11T04:23:18.7480511Z         _lazy_init()
2025-04-11T04:23:18.7480606Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7480713Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7480818Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7481109Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7481243Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7481489Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7481495Z 
2025-04-11T04:23:18.7481729Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7481898Z _____________ test_flash_decoding[False-False-5-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.7481902Z 
2025-04-11T04:23:18.7482050Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7482210Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7482303Z use_new_kcache_layout = False
2025-04-11T04:23:18.7482308Z 
2025-04-11T04:23:18.7482508Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7482614Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7482730Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7482872Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7482987Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7483150Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7483289Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7483389Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7483527Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7483675Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7483761Z     def test_flash_decoding(
2025-04-11T04:23:18.7483888Z         bsz: int,
2025-04-11T04:23:18.7483972Z         block_size: int,
2025-04-11T04:23:18.7484065Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7484146Z         num_attn_heads: int,
2025-04-11T04:23:18.7484231Z         kv_group_num: int,
2025-04-11T04:23:18.7484315Z         same_context_len: bool,
2025-04-11T04:23:18.7484393Z         q_len: int,
2025-04-11T04:23:18.7484483Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7484568Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7484642Z     ):
2025-04-11T04:23:18.7484753Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7484943Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7485129Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7485299Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7485468Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7485625Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7485701Z     
2025-04-11T04:23:18.7485787Z         torch.manual_seed(123)
2025-04-11T04:23:18.7485880Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7485974Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7485978Z 
2025-04-11T04:23:18.7486131Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7486251Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7486255Z 
2025-04-11T04:23:18.7486329Z device = None
2025-04-11T04:23:18.7486334Z 
2025-04-11T04:23:18.7486453Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7486602Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7486675Z     
2025-04-11T04:23:18.7486748Z         Args:
2025-04-11T04:23:18.7486914Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7487082Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7487187Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7487348Z         """
2025-04-11T04:23:18.7487427Z         _lazy_init()
2025-04-11T04:23:18.7487521Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7487630Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7487733Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7488019Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7488156Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7488318Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7488324Z 
2025-04-11T04:23:18.7488563Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7488733Z ____________ test_flash_decoding[False-False-5-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.7488738Z 
2025-04-11T04:23:18.7488887Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7489054Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7489200Z use_new_kcache_layout = False
2025-04-11T04:23:18.7489205Z 
2025-04-11T04:23:18.7489402Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7489509Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7489626Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7489769Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7489885Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7490048Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7490192Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7490293Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7490430Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7490581Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7490668Z     def test_flash_decoding(
2025-04-11T04:23:18.7490744Z         bsz: int,
2025-04-11T04:23:18.7490824Z         block_size: int,
2025-04-11T04:23:18.7490915Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7490995Z         num_attn_heads: int,
2025-04-11T04:23:18.7491078Z         kv_group_num: int,
2025-04-11T04:23:18.7491162Z         same_context_len: bool,
2025-04-11T04:23:18.7491234Z         q_len: int,
2025-04-11T04:23:18.7491325Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7491410Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7491485Z     ):
2025-04-11T04:23:18.7491592Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7491785Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7491967Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7492137Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7492302Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7492458Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7492532Z     
2025-04-11T04:23:18.7492615Z         torch.manual_seed(123)
2025-04-11T04:23:18.7492705Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7492792Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7492796Z 
2025-04-11T04:23:18.7492948Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7493064Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7493068Z 
2025-04-11T04:23:18.7493143Z device = None
2025-04-11T04:23:18.7493147Z 
2025-04-11T04:23:18.7493265Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7493504Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7493576Z     
2025-04-11T04:23:18.7493648Z         Args:
2025-04-11T04:23:18.7493814Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7493981Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7494086Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7494161Z         """
2025-04-11T04:23:18.7494237Z         _lazy_init()
2025-04-11T04:23:18.7494332Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7494435Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7494538Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7494828Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7494961Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7495123Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7495181Z 
2025-04-11T04:23:18.7495423Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7495591Z _____________ test_flash_decoding[False-False-5-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.7495595Z 
2025-04-11T04:23:18.7495744Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7495909Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7496046Z use_new_kcache_layout = False
2025-04-11T04:23:18.7496050Z 
2025-04-11T04:23:18.7496247Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7496352Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7496466Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7496610Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7496724Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7496840Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7496975Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7497075Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7497215Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7497362Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7497450Z     def test_flash_decoding(
2025-04-11T04:23:18.7497526Z         bsz: int,
2025-04-11T04:23:18.7497606Z         block_size: int,
2025-04-11T04:23:18.7497698Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7497779Z         num_attn_heads: int,
2025-04-11T04:23:18.7497862Z         kv_group_num: int,
2025-04-11T04:23:18.7497947Z         same_context_len: bool,
2025-04-11T04:23:18.7498029Z         q_len: int,
2025-04-11T04:23:18.7498113Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7498200Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7498276Z     ):
2025-04-11T04:23:18.7498388Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7498580Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7498761Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7498930Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7499094Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7499249Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7499322Z     
2025-04-11T04:23:18.7499406Z         torch.manual_seed(123)
2025-04-11T04:23:18.7499498Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7499690Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7499694Z 
2025-04-11T04:23:18.7499849Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7499966Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7499970Z 
2025-04-11T04:23:18.7500045Z device = None
2025-04-11T04:23:18.7500050Z 
2025-04-11T04:23:18.7500173Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7500320Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7500391Z     
2025-04-11T04:23:18.7500469Z         Args:
2025-04-11T04:23:18.7500637Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7500807Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7500916Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7500996Z         """
2025-04-11T04:23:18.7501072Z         _lazy_init()
2025-04-11T04:23:18.7501170Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7501269Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7501445Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7501735Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7501869Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7502028Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7502081Z 
2025-04-11T04:23:18.7502324Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7502495Z ____________ test_flash_decoding[False-False-5-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.7502499Z 
2025-04-11T04:23:18.7502646Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7502817Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7502907Z use_new_kcache_layout = False
2025-04-11T04:23:18.7502912Z 
2025-04-11T04:23:18.7503109Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7503215Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7503331Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7503474Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7503589Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7503705Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7503839Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7503940Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7504077Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7504230Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7504321Z     def test_flash_decoding(
2025-04-11T04:23:18.7504398Z         bsz: int,
2025-04-11T04:23:18.7504482Z         block_size: int,
2025-04-11T04:23:18.7504569Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7504650Z         num_attn_heads: int,
2025-04-11T04:23:18.7504736Z         kv_group_num: int,
2025-04-11T04:23:18.7504819Z         same_context_len: bool,
2025-04-11T04:23:18.7504895Z         q_len: int,
2025-04-11T04:23:18.7504978Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7505064Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7505139Z     ):
2025-04-11T04:23:18.7505248Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7505442Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7505619Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7505876Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7506045Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7506198Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7506272Z     
2025-04-11T04:23:18.7506357Z         torch.manual_seed(123)
2025-04-11T04:23:18.7506448Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7506537Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7506541Z 
2025-04-11T04:23:18.7506695Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7506810Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7506814Z 
2025-04-11T04:23:18.7506888Z device = None
2025-04-11T04:23:18.7506892Z 
2025-04-11T04:23:18.7507011Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7507163Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7507233Z     
2025-04-11T04:23:18.7507306Z         Args:
2025-04-11T04:23:18.7507524Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7507692Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7507796Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7507873Z         """
2025-04-11T04:23:18.7507949Z         _lazy_init()
2025-04-11T04:23:18.7508045Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7508200Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7508304Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7508633Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7508769Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7508932Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7508939Z 
2025-04-11T04:23:18.7509178Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7509350Z ____________ test_flash_decoding[False-False-5-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.7509354Z 
2025-04-11T04:23:18.7509504Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7509669Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7509757Z use_new_kcache_layout = False
2025-04-11T04:23:18.7509761Z 
2025-04-11T04:23:18.7509961Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7510067Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7510183Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7510327Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7510441Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7510558Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7510693Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7510793Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7510930Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7511078Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7511168Z     def test_flash_decoding(
2025-04-11T04:23:18.7511242Z         bsz: int,
2025-04-11T04:23:18.7511326Z         block_size: int,
2025-04-11T04:23:18.7511413Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7511494Z         num_attn_heads: int,
2025-04-11T04:23:18.7511580Z         kv_group_num: int,
2025-04-11T04:23:18.7511664Z         same_context_len: bool,
2025-04-11T04:23:18.7511833Z         q_len: int,
2025-04-11T04:23:18.7511917Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7512003Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7512078Z     ):
2025-04-11T04:23:18.7512188Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7512379Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7512560Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7512736Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7512904Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7513060Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7513134Z     
2025-04-11T04:23:18.7513217Z         torch.manual_seed(123)
2025-04-11T04:23:18.7513308Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7513400Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7513404Z 
2025-04-11T04:23:18.7513559Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7513724Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7513728Z 
2025-04-11T04:23:18.7513803Z device = None
2025-04-11T04:23:18.7513807Z 
2025-04-11T04:23:18.7513928Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7514078Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7514151Z     
2025-04-11T04:23:18.7514261Z         Args:
2025-04-11T04:23:18.7514430Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7514594Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7514698Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7514779Z         """
2025-04-11T04:23:18.7514855Z         _lazy_init()
2025-04-11T04:23:18.7514950Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7515049Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7515154Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7515439Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7515573Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7515732Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7515738Z 
2025-04-11T04:23:18.7515978Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7516150Z ____________ test_flash_decoding[False-False-5-False-1-16-16-16-16] ____________
2025-04-11T04:23:18.7516154Z 
2025-04-11T04:23:18.7516303Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7516470Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7516555Z use_new_kcache_layout = False
2025-04-11T04:23:18.7516562Z 
2025-04-11T04:23:18.7516763Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7516866Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7516983Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7517123Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7517237Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7517353Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7517488Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7517589Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7517725Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7517962Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7518051Z     def test_flash_decoding(
2025-04-11T04:23:18.7518127Z         bsz: int,
2025-04-11T04:23:18.7518212Z         block_size: int,
2025-04-11T04:23:18.7518299Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7518378Z         num_attn_heads: int,
2025-04-11T04:23:18.7518464Z         kv_group_num: int,
2025-04-11T04:23:18.7518548Z         same_context_len: bool,
2025-04-11T04:23:18.7518625Z         q_len: int,
2025-04-11T04:23:18.7518708Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7518793Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7518866Z     ):
2025-04-11T04:23:18.7518975Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7519169Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7519349Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7519524Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7519686Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7519895Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7519968Z     
2025-04-11T04:23:18.7520051Z         torch.manual_seed(123)
2025-04-11T04:23:18.7520143Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7520233Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7520237Z 
2025-04-11T04:23:18.7520394Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7520553Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7520558Z 
2025-04-11T04:23:18.7520632Z device = None
2025-04-11T04:23:18.7520644Z 
2025-04-11T04:23:18.7520760Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7520913Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7520985Z     
2025-04-11T04:23:18.7521057Z         Args:
2025-04-11T04:23:18.7521227Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7521393Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7521497Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7521571Z         """
2025-04-11T04:23:18.7521647Z         _lazy_init()
2025-04-11T04:23:18.7521741Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7521842Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7521946Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7522229Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7522362Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7522527Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7522534Z 
2025-04-11T04:23:18.7522774Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7522947Z ____________ test_flash_decoding[False-False-5-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.7522952Z 
2025-04-11T04:23:18.7523103Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7523271Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7523361Z use_new_kcache_layout = False
2025-04-11T04:23:18.7523365Z 
2025-04-11T04:23:18.7523571Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7523674Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7523792Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7524034Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7524149Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7524265Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7524398Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7524500Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7524633Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7524781Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7524871Z     def test_flash_decoding(
2025-04-11T04:23:18.7524947Z         bsz: int,
2025-04-11T04:23:18.7525032Z         block_size: int,
2025-04-11T04:23:18.7525121Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7525202Z         num_attn_heads: int,
2025-04-11T04:23:18.7525289Z         kv_group_num: int,
2025-04-11T04:23:18.7525374Z         same_context_len: bool,
2025-04-11T04:23:18.7525454Z         q_len: int,
2025-04-11T04:23:18.7525538Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7525624Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7525747Z     ):
2025-04-11T04:23:18.7525858Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7526051Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7526231Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7526404Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7526619Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7526776Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7526850Z     
2025-04-11T04:23:18.7526936Z         torch.manual_seed(123)
2025-04-11T04:23:18.7527024Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7527117Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7527121Z 
2025-04-11T04:23:18.7527276Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7527388Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7527392Z 
2025-04-11T04:23:18.7527467Z device = None
2025-04-11T04:23:18.7527474Z 
2025-04-11T04:23:18.7527590Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7527739Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7527811Z     
2025-04-11T04:23:18.7527885Z         Args:
2025-04-11T04:23:18.7528053Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7528217Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7528319Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7528397Z         """
2025-04-11T04:23:18.7528472Z         _lazy_init()
2025-04-11T04:23:18.7528568Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7528668Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7528776Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7529062Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7529197Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7529353Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7529359Z 
2025-04-11T04:23:18.7529596Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7529771Z ____________ test_flash_decoding[False-False-5-False-1-16-16-32-16] ____________
2025-04-11T04:23:18.7529776Z 
2025-04-11T04:23:18.7529925Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7530182Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7530269Z use_new_kcache_layout = False
2025-04-11T04:23:18.7530276Z 
2025-04-11T04:23:18.7530478Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7530580Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7530695Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7530838Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7530953Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7531070Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7531205Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7531310Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7531443Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7531596Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7531685Z     def test_flash_decoding(
2025-04-11T04:23:18.7531811Z         bsz: int,
2025-04-11T04:23:18.7531896Z         block_size: int,
2025-04-11T04:23:18.7531985Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7532065Z         num_attn_heads: int,
2025-04-11T04:23:18.7532152Z         kv_group_num: int,
2025-04-11T04:23:18.7532236Z         same_context_len: bool,
2025-04-11T04:23:18.7532316Z         q_len: int,
2025-04-11T04:23:18.7532399Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7532489Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7532611Z     ):
2025-04-11T04:23:18.7532723Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7532919Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7533101Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7533279Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7533441Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7533603Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7533672Z     
2025-04-11T04:23:18.7533759Z         torch.manual_seed(123)
2025-04-11T04:23:18.7533852Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7533942Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7533946Z 
2025-04-11T04:23:18.7534106Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7534221Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7534225Z 
2025-04-11T04:23:18.7534303Z device = None
2025-04-11T04:23:18.7534307Z 
2025-04-11T04:23:18.7534424Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7534575Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7534648Z     
2025-04-11T04:23:18.7534720Z         Args:
2025-04-11T04:23:18.7534890Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7535055Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7535162Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7535233Z         """
2025-04-11T04:23:18.7535307Z         _lazy_init()
2025-04-11T04:23:18.7535404Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7535506Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7535612Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7535894Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7536028Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7536273Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7536278Z 
2025-04-11T04:23:18.7536517Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7536690Z _____________ test_flash_decoding[False-False-5-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.7536694Z 
2025-04-11T04:23:18.7536844Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7537011Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7537101Z use_new_kcache_layout = False
2025-04-11T04:23:18.7537105Z 
2025-04-11T04:23:18.7537306Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7537411Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7537526Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7537668Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7537780Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7537950Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7538086Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7538190Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7538326Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7538476Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7538563Z     def test_flash_decoding(
2025-04-11T04:23:18.7538777Z         bsz: int,
2025-04-11T04:23:18.7538862Z         block_size: int,
2025-04-11T04:23:18.7538950Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7539034Z         num_attn_heads: int,
2025-04-11T04:23:18.7539116Z         kv_group_num: int,
2025-04-11T04:23:18.7539199Z         same_context_len: bool,
2025-04-11T04:23:18.7539282Z         q_len: int,
2025-04-11T04:23:18.7539365Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7539454Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7539525Z     ):
2025-04-11T04:23:18.7539636Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7539827Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7540006Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7540179Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7540340Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7540499Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7540568Z     
2025-04-11T04:23:18.7540651Z         torch.manual_seed(123)
2025-04-11T04:23:18.7540743Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7540837Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7540842Z 
2025-04-11T04:23:18.7540999Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7541110Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7541115Z 
2025-04-11T04:23:18.7541192Z device = None
2025-04-11T04:23:18.7541196Z 
2025-04-11T04:23:18.7541309Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7541458Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7541531Z     
2025-04-11T04:23:18.7541602Z         Args:
2025-04-11T04:23:18.7541772Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7541933Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7542041Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7542113Z         """
2025-04-11T04:23:18.7542276Z         _lazy_init()
2025-04-11T04:23:18.7542375Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7542476Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7542585Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7542868Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7543003Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7543164Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7543170Z 
2025-04-11T04:23:18.7543409Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7543581Z ____________ test_flash_decoding[False-False-5-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.7543586Z 
2025-04-11T04:23:18.7543735Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7543906Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7543994Z use_new_kcache_layout = False
2025-04-11T04:23:18.7544045Z 
2025-04-11T04:23:18.7544247Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7544351Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7544470Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7544610Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7544724Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7544893Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7545030Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7545139Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7545277Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7545432Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7545527Z     def test_flash_decoding(
2025-04-11T04:23:18.7545604Z         bsz: int,
2025-04-11T04:23:18.7545690Z         block_size: int,
2025-04-11T04:23:18.7545777Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7545861Z         num_attn_heads: int,
2025-04-11T04:23:18.7545941Z         kv_group_num: int,
2025-04-11T04:23:18.7546024Z         same_context_len: bool,
2025-04-11T04:23:18.7546104Z         q_len: int,
2025-04-11T04:23:18.7546187Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7546281Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7546353Z     ):
2025-04-11T04:23:18.7546460Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7546654Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7546834Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7547012Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7547174Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7547336Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7547405Z     
2025-04-11T04:23:18.7547491Z         torch.manual_seed(123)
2025-04-11T04:23:18.7547583Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7547673Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7547677Z 
2025-04-11T04:23:18.7547836Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7547947Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7547951Z 
2025-04-11T04:23:18.7548027Z device = None
2025-04-11T04:23:18.7548031Z 
2025-04-11T04:23:18.7548145Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7548389Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7548496Z     
2025-04-11T04:23:18.7548569Z         Args:
2025-04-11T04:23:18.7548741Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7548904Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7549014Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7549085Z         """
2025-04-11T04:23:18.7549160Z         _lazy_init()
2025-04-11T04:23:18.7549258Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7549360Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7549472Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7549761Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7549904Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7550065Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7550069Z 
2025-04-11T04:23:18.7550370Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7550545Z _____________ test_flash_decoding[False-False-5-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.7550549Z 
2025-04-11T04:23:18.7550701Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7550870Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7551013Z use_new_kcache_layout = False
2025-04-11T04:23:18.7551017Z 
2025-04-11T04:23:18.7551217Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7551323Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7551441Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7551580Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7551694Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7551811Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7551944Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7552048Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7552180Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7552331Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7552416Z     def test_flash_decoding(
2025-04-11T04:23:18.7552491Z         bsz: int,
2025-04-11T04:23:18.7552574Z         block_size: int,
2025-04-11T04:23:18.7552660Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7552744Z         num_attn_heads: int,
2025-04-11T04:23:18.7552824Z         kv_group_num: int,
2025-04-11T04:23:18.7552906Z         same_context_len: bool,
2025-04-11T04:23:18.7552987Z         q_len: int,
2025-04-11T04:23:18.7553071Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7553160Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7553232Z     ):
2025-04-11T04:23:18.7553340Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7553532Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7553711Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7553885Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7554050Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7554208Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7554277Z     
2025-04-11T04:23:18.7554364Z         torch.manual_seed(123)
2025-04-11T04:23:18.7554455Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7554642Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7554646Z 
2025-04-11T04:23:18.7554807Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7554920Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7554924Z 
2025-04-11T04:23:18.7555004Z device = None
2025-04-11T04:23:18.7555008Z 
2025-04-11T04:23:18.7555123Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7555278Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7555347Z     
2025-04-11T04:23:18.7555420Z         Args:
2025-04-11T04:23:18.7555592Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7555756Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7555864Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7555936Z         """
2025-04-11T04:23:18.7556017Z         _lazy_init()
2025-04-11T04:23:18.7556114Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7556213Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7556375Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7556657Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7556794Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7556948Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7557002Z 
2025-04-11T04:23:18.7557241Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7557414Z ____________ test_flash_decoding[False-False-5-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.7557417Z 
2025-04-11T04:23:18.7557565Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7557734Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7557821Z use_new_kcache_layout = False
2025-04-11T04:23:18.7557827Z 
2025-04-11T04:23:18.7558033Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7558137Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7558257Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7558397Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7558515Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7558635Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7558769Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7558874Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7559012Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7559169Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7559253Z     def test_flash_decoding(
2025-04-11T04:23:18.7559329Z         bsz: int,
2025-04-11T04:23:18.7559414Z         block_size: int,
2025-04-11T04:23:18.7559502Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7559586Z         num_attn_heads: int,
2025-04-11T04:23:18.7559667Z         kv_group_num: int,
2025-04-11T04:23:18.7559753Z         same_context_len: bool,
2025-04-11T04:23:18.7559831Z         q_len: int,
2025-04-11T04:23:18.7559916Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7560004Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7560078Z     ):
2025-04-11T04:23:18.7560186Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7560384Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7560562Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7560840Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7561004Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7561164Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7561234Z     
2025-04-11T04:23:18.7561324Z         torch.manual_seed(123)
2025-04-11T04:23:18.7561411Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7561502Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7561506Z 
2025-04-11T04:23:18.7561669Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7561780Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7561784Z 
2025-04-11T04:23:18.7561861Z device = None
2025-04-11T04:23:18.7561865Z 
2025-04-11T04:23:18.7561981Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7562139Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7562208Z     
2025-04-11T04:23:18.7562279Z         Args:
2025-04-11T04:23:18.7562500Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7562667Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7562775Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7562848Z         """
2025-04-11T04:23:18.7562924Z         _lazy_init()
2025-04-11T04:23:18.7563023Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7563173Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7563284Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7563564Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7563702Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7563860Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7563865Z 
2025-04-11T04:23:18.7564107Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7564276Z ____________ test_flash_decoding[False-False-5-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.7564280Z 
2025-04-11T04:23:18.7564431Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7564602Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7564693Z use_new_kcache_layout = False
2025-04-11T04:23:18.7564697Z 
2025-04-11T04:23:18.7564905Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7565009Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7565129Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7565271Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7565386Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7565504Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7565637Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7565743Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7565880Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7566032Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7566118Z     def test_flash_decoding(
2025-04-11T04:23:18.7566193Z         bsz: int,
2025-04-11T04:23:18.7566276Z         block_size: int,
2025-04-11T04:23:18.7566364Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7566447Z         num_attn_heads: int,
2025-04-11T04:23:18.7566528Z         kv_group_num: int,
2025-04-11T04:23:18.7566611Z         same_context_len: bool,
2025-04-11T04:23:18.7566777Z         q_len: int,
2025-04-11T04:23:18.7566860Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7566950Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7567023Z     ):
2025-04-11T04:23:18.7567134Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7567324Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7567503Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7567676Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7567839Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7568000Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7568068Z     
2025-04-11T04:23:18.7568158Z         torch.manual_seed(123)
2025-04-11T04:23:18.7568244Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7568336Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7568340Z 
2025-04-11T04:23:18.7568496Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7568657Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7568662Z 
2025-04-11T04:23:18.7568741Z device = None
2025-04-11T04:23:18.7568745Z 
2025-04-11T04:23:18.7568860Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7569014Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7569083Z     
2025-04-11T04:23:18.7569157Z         Args:
2025-04-11T04:23:18.7569392Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7569557Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7569666Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7569737Z         """
2025-04-11T04:23:18.7569820Z         _lazy_init()
2025-04-11T04:23:18.7569913Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7570012Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7570121Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7570402Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7570538Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7570695Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7570701Z 
2025-04-11T04:23:18.7570942Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7571112Z ____________ test_flash_decoding[False-False-5-False-4-16-16-16-16] ____________
2025-04-11T04:23:18.7571116Z 
2025-04-11T04:23:18.7571265Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7571433Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7571519Z use_new_kcache_layout = False
2025-04-11T04:23:18.7571526Z 
2025-04-11T04:23:18.7571727Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7571829Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7571947Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7572084Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7572201Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7572314Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7572450Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7572554Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7572685Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7572924Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7573010Z     def test_flash_decoding(
2025-04-11T04:23:18.7573085Z         bsz: int,
2025-04-11T04:23:18.7573170Z         block_size: int,
2025-04-11T04:23:18.7573258Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7573346Z         num_attn_heads: int,
2025-04-11T04:23:18.7573427Z         kv_group_num: int,
2025-04-11T04:23:18.7573515Z         same_context_len: bool,
2025-04-11T04:23:18.7573589Z         q_len: int,
2025-04-11T04:23:18.7573672Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7573763Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7573834Z     ):
2025-04-11T04:23:18.7573946Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7574135Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7574312Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7574489Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7574651Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7574862Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7574931Z     
2025-04-11T04:23:18.7575018Z         torch.manual_seed(123)
2025-04-11T04:23:18.7575105Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7575194Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7575198Z 
2025-04-11T04:23:18.7575355Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7575517Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7575521Z 
2025-04-11T04:23:18.7575599Z device = None
2025-04-11T04:23:18.7575603Z 
2025-04-11T04:23:18.7575718Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7575870Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7575944Z     
2025-04-11T04:23:18.7576016Z         Args:
2025-04-11T04:23:18.7576183Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7576345Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7576453Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7576524Z         """
2025-04-11T04:23:18.7576603Z         _lazy_init()
2025-04-11T04:23:18.7576695Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7576794Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7576903Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7577182Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7577322Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7577480Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7577484Z 
2025-04-11T04:23:18.7577727Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7577892Z ____________ test_flash_decoding[False-False-5-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.7577896Z 
2025-04-11T04:23:18.7578048Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7578210Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7578302Z use_new_kcache_layout = False
2025-04-11T04:23:18.7578306Z 
2025-04-11T04:23:18.7578507Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7578609Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7578727Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7578952Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7579070Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7579182Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7579317Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7579423Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7579556Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7579706Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7579792Z     def test_flash_decoding(
2025-04-11T04:23:18.7579868Z         bsz: int,
2025-04-11T04:23:18.7579951Z         block_size: int,
2025-04-11T04:23:18.7580039Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7580123Z         num_attn_heads: int,
2025-04-11T04:23:18.7580203Z         kv_group_num: int,
2025-04-11T04:23:18.7580290Z         same_context_len: bool,
2025-04-11T04:23:18.7580368Z         q_len: int,
2025-04-11T04:23:18.7580453Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7580544Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7580664Z     ):
2025-04-11T04:23:18.7580778Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7580966Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7581148Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7581320Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7581538Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7581699Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7581769Z     
2025-04-11T04:23:18.7581857Z         torch.manual_seed(123)
2025-04-11T04:23:18.7581945Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7582039Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7582043Z 
2025-04-11T04:23:18.7582202Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7582313Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7582317Z 
2025-04-11T04:23:18.7582397Z device = None
2025-04-11T04:23:18.7582400Z 
2025-04-11T04:23:18.7582516Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7582667Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7582736Z     
2025-04-11T04:23:18.7582808Z         Args:
2025-04-11T04:23:18.7582978Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7583141Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7583248Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7583320Z         """
2025-04-11T04:23:18.7583403Z         _lazy_init()
2025-04-11T04:23:18.7583495Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7583594Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7583703Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7583986Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7584125Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7584279Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7584286Z 
2025-04-11T04:23:18.7584527Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7584694Z ____________ test_flash_decoding[False-False-5-False-4-16-16-32-16] ____________
2025-04-11T04:23:18.7584698Z 
2025-04-11T04:23:18.7584851Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7585099Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7585188Z use_new_kcache_layout = False
2025-04-11T04:23:18.7585194Z 
2025-04-11T04:23:18.7585398Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7585500Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7585619Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7585754Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7585871Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7585984Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7586116Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7586221Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7586355Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7586512Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7586597Z     def test_flash_decoding(
2025-04-11T04:23:18.7586675Z         bsz: int,
2025-04-11T04:23:18.7586812Z         block_size: int,
2025-04-11T04:23:18.7586901Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7586987Z         num_attn_heads: int,
2025-04-11T04:23:18.7587068Z         kv_group_num: int,
2025-04-11T04:23:18.7587154Z         same_context_len: bool,
2025-04-11T04:23:18.7587229Z         q_len: int,
2025-04-11T04:23:18.7587311Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7587400Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7587522Z     ):
2025-04-11T04:23:18.7587633Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7587823Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7588001Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7588181Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7588344Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7588534Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7588604Z     
2025-04-11T04:23:18.7588694Z         torch.manual_seed(123)
2025-04-11T04:23:18.7588783Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7588872Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7588881Z 
2025-04-11T04:23:18.7589035Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7589147Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7589151Z 
2025-04-11T04:23:18.7589230Z device = None
2025-04-11T04:23:18.7589234Z 
2025-04-11T04:23:18.7589349Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7589500Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7589574Z     
2025-04-11T04:23:18.7589648Z         Args:
2025-04-11T04:23:18.7589815Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7589979Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7590087Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7590158Z         """
2025-04-11T04:23:18.7590237Z         _lazy_init()
2025-04-11T04:23:18.7590330Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7590430Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7590539Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7590821Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7590960Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7591214Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7591219Z 
2025-04-11T04:23:18.7591462Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7591617Z ________________ test_copy_kv_to_caches[True-1-True-16-16-16-7] ________________
2025-04-11T04:23:18.7591622Z 
2025-04-11T04:23:18.7591773Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7591930Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7591935Z 
2025-04-11T04:23:18.7592146Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7592255Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7592377Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7592516Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7592632Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7592776Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7592963Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7593114Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7593207Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7593280Z         bsz: int,
2025-04-11T04:23:18.7593363Z         block_size: int,
2025-04-11T04:23:18.7593451Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7593535Z         num_kv_heads: int,
2025-04-11T04:23:18.7593619Z         same_context_len: bool,
2025-04-11T04:23:18.7593753Z         n_tokens: int,
2025-04-11T04:23:18.7593846Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7593917Z     ):
2025-04-11T04:23:18.7594005Z         torch.manual_seed(123)
2025-04-11T04:23:18.7594093Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7594180Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7594187Z 
2025-04-11T04:23:18.7594341Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7594453Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7594460Z 
2025-04-11T04:23:18.7594538Z device = None
2025-04-11T04:23:18.7594542Z 
2025-04-11T04:23:18.7594656Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7594809Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7594878Z     
2025-04-11T04:23:18.7594949Z         Args:
2025-04-11T04:23:18.7595118Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7595284Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7595391Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7595462Z         """
2025-04-11T04:23:18.7595540Z         _lazy_init()
2025-04-11T04:23:18.7595636Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7595734Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7595844Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7596126Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7596261Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7596416Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7596420Z 
2025-04-11T04:23:18.7596660Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7596816Z _______________ test_copy_kv_to_caches[True-1-True-16-16-16-32] ________________
2025-04-11T04:23:18.7596820Z 
2025-04-11T04:23:18.7596972Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7597124Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7597213Z 
2025-04-11T04:23:18.7597415Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7597527Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7597649Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7597788Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7597899Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7598041Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7598148Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7598299Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7598390Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7598463Z         bsz: int,
2025-04-11T04:23:18.7598547Z         block_size: int,
2025-04-11T04:23:18.7598635Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7598723Z         num_kv_heads: int,
2025-04-11T04:23:18.7598810Z         same_context_len: bool,
2025-04-11T04:23:18.7598887Z         n_tokens: int,
2025-04-11T04:23:18.7599037Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7599110Z     ):
2025-04-11T04:23:18.7599198Z         torch.manual_seed(123)
2025-04-11T04:23:18.7599285Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7599373Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7599377Z 
2025-04-11T04:23:18.7599534Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7599644Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7599700Z 
2025-04-11T04:23:18.7599781Z device = None
2025-04-11T04:23:18.7599785Z 
2025-04-11T04:23:18.7599902Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7600054Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7600124Z     
2025-04-11T04:23:18.7600203Z         Args:
2025-04-11T04:23:18.7600373Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7600540Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7600647Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7600719Z         """
2025-04-11T04:23:18.7600796Z         _lazy_init()
2025-04-11T04:23:18.7600891Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7600991Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7601097Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7601379Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7601515Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7601671Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7601679Z 
2025-04-11T04:23:18.7601917Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7602070Z ________________ test_copy_kv_to_caches[True-1-True-16-16-32-7] ________________
2025-04-11T04:23:18.7602074Z 
2025-04-11T04:23:18.7602221Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7602375Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7602379Z 
2025-04-11T04:23:18.7602576Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7602686Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7602808Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7602943Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7603054Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7603280Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7603393Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7603545Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7603637Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7603711Z         bsz: int,
2025-04-11T04:23:18.7603796Z         block_size: int,
2025-04-11T04:23:18.7603885Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7603967Z         num_kv_heads: int,
2025-04-11T04:23:18.7604055Z         same_context_len: bool,
2025-04-11T04:23:18.7604133Z         n_tokens: int,
2025-04-11T04:23:18.7604228Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7604298Z     ):
2025-04-11T04:23:18.7604383Z         torch.manual_seed(123)
2025-04-11T04:23:18.7604475Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7604565Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7604569Z 
2025-04-11T04:23:18.7604724Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7604838Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7604842Z 
2025-04-11T04:23:18.7604972Z device = None
2025-04-11T04:23:18.7604977Z 
2025-04-11T04:23:18.7605095Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7605245Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7605320Z     
2025-04-11T04:23:18.7605395Z         Args:
2025-04-11T04:23:18.7605568Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7605734Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7605890Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7605961Z         """
2025-04-11T04:23:18.7606037Z         _lazy_init()
2025-04-11T04:23:18.7606137Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7606235Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7606346Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7606626Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7606767Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7606924Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7606928Z 
2025-04-11T04:23:18.7607166Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7607325Z _______________ test_copy_kv_to_caches[True-1-True-16-16-32-32] ________________
2025-04-11T04:23:18.7607331Z 
2025-04-11T04:23:18.7607479Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7607633Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7607638Z 
2025-04-11T04:23:18.7607841Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7607949Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7608074Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7608213Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7608324Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7608462Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7608573Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7608721Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7608814Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7608887Z         bsz: int,
2025-04-11T04:23:18.7608970Z         block_size: int,
2025-04-11T04:23:18.7609057Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7609138Z         num_kv_heads: int,
2025-04-11T04:23:18.7609223Z         same_context_len: bool,
2025-04-11T04:23:18.7609388Z         n_tokens: int,
2025-04-11T04:23:18.7609480Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7609550Z     ):
2025-04-11T04:23:18.7609638Z         torch.manual_seed(123)
2025-04-11T04:23:18.7609729Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7609817Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7609821Z 
2025-04-11T04:23:18.7609976Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7610086Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7610091Z 
2025-04-11T04:23:18.7610168Z device = None
2025-04-11T04:23:18.7610174Z 
2025-04-11T04:23:18.7610288Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7610438Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7610512Z     
2025-04-11T04:23:18.7610583Z         Args:
2025-04-11T04:23:18.7610752Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7610921Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7611080Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7611152Z         """
2025-04-11T04:23:18.7611229Z         _lazy_init()
2025-04-11T04:23:18.7611326Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7611427Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7611533Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7611812Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7611999Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7612158Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7612162Z 
2025-04-11T04:23:18.7612395Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7612555Z ________________ test_copy_kv_to_caches[True-1-True-16-16-64-7] ________________
2025-04-11T04:23:18.7612561Z 
2025-04-11T04:23:18.7612710Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7612864Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7612868Z 
2025-04-11T04:23:18.7613066Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7613172Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7613294Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7613431Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7613546Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7613682Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7613793Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7613942Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7614029Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7614105Z         bsz: int,
2025-04-11T04:23:18.7614185Z         block_size: int,
2025-04-11T04:23:18.7614278Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7614359Z         num_kv_heads: int,
2025-04-11T04:23:18.7614444Z         same_context_len: bool,
2025-04-11T04:23:18.7614522Z         n_tokens: int,
2025-04-11T04:23:18.7614608Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7614680Z     ):
2025-04-11T04:23:18.7614764Z         torch.manual_seed(123)
2025-04-11T04:23:18.7614857Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7614945Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7614948Z 
2025-04-11T04:23:18.7615099Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7615210Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7615315Z 
2025-04-11T04:23:18.7615393Z device = None
2025-04-11T04:23:18.7615400Z 
2025-04-11T04:23:18.7615514Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7615663Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7615737Z     
2025-04-11T04:23:18.7615807Z         Args:
2025-04-11T04:23:18.7615976Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7616140Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7616243Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7616319Z         """
2025-04-11T04:23:18.7616394Z         _lazy_init()
2025-04-11T04:23:18.7616487Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7616586Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7616691Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7616977Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7617164Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7617322Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7617326Z 
2025-04-11T04:23:18.7617565Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7617721Z _______________ test_copy_kv_to_caches[True-1-True-16-16-64-32] ________________
2025-04-11T04:23:18.7617788Z 
2025-04-11T04:23:18.7617938Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7618093Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7618097Z 
2025-04-11T04:23:18.7618294Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7618401Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7618522Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7618659Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7618773Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7618909Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7619017Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7619165Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7619253Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7619329Z         bsz: int,
2025-04-11T04:23:18.7619408Z         block_size: int,
2025-04-11T04:23:18.7619499Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7619579Z         num_kv_heads: int,
2025-04-11T04:23:18.7619664Z         same_context_len: bool,
2025-04-11T04:23:18.7619741Z         n_tokens: int,
2025-04-11T04:23:18.7619825Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7619902Z     ):
2025-04-11T04:23:18.7619986Z         torch.manual_seed(123)
2025-04-11T04:23:18.7620077Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7620168Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7620172Z 
2025-04-11T04:23:18.7620320Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7620432Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7620436Z 
2025-04-11T04:23:18.7620509Z device = None
2025-04-11T04:23:18.7620513Z 
2025-04-11T04:23:18.7620633Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7620781Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7620854Z     
2025-04-11T04:23:18.7620926Z         Args:
2025-04-11T04:23:18.7621092Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7621257Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7621445Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7621524Z         """
2025-04-11T04:23:18.7621601Z         _lazy_init()
2025-04-11T04:23:18.7621695Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7621793Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7621896Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7622180Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7622312Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7622471Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7622476Z 
2025-04-11T04:23:18.7622709Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7622864Z _______________ test_copy_kv_to_caches[True-1-False-16-16-16-7] ________________
2025-04-11T04:23:18.7622873Z 
2025-04-11T04:23:18.7623018Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7623226Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7623230Z 
2025-04-11T04:23:18.7623425Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7623530Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7623650Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7623783Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7623945Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7624080Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7624188Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7624338Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7624429Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7624506Z         bsz: int,
2025-04-11T04:23:18.7624585Z         block_size: int,
2025-04-11T04:23:18.7624678Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7624759Z         num_kv_heads: int,
2025-04-11T04:23:18.7624844Z         same_context_len: bool,
2025-04-11T04:23:18.7624920Z         n_tokens: int,
2025-04-11T04:23:18.7625007Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7625081Z     ):
2025-04-11T04:23:18.7625164Z         torch.manual_seed(123)
2025-04-11T04:23:18.7625253Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7625342Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7625346Z 
2025-04-11T04:23:18.7625495Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7625607Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7625611Z 
2025-04-11T04:23:18.7625685Z device = None
2025-04-11T04:23:18.7625693Z 
2025-04-11T04:23:18.7625811Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7625959Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7626033Z     
2025-04-11T04:23:18.7626106Z         Args:
2025-04-11T04:23:18.7626272Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7626440Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7626542Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7626617Z         """
2025-04-11T04:23:18.7626694Z         _lazy_init()
2025-04-11T04:23:18.7626788Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7626886Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7626986Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7627274Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7627495Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7627656Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7627661Z 
2025-04-11T04:23:18.7627896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7628053Z _______________ test_copy_kv_to_caches[True-1-False-16-16-16-32] _______________
2025-04-11T04:23:18.7628057Z 
2025-04-11T04:23:18.7628205Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7628361Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7628365Z 
2025-04-11T04:23:18.7628597Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7628699Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7628824Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7628963Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7629075Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7629276Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7629385Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7629532Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7629620Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7629697Z         bsz: int,
2025-04-11T04:23:18.7629776Z         block_size: int,
2025-04-11T04:23:18.7629925Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7630005Z         num_kv_heads: int,
2025-04-11T04:23:18.7630089Z         same_context_len: bool,
2025-04-11T04:23:18.7630171Z         n_tokens: int,
2025-04-11T04:23:18.7630255Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7630327Z     ):
2025-04-11T04:23:18.7630410Z         torch.manual_seed(123)
2025-04-11T04:23:18.7630499Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7630592Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7630598Z 
2025-04-11T04:23:18.7630746Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7630859Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7630863Z 
2025-04-11T04:23:18.7630937Z device = None
2025-04-11T04:23:18.7630941Z 
2025-04-11T04:23:18.7631058Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7631206Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7631280Z     
2025-04-11T04:23:18.7631350Z         Args:
2025-04-11T04:23:18.7631515Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7631685Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7631787Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7631866Z         """
2025-04-11T04:23:18.7631942Z         _lazy_init()
2025-04-11T04:23:18.7632033Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7632138Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7632240Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7632523Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7632654Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7632811Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7632818Z 
2025-04-11T04:23:18.7633053Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7633208Z _______________ test_copy_kv_to_caches[True-1-False-16-16-32-7] ________________
2025-04-11T04:23:18.7633212Z 
2025-04-11T04:23:18.7633453Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7633606Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7633615Z 
2025-04-11T04:23:18.7633812Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7633916Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7634043Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7634177Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7634293Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7634430Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7634536Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7634687Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7634775Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7634858Z         bsz: int,
2025-04-11T04:23:18.7634938Z         block_size: int,
2025-04-11T04:23:18.7635028Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7635257Z         num_kv_heads: int,
2025-04-11T04:23:18.7635340Z         same_context_len: bool,
2025-04-11T04:23:18.7635422Z         n_tokens: int,
2025-04-11T04:23:18.7635510Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7635584Z     ):
2025-04-11T04:23:18.7635667Z         torch.manual_seed(123)
2025-04-11T04:23:18.7635753Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7635846Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7635850Z 
2025-04-11T04:23:18.7636049Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7636169Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7636173Z 
2025-04-11T04:23:18.7636247Z device = None
2025-04-11T04:23:18.7636251Z 
2025-04-11T04:23:18.7636371Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7636523Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7636592Z     
2025-04-11T04:23:18.7636666Z         Args:
2025-04-11T04:23:18.7636835Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7637006Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7637108Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7637183Z         """
2025-04-11T04:23:18.7637257Z         _lazy_init()
2025-04-11T04:23:18.7637349Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7637454Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7637557Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7637860Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7637991Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7638154Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7638161Z 
2025-04-11T04:23:18.7638397Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7638552Z _______________ test_copy_kv_to_caches[True-1-False-16-16-32-32] _______________
2025-04-11T04:23:18.7638560Z 
2025-04-11T04:23:18.7638707Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7638859Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7638865Z 
2025-04-11T04:23:18.7639065Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7639168Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7639292Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7639424Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7639638Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7639776Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7639883Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7640035Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7640120Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7640197Z         bsz: int,
2025-04-11T04:23:18.7640275Z         block_size: int,
2025-04-11T04:23:18.7640364Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7640447Z         num_kv_heads: int,
2025-04-11T04:23:18.7640532Z         same_context_len: bool,
2025-04-11T04:23:18.7640612Z         n_tokens: int,
2025-04-11T04:23:18.7640699Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7640771Z     ):
2025-04-11T04:23:18.7640853Z         torch.manual_seed(123)
2025-04-11T04:23:18.7640940Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7641037Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7641041Z 
2025-04-11T04:23:18.7641190Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7641353Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7641358Z 
2025-04-11T04:23:18.7641432Z device = None
2025-04-11T04:23:18.7641437Z 
2025-04-11T04:23:18.7641556Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7641702Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7649760Z     
2025-04-11T04:23:18.7649879Z         Args:
2025-04-11T04:23:18.7650178Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7650355Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7650461Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7650540Z         """
2025-04-11T04:23:18.7650633Z         _lazy_init()
2025-04-11T04:23:18.7650728Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7650836Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7650945Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7651242Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7651379Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7651540Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7651545Z 
2025-04-11T04:23:18.7651786Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7651945Z _______________ test_copy_kv_to_caches[True-1-False-16-16-64-7] ________________
2025-04-11T04:23:18.7651949Z 
2025-04-11T04:23:18.7652097Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7652256Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7652264Z 
2025-04-11T04:23:18.7652462Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7652569Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7652696Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7652832Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7652948Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7653087Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7653197Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7653351Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7653438Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7653517Z         bsz: int,
2025-04-11T04:23:18.7653597Z         block_size: int,
2025-04-11T04:23:18.7653788Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7653869Z         num_kv_heads: int,
2025-04-11T04:23:18.7653954Z         same_context_len: bool,
2025-04-11T04:23:18.7654038Z         n_tokens: int,
2025-04-11T04:23:18.7654126Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7654200Z     ):
2025-04-11T04:23:18.7654285Z         torch.manual_seed(123)
2025-04-11T04:23:18.7654372Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7654464Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7654468Z 
2025-04-11T04:23:18.7654620Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7654738Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7654744Z 
2025-04-11T04:23:18.7654820Z device = None
2025-04-11T04:23:18.7654825Z 
2025-04-11T04:23:18.7654944Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7655095Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7655168Z     
2025-04-11T04:23:18.7655245Z         Args:
2025-04-11T04:23:18.7655412Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7655645Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7655749Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7655825Z         """
2025-04-11T04:23:18.7655902Z         _lazy_init()
2025-04-11T04:23:18.7655995Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7656101Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7656257Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7656545Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7656677Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7656836Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7656844Z 
2025-04-11T04:23:18.7657082Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7657239Z _______________ test_copy_kv_to_caches[True-1-False-16-16-64-32] _______________
2025-04-11T04:23:18.7657247Z 
2025-04-11T04:23:18.7657396Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7657546Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7657550Z 
2025-04-11T04:23:18.7657752Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7657855Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7657981Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7658114Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7658228Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7658371Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7658480Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7658633Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7658720Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7658796Z         bsz: int,
2025-04-11T04:23:18.7658876Z         block_size: int,
2025-04-11T04:23:18.7658964Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7659050Z         num_kv_heads: int,
2025-04-11T04:23:18.7659133Z         same_context_len: bool,
2025-04-11T04:23:18.7659213Z         n_tokens: int,
2025-04-11T04:23:18.7659301Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7659374Z     ):
2025-04-11T04:23:18.7659460Z         torch.manual_seed(123)
2025-04-11T04:23:18.7659548Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7659640Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7659643Z 
2025-04-11T04:23:18.7659792Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7659991Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7659998Z 
2025-04-11T04:23:18.7660073Z device = None
2025-04-11T04:23:18.7660077Z 
2025-04-11T04:23:18.7660196Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7660348Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7660416Z     
2025-04-11T04:23:18.7660491Z         Args:
2025-04-11T04:23:18.7660660Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7660831Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7660934Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7661008Z         """
2025-04-11T04:23:18.7661086Z         _lazy_init()
2025-04-11T04:23:18.7661179Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7661287Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7661391Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7661727Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7661860Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7662015Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7662022Z 
2025-04-11T04:23:18.7662256Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7662462Z ________________ test_copy_kv_to_caches[True-5-True-16-16-16-7] ________________
2025-04-11T04:23:18.7662466Z 
2025-04-11T04:23:18.7662613Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7662766Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7662778Z 
2025-04-11T04:23:18.7662974Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7663076Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7663201Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7663333Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7663446Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7663581Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7663687Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7663840Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7663926Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7664002Z         bsz: int,
2025-04-11T04:23:18.7664081Z         block_size: int,
2025-04-11T04:23:18.7664172Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7664252Z         num_kv_heads: int,
2025-04-11T04:23:18.7664340Z         same_context_len: bool,
2025-04-11T04:23:18.7664419Z         n_tokens: int,
2025-04-11T04:23:18.7664505Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7664579Z     ):
2025-04-11T04:23:18.7664662Z         torch.manual_seed(123)
2025-04-11T04:23:18.7664748Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7664838Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7664842Z 
2025-04-11T04:23:18.7664989Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7665103Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7665107Z 
2025-04-11T04:23:18.7665182Z device = None
2025-04-11T04:23:18.7665187Z 
2025-04-11T04:23:18.7665304Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7665452Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7665521Z     
2025-04-11T04:23:18.7665596Z         Args:
2025-04-11T04:23:18.7665855Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7666024Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7666128Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7666203Z         """
2025-04-11T04:23:18.7666279Z         _lazy_init()
2025-04-11T04:23:18.7666371Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7666474Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7666578Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7666861Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7666997Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7667157Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7667161Z 
2025-04-11T04:23:18.7667397Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7667548Z _______________ test_copy_kv_to_caches[True-5-True-16-16-16-32] ________________
2025-04-11T04:23:18.7667605Z 
2025-04-11T04:23:18.7667753Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7667904Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7667908Z 
2025-04-11T04:23:18.7668106Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7668210Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7668438Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7668572Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7668686Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7668824Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7668934Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7669085Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7669173Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7669250Z         bsz: int,
2025-04-11T04:23:18.7669329Z         block_size: int,
2025-04-11T04:23:18.7669418Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7669503Z         num_kv_heads: int,
2025-04-11T04:23:18.7669586Z         same_context_len: bool,
2025-04-11T04:23:18.7669668Z         n_tokens: int,
2025-04-11T04:23:18.7669755Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7669829Z     ):
2025-04-11T04:23:18.7669913Z         torch.manual_seed(123)
2025-04-11T04:23:18.7670000Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7670091Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7670095Z 
2025-04-11T04:23:18.7670243Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7670357Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7670365Z 
2025-04-11T04:23:18.7670440Z device = None
2025-04-11T04:23:18.7670444Z 
2025-04-11T04:23:18.7670563Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7670710Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7670779Z     
2025-04-11T04:23:18.7670855Z         Args:
2025-04-11T04:23:18.7671020Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7671189Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7671292Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7671366Z         """
2025-04-11T04:23:18.7671441Z         _lazy_init()
2025-04-11T04:23:18.7671533Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7671635Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7671736Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7672117Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7672252Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7672408Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7672416Z 
2025-04-11T04:23:18.7672658Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7672809Z ________________ test_copy_kv_to_caches[True-5-True-16-16-32-7] ________________
2025-04-11T04:23:18.7672815Z 
2025-04-11T04:23:18.7672966Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7673116Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7673120Z 
2025-04-11T04:23:18.7673324Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7673432Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7673558Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7673747Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7673858Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7673999Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7674107Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7674259Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7674346Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7674476Z         bsz: int,
2025-04-11T04:23:18.7674556Z         block_size: int,
2025-04-11T04:23:18.7674642Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7674726Z         num_kv_heads: int,
2025-04-11T04:23:18.7674809Z         same_context_len: bool,
2025-04-11T04:23:18.7674889Z         n_tokens: int,
2025-04-11T04:23:18.7674979Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7675050Z     ):
2025-04-11T04:23:18.7675138Z         torch.manual_seed(123)
2025-04-11T04:23:18.7675227Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7675317Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7675321Z 
2025-04-11T04:23:18.7675469Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7675581Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7675586Z 
2025-04-11T04:23:18.7675660Z device = None
2025-04-11T04:23:18.7675664Z 
2025-04-11T04:23:18.7675779Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7675934Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7676002Z     
2025-04-11T04:23:18.7676077Z         Args:
2025-04-11T04:23:18.7676243Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7676417Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7676519Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7676592Z         """
2025-04-11T04:23:18.7676671Z         _lazy_init()
2025-04-11T04:23:18.7676764Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7676866Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7676968Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7677252Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7677388Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7677543Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7677548Z 
2025-04-11T04:23:18.7677784Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7678021Z _______________ test_copy_kv_to_caches[True-5-True-16-16-32-32] ________________
2025-04-11T04:23:18.7678025Z 
2025-04-11T04:23:18.7678176Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7678329Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7678334Z 
2025-04-11T04:23:18.7678533Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7678636Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7678757Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7678893Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7679004Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7679148Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7679255Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7679407Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7679497Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7679573Z         bsz: int,
2025-04-11T04:23:18.7679708Z         block_size: int,
2025-04-11T04:23:18.7679796Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7679881Z         num_kv_heads: int,
2025-04-11T04:23:18.7679964Z         same_context_len: bool,
2025-04-11T04:23:18.7680044Z         n_tokens: int,
2025-04-11T04:23:18.7680132Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7680201Z     ):
2025-04-11T04:23:18.7680289Z         torch.manual_seed(123)
2025-04-11T04:23:18.7680376Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7680519Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7680524Z 
2025-04-11T04:23:18.7680675Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7680785Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7680793Z 
2025-04-11T04:23:18.7680870Z device = None
2025-04-11T04:23:18.7680874Z 
2025-04-11T04:23:18.7680987Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7681140Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7681211Z     
2025-04-11T04:23:18.7681286Z         Args:
2025-04-11T04:23:18.7681454Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7681622Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7681725Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7681797Z         """
2025-04-11T04:23:18.7681876Z         _lazy_init()
2025-04-11T04:23:18.7681967Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7682068Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7682170Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7682453Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7682593Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7682749Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7682753Z 
2025-04-11T04:23:18.7682995Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7683153Z ________________ test_copy_kv_to_caches[True-5-True-16-16-64-7] ________________
2025-04-11T04:23:18.7683158Z 
2025-04-11T04:23:18.7683306Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7683459Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7683464Z 
2025-04-11T04:23:18.7683663Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7683765Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7683967Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7684103Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7684216Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7684357Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7684463Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7684614Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7684700Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7684773Z         bsz: int,
2025-04-11T04:23:18.7684858Z         block_size: int,
2025-04-11T04:23:18.7684945Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7685030Z         num_kv_heads: int,
2025-04-11T04:23:18.7685113Z         same_context_len: bool,
2025-04-11T04:23:18.7685190Z         n_tokens: int,
2025-04-11T04:23:18.7685280Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7685348Z     ):
2025-04-11T04:23:18.7685444Z         torch.manual_seed(123)
2025-04-11T04:23:18.7685532Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7685621Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7685696Z 
2025-04-11T04:23:18.7685847Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7685955Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7685960Z 
2025-04-11T04:23:18.7686037Z device = None
2025-04-11T04:23:18.7686042Z 
2025-04-11T04:23:18.7686159Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7686312Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7686434Z     
2025-04-11T04:23:18.7686509Z         Args:
2025-04-11T04:23:18.7686676Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7686841Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7686950Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7687019Z         """
2025-04-11T04:23:18.7687101Z         _lazy_init()
2025-04-11T04:23:18.7687195Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7687298Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7687401Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7687684Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7687819Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7687976Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7687981Z 
2025-04-11T04:23:18.7688220Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7688370Z _______________ test_copy_kv_to_caches[True-5-True-16-16-64-32] ________________
2025-04-11T04:23:18.7688378Z 
2025-04-11T04:23:18.7688525Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7688674Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7688681Z 
2025-04-11T04:23:18.7688881Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7688984Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7689103Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7689240Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7689349Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7689492Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7689597Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7689747Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7689833Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7689993Z         bsz: int,
2025-04-11T04:23:18.7690077Z         block_size: int,
2025-04-11T04:23:18.7690164Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7690250Z         num_kv_heads: int,
2025-04-11T04:23:18.7690332Z         same_context_len: bool,
2025-04-11T04:23:18.7690407Z         n_tokens: int,
2025-04-11T04:23:18.7690498Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7690565Z     ):
2025-04-11T04:23:18.7690653Z         torch.manual_seed(123)
2025-04-11T04:23:18.7690739Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7690825Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7690835Z 
2025-04-11T04:23:18.7690982Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7691093Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7691097Z 
2025-04-11T04:23:18.7691175Z device = None
2025-04-11T04:23:18.7691179Z 
2025-04-11T04:23:18.7691293Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7691448Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7691516Z     
2025-04-11T04:23:18.7691657Z         Args:
2025-04-11T04:23:18.7691822Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7691988Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7692096Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7692166Z         """
2025-04-11T04:23:18.7692243Z         _lazy_init()
2025-04-11T04:23:18.7692334Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7692484Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7692589Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7692868Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7693009Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7693163Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7693170Z 
2025-04-11T04:23:18.7693406Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7693559Z _______________ test_copy_kv_to_caches[True-5-False-16-16-16-7] ________________
2025-04-11T04:23:18.7693563Z 
2025-04-11T04:23:18.7693711Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7693862Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7693868Z 
2025-04-11T04:23:18.7694066Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7694170Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7694290Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7694436Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7694545Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7694686Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7694792Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7694940Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7695025Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7695097Z         bsz: int,
2025-04-11T04:23:18.7695181Z         block_size: int,
2025-04-11T04:23:18.7695267Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7695351Z         num_kv_heads: int,
2025-04-11T04:23:18.7695434Z         same_context_len: bool,
2025-04-11T04:23:18.7695511Z         n_tokens: int,
2025-04-11T04:23:18.7695600Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7695670Z     ):
2025-04-11T04:23:18.7695759Z         torch.manual_seed(123)
2025-04-11T04:23:18.7695844Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7696029Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7696033Z 
2025-04-11T04:23:18.7696181Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7696293Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7696300Z 
2025-04-11T04:23:18.7696373Z device = None
2025-04-11T04:23:18.7696377Z 
2025-04-11T04:23:18.7696492Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7696643Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7696712Z     
2025-04-11T04:23:18.7696788Z         Args:
2025-04-11T04:23:18.7696954Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7697118Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7697225Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7697300Z         """
2025-04-11T04:23:18.7697379Z         _lazy_init()
2025-04-11T04:23:18.7697471Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7697573Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7697728Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7698011Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7698149Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7698306Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7698360Z 
2025-04-11T04:23:18.7698598Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7698750Z _______________ test_copy_kv_to_caches[True-5-False-16-16-16-32] _______________
2025-04-11T04:23:18.7698754Z 
2025-04-11T04:23:18.7698904Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7699058Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7699062Z 
2025-04-11T04:23:18.7699263Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7699367Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7699488Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7699626Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7699735Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7699877Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7699984Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7700134Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7700220Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7700292Z         bsz: int,
2025-04-11T04:23:18.7700375Z         block_size: int,
2025-04-11T04:23:18.7700465Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7700550Z         num_kv_heads: int,
2025-04-11T04:23:18.7700632Z         same_context_len: bool,
2025-04-11T04:23:18.7700711Z         n_tokens: int,
2025-04-11T04:23:18.7700800Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7700868Z     ):
2025-04-11T04:23:18.7700955Z         torch.manual_seed(123)
2025-04-11T04:23:18.7701041Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7701129Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7701137Z 
2025-04-11T04:23:18.7701285Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7701397Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7701401Z 
2025-04-11T04:23:18.7701479Z device = None
2025-04-11T04:23:18.7701483Z 
2025-04-11T04:23:18.7701597Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7701749Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7701908Z     
2025-04-11T04:23:18.7701984Z         Args:
2025-04-11T04:23:18.7702151Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7702319Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7702427Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7702498Z         """
2025-04-11T04:23:18.7702577Z         _lazy_init()
2025-04-11T04:23:18.7702670Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7702769Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7702877Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7703156Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7703292Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7703450Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7703455Z 
2025-04-11T04:23:18.7703691Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7703896Z _______________ test_copy_kv_to_caches[True-5-False-16-16-32-7] ________________
2025-04-11T04:23:18.7703900Z 
2025-04-11T04:23:18.7704046Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7704197Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7704201Z 
2025-04-11T04:23:18.7704399Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7704555Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7704675Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7704811Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7704921Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7705065Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7705170Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7705318Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7705411Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7705483Z         bsz: int,
2025-04-11T04:23:18.7705566Z         block_size: int,
2025-04-11T04:23:18.7705652Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7705737Z         num_kv_heads: int,
2025-04-11T04:23:18.7705819Z         same_context_len: bool,
2025-04-11T04:23:18.7705898Z         n_tokens: int,
2025-04-11T04:23:18.7705990Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7706059Z     ):
2025-04-11T04:23:18.7706147Z         torch.manual_seed(123)
2025-04-11T04:23:18.7706233Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7706318Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7706322Z 
2025-04-11T04:23:18.7706478Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7706588Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7706594Z 
2025-04-11T04:23:18.7706671Z device = None
2025-04-11T04:23:18.7706675Z 
2025-04-11T04:23:18.7706789Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7706938Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7707007Z     
2025-04-11T04:23:18.7707078Z         Args:
2025-04-11T04:23:18.7707248Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7707417Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7707528Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7707600Z         """
2025-04-11T04:23:18.7707678Z         _lazy_init()
2025-04-11T04:23:18.7707769Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7707957Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7708062Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7708350Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7708531Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7708686Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7708691Z 
2025-04-11T04:23:18.7708928Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7709086Z _______________ test_copy_kv_to_caches[True-5-False-16-16-32-32] _______________
2025-04-11T04:23:18.7709090Z 
2025-04-11T04:23:18.7709241Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7709394Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7709402Z 
2025-04-11T04:23:18.7709597Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7709763Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7709884Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7710020Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7710131Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7710273Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7710378Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7710576Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7710668Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7710741Z         bsz: int,
2025-04-11T04:23:18.7710822Z         block_size: int,
2025-04-11T04:23:18.7710908Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7710993Z         num_kv_heads: int,
2025-04-11T04:23:18.7711080Z         same_context_len: bool,
2025-04-11T04:23:18.7711156Z         n_tokens: int,
2025-04-11T04:23:18.7711244Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7711315Z     ):
2025-04-11T04:23:18.7711403Z         torch.manual_seed(123)
2025-04-11T04:23:18.7711489Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7711578Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7711582Z 
2025-04-11T04:23:18.7711733Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7711842Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7711848Z 
2025-04-11T04:23:18.7711925Z device = None
2025-04-11T04:23:18.7711929Z 
2025-04-11T04:23:18.7712043Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7712193Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7712262Z     
2025-04-11T04:23:18.7712332Z         Args:
2025-04-11T04:23:18.7712505Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7712672Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7712780Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7712851Z         """
2025-04-11T04:23:18.7712926Z         _lazy_init()
2025-04-11T04:23:18.7713020Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7713120Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7713225Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7713508Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7713643Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7713798Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7713915Z 
2025-04-11T04:23:18.7714158Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7714312Z _______________ test_copy_kv_to_caches[True-5-False-16-16-64-7] ________________
2025-04-11T04:23:18.7714318Z 
2025-04-11T04:23:18.7714462Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7714617Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7714621Z 
2025-04-11T04:23:18.7714816Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7714922Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7715045Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7715179Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7715288Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7715423Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7715536Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7715682Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7715827Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7715901Z         bsz: int,
2025-04-11T04:23:18.7715983Z         block_size: int,
2025-04-11T04:23:18.7716070Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7716150Z         num_kv_heads: int,
2025-04-11T04:23:18.7716239Z         same_context_len: bool,
2025-04-11T04:23:18.7716316Z         n_tokens: int,
2025-04-11T04:23:18.7716407Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7716527Z     ):
2025-04-11T04:23:18.7716615Z         torch.manual_seed(123)
2025-04-11T04:23:18.7716706Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7716795Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7716799Z 
2025-04-11T04:23:18.7716951Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7717065Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7717069Z 
2025-04-11T04:23:18.7717147Z device = None
2025-04-11T04:23:18.7717153Z 
2025-04-11T04:23:18.7717269Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7717414Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7717486Z     
2025-04-11T04:23:18.7717557Z         Args:
2025-04-11T04:23:18.7717727Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7717891Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7717999Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7718070Z         """
2025-04-11T04:23:18.7718146Z         _lazy_init()
2025-04-11T04:23:18.7718243Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7718342Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7718453Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7718734Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7718874Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7719030Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7719035Z 
2025-04-11T04:23:18.7719269Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7719424Z _______________ test_copy_kv_to_caches[True-5-False-16-16-64-32] _______________
2025-04-11T04:23:18.7719430Z 
2025-04-11T04:23:18.7719576Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7719733Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7719738Z 
2025-04-11T04:23:18.7719931Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7720123Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7720243Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7720381Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7720491Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7720630Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7720741Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7720889Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7720979Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7721052Z         bsz: int,
2025-04-11T04:23:18.7721132Z         block_size: int,
2025-04-11T04:23:18.7721222Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7721302Z         num_kv_heads: int,
2025-04-11T04:23:18.7721391Z         same_context_len: bool,
2025-04-11T04:23:18.7721473Z         n_tokens: int,
2025-04-11T04:23:18.7721563Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7721633Z     ):
2025-04-11T04:23:18.7721717Z         torch.manual_seed(123)
2025-04-11T04:23:18.7721861Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7721951Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7721956Z 
2025-04-11T04:23:18.7722107Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7722217Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7722222Z 
2025-04-11T04:23:18.7722298Z device = None
2025-04-11T04:23:18.7722302Z 
2025-04-11T04:23:18.7722455Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7722602Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7722675Z     
2025-04-11T04:23:18.7722745Z         Args:
2025-04-11T04:23:18.7722915Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7723083Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7723188Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7723261Z         """
2025-04-11T04:23:18.7723335Z         _lazy_init()
2025-04-11T04:23:18.7723431Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7723530Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7723635Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7723916Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7724050Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7724207Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7724211Z 
2025-04-11T04:23:18.7724443Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7724601Z _______________ test_copy_kv_to_caches[False-1-True-16-16-16-7] ________________
2025-04-11T04:23:18.7724605Z 
2025-04-11T04:23:18.7724751Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7724906Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7724910Z 
2025-04-11T04:23:18.7725106Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7725209Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7725330Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7725463Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7725576Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7725714Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7725824Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7726058Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7726146Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7726225Z         bsz: int,
2025-04-11T04:23:18.7726304Z         block_size: int,
2025-04-11T04:23:18.7726398Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7726477Z         num_kv_heads: int,
2025-04-11T04:23:18.7726564Z         same_context_len: bool,
2025-04-11T04:23:18.7726643Z         n_tokens: int,
2025-04-11T04:23:18.7726729Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7726802Z     ):
2025-04-11T04:23:18.7726887Z         torch.manual_seed(123)
2025-04-11T04:23:18.7726979Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7727068Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7727073Z 
2025-04-11T04:23:18.7727223Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7727332Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7727340Z 
2025-04-11T04:23:18.7727413Z device = None
2025-04-11T04:23:18.7727421Z 
2025-04-11T04:23:18.7727535Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7727733Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7727806Z     
2025-04-11T04:23:18.7727875Z         Args:
2025-04-11T04:23:18.7728044Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7728208Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7728311Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7728434Z         """
2025-04-11T04:23:18.7728511Z         _lazy_init()
2025-04-11T04:23:18.7728608Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7728709Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7728816Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7729104Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7729237Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7729399Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7729404Z 
2025-04-11T04:23:18.7729638Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7729793Z _______________ test_copy_kv_to_caches[False-1-True-16-16-16-32] _______________
2025-04-11T04:23:18.7729798Z 
2025-04-11T04:23:18.7729943Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7730100Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7730103Z 
2025-04-11T04:23:18.7730298Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7730402Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7730528Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7730659Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7730776Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7730913Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7731024Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7731171Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7731259Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7731332Z         bsz: int,
2025-04-11T04:23:18.7731414Z         block_size: int,
2025-04-11T04:23:18.7731506Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7731588Z         num_kv_heads: int,
2025-04-11T04:23:18.7731673Z         same_context_len: bool,
2025-04-11T04:23:18.7731749Z         n_tokens: int,
2025-04-11T04:23:18.7731833Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7731991Z     ):
2025-04-11T04:23:18.7732076Z         torch.manual_seed(123)
2025-04-11T04:23:18.7732166Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7732255Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7732259Z 
2025-04-11T04:23:18.7732406Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7732521Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7732525Z 
2025-04-11T04:23:18.7732598Z device = None
2025-04-11T04:23:18.7732602Z 
2025-04-11T04:23:18.7732719Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7732867Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7732941Z     
2025-04-11T04:23:18.7733011Z         Args:
2025-04-11T04:23:18.7733177Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7733339Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7733443Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7733519Z         """
2025-04-11T04:23:18.7733646Z         _lazy_init()
2025-04-11T04:23:18.7733742Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7733842Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7733943Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7734229Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7734361Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7734574Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7734579Z 
2025-04-11T04:23:18.7734814Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7734969Z _______________ test_copy_kv_to_caches[False-1-True-16-16-32-7] ________________
2025-04-11T04:23:18.7734976Z 
2025-04-11T04:23:18.7735122Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7735279Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7735283Z 
2025-04-11T04:23:18.7735480Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7735585Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7735706Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7735837Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7735956Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7736092Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7736202Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7736349Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7736439Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7736516Z         bsz: int,
2025-04-11T04:23:18.7736595Z         block_size: int,
2025-04-11T04:23:18.7736687Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7736767Z         num_kv_heads: int,
2025-04-11T04:23:18.7736852Z         same_context_len: bool,
2025-04-11T04:23:18.7736928Z         n_tokens: int,
2025-04-11T04:23:18.7737014Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7737085Z     ):
2025-04-11T04:23:18.7737169Z         torch.manual_seed(123)
2025-04-11T04:23:18.7737259Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7737347Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7737353Z 
2025-04-11T04:23:18.7737500Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7737613Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7737618Z 
2025-04-11T04:23:18.7737694Z device = None
2025-04-11T04:23:18.7737699Z 
2025-04-11T04:23:18.7738012Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7738163Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7738238Z     
2025-04-11T04:23:18.7738309Z         Args:
2025-04-11T04:23:18.7738478Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7738650Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7738757Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7738834Z         """
2025-04-11T04:23:18.7738911Z         _lazy_init()
2025-04-11T04:23:18.7739013Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7739113Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7739216Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7739507Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7739642Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7739801Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7739856Z 
2025-04-11T04:23:18.7740091Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7740247Z _______________ test_copy_kv_to_caches[False-1-True-16-16-32-32] _______________
2025-04-11T04:23:18.7740251Z 
2025-04-11T04:23:18.7740396Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7740549Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7740601Z 
2025-04-11T04:23:18.7740799Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7740901Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7741025Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7741161Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7741275Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7741412Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7741522Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7741668Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7741755Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7741834Z         bsz: int,
2025-04-11T04:23:18.7741916Z         block_size: int,
2025-04-11T04:23:18.7742009Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7742093Z         num_kv_heads: int,
2025-04-11T04:23:18.7742176Z         same_context_len: bool,
2025-04-11T04:23:18.7742256Z         n_tokens: int,
2025-04-11T04:23:18.7742343Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7742414Z     ):
2025-04-11T04:23:18.7742498Z         torch.manual_seed(123)
2025-04-11T04:23:18.7742590Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7742681Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7742685Z 
2025-04-11T04:23:18.7742832Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7742948Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7742952Z 
2025-04-11T04:23:18.7743026Z device = None
2025-04-11T04:23:18.7743030Z 
2025-04-11T04:23:18.7743148Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7743296Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7743370Z     
2025-04-11T04:23:18.7743441Z         Args:
2025-04-11T04:23:18.7743605Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7743774Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7743875Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7744045Z         """
2025-04-11T04:23:18.7744121Z         _lazy_init()
2025-04-11T04:23:18.7744212Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7744318Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7744422Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7744707Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7744840Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7745001Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7745008Z 
2025-04-11T04:23:18.7745242Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7745396Z _______________ test_copy_kv_to_caches[False-1-True-16-16-64-7] ________________
2025-04-11T04:23:18.7745400Z 
2025-04-11T04:23:18.7745545Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7745699Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7745757Z 
2025-04-11T04:23:18.7745959Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7746060Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7746185Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7746317Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7746430Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7746617Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7746723Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7746875Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7746962Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7747039Z         bsz: int,
2025-04-11T04:23:18.7747123Z         block_size: int,
2025-04-11T04:23:18.7747213Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7747295Z         num_kv_heads: int,
2025-04-11T04:23:18.7747379Z         same_context_len: bool,
2025-04-11T04:23:18.7747459Z         n_tokens: int,
2025-04-11T04:23:18.7747545Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7747617Z     ):
2025-04-11T04:23:18.7747701Z         torch.manual_seed(123)
2025-04-11T04:23:18.7747784Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7747874Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7747878Z 
2025-04-11T04:23:18.7748025Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7748141Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7748146Z 
2025-04-11T04:23:18.7748219Z device = None
2025-04-11T04:23:18.7748223Z 
2025-04-11T04:23:18.7748341Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7748538Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7748613Z     
2025-04-11T04:23:18.7748688Z         Args:
2025-04-11T04:23:18.7748857Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7749028Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7749131Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7749205Z         """
2025-04-11T04:23:18.7749281Z         _lazy_init()
2025-04-11T04:23:18.7749373Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7749479Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7749586Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7749878Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7750010Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7750270Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7750275Z 
2025-04-11T04:23:18.7750509Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7750661Z _______________ test_copy_kv_to_caches[False-1-True-16-16-64-32] _______________
2025-04-11T04:23:18.7750669Z 
2025-04-11T04:23:18.7750813Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7750963Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7750967Z 
2025-04-11T04:23:18.7751167Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7751269Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7751394Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7751524Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7751643Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7751778Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7751940Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7752092Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7752178Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7752254Z         bsz: int,
2025-04-11T04:23:18.7752333Z         block_size: int,
2025-04-11T04:23:18.7752420Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7752506Z         num_kv_heads: int,
2025-04-11T04:23:18.7752645Z         same_context_len: bool,
2025-04-11T04:23:18.7752726Z         n_tokens: int,
2025-04-11T04:23:18.7752813Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7752885Z     ):
2025-04-11T04:23:18.7752971Z         torch.manual_seed(123)
2025-04-11T04:23:18.7753058Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7753148Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7753155Z 
2025-04-11T04:23:18.7753303Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7753416Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7753422Z 
2025-04-11T04:23:18.7753495Z device = None
2025-04-11T04:23:18.7753499Z 
2025-04-11T04:23:18.7753615Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7753762Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7753830Z     
2025-04-11T04:23:18.7753904Z         Args:
2025-04-11T04:23:18.7754069Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7754239Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7754342Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7754416Z         """
2025-04-11T04:23:18.7754491Z         _lazy_init()
2025-04-11T04:23:18.7754586Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7754688Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7754790Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7755072Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7755204Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7755359Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7755366Z 
2025-04-11T04:23:18.7755600Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7755756Z _______________ test_copy_kv_to_caches[False-1-False-16-16-16-7] _______________
2025-04-11T04:23:18.7755761Z 
2025-04-11T04:23:18.7755911Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7756066Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7756151Z 
2025-04-11T04:23:18.7756355Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7756460Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7756586Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7756717Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7756829Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7756968Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7757077Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7757228Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7757313Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7757389Z         bsz: int,
2025-04-11T04:23:18.7757467Z         block_size: int,
2025-04-11T04:23:18.7757552Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7757640Z         num_kv_heads: int,
2025-04-11T04:23:18.7757722Z         same_context_len: bool,
2025-04-11T04:23:18.7757802Z         n_tokens: int,
2025-04-11T04:23:18.7757941Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7758009Z     ):
2025-04-11T04:23:18.7758097Z         torch.manual_seed(123)
2025-04-11T04:23:18.7758184Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7758276Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7758280Z 
2025-04-11T04:23:18.7758425Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7758538Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7758604Z 
2025-04-11T04:23:18.7758680Z device = None
2025-04-11T04:23:18.7758684Z 
2025-04-11T04:23:18.7758798Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7758949Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7759023Z     
2025-04-11T04:23:18.7759097Z         Args:
2025-04-11T04:23:18.7759261Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7759429Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7759530Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7759601Z         """
2025-04-11T04:23:18.7759679Z         _lazy_init()
2025-04-11T04:23:18.7759771Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7759875Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7759978Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7760268Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7760401Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7760554Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7760562Z 
2025-04-11T04:23:18.7760796Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7760952Z ______________ test_copy_kv_to_caches[False-1-False-16-16-16-32] _______________
2025-04-11T04:23:18.7760956Z 
2025-04-11T04:23:18.7761107Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7761262Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7761267Z 
2025-04-11T04:23:18.7761467Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7761572Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7761697Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7761829Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7761938Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7762199Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7762305Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7762461Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7762548Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7762623Z         bsz: int,
2025-04-11T04:23:18.7762703Z         block_size: int,
2025-04-11T04:23:18.7762790Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7762873Z         num_kv_heads: int,
2025-04-11T04:23:18.7762955Z         same_context_len: bool,
2025-04-11T04:23:18.7763036Z         n_tokens: int,
2025-04-11T04:23:18.7763127Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7763195Z     ):
2025-04-11T04:23:18.7763284Z         torch.manual_seed(123)
2025-04-11T04:23:18.7763369Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7763459Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7763464Z 
2025-04-11T04:23:18.7763612Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7763731Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7763735Z 
2025-04-11T04:23:18.7763861Z device = None
2025-04-11T04:23:18.7763865Z 
2025-04-11T04:23:18.7763982Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7764139Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7764209Z     
2025-04-11T04:23:18.7764284Z         Args:
2025-04-11T04:23:18.7764449Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7764615Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7764768Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7764840Z         """
2025-04-11T04:23:18.7764919Z         _lazy_init()
2025-04-11T04:23:18.7765010Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7765113Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7765221Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7765500Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7765638Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7765795Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7765800Z 
2025-04-11T04:23:18.7766039Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7766194Z _______________ test_copy_kv_to_caches[False-1-False-16-16-32-7] _______________
2025-04-11T04:23:18.7766198Z 
2025-04-11T04:23:18.7766346Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7766499Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7766504Z 
2025-04-11T04:23:18.7766709Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7766812Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7766938Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7767070Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7767181Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7767325Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7767431Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7767584Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7767670Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7767742Z         bsz: int,
2025-04-11T04:23:18.7767825Z         block_size: int,
2025-04-11T04:23:18.7767912Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7767995Z         num_kv_heads: int,
2025-04-11T04:23:18.7768078Z         same_context_len: bool,
2025-04-11T04:23:18.7768244Z         n_tokens: int,
2025-04-11T04:23:18.7768333Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7768405Z     ):
2025-04-11T04:23:18.7768492Z         torch.manual_seed(123)
2025-04-11T04:23:18.7768579Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7768671Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7768675Z 
2025-04-11T04:23:18.7768825Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7768934Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7768941Z 
2025-04-11T04:23:18.7769015Z device = None
2025-04-11T04:23:18.7769022Z 
2025-04-11T04:23:18.7769140Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7769291Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7769359Z     
2025-04-11T04:23:18.7769433Z         Args:
2025-04-11T04:23:18.7769600Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7769768Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7769923Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7769999Z         """
2025-04-11T04:23:18.7770081Z         _lazy_init()
2025-04-11T04:23:18.7770178Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7770286Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7770395Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7770683Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7770871Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7771027Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7771031Z 
2025-04-11T04:23:18.7771270Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7771429Z ______________ test_copy_kv_to_caches[False-1-False-16-16-32-32] _______________
2025-04-11T04:23:18.7771434Z 
2025-04-11T04:23:18.7771585Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7771739Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7771743Z 
2025-04-11T04:23:18.7771943Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7772046Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7772169Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7772303Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7772414Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7772556Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7772662Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7772817Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7772900Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7772976Z         bsz: int,
2025-04-11T04:23:18.7773062Z         block_size: int,
2025-04-11T04:23:18.7773148Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7773232Z         num_kv_heads: int,
2025-04-11T04:23:18.7773314Z         same_context_len: bool,
2025-04-11T04:23:18.7773390Z         n_tokens: int,
2025-04-11T04:23:18.7773480Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7773548Z     ):
2025-04-11T04:23:18.7773636Z         torch.manual_seed(123)
2025-04-11T04:23:18.7773721Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7773807Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7773814Z 
2025-04-11T04:23:18.7773959Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7774068Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7774162Z 
2025-04-11T04:23:18.7774245Z device = None
2025-04-11T04:23:18.7774250Z 
2025-04-11T04:23:18.7774367Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7774521Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7774591Z     
2025-04-11T04:23:18.7774663Z         Args:
2025-04-11T04:23:18.7774828Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7774992Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7775103Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7775174Z         """
2025-04-11T04:23:18.7775252Z         _lazy_init()
2025-04-11T04:23:18.7775344Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7775443Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7775549Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7775837Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7776029Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7776185Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7776189Z 
2025-04-11T04:23:18.7776426Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7776578Z _______________ test_copy_kv_to_caches[False-1-False-16-16-64-7] _______________
2025-04-11T04:23:18.7776630Z 
2025-04-11T04:23:18.7776782Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7776934Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7776938Z 
2025-04-11T04:23:18.7777136Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7777242Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7777363Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7777502Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7777613Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7777753Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7777858Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7778006Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7778096Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7778172Z         bsz: int,
2025-04-11T04:23:18.7778254Z         block_size: int,
2025-04-11T04:23:18.7778340Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7778424Z         num_kv_heads: int,
2025-04-11T04:23:18.7778507Z         same_context_len: bool,
2025-04-11T04:23:18.7778584Z         n_tokens: int,
2025-04-11T04:23:18.7778678Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7778748Z     ):
2025-04-11T04:23:18.7778835Z         torch.manual_seed(123)
2025-04-11T04:23:18.7778921Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7779011Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7779014Z 
2025-04-11T04:23:18.7779165Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7779274Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7779278Z 
2025-04-11T04:23:18.7779357Z device = None
2025-04-11T04:23:18.7779361Z 
2025-04-11T04:23:18.7779474Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7779628Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7779696Z     
2025-04-11T04:23:18.7779768Z         Args:
2025-04-11T04:23:18.7779935Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7780099Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7780296Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7780371Z         """
2025-04-11T04:23:18.7780451Z         _lazy_init()
2025-04-11T04:23:18.7780544Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7780644Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7780751Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7781034Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7781175Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7781330Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7781334Z 
2025-04-11T04:23:18.7781575Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7781733Z ______________ test_copy_kv_to_caches[False-1-False-16-16-64-32] _______________
2025-04-11T04:23:18.7781737Z 
2025-04-11T04:23:18.7781886Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7782111Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7782115Z 
2025-04-11T04:23:18.7782314Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7782420Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7782539Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7782681Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7782841Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7782982Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7783088Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7783234Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7783329Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7783402Z         bsz: int,
2025-04-11T04:23:18.7783489Z         block_size: int,
2025-04-11T04:23:18.7783574Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7783654Z         num_kv_heads: int,
2025-04-11T04:23:18.7783741Z         same_context_len: bool,
2025-04-11T04:23:18.7783817Z         n_tokens: int,
2025-04-11T04:23:18.7783906Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7783974Z     ):
2025-04-11T04:23:18.7784059Z         torch.manual_seed(123)
2025-04-11T04:23:18.7784144Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7784232Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7784236Z 
2025-04-11T04:23:18.7784386Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7784496Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7784500Z 
2025-04-11T04:23:18.7784577Z device = None
2025-04-11T04:23:18.7784584Z 
2025-04-11T04:23:18.7784698Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7784847Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7784916Z     
2025-04-11T04:23:18.7784987Z         Args:
2025-04-11T04:23:18.7785155Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7785322Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7785428Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7785498Z         """
2025-04-11T04:23:18.7785577Z         _lazy_init()
2025-04-11T04:23:18.7785669Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7785769Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7785873Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7786154Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7786382Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7786540Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7786545Z 
2025-04-11T04:23:18.7786782Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7786933Z _______________ test_copy_kv_to_caches[False-5-True-16-16-16-7] ________________
2025-04-11T04:23:18.7786937Z 
2025-04-11T04:23:18.7787082Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7787239Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7787243Z 
2025-04-11T04:23:18.7787441Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7787547Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7787671Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7787806Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7787972Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7788108Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7788216Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7788363Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7788487Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7788561Z         bsz: int,
2025-04-11T04:23:18.7788644Z         block_size: int,
2025-04-11T04:23:18.7788792Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7788873Z         num_kv_heads: int,
2025-04-11T04:23:18.7788960Z         same_context_len: bool,
2025-04-11T04:23:18.7789035Z         n_tokens: int,
2025-04-11T04:23:18.7789124Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7789193Z     ):
2025-04-11T04:23:18.7789280Z         torch.manual_seed(123)
2025-04-11T04:23:18.7789371Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7789458Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7789464Z 
2025-04-11T04:23:18.7789617Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7789727Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7789731Z 
2025-04-11T04:23:18.7789807Z device = None
2025-04-11T04:23:18.7789811Z 
2025-04-11T04:23:18.7789926Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7790076Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7790146Z     
2025-04-11T04:23:18.7790217Z         Args:
2025-04-11T04:23:18.7790386Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7790550Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7790655Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7790730Z         """
2025-04-11T04:23:18.7790805Z         _lazy_init()
2025-04-11T04:23:18.7790903Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7791003Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7791109Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7791394Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7791531Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7791684Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7791690Z 
2025-04-11T04:23:18.7791925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7792080Z _______________ test_copy_kv_to_caches[False-5-True-16-16-16-32] _______________
2025-04-11T04:23:18.7792174Z 
2025-04-11T04:23:18.7792323Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7792479Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7792485Z 
2025-04-11T04:23:18.7792681Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7792788Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7792907Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7793040Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7793151Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7793289Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7793398Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7793542Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7793632Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7793709Z         bsz: int,
2025-04-11T04:23:18.7793789Z         block_size: int,
2025-04-11T04:23:18.7793876Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7794017Z         num_kv_heads: int,
2025-04-11T04:23:18.7794103Z         same_context_len: bool,
2025-04-11T04:23:18.7794179Z         n_tokens: int,
2025-04-11T04:23:18.7794267Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7794337Z     ):
2025-04-11T04:23:18.7794420Z         torch.manual_seed(123)
2025-04-11T04:23:18.7794510Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7794596Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7794599Z 
2025-04-11T04:23:18.7794801Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7794912Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7794916Z 
2025-04-11T04:23:18.7794992Z device = None
2025-04-11T04:23:18.7794996Z 
2025-04-11T04:23:18.7795110Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7795259Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7795331Z     
2025-04-11T04:23:18.7795403Z         Args:
2025-04-11T04:23:18.7795570Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7795733Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7795838Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7795909Z         """
2025-04-11T04:23:18.7795985Z         _lazy_init()
2025-04-11T04:23:18.7796081Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7796182Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7796288Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7796571Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7796708Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7796867Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7796873Z 
2025-04-11T04:23:18.7797107Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7797261Z _______________ test_copy_kv_to_caches[False-5-True-16-16-32-7] ________________
2025-04-11T04:23:18.7797265Z 
2025-04-11T04:23:18.7797407Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7797559Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7797564Z 
2025-04-11T04:23:18.7797760Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7797866Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7797985Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7798119Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7798317Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7798451Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7798561Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7798706Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7798793Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7798866Z         bsz: int,
2025-04-11T04:23:18.7798944Z         block_size: int,
2025-04-11T04:23:18.7799034Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7799115Z         num_kv_heads: int,
2025-04-11T04:23:18.7799202Z         same_context_len: bool,
2025-04-11T04:23:18.7799279Z         n_tokens: int,
2025-04-11T04:23:18.7799366Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7799438Z     ):
2025-04-11T04:23:18.7799521Z         torch.manual_seed(123)
2025-04-11T04:23:18.7799610Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7799700Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7799704Z 
2025-04-11T04:23:18.7799853Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7800017Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7800022Z 
2025-04-11T04:23:18.7800097Z device = None
2025-04-11T04:23:18.7800104Z 
2025-04-11T04:23:18.7800220Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7800366Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7800438Z     
2025-04-11T04:23:18.7800508Z         Args:
2025-04-11T04:23:18.7800711Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7800876Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7800980Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7801054Z         """
2025-04-11T04:23:18.7801137Z         _lazy_init()
2025-04-11T04:23:18.7801233Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7801337Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7801448Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7801731Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7801863Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7802024Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7802030Z 
2025-04-11T04:23:18.7802268Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7802426Z _______________ test_copy_kv_to_caches[False-5-True-16-16-32-32] _______________
2025-04-11T04:23:18.7802430Z 
2025-04-11T04:23:18.7802575Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7802734Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7802737Z 
2025-04-11T04:23:18.7802933Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7803040Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7803159Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7803291Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7803405Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7803544Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7803655Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7803804Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7803894Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7803967Z         bsz: int,
2025-04-11T04:23:18.7804048Z         block_size: int,
2025-04-11T04:23:18.7804256Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7804339Z         num_kv_heads: int,
2025-04-11T04:23:18.7804426Z         same_context_len: bool,
2025-04-11T04:23:18.7804507Z         n_tokens: int,
2025-04-11T04:23:18.7804599Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7804674Z     ):
2025-04-11T04:23:18.7804759Z         torch.manual_seed(123)
2025-04-11T04:23:18.7804849Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7804939Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7804944Z 
2025-04-11T04:23:18.7805101Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7805213Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7805217Z 
2025-04-11T04:23:18.7805292Z device = None
2025-04-11T04:23:18.7805296Z 
2025-04-11T04:23:18.7805418Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7805566Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7805642Z     
2025-04-11T04:23:18.7805713Z         Args:
2025-04-11T04:23:18.7805885Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7806101Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7806204Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7806278Z         """
2025-04-11T04:23:18.7806355Z         _lazy_init()
2025-04-11T04:23:18.7806450Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7806550Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7806705Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7806990Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7807123Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7807281Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7807289Z 
2025-04-11T04:23:18.7807525Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7807683Z _______________ test_copy_kv_to_caches[False-5-True-16-16-64-7] ________________
2025-04-11T04:23:18.7808939Z 
2025-04-11T04:23:18.7809092Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7809244Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7809251Z 
2025-04-11T04:23:18.7809450Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7809557Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7809680Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7809811Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7809924Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7810066Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7810172Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7810325Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7810411Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7810509Z         bsz: int,
2025-04-11T04:23:18.7810592Z         block_size: int,
2025-04-11T04:23:18.7810678Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7810759Z         num_kv_heads: int,
2025-04-11T04:23:18.7810844Z         same_context_len: bool,
2025-04-11T04:23:18.7810922Z         n_tokens: int,
2025-04-11T04:23:18.7811011Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7811080Z     ):
2025-04-11T04:23:18.7811168Z         torch.manual_seed(123)
2025-04-11T04:23:18.7811254Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7811341Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7811345Z 
2025-04-11T04:23:18.7811558Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7811669Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7811675Z 
2025-04-11T04:23:18.7811754Z device = None
2025-04-11T04:23:18.7811758Z 
2025-04-11T04:23:18.7811875Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7812027Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7812097Z     
2025-04-11T04:23:18.7812168Z         Args:
2025-04-11T04:23:18.7812334Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7812502Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7812610Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7812680Z         """
2025-04-11T04:23:18.7812758Z         _lazy_init()
2025-04-11T04:23:18.7812848Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7812949Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7813056Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7813393Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7813529Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7813687Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7813692Z 
2025-04-11T04:23:18.7813933Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7814135Z _______________ test_copy_kv_to_caches[False-5-True-16-16-64-32] _______________
2025-04-11T04:23:18.7814139Z 
2025-04-11T04:23:18.7814282Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7814437Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7814444Z 
2025-04-11T04:23:18.7814639Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7814749Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7814870Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7815004Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7815186Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7815328Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7815433Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7815581Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7815671Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7815745Z         bsz: int,
2025-04-11T04:23:18.7815828Z         block_size: int,
2025-04-11T04:23:18.7815914Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7815996Z         num_kv_heads: int,
2025-04-11T04:23:18.7816085Z         same_context_len: bool,
2025-04-11T04:23:18.7816161Z         n_tokens: int,
2025-04-11T04:23:18.7816250Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7816322Z     ):
2025-04-11T04:23:18.7816405Z         torch.manual_seed(123)
2025-04-11T04:23:18.7816496Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7816586Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7816590Z 
2025-04-11T04:23:18.7816741Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7816851Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7816857Z 
2025-04-11T04:23:18.7816934Z device = None
2025-04-11T04:23:18.7816938Z 
2025-04-11T04:23:18.7817053Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7817203Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7817272Z     
2025-04-11T04:23:18.7817343Z         Args:
2025-04-11T04:23:18.7817565Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7817727Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7817837Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7817908Z         """
2025-04-11T04:23:18.7817985Z         _lazy_init()
2025-04-11T04:23:18.7818080Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7818181Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7818288Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7818569Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7818709Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7818865Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7818869Z 
2025-04-11T04:23:18.7819108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7819266Z _______________ test_copy_kv_to_caches[False-5-False-16-16-16-7] _______________
2025-04-11T04:23:18.7819320Z 
2025-04-11T04:23:18.7819467Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7819628Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7819632Z 
2025-04-11T04:23:18.7819831Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7819939Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7820111Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7820246Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7820356Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7820495Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7820606Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7820752Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7820843Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7820917Z         bsz: int,
2025-04-11T04:23:18.7820999Z         block_size: int,
2025-04-11T04:23:18.7821139Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7821220Z         num_kv_heads: int,
2025-04-11T04:23:18.7821306Z         same_context_len: bool,
2025-04-11T04:23:18.7821383Z         n_tokens: int,
2025-04-11T04:23:18.7821471Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7821543Z     ):
2025-04-11T04:23:18.7821627Z         torch.manual_seed(123)
2025-04-11T04:23:18.7821716Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7821804Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7821808Z 
2025-04-11T04:23:18.7821960Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7822071Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7822078Z 
2025-04-11T04:23:18.7822157Z device = None
2025-04-11T04:23:18.7822162Z 
2025-04-11T04:23:18.7822279Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7822427Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7822501Z     
2025-04-11T04:23:18.7822571Z         Args:
2025-04-11T04:23:18.7822739Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7822904Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7823013Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7823083Z         """
2025-04-11T04:23:18.7823159Z         _lazy_init()
2025-04-11T04:23:18.7823253Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7823353Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7823516Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7823796Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7823934Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7824092Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7824098Z 
2025-04-11T04:23:18.7824332Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7824489Z ______________ test_copy_kv_to_caches[False-5-False-16-16-16-32] _______________
2025-04-11T04:23:18.7824495Z 
2025-04-11T04:23:18.7824641Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7824799Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7824804Z 
2025-04-11T04:23:18.7825000Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7825109Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7825229Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7825417Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7825528Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7825669Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7825777Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7825924Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7826065Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7826138Z         bsz: int,
2025-04-11T04:23:18.7826219Z         block_size: int,
2025-04-11T04:23:18.7826311Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7826391Z         num_kv_heads: int,
2025-04-11T04:23:18.7826483Z         same_context_len: bool,
2025-04-11T04:23:18.7826561Z         n_tokens: int,
2025-04-11T04:23:18.7826656Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7826725Z     ):
2025-04-11T04:23:18.7826811Z         torch.manual_seed(123)
2025-04-11T04:23:18.7826902Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7826991Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7826995Z 
2025-04-11T04:23:18.7827220Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7827331Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7827336Z 
2025-04-11T04:23:18.7827411Z device = None
2025-04-11T04:23:18.7827415Z 
2025-04-11T04:23:18.7827540Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7827692Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7827760Z     
2025-04-11T04:23:18.7827834Z         Args:
2025-04-11T04:23:18.7827999Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7828163Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7828268Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7828343Z         """
2025-04-11T04:23:18.7828453Z         _lazy_init()
2025-04-11T04:23:18.7828547Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7828652Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7828754Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7829035Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7829174Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7829331Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7829336Z 
2025-04-11T04:23:18.7829574Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7829787Z _______________ test_copy_kv_to_caches[False-5-False-16-16-32-7] _______________
2025-04-11T04:23:18.7829791Z 
2025-04-11T04:23:18.7829940Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7830091Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7830097Z 
2025-04-11T04:23:18.7830298Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7830402Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7830521Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7830658Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7830768Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7830907Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7831012Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7831165Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7831252Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7831325Z         bsz: int,
2025-04-11T04:23:18.7831466Z         block_size: int,
2025-04-11T04:23:18.7831554Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7831636Z         num_kv_heads: int,
2025-04-11T04:23:18.7831721Z         same_context_len: bool,
2025-04-11T04:23:18.7831797Z         n_tokens: int,
2025-04-11T04:23:18.7831886Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7831955Z     ):
2025-04-11T04:23:18.7832042Z         torch.manual_seed(123)
2025-04-11T04:23:18.7832182Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7832272Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7832280Z 
2025-04-11T04:23:18.7832425Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7832534Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7832540Z 
2025-04-11T04:23:18.7832620Z device = None
2025-04-11T04:23:18.7832624Z 
2025-04-11T04:23:18.7832739Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7832896Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7832965Z     
2025-04-11T04:23:18.7833039Z         Args:
2025-04-11T04:23:18.7833349Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7833515Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7833622Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7833695Z         """
2025-04-11T04:23:18.7833776Z         _lazy_init()
2025-04-11T04:23:18.7833869Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7833969Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7834076Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7834354Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7834494Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7834654Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7834658Z 
2025-04-11T04:23:18.7834898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7835053Z ______________ test_copy_kv_to_caches[False-5-False-16-16-32-32] _______________
2025-04-11T04:23:18.7835057Z 
2025-04-11T04:23:18.7835207Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7835362Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7835367Z 
2025-04-11T04:23:18.7835563Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7835670Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7835844Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7835981Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7836095Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7836234Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7836342Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7836490Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7836580Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7836652Z         bsz: int,
2025-04-11T04:23:18.7836739Z         block_size: int,
2025-04-11T04:23:18.7836825Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7836910Z         num_kv_heads: int,
2025-04-11T04:23:18.7836992Z         same_context_len: bool,
2025-04-11T04:23:18.7837068Z         n_tokens: int,
2025-04-11T04:23:18.7837158Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7837229Z     ):
2025-04-11T04:23:18.7837317Z         torch.manual_seed(123)
2025-04-11T04:23:18.7837403Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7837493Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7837549Z 
2025-04-11T04:23:18.7837704Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7837815Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7837821Z 
2025-04-11T04:23:18.7837900Z device = None
2025-04-11T04:23:18.7837904Z 
2025-04-11T04:23:18.7838019Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7838171Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7838297Z     
2025-04-11T04:23:18.7838369Z         Args:
2025-04-11T04:23:18.7838537Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7838703Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7838811Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7838881Z         """
2025-04-11T04:23:18.7838959Z         _lazy_init()
2025-04-11T04:23:18.7839054Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7839153Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7839311Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7839594Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7839731Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7839890Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7839894Z 
2025-04-11T04:23:18.7840134Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7840286Z _______________ test_copy_kv_to_caches[False-5-False-16-16-64-7] _______________
2025-04-11T04:23:18.7840292Z 
2025-04-11T04:23:18.7840438Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7840597Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7840602Z 
2025-04-11T04:23:18.7840800Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7840913Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7841033Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7841168Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7841281Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7841423Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7841530Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7841678Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7841821Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7841895Z         bsz: int,
2025-04-11T04:23:18.7841981Z         block_size: int,
2025-04-11T04:23:18.7842068Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7842151Z         num_kv_heads: int,
2025-04-11T04:23:18.7842240Z         same_context_len: bool,
2025-04-11T04:23:18.7842316Z         n_tokens: int,
2025-04-11T04:23:18.7842407Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7842476Z     ):
2025-04-11T04:23:18.7842559Z         torch.manual_seed(123)
2025-04-11T04:23:18.7842648Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7842736Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7842743Z 
2025-04-11T04:23:18.7842898Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7843006Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7843010Z 
2025-04-11T04:23:18.7843086Z device = None
2025-04-11T04:23:18.7843090Z 
2025-04-11T04:23:18.7843212Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7843362Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7843482Z     
2025-04-11T04:23:18.7843554Z         Args:
2025-04-11T04:23:18.7843727Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7843893Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7843999Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7844072Z         """
2025-04-11T04:23:18.7844146Z         _lazy_init()
2025-04-11T04:23:18.7844295Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7844397Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7844506Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7844789Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7844928Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7845089Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7845095Z 
2025-04-11T04:23:18.7845332Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7845537Z ______________ test_copy_kv_to_caches[False-5-False-16-16-64-32] _______________
2025-04-11T04:23:18.7845542Z 
2025-04-11T04:23:18.7845687Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7845843Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7845850Z 
2025-04-11T04:23:18.7846046Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7846154Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7846273Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7846410Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7846522Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7846664Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7846774Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7846925Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7847015Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7847089Z         bsz: int,
2025-04-11T04:23:18.7847172Z         block_size: int,
2025-04-11T04:23:18.7847259Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7847342Z         num_kv_heads: int,
2025-04-11T04:23:18.7847430Z         same_context_len: bool,
2025-04-11T04:23:18.7847508Z         n_tokens: int,
2025-04-11T04:23:18.7847598Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7847667Z     ):
2025-04-11T04:23:18.7847754Z         torch.manual_seed(123)
2025-04-11T04:23:18.7847901Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7847991Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7847995Z 
2025-04-11T04:23:18.7848148Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7848260Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7848265Z 
2025-04-11T04:23:18.7848344Z device = None
2025-04-11T04:23:18.7848348Z 
2025-04-11T04:23:18.7848467Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7848614Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7848686Z     
2025-04-11T04:23:18.7848760Z         Args:
2025-04-11T04:23:18.7848931Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7849095Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7849202Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7849275Z         """
2025-04-11T04:23:18.7849354Z         _lazy_init()
2025-04-11T04:23:18.7849451Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7849605Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7849714Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7849997Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7850138Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7850295Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7850362Z 
2025-04-11T04:23:18.7850600Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7850736Z _______________________________ test_layer_norm ________________________________
2025-04-11T04:23:18.7850740Z 
2025-04-11T04:23:18.7850834Z kwargs = {}, val = 2, arg_map = {'M': 2}
2025-04-11T04:23:18.7851196Z partial_func = functools.partial(<function parameterize.<locals>._wrapper.<locals>._execute_function_by_param at 0x7fb5b8741750>, M=2)
2025-04-11T04:23:18.7851202Z 
2025-04-11T04:23:18.7851307Z     def _execute_function_by_param(**kwargs):
2025-04-11T04:23:18.7851391Z         for val in values:
2025-04-11T04:23:18.7851532Z             arg_map = {argument: val}
2025-04-11T04:23:18.7851640Z             partial_func = partial(func, **arg_map)
2025-04-11T04:23:18.7851728Z >           partial_func(**kwargs)
2025-04-11T04:23:18.7851734Z 
2025-04-11T04:23:18.7851825Z colossalai/testing/utils.py:64: 
2025-04-11T04:23:18.7851940Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7852098Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T04:23:18.7852185Z     partial_func(**kwargs)
2025-04-11T04:23:18.7852292Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7852298Z 
2025-04-11T04:23:18.7852371Z M = 2, N = 64
2025-04-11T04:23:18.7852375Z 
2025-04-11T04:23:18.7852464Z     @pytest.mark.skipif(
2025-04-11T04:23:18.7852712Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T04:23:18.7852789Z     )
2025-04-11T04:23:18.7852882Z     @parameterize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.7852977Z     @parameterize("N", [64, 128])
2025-04-11T04:23:18.7853066Z     def test_layer_norm(M, N):
2025-04-11T04:23:18.7853156Z         dtype = torch.float16
2025-04-11T04:23:18.7853233Z         eps = 1e-5
2025-04-11T04:23:18.7853313Z         x_shape = (M, N)
2025-04-11T04:23:18.7853405Z         w_shape = (x_shape[-1],)
2025-04-11T04:23:18.7853545Z >       weight = torch.ones(w_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.7853652Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7853935Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7854126Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7854287Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7854294Z 
2025-04-11T04:23:18.7854475Z tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py:30: RuntimeError
2025-04-11T04:23:18.7854631Z ___________________ test_rotary_emb[True-dtype0-64-32-64-4] ____________________
2025-04-11T04:23:18.7854635Z 
2025-04-11T04:23:18.7854776Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
2025-04-11T04:23:18.7854871Z use_new_kcache_layout = True
2025-04-11T04:23:18.7854876Z 
2025-04-11T04:23:18.7854967Z     @pytest.mark.skipif(
2025-04-11T04:23:18.7855210Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T04:23:18.7855284Z     )
2025-04-11T04:23:18.7855395Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.7855509Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.7855607Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.7855706Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.7855884Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T04:23:18.7856038Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7856217Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
2025-04-11T04:23:18.7856313Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.7856417Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.7856598Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.7856726Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.7856819Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.7856985Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.7857082Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.7857201Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.7857307Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.7857406Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.7857490Z         cos_2 = cos[:, :32]
2025-04-11T04:23:18.7857625Z         sin_2 = sin[:, :32]
2025-04-11T04:23:18.7857745Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.7857880Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.7858095Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.7858227Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.7858296Z     
2025-04-11T04:23:18.7858377Z         # create data
2025-04-11T04:23:18.7858456Z         block_size = 32
2025-04-11T04:23:18.7858544Z         max_num_blocks_per_seq = 4
2025-04-11T04:23:18.7858641Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.7858783Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.7858897Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7859185Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7859321Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7859484Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7859489Z 
2025-04-11T04:23:18.7859691Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
2025-04-11T04:23:18.7859851Z ___________________ test_rotary_emb[False-dtype0-64-32-64-4] ___________________
2025-04-11T04:23:18.7859855Z 
2025-04-11T04:23:18.7859993Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
2025-04-11T04:23:18.7860086Z use_new_kcache_layout = False
2025-04-11T04:23:18.7860139Z 
2025-04-11T04:23:18.7860229Z     @pytest.mark.skipif(
2025-04-11T04:23:18.7860467Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T04:23:18.7860540Z     )
2025-04-11T04:23:18.7860649Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.7860756Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.7860853Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.7860948Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.7861071Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T04:23:18.7861229Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7861406Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
2025-04-11T04:23:18.7861500Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.7861606Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.7861733Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.7861862Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.7862008Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.7862181Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.7862271Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.7862389Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.7862491Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.7862587Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.7862736Z         cos_2 = cos[:, :32]
2025-04-11T04:23:18.7862817Z         sin_2 = sin[:, :32]
2025-04-11T04:23:18.7862932Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.7863062Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.7863270Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.7863399Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.7863467Z     
2025-04-11T04:23:18.7863550Z         # create data
2025-04-11T04:23:18.7863629Z         block_size = 32
2025-04-11T04:23:18.7863716Z         max_num_blocks_per_seq = 4
2025-04-11T04:23:18.7863811Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.7864001Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.7864107Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7864390Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7864530Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7864687Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7864692Z 
2025-04-11T04:23:18.7864891Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
2025-04-11T04:23:18.7865043Z _____________________ test_get_xine_cache[dtype0-64-64-4] ______________________
2025-04-11T04:23:18.7865050Z 
2025-04-11T04:23:18.7865196Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T04:23:18.7865201Z 
2025-04-11T04:23:18.7865291Z     @pytest.mark.skipif(
2025-04-11T04:23:18.7865528Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T04:23:18.7865601Z     )
2025-04-11T04:23:18.7865706Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.7865817Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T04:23:18.7865928Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T04:23:18.7866053Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T04:23:18.7866205Z     def test_get_xine_cache(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T04:23:18.7866309Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T04:23:18.7866546Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T04:23:18.7866647Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7866929Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7867066Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7867221Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7867226Z 
2025-04-11T04:23:18.7867397Z tests/test_infer/test_kernels/triton/test_xine_copy.py:50: RuntimeError
2025-04-11T04:23:18.7867548Z _____________________ test_models_lazy_init[cuda-subset0] ______________________
2025-04-11T04:23:18.7867552Z 
2025-04-11T04:23:18.7867968Z subset = ['custom_hanging_param_model', 'custom_nested_model', 'custom_repeated_computed_layers', 'custom_simple_net', 'diffusers_clip_text_model', 'diffusers_auto_encoder_kl', ...]
2025-04-11T04:23:18.7868057Z default_device = 'cuda'
2025-04-11T04:23:18.7868062Z 
2025-04-11T04:23:18.7868235Z     @pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
2025-04-11T04:23:18.7868384Z     @pytest.mark.parametrize(
2025-04-11T04:23:18.7868488Z         "subset",
2025-04-11T04:23:18.7868562Z         (
2025-04-11T04:23:18.7868646Z             [COMMON_MODELS]
2025-04-11T04:23:18.7868729Z             if IS_FAST_TEST
2025-04-11T04:23:18.7868931Z             else ["torchvision", "diffusers", "timm", "transformers", "torchaudio", "deepfm", "dlrm"]
2025-04-11T04:23:18.7869007Z         ),
2025-04-11T04:23:18.7869134Z     )
2025-04-11T04:23:18.7869281Z     @pytest.mark.parametrize("default_device", ["cpu", "cuda"])
2025-04-11T04:23:18.7869405Z     def test_models_lazy_init(subset, default_device):
2025-04-11T04:23:18.7869562Z         sub_model_zoo = model_zoo.get_sub_registry(subset, allow_empty=True)
2025-04-11T04:23:18.7869672Z         for name, entry in sub_model_zoo.items():
2025-04-11T04:23:18.7869841Z             # TODO(ver217): lazy init does not support weight norm, skip these models
2025-04-11T04:23:18.7869924Z             if name in (
2025-04-11T04:23:18.7870022Z                 "torchaudio_wav2vec2_base",
2025-04-11T04:23:18.7870115Z                 "torchaudio_hubert_base",
2025-04-11T04:23:18.7870200Z                 "timm_beit",
2025-04-11T04:23:18.7870351Z                 "timm_vision_transformer",
2025-04-11T04:23:18.7870434Z                 "timm_deit",
2025-04-11T04:23:18.7870515Z                 "timm_beitv2",
2025-04-11T04:23:18.7870594Z                 "timm_deit3",
2025-04-11T04:23:18.7870677Z                 "timm_convit",
2025-04-11T04:23:18.7870766Z                 "timm_tnt_b_patch16_224",
2025-04-11T04:23:18.7870977Z             ) or name.startswith(("transformers_vit", "transformers_blip2", "transformers_whisper")):
2025-04-11T04:23:18.7871054Z                 continue
2025-04-11T04:23:18.7871218Z >           check_lazy_init(entry, verbose=True, default_device=default_device)
2025-04-11T04:23:18.7871224Z 
2025-04-11T04:23:18.7871319Z tests/test_lazy/test_models.py:33: 
2025-04-11T04:23:18.7871432Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7871578Z tests/test_lazy/lazy_init_utils.py:77: in check_lazy_init
2025-04-11T04:23:18.7871658Z     model = model_fn()
2025-04-11T04:23:18.7871824Z tests/kit/model_zoo/custom/hanging_param_model.py:17: in __init__
2025-04-11T04:23:18.7871916Z     self.proj1 = nn.Linear(4, 8)
2025-04-11T04:23:18.7872170Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:98: in __init__
2025-04-11T04:23:18.7872368Z     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
2025-04-11T04:23:18.7872477Z colossalai/lazy/lazy_init.py:506: in wrapper
2025-04-11T04:23:18.7872595Z     return self.tensor_cls(target, *args, **kwargs)
2025-04-11T04:23:18.7872705Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7872766Z 
2025-04-11T04:23:18.7872890Z cls = <class 'colossalai.lazy.lazy_init._MyTensor'>
2025-04-11T04:23:18.7873035Z func = <built-in method empty of type object at 0x7fb8e3cd9840>
2025-04-11T04:23:18.7873135Z concrete_data = None, args = ((8, 4),)
2025-04-11T04:23:18.7873234Z kwargs = {'device': 'cuda', 'dtype': None}
2025-04-11T04:23:18.7873240Z 
2025-04-11T04:23:18.7873405Z     def __new__(cls, func, *args, concrete_data=None, **kwargs) -> "_MyTensor":
2025-04-11T04:23:18.7873497Z         cls._pre_op_fn()
2025-04-11T04:23:18.7873587Z         if concrete_data is not None:
2025-04-11T04:23:18.7873682Z             # uniform api as LazyTensor
2025-04-11T04:23:18.7873769Z             data = concrete_data
2025-04-11T04:23:18.7873844Z         else:
2025-04-11T04:23:18.7873945Z             kwargs["device"] = cls.default_device
2025-04-11T04:23:18.7874033Z >           data = func(*args, **kwargs)
2025-04-11T04:23:18.7874140Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7874429Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7874645Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7874805Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7874811Z 
2025-04-11T04:23:18.7874922Z colossalai/lazy/lazy_init.py:93: RuntimeError
2025-04-11T04:23:18.7875053Z ________________________________ test_lazy_ops _________________________________
2025-04-11T04:23:18.7875057Z 
2025-04-11T04:23:18.7875224Z     @pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
2025-04-11T04:23:18.7875362Z     def test_lazy_ops():
2025-04-11T04:23:18.7875450Z         with LazyInitContext():
2025-04-11T04:23:18.7875539Z             x = torch.rand(2, 3)
2025-04-11T04:23:18.7875632Z             assert tuple(x.shape) == (2, 3)
2025-04-11T04:23:18.7875728Z             assert x.device.type == "cpu"
2025-04-11T04:23:18.7875816Z             x.requires_grad is False
2025-04-11T04:23:18.7875892Z             y = x.cuda()
2025-04-11T04:23:18.7875986Z             assert tuple(y.shape) == (2, 3)
2025-04-11T04:23:18.7876078Z             assert y.device.type == "cuda"
2025-04-11T04:23:18.7876173Z             assert y.requires_grad is False
2025-04-11T04:23:18.7876310Z             assert x.cpu() is x
2025-04-11T04:23:18.7876408Z             p = Parameter(torch.empty(2, 3))
2025-04-11T04:23:18.7876506Z             assert tuple(p.shape) == (2, 3)
2025-04-11T04:23:18.7876597Z             assert p.device.type == "cpu"
2025-04-11T04:23:18.7876690Z             assert p.requires_grad is True
2025-04-11T04:23:18.7876784Z             assert isinstance(p, Parameter)
2025-04-11T04:23:18.7876867Z         x.materialize()
2025-04-11T04:23:18.7876955Z         assert tuple(x.shape) == (2, 3)
2025-04-11T04:23:18.7877045Z         assert x.device.type == "cpu"
2025-04-11T04:23:18.7877135Z         assert x.requires_grad is False
2025-04-11T04:23:18.7877217Z >       y.materialize()
2025-04-11T04:23:18.7877221Z 
2025-04-11T04:23:18.7877313Z tests/test_lazy/test_ops.py:33: 
2025-04-11T04:23:18.7877423Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7877537Z colossalai/lazy/lazy_init.py:217: in materialize
2025-04-11T04:23:18.7877633Z     target = self._materialize_data()
2025-04-11T04:23:18.7877762Z colossalai/lazy/lazy_init.py:242: in _materialize_data
2025-04-11T04:23:18.7877844Z     init_val = func(
2025-04-11T04:23:18.7877951Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7877958Z 
2025-04-11T04:23:18.7878045Z t = tensor([[0.8823, 0.9150, 0.3829],
2025-04-11T04:23:18.7878124Z         [0.9593, 0.3904, 0.6009]])
2025-04-11T04:23:18.7878215Z kw = {'device': device(type='cuda')}
2025-04-11T04:23:18.7878222Z 
2025-04-11T04:23:18.7878317Z     def factory_fn(t: torch.Tensor, **kw):
2025-04-11T04:23:18.7878402Z >       return t.to(*args, **kwargs)
2025-04-11T04:23:18.7878561Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7878838Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7878974Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7879131Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7879136Z 
2025-04-11T04:23:18.7879246Z colossalai/lazy/lazy_init.py:380: RuntimeError
2025-04-11T04:23:18.7879378Z _____________________________ test_torch_ddp_lora ______________________________
2025-04-11T04:23:18.7879384Z 
2025-04-11T04:23:18.7879471Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.7880093Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.7880102Z 
2025-04-11T04:23:18.7880202Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.7880284Z         try_count = 0
2025-04-11T04:23:18.7880433Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.7880514Z             max_try, int
2025-04-11T04:23:18.7880660Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.7880731Z     
2025-04-11T04:23:18.7880847Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.7880920Z             try:
2025-04-11T04:23:18.7881005Z                 try_count += 1
2025-04-11T04:23:18.7881146Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.7881228Z                 return ret
2025-04-11T04:23:18.7881319Z             except exception_type as e:
2025-04-11T04:23:18.7881418Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.7881608Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.7881728Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.7881877Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.7882031Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.7882163Z                     continue
2025-04-11T04:23:18.7882238Z                 else:
2025-04-11T04:23:18.7882460Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.7882543Z >                   raise e
2025-04-11T04:23:18.7882549Z 
2025-04-11T04:23:18.7882641Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.7882756Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7882883Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.7882971Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.7883111Z tests/test_lora/test_lora.py:108: in test_torch_ddp_lora
2025-04-11T04:23:18.7883191Z     spawn(run_dist, 2)
2025-04-11T04:23:18.7883297Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.7883396Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.7883656Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.7883836Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.7884122Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.7884214Z     while not context.join():
2025-04-11T04:23:18.7884324Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7884329Z 
2025-04-11T04:23:18.7884528Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5e92b00>
2025-04-11T04:23:18.7884607Z timeout = None
2025-04-11T04:23:18.7884612Z 
2025-04-11T04:23:18.7884760Z     def join(self, timeout=None):
2025-04-11T04:23:18.7884886Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.7884961Z     
2025-04-11T04:23:18.7885115Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.7885256Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.7885422Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.7885513Z         of the first process exiting.
2025-04-11T04:23:18.7885585Z     
2025-04-11T04:23:18.7885731Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.7885868Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.7885939Z     
2025-04-11T04:23:18.7886011Z         Args:
2025-04-11T04:23:18.7886153Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.7886223Z         """
2025-04-11T04:23:18.7886366Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.7886455Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.7886532Z             return True
2025-04-11T04:23:18.7886658Z     
2025-04-11T04:23:18.7886792Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.7886912Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.7887006Z             self.sentinels.keys(),
2025-04-11T04:23:18.7887089Z             timeout=timeout,
2025-04-11T04:23:18.7887165Z         )
2025-04-11T04:23:18.7887232Z     
2025-04-11T04:23:18.7887315Z         error_index = None
2025-04-11T04:23:18.7887456Z         for sentinel in ready:
2025-04-11T04:23:18.7887562Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.7887663Z             process = self.processes[index]
2025-04-11T04:23:18.7887747Z             process.join()
2025-04-11T04:23:18.7887842Z             if process.exitcode != 0:
2025-04-11T04:23:18.7887928Z                 error_index = index
2025-04-11T04:23:18.7888009Z                 break
2025-04-11T04:23:18.7888078Z     
2025-04-11T04:23:18.7888168Z         # Return if there was no error.
2025-04-11T04:23:18.7888258Z         if error_index is None:
2025-04-11T04:23:18.7888394Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.7888542Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.7888610Z     
2025-04-11T04:23:18.7888752Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.7888850Z         for process in self.processes:
2025-04-11T04:23:18.7888936Z             if process.is_alive():
2025-04-11T04:23:18.7889031Z                 process.terminate()
2025-04-11T04:23:18.7889117Z             process.join()
2025-04-11T04:23:18.7889188Z     
2025-04-11T04:23:18.7889328Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.7889443Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.7889554Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.7889676Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.7889764Z             if exitcode < 0:
2025-04-11T04:23:18.7889871Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.7889976Z                 raise ProcessExitedException(
2025-04-11T04:23:18.7890130Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.7890225Z                     error_index=error_index,
2025-04-11T04:23:18.7890327Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.7890418Z                     exit_code=exitcode,
2025-04-11T04:23:18.7890506Z                     signal_name=name,
2025-04-11T04:23:18.7890579Z                 )
2025-04-11T04:23:18.7890652Z             else:
2025-04-11T04:23:18.7890760Z                 raise ProcessExitedException(
2025-04-11T04:23:18.7890924Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.7891071Z                     error_index=error_index,
2025-04-11T04:23:18.7891173Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.7891263Z                     exit_code=exitcode,
2025-04-11T04:23:18.7891339Z                 )
2025-04-11T04:23:18.7891408Z     
2025-04-11T04:23:18.7891549Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.7891719Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.7891807Z         msg += original_trace
2025-04-11T04:23:18.7891979Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.7892142Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.7892217Z E       
2025-04-11T04:23:18.7892345Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.7892445Z E       Traceback (most recent call last):
2025-04-11T04:23:18.7892749Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.7892834Z E           fn(i, *args)
2025-04-11T04:23:18.7893098Z E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
2025-04-11T04:23:18.7893180Z E           run_lora_test()
2025-04-11T04:23:18.7893401Z E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
2025-04-11T04:23:18.7893577Z E           check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
2025-04-11T04:23:18.7893799Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.7893950Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.7894214Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.7894314Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.7894595Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.7894700Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7894803Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7895087Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7895270Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7895430Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7895437Z 
2025-04-11T04:23:18.7895740Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.7895898Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.7896056Z [04/11/25 04:17:31] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.7896187Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.7896296Z                              :75 launch                                         
2025-04-11T04:23:18.7896435Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.7896562Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.7896755Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.7896903Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.7898052Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.7898294Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.7899422Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.7899596Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.7900297Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.7900386Z   warnings.warn(
2025-04-11T04:23:18.7901082Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.7901221Z   warnings.warn(
2025-04-11T04:23:18.7902060Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7902190Z   warnings.warn(
2025-04-11T04:23:18.7903009Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7903093Z   warnings.warn(
2025-04-11T04:23:18.7903903Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7904030Z   warnings.warn(
2025-04-11T04:23:18.7904842Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7904921Z   warnings.warn(
2025-04-11T04:23:18.7905713Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7905793Z   warnings.warn(
2025-04-11T04:23:18.7906606Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7906686Z   warnings.warn(
2025-04-11T04:23:18.7906992Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:61905 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.7907550Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.7907682Z   warnings.warn(
2025-04-11T04:23:18.7908215Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.7908296Z   warnings.warn(
2025-04-11T04:23:18.7908864Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.7908945Z   warnings.warn(
2025-04-11T04:23:18.7909468Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.7909544Z   warnings.warn(
2025-04-11T04:23:18.7909693Z _________________________ test_moe_kernel[data_type0] __________________________
2025-04-11T04:23:18.7909697Z 
2025-04-11T04:23:18.7909785Z data_type = torch.float32
2025-04-11T04:23:18.7909848Z 
2025-04-11T04:23:18.7910026Z     @pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
2025-04-11T04:23:18.7910118Z     def test_moe_kernel(data_type):
2025-04-11T04:23:18.7910211Z         torch.manual_seed(1024)
2025-04-11T04:23:18.7910292Z >       run_moe_cumsum()
2025-04-11T04:23:18.7910297Z 
2025-04-11T04:23:18.7910390Z tests/test_moe/test_kernel.py:93: 
2025-04-11T04:23:18.7910506Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7910563Z 
2025-04-11T04:23:18.7910647Z     def run_moe_cumsum():
2025-04-11T04:23:18.7910737Z         test_mask = torch.tensor(
2025-04-11T04:23:18.7910808Z             [
2025-04-11T04:23:18.7910886Z                 [0, 1, 0, 0],
2025-04-11T04:23:18.7910965Z                 [1, 0, 0, 0],
2025-04-11T04:23:18.7911041Z                 [0, 1, 0, 0],
2025-04-11T04:23:18.7911117Z                 [1, 0, 0, 0],
2025-04-11T04:23:18.7911188Z             ],
2025-04-11T04:23:18.7911279Z             dtype=torch.int32,
2025-04-11T04:23:18.7911354Z >       ).to("cuda")
2025-04-11T04:23:18.7911460Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7911806Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7911941Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7912105Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7912112Z 
2025-04-11T04:23:18.7912223Z tests/test_moe/test_kernel.py:29: RuntimeError
2025-04-11T04:23:18.7912366Z _________________________ test_moe_kernel[data_type1] __________________________
2025-04-11T04:23:18.7912371Z 
2025-04-11T04:23:18.7912456Z data_type = torch.float16
2025-04-11T04:23:18.7912460Z 
2025-04-11T04:23:18.7912628Z     @pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
2025-04-11T04:23:18.7912722Z     def test_moe_kernel(data_type):
2025-04-11T04:23:18.7912811Z         torch.manual_seed(1024)
2025-04-11T04:23:18.7912895Z >       run_moe_cumsum()
2025-04-11T04:23:18.7912900Z 
2025-04-11T04:23:18.7912992Z tests/test_moe/test_kernel.py:93: 
2025-04-11T04:23:18.7913104Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7913108Z 
2025-04-11T04:23:18.7913186Z     def run_moe_cumsum():
2025-04-11T04:23:18.7913273Z         test_mask = torch.tensor(
2025-04-11T04:23:18.7913348Z             [
2025-04-11T04:23:18.7913426Z                 [0, 1, 0, 0],
2025-04-11T04:23:18.7913505Z                 [1, 0, 0, 0],
2025-04-11T04:23:18.7913578Z                 [0, 1, 0, 0],
2025-04-11T04:23:18.7913653Z                 [1, 0, 0, 0],
2025-04-11T04:23:18.7913723Z             ],
2025-04-11T04:23:18.7913810Z             dtype=torch.int32,
2025-04-11T04:23:18.7913945Z >       ).to("cuda")
2025-04-11T04:23:18.7914049Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7914334Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7914469Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7914627Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7914637Z 
2025-04-11T04:23:18.7914746Z tests/test_moe/test_kernel.py:29: RuntimeError
2025-04-11T04:23:18.7914885Z __________________________ test_mixtral_moe_layer[4] ___________________________
2025-04-11T04:23:18.7914892Z 
2025-04-11T04:23:18.7914970Z world_size = 4
2025-04-11T04:23:18.7914974Z 
2025-04-11T04:23:18.7915083Z     @pytest.mark.parametrize("world_size", [4])
2025-04-11T04:23:18.7915194Z     def test_mixtral_moe_layer(world_size: int):
2025-04-11T04:23:18.7915284Z >       spawn(run_dist, world_size)
2025-04-11T04:23:18.7915291Z 
2025-04-11T04:23:18.7915399Z tests/test_moe/test_moe_checkpoint.py:171: 
2025-04-11T04:23:18.7915505Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7915656Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.7915758Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.7916021Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.7916201Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.7916490Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.7916631Z     while not context.join():
2025-04-11T04:23:18.7916737Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7916742Z 
2025-04-11T04:23:18.7916939Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f0aa70>
2025-04-11T04:23:18.7917018Z timeout = None
2025-04-11T04:23:18.7917023Z 
2025-04-11T04:23:18.7917111Z     def join(self, timeout=None):
2025-04-11T04:23:18.7917240Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.7917310Z     
2025-04-11T04:23:18.7917459Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.7917651Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.7917814Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.7917906Z         of the first process exiting.
2025-04-11T04:23:18.7917975Z     
2025-04-11T04:23:18.7918128Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.7918264Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.7918335Z     
2025-04-11T04:23:18.7918407Z         Args:
2025-04-11T04:23:18.7918544Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.7918622Z         """
2025-04-11T04:23:18.7918757Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.7918854Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.7918930Z             return True
2025-04-11T04:23:18.7918999Z     
2025-04-11T04:23:18.7919132Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.7919250Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.7919342Z             self.sentinels.keys(),
2025-04-11T04:23:18.7919424Z             timeout=timeout,
2025-04-11T04:23:18.7919497Z         )
2025-04-11T04:23:18.7919567Z     
2025-04-11T04:23:18.7919649Z         error_index = None
2025-04-11T04:23:18.7919738Z         for sentinel in ready:
2025-04-11T04:23:18.7919843Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.7919942Z             process = self.processes[index]
2025-04-11T04:23:18.7920026Z             process.join()
2025-04-11T04:23:18.7920169Z             if process.exitcode != 0:
2025-04-11T04:23:18.7920258Z                 error_index = index
2025-04-11T04:23:18.7920333Z                 break
2025-04-11T04:23:18.7920407Z     
2025-04-11T04:23:18.7920497Z         # Return if there was no error.
2025-04-11T04:23:18.7920581Z         if error_index is None:
2025-04-11T04:23:18.7920717Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.7920813Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.7920884Z     
2025-04-11T04:23:18.7921025Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.7921124Z         for process in self.processes:
2025-04-11T04:23:18.7921213Z             if process.is_alive():
2025-04-11T04:23:18.7921302Z                 process.terminate()
2025-04-11T04:23:18.7921390Z             process.join()
2025-04-11T04:23:18.7921457Z     
2025-04-11T04:23:18.7921600Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.7921716Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.7921823Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.7921997Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.7922081Z             if exitcode < 0:
2025-04-11T04:23:18.7922192Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.7922300Z                 raise ProcessExitedException(
2025-04-11T04:23:18.7922453Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.7922548Z                     error_index=error_index,
2025-04-11T04:23:18.7922697Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.7922789Z                     exit_code=exitcode,
2025-04-11T04:23:18.7922875Z                     signal_name=name,
2025-04-11T04:23:18.7922949Z                 )
2025-04-11T04:23:18.7923023Z             else:
2025-04-11T04:23:18.7923125Z                 raise ProcessExitedException(
2025-04-11T04:23:18.7923293Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.7923385Z                     error_index=error_index,
2025-04-11T04:23:18.7923488Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.7923573Z                     exit_code=exitcode,
2025-04-11T04:23:18.7923718Z                 )
2025-04-11T04:23:18.7923787Z     
2025-04-11T04:23:18.7923920Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.7924095Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.7924178Z         msg += original_trace
2025-04-11T04:23:18.7924357Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.7924520Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.7924595Z E       
2025-04-11T04:23:18.7924721Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.7924823Z E       Traceback (most recent call last):
2025-04-11T04:23:18.7925130Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.7925213Z E           fn(i, *args)
2025-04-11T04:23:18.7925452Z E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
2025-04-11T04:23:18.7925547Z E           check_moe_checkpoint()
2025-04-11T04:23:18.7925807Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.7925905Z E           partial_func(**kwargs)
2025-04-11T04:23:18.7926172Z E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
2025-04-11T04:23:18.7926309Z E           dist.broadcast_object_list(broadcast_objects, src=0)
2025-04-11T04:23:18.7926611Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.7926759Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.7927121Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
2025-04-11T04:23:18.7927341Z E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T04:23:18.7927670Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
2025-04-11T04:23:18.7927876Z E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T04:23:18.7928227Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
2025-04-11T04:23:18.7928365Z E           byte_tensor = torch.ByteTensor(byte_storage).to(device)
2025-04-11T04:23:18.7928472Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7928757Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7928958Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7929114Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7929120Z 
2025-04-11T04:23:18.7929436Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.7929587Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.7929795Z [04/11/25 04:17:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.7929924Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.7930029Z                              :75 launch                                         
2025-04-11T04:23:18.7930169Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.7930293Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.7930439Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.7930733Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.7931073Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.7931241Z _____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] ______________
2025-04-11T04:23:18.7931248Z 
2025-04-11T04:23:18.7931391Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7931477Z g_dtype = torch.float16
2025-04-11T04:23:18.7931481Z 
2025-04-11T04:23:18.7931602Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7931735Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7931910Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7932067Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7932157Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7932298Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7932387Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7932527Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7932615Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7932701Z >       check_adam_kernel(
2025-04-11T04:23:18.7932962Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7933033Z         )
2025-04-11T04:23:18.7933037Z 
2025-04-11T04:23:18.7933152Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7933313Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7933474Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7933632Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7933738Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7933744Z 
2025-04-11T04:23:18.7933926Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c57c10>, lr = 0.001
2025-04-11T04:23:18.7934082Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7934089Z 
2025-04-11T04:23:18.7934338Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7934483Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7934646Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7934720Z     
2025-04-11T04:23:18.7934832Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7934955Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7935254Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7935365Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7935646Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7935778Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7936083Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7936087Z 
2025-04-11T04:23:18.7936227Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7936394Z ______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] ______________
2025-04-11T04:23:18.7936400Z 
2025-04-11T04:23:18.7936529Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7936619Z g_dtype = torch.float16
2025-04-11T04:23:18.7936625Z 
2025-04-11T04:23:18.7936747Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7936874Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7937096Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7937246Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7937339Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7937467Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7937560Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7937694Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7937785Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7937867Z >       check_adam_kernel(
2025-04-11T04:23:18.7938124Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7938202Z         )
2025-04-11T04:23:18.7938208Z 
2025-04-11T04:23:18.7938319Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7938433Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7938590Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7938738Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7938846Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7938852Z 
2025-04-11T04:23:18.7939026Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5ad1e40>, lr = 0.001
2025-04-11T04:23:18.7939182Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7939187Z 
2025-04-11T04:23:18.7939427Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7939625Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7939789Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7939863Z     
2025-04-11T04:23:18.7939973Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7940096Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7940337Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7940447Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7940730Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7940864Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7941022Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7941029Z 
2025-04-11T04:23:18.7941164Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7941382Z _____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] ______________
2025-04-11T04:23:18.7941386Z 
2025-04-11T04:23:18.7941516Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.7941606Z g_dtype = torch.float16
2025-04-11T04:23:18.7941610Z 
2025-04-11T04:23:18.7941731Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7941857Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7942082Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7942229Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7942318Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7942448Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7942540Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7942672Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7942756Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7942846Z >       check_adam_kernel(
2025-04-11T04:23:18.7943105Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7943233Z         )
2025-04-11T04:23:18.7943238Z 
2025-04-11T04:23:18.7943354Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7943467Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7943626Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7943773Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7943885Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7943889Z 
2025-04-11T04:23:18.7944069Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a6fd00>, lr = 0.001
2025-04-11T04:23:18.7944221Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.7944228Z 
2025-04-11T04:23:18.7944470Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7944617Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7944778Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7944850Z     
2025-04-11T04:23:18.7944959Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7945078Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7945322Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7945427Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7945771Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7945905Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7946065Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7946069Z 
2025-04-11T04:23:18.7946205Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7946366Z ______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] ______________
2025-04-11T04:23:18.7946371Z 
2025-04-11T04:23:18.7946502Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.7946587Z g_dtype = torch.float16
2025-04-11T04:23:18.7946592Z 
2025-04-11T04:23:18.7946715Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7946838Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7947008Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7947158Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7947244Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7947425Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7947510Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7947644Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7947730Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7947814Z >       check_adam_kernel(
2025-04-11T04:23:18.7948073Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7948211Z         )
2025-04-11T04:23:18.7948217Z 
2025-04-11T04:23:18.7948336Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7948479Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7948638Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7948786Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7948896Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7948902Z 
2025-04-11T04:23:18.7949075Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c77370>, lr = 0.001
2025-04-11T04:23:18.7949281Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.7949288Z 
2025-04-11T04:23:18.7949524Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7949664Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7949828Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7949899Z     
2025-04-11T04:23:18.7950011Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7950127Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7950370Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7950475Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7950756Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7950895Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7951051Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7951055Z 
2025-04-11T04:23:18.7951194Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7951355Z _____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] ______________
2025-04-11T04:23:18.7951359Z 
2025-04-11T04:23:18.7951492Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7951575Z g_dtype = torch.float32
2025-04-11T04:23:18.7951633Z 
2025-04-11T04:23:18.7951760Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7951885Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7952055Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7952206Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7952294Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7952426Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7952511Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7952645Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7952730Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7952811Z >       check_adam_kernel(
2025-04-11T04:23:18.7953069Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7953141Z         )
2025-04-11T04:23:18.7953147Z 
2025-04-11T04:23:18.7953261Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7953368Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7953583Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7953729Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7953838Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7953842Z 
2025-04-11T04:23:18.7954021Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a84040>, lr = 0.001
2025-04-11T04:23:18.7954226Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7954231Z 
2025-04-11T04:23:18.7954471Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7954610Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7954774Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7954843Z     
2025-04-11T04:23:18.7954949Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7955069Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7955311Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7955471Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7955751Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7955890Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7956048Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7956053Z 
2025-04-11T04:23:18.7956192Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7956359Z ______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] ______________
2025-04-11T04:23:18.7956364Z 
2025-04-11T04:23:18.7956492Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7956584Z g_dtype = torch.float32
2025-04-11T04:23:18.7956588Z 
2025-04-11T04:23:18.7956711Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7956842Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7957012Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7957165Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7957261Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7957391Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7957481Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7957612Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7957703Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7957837Z >       check_adam_kernel(
2025-04-11T04:23:18.7958091Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7958163Z         )
2025-04-11T04:23:18.7958168Z 
2025-04-11T04:23:18.7958278Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7958390Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7958544Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7958690Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7958797Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7958801Z 
2025-04-11T04:23:18.7958976Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3f790>, lr = 0.001
2025-04-11T04:23:18.7959124Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7959130Z 
2025-04-11T04:23:18.7959368Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7959556Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7959717Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7959792Z     
2025-04-11T04:23:18.7959900Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7960018Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7960258Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7960411Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7960692Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7960825Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7960986Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7960991Z 
2025-04-11T04:23:18.7961125Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7961291Z _____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] ______________
2025-04-11T04:23:18.7961343Z 
2025-04-11T04:23:18.7961472Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.7961559Z g_dtype = torch.float32
2025-04-11T04:23:18.7961563Z 
2025-04-11T04:23:18.7961682Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7961811Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7961976Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7962123Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7962212Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7962339Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7962427Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7962557Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7962645Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7962727Z >       check_adam_kernel(
2025-04-11T04:23:18.7962981Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7963056Z         )
2025-04-11T04:23:18.7963061Z 
2025-04-11T04:23:18.7963172Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7963285Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7963439Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7963587Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7963692Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7963744Z 
2025-04-11T04:23:18.7963921Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a854b0>, lr = 0.001
2025-04-11T04:23:18.7964075Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.7964080Z 
2025-04-11T04:23:18.7964316Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7964460Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7964620Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7964694Z     
2025-04-11T04:23:18.7964802Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7964917Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7965161Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7965267Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7965554Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7965741Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7965900Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7965906Z 
2025-04-11T04:23:18.7966042Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7966204Z ______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] ______________
2025-04-11T04:23:18.7966243Z 
2025-04-11T04:23:18.7966372Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.7966455Z g_dtype = torch.float32
2025-04-11T04:23:18.7966464Z 
2025-04-11T04:23:18.7966580Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7966705Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7966875Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7967023Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7967114Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7967239Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7967375Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7967510Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7967595Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7967681Z >       check_adam_kernel(
2025-04-11T04:23:18.7967935Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7968009Z         )
2025-04-11T04:23:18.7968013Z 
2025-04-11T04:23:18.7968124Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7968233Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7968394Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7968537Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7968653Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7968657Z 
2025-04-11T04:23:18.7968834Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3fbb0>, lr = 0.001
2025-04-11T04:23:18.7968986Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.7968990Z 
2025-04-11T04:23:18.7969226Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7969369Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7969530Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7969599Z     
2025-04-11T04:23:18.7969710Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7969878Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7970120Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7970229Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7970514Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7970650Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7970804Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7970813Z 
2025-04-11T04:23:18.7970947Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7971108Z _____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] ______________
2025-04-11T04:23:18.7971112Z 
2025-04-11T04:23:18.7971245Z adamw = False, weight_decay = 0.0, p_dtype = torch.float16
2025-04-11T04:23:18.7971330Z g_dtype = torch.float16
2025-04-11T04:23:18.7971335Z 
2025-04-11T04:23:18.7971455Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7971646Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7971815Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7971965Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7972052Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7972181Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7972266Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7972450Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7972534Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7972618Z >       check_adam_kernel(
2025-04-11T04:23:18.7972874Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7972946Z         )
2025-04-11T04:23:18.7972950Z 
2025-04-11T04:23:18.7973063Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7973173Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7973330Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7973523Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7973633Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7973637Z 
2025-04-11T04:23:18.7973811Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a85150>, lr = 0.001
2025-04-11T04:23:18.7973963Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7973970Z 
2025-04-11T04:23:18.7974208Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7974347Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7974512Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7974583Z     
2025-04-11T04:23:18.7974694Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7974808Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7975053Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7975160Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7975445Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7975587Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7975744Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7975748Z 
2025-04-11T04:23:18.7975939Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7976098Z ______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] ______________
2025-04-11T04:23:18.7976105Z 
2025-04-11T04:23:18.7976233Z adamw = True, weight_decay = 0.0, p_dtype = torch.float16
2025-04-11T04:23:18.7976318Z g_dtype = torch.float16
2025-04-11T04:23:18.7976325Z 
2025-04-11T04:23:18.7976441Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7976569Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7976732Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7976888Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7976976Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7977107Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7977193Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7977322Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7977412Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7977495Z >       check_adam_kernel(
2025-04-11T04:23:18.7977804Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7977876Z         )
2025-04-11T04:23:18.7977882Z 
2025-04-11T04:23:18.7977996Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7978106Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7978261Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7978460Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7978569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7978573Z 
2025-04-11T04:23:18.7978750Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5819268c0>, lr = 0.001
2025-04-11T04:23:18.7978900Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7978905Z 
2025-04-11T04:23:18.7979143Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7979283Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7979501Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7979571Z     
2025-04-11T04:23:18.7979679Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7979797Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7980037Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7980144Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7980423Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7980559Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7980714Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7980721Z 
2025-04-11T04:23:18.7980853Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7981020Z _____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] ______________
2025-04-11T04:23:18.7981025Z 
2025-04-11T04:23:18.7981152Z adamw = False, weight_decay = 0.1, p_dtype = torch.float16
2025-04-11T04:23:18.7981239Z g_dtype = torch.float16
2025-04-11T04:23:18.7981245Z 
2025-04-11T04:23:18.7981363Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7981489Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7981655Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7981805Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7981941Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7982067Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7982156Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7982284Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7982371Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7982455Z >       check_adam_kernel(
2025-04-11T04:23:18.7982708Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7982783Z         )
2025-04-11T04:23:18.7982789Z 
2025-04-11T04:23:18.7982899Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7983010Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7983164Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7983313Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7983420Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7983424Z 
2025-04-11T04:23:18.7983653Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb581d7a710>, lr = 0.001
2025-04-11T04:23:18.7983800Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.7983807Z 
2025-04-11T04:23:18.7984042Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7984181Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7984392Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7984466Z     
2025-04-11T04:23:18.7984577Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7984695Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7984931Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7985040Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7985320Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7985452Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7985660Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7985664Z 
2025-04-11T04:23:18.7985799Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7985964Z ______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] ______________
2025-04-11T04:23:18.7985968Z 
2025-04-11T04:23:18.7986094Z adamw = True, weight_decay = 0.1, p_dtype = torch.float16
2025-04-11T04:23:18.7986180Z g_dtype = torch.float16
2025-04-11T04:23:18.7986185Z 
2025-04-11T04:23:18.7986302Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7986427Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7986593Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7986742Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7986831Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7986959Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7987046Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7987174Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7987257Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7987342Z >       check_adam_kernel(
2025-04-11T04:23:18.7987592Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7987666Z         )
2025-04-11T04:23:18.7987671Z 
2025-04-11T04:23:18.7987781Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7987959Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7988114Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7988261Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7988369Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7988376Z 
2025-04-11T04:23:18.7988590Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5e929e0>, lr = 0.001
2025-04-11T04:23:18.7988744Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.7988750Z 
2025-04-11T04:23:18.7988984Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7989128Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7989288Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7989364Z     
2025-04-11T04:23:18.7989471Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7989583Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7989888Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7989996Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7990299Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7990432Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7990645Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7990650Z 
2025-04-11T04:23:18.7990783Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7990946Z _____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] ______________
2025-04-11T04:23:18.7990952Z 
2025-04-11T04:23:18.7991081Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7991167Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.7991174Z 
2025-04-11T04:23:18.7991296Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7991419Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7991653Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7991803Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7991891Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7992019Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7992105Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7992239Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7992323Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7992408Z >       check_adam_kernel(
2025-04-11T04:23:18.7992669Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7992742Z         )
2025-04-11T04:23:18.7992750Z 
2025-04-11T04:23:18.7992863Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7992974Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7993133Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7993276Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7993385Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7993391Z 
2025-04-11T04:23:18.7993564Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c54cd0>, lr = 0.001
2025-04-11T04:23:18.7993715Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7993719Z 
2025-04-11T04:23:18.7993956Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7994171Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7994334Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7994403Z     
2025-04-11T04:23:18.7994517Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7994633Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7994879Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7994985Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7995272Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7995404Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7995559Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7995565Z 
2025-04-11T04:23:18.7995705Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7995917Z ______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] ______________
2025-04-11T04:23:18.7995921Z 
2025-04-11T04:23:18.7996053Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7996142Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.7996147Z 
2025-04-11T04:23:18.7996267Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7996391Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7996607Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7996759Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7996849Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7996984Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7997075Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7997211Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7997298Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7997384Z >       check_adam_kernel(
2025-04-11T04:23:18.7997647Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7997767Z         )
2025-04-11T04:23:18.7997771Z 
2025-04-11T04:23:18.7997885Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7997993Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7998152Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7998294Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7998399Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7998406Z 
2025-04-11T04:23:18.7998579Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c74c70>, lr = 0.001
2025-04-11T04:23:18.7998725Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7998731Z 
2025-04-11T04:23:18.7998971Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7999111Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7999274Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7999344Z     
2025-04-11T04:23:18.7999457Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7999574Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7999811Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7999919Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8000252Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8000391Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8000547Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8000553Z 
2025-04-11T04:23:18.8000689Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8000847Z _____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] ______________
2025-04-11T04:23:18.8000851Z 
2025-04-11T04:23:18.8000983Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.8001071Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8001076Z 
2025-04-11T04:23:18.8001193Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8001323Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8001488Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8001642Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8001728Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8001908Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8001994Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8002126Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8002215Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8002297Z >       check_adam_kernel(
2025-04-11T04:23:18.8002552Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8002675Z         )
2025-04-11T04:23:18.8002680Z 
2025-04-11T04:23:18.8002797Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8002908Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8003067Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8003221Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8003330Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8003336Z 
2025-04-11T04:23:18.8003518Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a89ed0>, lr = 0.001
2025-04-11T04:23:18.8003715Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.8003719Z 
2025-04-11T04:23:18.8003959Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8004101Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8004263Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8004332Z     
2025-04-11T04:23:18.8004440Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8004560Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8004802Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8004911Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8005193Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8005335Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8005490Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8005495Z 
2025-04-11T04:23:18.8005630Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8005792Z ______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] ______________
2025-04-11T04:23:18.8005797Z 
2025-04-11T04:23:18.8005924Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.8006014Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8006071Z 
2025-04-11T04:23:18.8006192Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8006318Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8006486Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8006634Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8006728Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8006852Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8006939Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8007069Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8007158Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8007240Z >       check_adam_kernel(
2025-04-11T04:23:18.8007492Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8007566Z         )
2025-04-11T04:23:18.8007572Z 
2025-04-11T04:23:18.8007682Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8007794Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8007998Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8008146Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8008254Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8008257Z 
2025-04-11T04:23:18.8008434Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3c2b0>, lr = 0.001
2025-04-11T04:23:18.8008620Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.8008624Z 
2025-04-11T04:23:18.8008860Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8009000Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8009160Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8009232Z     
2025-04-11T04:23:18.8009340Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8009457Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8009696Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8009847Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8010129Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8010265Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8010421Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8010426Z 
2025-04-11T04:23:18.8010559Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8010722Z _____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] ______________
2025-04-11T04:23:18.8010727Z 
2025-04-11T04:23:18.8010861Z adamw = False, weight_decay = 0.0, p_dtype = torch.bfloat16
2025-04-11T04:23:18.8010951Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8010955Z 
2025-04-11T04:23:18.8011074Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8011201Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8011369Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8011516Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8011607Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8011734Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8011821Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8011952Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8012034Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8012174Z >       check_adam_kernel(
2025-04-11T04:23:18.8012426Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8012501Z         )
2025-04-11T04:23:18.8012506Z 
2025-04-11T04:23:18.8012615Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8012729Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8012883Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8013027Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8013139Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8013143Z 
2025-04-11T04:23:18.8013314Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a88880>, lr = 0.001
2025-04-11T04:23:18.8013465Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.8013472Z 
2025-04-11T04:23:18.8013708Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8013899Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8014059Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8014133Z     
2025-04-11T04:23:18.8014242Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8014357Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8014601Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8014756Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8015042Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8015175Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8015335Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8015341Z 
2025-04-11T04:23:18.8015475Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8015633Z ______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] ______________
2025-04-11T04:23:18.8015705Z 
2025-04-11T04:23:18.8015837Z adamw = True, weight_decay = 0.0, p_dtype = torch.bfloat16
2025-04-11T04:23:18.8015922Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8015927Z 
2025-04-11T04:23:18.8016050Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8016176Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8016347Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8016498Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8016587Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8016715Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8016801Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8016936Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8017019Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8017103Z >       check_adam_kernel(
2025-04-11T04:23:18.8017358Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8017429Z         )
2025-04-11T04:23:18.8017437Z 
2025-04-11T04:23:18.8017549Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8017659Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8017816Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8017960Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8018071Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8018124Z 
2025-04-11T04:23:18.8018301Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3fdf0>, lr = 0.001
2025-04-11T04:23:18.8018455Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.8018460Z 
2025-04-11T04:23:18.8018698Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8018836Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8019000Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8019070Z     
2025-04-11T04:23:18.8019185Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8019299Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8019540Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8019648Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8019927Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8020115Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8020271Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8020277Z 
2025-04-11T04:23:18.8020414Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8020575Z _____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] ______________
2025-04-11T04:23:18.8020612Z 
2025-04-11T04:23:18.8020751Z adamw = False, weight_decay = 0.1, p_dtype = torch.bfloat16
2025-04-11T04:23:18.8020836Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8020841Z 
2025-04-11T04:23:18.8020961Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8021085Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8021254Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8021406Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8021494Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8021621Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8021758Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8021891Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8021974Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8022056Z >       check_adam_kernel(
2025-04-11T04:23:18.8022315Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8022385Z         )
2025-04-11T04:23:18.8022389Z 
2025-04-11T04:23:18.8022503Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8022611Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8022771Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8022915Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8023021Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8023027Z 
2025-04-11T04:23:18.8023205Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a885b0>, lr = 0.001
2025-04-11T04:23:18.8023354Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.8023359Z 
2025-04-11T04:23:18.8023598Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8023737Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8023900Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8023968Z     
2025-04-11T04:23:18.8024131Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8024246Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8024487Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8024596Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8024875Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8025013Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8025167Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8025173Z 
2025-04-11T04:23:18.8025309Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8025469Z ______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] ______________
2025-04-11T04:23:18.8025474Z 
2025-04-11T04:23:18.8025604Z adamw = True, weight_decay = 0.1, p_dtype = torch.bfloat16
2025-04-11T04:23:18.8025693Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8025698Z 
2025-04-11T04:23:18.8025814Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8025994Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8026159Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8026313Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8026398Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8026524Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8026662Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8026793Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8026884Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8026968Z >       check_adam_kernel(
2025-04-11T04:23:18.8027229Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8027302Z         )
2025-04-11T04:23:18.8027306Z 
2025-04-11T04:23:18.8027417Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8027528Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8027682Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8027880Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8027986Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8027990Z 
2025-04-11T04:23:18.8028168Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3e350>, lr = 0.001
2025-04-11T04:23:18.8028317Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.8028322Z 
2025-04-11T04:23:18.8028593Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8028734Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8028896Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8028972Z     
2025-04-11T04:23:18.8029081Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8029203Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8029443Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8029550Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8029834Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8029970Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8030129Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8030191Z 
2025-04-11T04:23:18.8030328Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8030513Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] ______
2025-04-11T04:23:18.8030518Z 
2025-04-11T04:23:18.8030675Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8030848Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8030934Z g_dtype = torch.float32
2025-04-11T04:23:18.8030938Z 
2025-04-11T04:23:18.8031106Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8031231Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8031387Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8031480Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8031636Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8031728Z         device: torch.device,
2025-04-11T04:23:18.8031806Z         adamw: bool,
2025-04-11T04:23:18.8031898Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8031982Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8032208Z     ) -> None:
2025-04-11T04:23:18.8032473Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8032572Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8032577Z 
2025-04-11T04:23:18.8032692Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8032803Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8033121Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8033215Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8033453Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8033546Z     return self._apply(convert)
2025-04-11T04:23:18.8033790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8033878Z     module._apply(fn)
2025-04-11T04:23:18.8034123Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8034208Z     module._apply(fn)
2025-04-11T04:23:18.8034492Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8034571Z     module._apply(fn)
2025-04-11T04:23:18.8034804Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8034897Z     param_applied = fn(param)
2025-04-11T04:23:18.8035006Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8035010Z 
2025-04-11T04:23:18.8035099Z t = Parameter containing:
2025-04-11T04:23:18.8035238Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8035337Z         [ 0.0028, -0.0014,...0,  0.0213, -0.0091],
2025-04-11T04:23:18.8035463Z         [-0.0226, -0.0230, -0.0057,  ..., -0.0094, -0.0239, -0.0399]],
2025-04-11T04:23:18.8035552Z        requires_grad=True)
2025-04-11T04:23:18.8035556Z 
2025-04-11T04:23:18.8035633Z     def convert(t):
2025-04-11T04:23:18.8035767Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8035951Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8036073Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8036280Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8036388Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8036672Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8036808Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8037024Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8037030Z 
2025-04-11T04:23:18.8037282Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8037462Z _______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] _______
2025-04-11T04:23:18.8037468Z 
2025-04-11T04:23:18.8037615Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8037786Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8037871Z g_dtype = torch.float32
2025-04-11T04:23:18.8037876Z 
2025-04-11T04:23:18.8038041Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8038161Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8038314Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8038412Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8038565Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8038704Z         device: torch.device,
2025-04-11T04:23:18.8038781Z         adamw: bool,
2025-04-11T04:23:18.8038867Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8038953Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8039030Z     ) -> None:
2025-04-11T04:23:18.8039288Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8039385Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8039453Z 
2025-04-11T04:23:18.8039566Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8039674Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8039916Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8040013Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8040240Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8040333Z     return self._apply(convert)
2025-04-11T04:23:18.8040562Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8040696Z     module._apply(fn)
2025-04-11T04:23:18.8040926Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8041004Z     module._apply(fn)
2025-04-11T04:23:18.8041235Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8041315Z     module._apply(fn)
2025-04-11T04:23:18.8041545Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8041632Z     param_applied = fn(param)
2025-04-11T04:23:18.8041744Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8041751Z 
2025-04-11T04:23:18.8041838Z t = Parameter containing:
2025-04-11T04:23:18.8041970Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8042071Z         [-0.0048,  0.0237,...2, -0.0204,  0.0268],
2025-04-11T04:23:18.8042190Z         [ 0.0211,  0.0139,  0.0082,  ...,  0.0303, -0.0201, -0.0544]],
2025-04-11T04:23:18.8042279Z        requires_grad=True)
2025-04-11T04:23:18.8042283Z 
2025-04-11T04:23:18.8042359Z     def convert(t):
2025-04-11T04:23:18.8042491Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8042669Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8042788Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8043000Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8043106Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8043448Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8043585Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8043744Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8043751Z 
2025-04-11T04:23:18.8044003Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8044188Z _____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] ______
2025-04-11T04:23:18.8044193Z 
2025-04-11T04:23:18.8044360Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8044510Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8044598Z g_dtype = torch.float32
2025-04-11T04:23:18.8044602Z 
2025-04-11T04:23:18.8044767Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8044892Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8045043Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8045193Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8045350Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8045439Z         device: torch.device,
2025-04-11T04:23:18.8045519Z         adamw: bool,
2025-04-11T04:23:18.8045604Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8045694Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8045767Z     ) -> None:
2025-04-11T04:23:18.8046016Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8046165Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8046272Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8046352Z         lr = 1e-3
2025-04-11T04:23:18.8046436Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8046519Z         eps = 1e-8
2025-04-11T04:23:18.8046623Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8046853Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8047076Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8047143Z 
2025-04-11T04:23:18.8047254Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8047371Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8047377Z 
2025-04-11T04:23:18.8047458Z self = HybridAdam (
2025-04-11T04:23:18.8047543Z Parameter Group 0
2025-04-11T04:23:18.8047622Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8047706Z     bias_correction: True
2025-04-11T04:23:18.8047786Z     eps: 1e-08
2025-04-11T04:23:18.8047859Z     lr: 0.001
2025-04-11T04:23:18.8047949Z     weig...arameter Group 1
2025-04-11T04:23:18.8048027Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8048108Z     bias_correction: True
2025-04-11T04:23:18.8048186Z     eps: 1e-08
2025-04-11T04:23:18.8048257Z     lr: 0.001
2025-04-11T04:23:18.8048343Z     weight_decay: 0.0
2025-04-11T04:23:18.8048415Z )
2025-04-11T04:23:18.8048737Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8048888Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8049042Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8049145Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8049522Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8049528Z 
2025-04-11T04:23:18.8049606Z     def __init__(
2025-04-11T04:23:18.8049731Z         self,
2025-04-11T04:23:18.8049813Z         model_params,
2025-04-11T04:23:18.8049888Z         lr=1e-3,
2025-04-11T04:23:18.8049968Z         bias_correction=True,
2025-04-11T04:23:18.8050050Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8050124Z         eps=1e-8,
2025-04-11T04:23:18.8050205Z         weight_decay=0,
2025-04-11T04:23:18.8050283Z         adamw_mode=True,
2025-04-11T04:23:18.8050381Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8050487Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8050567Z         **defaults: Any,
2025-04-11T04:23:18.8050640Z     ):
2025-04-11T04:23:18.8050719Z         super().__init__(
2025-04-11T04:23:18.8050803Z             model_params,
2025-04-11T04:23:18.8050875Z             lr,
2025-04-11T04:23:18.8050955Z             bias_correction,
2025-04-11T04:23:18.8051032Z             betas,
2025-04-11T04:23:18.8051105Z             eps,
2025-04-11T04:23:18.8051184Z             weight_decay,
2025-04-11T04:23:18.8051260Z             adamw_mode,
2025-04-11T04:23:18.8051349Z             nvme_offload_fraction,
2025-04-11T04:23:18.8051434Z             nvme_offload_dir,
2025-04-11T04:23:18.8051505Z         )
2025-04-11T04:23:18.8051655Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8051771Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8051896Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8052114Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8052223Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8052515Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8052701Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8052864Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8052871Z 
2025-04-11T04:23:18.8053008Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8053193Z _____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] ______
2025-04-11T04:23:18.8053199Z 
2025-04-11T04:23:18.8053364Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8053533Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8053673Z g_dtype = torch.float32
2025-04-11T04:23:18.8053678Z 
2025-04-11T04:23:18.8053844Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8053966Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8054123Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8054216Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8054369Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8054453Z         device: torch.device,
2025-04-11T04:23:18.8054535Z         adamw: bool,
2025-04-11T04:23:18.8054619Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8054704Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8054780Z     ) -> None:
2025-04-11T04:23:18.8055030Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8055131Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8055136Z 
2025-04-11T04:23:18.8055244Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8055359Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8055607Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8055700Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8055929Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8056020Z     return self._apply(convert)
2025-04-11T04:23:18.8056303Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8056384Z     module._apply(fn)
2025-04-11T04:23:18.8056615Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8056694Z     module._apply(fn)
2025-04-11T04:23:18.8056921Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8056999Z     module._apply(fn)
2025-04-11T04:23:18.8057223Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8057312Z     param_applied = fn(param)
2025-04-11T04:23:18.8057420Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8057424Z 
2025-04-11T04:23:18.8057514Z t = Parameter containing:
2025-04-11T04:23:18.8057647Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8057748Z         [ 0.0210, -0.0131,...8, -0.0156, -0.0054],
2025-04-11T04:23:18.8057862Z         [ 0.0148,  0.0292,  0.0008,  ...,  0.0355, -0.0048, -0.0186]],
2025-04-11T04:23:18.8058000Z        requires_grad=True)
2025-04-11T04:23:18.8058004Z 
2025-04-11T04:23:18.8058081Z     def convert(t):
2025-04-11T04:23:18.8058212Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8058397Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8058515Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8058728Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8058883Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8059170Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8059306Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8059468Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8059478Z 
2025-04-11T04:23:18.8059732Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8059912Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] _______
2025-04-11T04:23:18.8059963Z 
2025-04-11T04:23:18.8060131Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8060298Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8060387Z g_dtype = torch.float32
2025-04-11T04:23:18.8060391Z 
2025-04-11T04:23:18.8060556Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8060676Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8060829Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8060920Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8061076Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8061163Z         device: torch.device,
2025-04-11T04:23:18.8061243Z         adamw: bool,
2025-04-11T04:23:18.8061326Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8061412Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8061488Z     ) -> None:
2025-04-11T04:23:18.8061735Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8061833Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8061840Z 
2025-04-11T04:23:18.8061946Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8062057Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8062298Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8062392Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8062667Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8062757Z     return self._apply(convert)
2025-04-11T04:23:18.8062989Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8063070Z     module._apply(fn)
2025-04-11T04:23:18.8063298Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8063377Z     module._apply(fn)
2025-04-11T04:23:18.8063602Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8063681Z     module._apply(fn)
2025-04-11T04:23:18.8063904Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8063994Z     param_applied = fn(param)
2025-04-11T04:23:18.8064101Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8064107Z 
2025-04-11T04:23:18.8064196Z t = Parameter containing:
2025-04-11T04:23:18.8064328Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8064491Z         [ 0.0168, -0.0116,...1,  0.0247, -0.0168],
2025-04-11T04:23:18.8064610Z         [ 0.0078, -0.0201,  0.0158,  ..., -0.0204,  0.0234,  0.0068]],
2025-04-11T04:23:18.8064694Z        requires_grad=True)
2025-04-11T04:23:18.8064699Z 
2025-04-11T04:23:18.8064779Z     def convert(t):
2025-04-11T04:23:18.8064907Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8065090Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8065260Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8065468Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8065574Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8065855Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8065994Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8066152Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8066200Z 
2025-04-11T04:23:18.8066456Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8066633Z _______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] ________
2025-04-11T04:23:18.8066637Z 
2025-04-11T04:23:18.8066791Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8066954Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8067040Z g_dtype = torch.float32
2025-04-11T04:23:18.8067044Z 
2025-04-11T04:23:18.8067207Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8067326Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8067484Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8067576Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8067733Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8067817Z         device: torch.device,
2025-04-11T04:23:18.8067897Z         adamw: bool,
2025-04-11T04:23:18.8067979Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8068058Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8068136Z     ) -> None:
2025-04-11T04:23:18.8068379Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8068533Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8068538Z 
2025-04-11T04:23:18.8068645Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8068756Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8069054Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8069147Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8069375Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8069466Z     return self._apply(convert)
2025-04-11T04:23:18.8069696Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8069777Z     module._apply(fn)
2025-04-11T04:23:18.8070006Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8070087Z     module._apply(fn)
2025-04-11T04:23:18.8070309Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8070392Z     module._apply(fn)
2025-04-11T04:23:18.8070617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8070709Z     param_applied = fn(param)
2025-04-11T04:23:18.8070816Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8070870Z 
2025-04-11T04:23:18.8070960Z t = Parameter containing:
2025-04-11T04:23:18.8071091Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8071188Z         [ 0.0171, -0.0037,...8, -0.0068,  0.0037],
2025-04-11T04:23:18.8071307Z         [ 0.0260, -0.0271, -0.0247,  ...,  0.0262,  0.0078,  0.0236]],
2025-04-11T04:23:18.8071391Z        requires_grad=True)
2025-04-11T04:23:18.8071434Z 
2025-04-11T04:23:18.8071514Z     def convert(t):
2025-04-11T04:23:18.8071642Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8071823Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8071938Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8072146Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8072256Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8072537Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8072728Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8072885Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8072889Z 
2025-04-11T04:23:18.8073146Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8073328Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] ______
2025-04-11T04:23:18.8073332Z 
2025-04-11T04:23:18.8073500Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8073652Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8073737Z g_dtype = torch.float32
2025-04-11T04:23:18.8073741Z 
2025-04-11T04:23:18.8073909Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8074032Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8074190Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8074281Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8074437Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8074521Z         device: torch.device,
2025-04-11T04:23:18.8074597Z         adamw: bool,
2025-04-11T04:23:18.8074688Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8074769Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8074846Z     ) -> None:
2025-04-11T04:23:18.8075096Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8075189Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8075349Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8075423Z         lr = 1e-3
2025-04-11T04:23:18.8075512Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8075588Z         eps = 1e-8
2025-04-11T04:23:18.8075693Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8075923Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8076138Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8076146Z 
2025-04-11T04:23:18.8076256Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8076365Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8076370Z 
2025-04-11T04:23:18.8076451Z self = HybridAdam (
2025-04-11T04:23:18.8076532Z Parameter Group 0
2025-04-11T04:23:18.8076613Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8076698Z     bias_correction: True
2025-04-11T04:23:18.8076774Z     eps: 1e-08
2025-04-11T04:23:18.8076851Z     lr: 0.001
2025-04-11T04:23:18.8076935Z     weig...arameter Group 1
2025-04-11T04:23:18.8077066Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8077148Z     bias_correction: True
2025-04-11T04:23:18.8077226Z     eps: 1e-08
2025-04-11T04:23:18.8077304Z     lr: 0.001
2025-04-11T04:23:18.8077382Z     weight_decay: 0.0
2025-04-11T04:23:18.8077455Z )
2025-04-11T04:23:18.8077772Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8077975Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8078123Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8078218Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8078593Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8078600Z 
2025-04-11T04:23:18.8078674Z     def __init__(
2025-04-11T04:23:18.8078753Z         self,
2025-04-11T04:23:18.8078830Z         model_params,
2025-04-11T04:23:18.8078907Z         lr=1e-3,
2025-04-11T04:23:18.8078989Z         bias_correction=True,
2025-04-11T04:23:18.8079122Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8079200Z         eps=1e-8,
2025-04-11T04:23:18.8079281Z         weight_decay=0,
2025-04-11T04:23:18.8079363Z         adamw_mode=True,
2025-04-11T04:23:18.8079459Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8079565Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8079648Z         **defaults: Any,
2025-04-11T04:23:18.8079717Z     ):
2025-04-11T04:23:18.8079805Z         super().__init__(
2025-04-11T04:23:18.8079884Z             model_params,
2025-04-11T04:23:18.8079954Z             lr,
2025-04-11T04:23:18.8080042Z             bias_correction,
2025-04-11T04:23:18.8080116Z             betas,
2025-04-11T04:23:18.8080192Z             eps,
2025-04-11T04:23:18.8080269Z             weight_decay,
2025-04-11T04:23:18.8080350Z             adamw_mode,
2025-04-11T04:23:18.8080439Z             nvme_offload_fraction,
2025-04-11T04:23:18.8080519Z             nvme_offload_dir,
2025-04-11T04:23:18.8080594Z         )
2025-04-11T04:23:18.8080687Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8080808Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8080930Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8081142Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8081254Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8081547Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8081740Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8081900Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8081907Z 
2025-04-11T04:23:18.8082045Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8082225Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] ______
2025-04-11T04:23:18.8082232Z 
2025-04-11T04:23:18.8082398Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8082564Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8082649Z g_dtype = torch.float32
2025-04-11T04:23:18.8082654Z 
2025-04-11T04:23:18.8082821Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8082943Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8083101Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8083193Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8083352Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8083486Z         device: torch.device,
2025-04-11T04:23:18.8083564Z         adamw: bool,
2025-04-11T04:23:18.8083653Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8083737Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8083816Z     ) -> None:
2025-04-11T04:23:18.8084071Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8084166Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8084224Z 
2025-04-11T04:23:18.8084333Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8084445Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8084700Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8084792Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8085024Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8085117Z     return self._apply(convert)
2025-04-11T04:23:18.8085356Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8085437Z     module._apply(fn)
2025-04-11T04:23:18.8085718Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8085800Z     module._apply(fn)
2025-04-11T04:23:18.8086033Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8086114Z     module._apply(fn)
2025-04-11T04:23:18.8086342Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8086434Z     param_applied = fn(param)
2025-04-11T04:23:18.8086542Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8086548Z 
2025-04-11T04:23:18.8086634Z t = Parameter containing:
2025-04-11T04:23:18.8086769Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8086867Z         [-0.0301, -0.0063,...5, -0.0105,  0.0078],
2025-04-11T04:23:18.8086987Z         [-0.0225,  0.0108,  0.0321,  ..., -0.0056, -0.0089, -0.0360]],
2025-04-11T04:23:18.8087071Z        requires_grad=True)
2025-04-11T04:23:18.8087076Z 
2025-04-11T04:23:18.8087155Z     def convert(t):
2025-04-11T04:23:18.8087287Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8087468Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8087591Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8087800Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8087909Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8088284Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8088423Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8088581Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8088588Z 
2025-04-11T04:23:18.8088838Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8089019Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] ______
2025-04-11T04:23:18.8089024Z 
2025-04-11T04:23:18.8089187Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8089360Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8089444Z g_dtype = torch.float16
2025-04-11T04:23:18.8089448Z 
2025-04-11T04:23:18.8089616Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8089738Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8089894Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8090034Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8090190Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8090282Z         device: torch.device,
2025-04-11T04:23:18.8090358Z         adamw: bool,
2025-04-11T04:23:18.8090445Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8090526Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8090601Z     ) -> None:
2025-04-11T04:23:18.8090855Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8091006Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8091011Z 
2025-04-11T04:23:18.8091123Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8091233Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8091481Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8091575Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8091809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8091948Z     return self._apply(convert)
2025-04-11T04:23:18.8092179Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8092263Z     module._apply(fn)
2025-04-11T04:23:18.8092500Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8092586Z     module._apply(fn)
2025-04-11T04:23:18.8092819Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8092900Z     module._apply(fn)
2025-04-11T04:23:18.8093132Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8093221Z     param_applied = fn(param)
2025-04-11T04:23:18.8093332Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8093338Z 
2025-04-11T04:23:18.8093424Z t = Parameter containing:
2025-04-11T04:23:18.8093559Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8093655Z         [-0.0063,  0.0127,...8,  0.0139, -0.0372],
2025-04-11T04:23:18.8093775Z         [-0.0001,  0.0211,  0.0425,  ..., -0.0074,  0.0182,  0.0033]],
2025-04-11T04:23:18.8093859Z        requires_grad=True)
2025-04-11T04:23:18.8093863Z 
2025-04-11T04:23:18.8093941Z     def convert(t):
2025-04-11T04:23:18.8094074Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8094251Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8094370Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8094629Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8094738Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8095018Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8095154Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8095314Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8095318Z 
2025-04-11T04:23:18.8095570Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8095754Z _______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] _______
2025-04-11T04:23:18.8095758Z 
2025-04-11T04:23:18.8095907Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8096078Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8096162Z g_dtype = torch.float16
2025-04-11T04:23:18.8096167Z 
2025-04-11T04:23:18.8096333Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8096505Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8096663Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8096758Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8096909Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8096994Z         device: torch.device,
2025-04-11T04:23:18.8097071Z         adamw: bool,
2025-04-11T04:23:18.8097208Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8097291Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8097364Z     ) -> None:
2025-04-11T04:23:18.8097618Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8097710Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8097717Z 
2025-04-11T04:23:18.8097829Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8097938Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8098181Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8098325Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8098548Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8098642Z     return self._apply(convert)
2025-04-11T04:23:18.8098869Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8098953Z     module._apply(fn)
2025-04-11T04:23:18.8099181Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8099261Z     module._apply(fn)
2025-04-11T04:23:18.8099491Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8099569Z     module._apply(fn)
2025-04-11T04:23:18.8099798Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8099887Z     param_applied = fn(param)
2025-04-11T04:23:18.8099997Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8100004Z 
2025-04-11T04:23:18.8100090Z t = Parameter containing:
2025-04-11T04:23:18.8100220Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8100318Z         [ 0.0058,  0.0119,...4, -0.0198,  0.0151],
2025-04-11T04:23:18.8100438Z         [-0.0479,  0.0136, -0.0425,  ..., -0.0021, -0.0081,  0.0171]],
2025-04-11T04:23:18.8100526Z        requires_grad=True)
2025-04-11T04:23:18.8100531Z 
2025-04-11T04:23:18.8100606Z     def convert(t):
2025-04-11T04:23:18.8100738Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8100966Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8101085Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8101291Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8101399Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8101686Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8101818Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8101979Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8101983Z 
2025-04-11T04:23:18.8102232Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8102418Z _____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] ______
2025-04-11T04:23:18.8102424Z 
2025-04-11T04:23:18.8102588Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8102743Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8102879Z g_dtype = torch.float16
2025-04-11T04:23:18.8102884Z 
2025-04-11T04:23:18.8103051Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8103178Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8103331Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8103424Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8103629Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8103715Z         device: torch.device,
2025-04-11T04:23:18.8103792Z         adamw: bool,
2025-04-11T04:23:18.8103876Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8103959Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8104034Z     ) -> None:
2025-04-11T04:23:18.8104285Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8104382Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8104489Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8104569Z         lr = 1e-3
2025-04-11T04:23:18.8104705Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8104787Z         eps = 1e-8
2025-04-11T04:23:18.8104891Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8105124Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8105340Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8105345Z 
2025-04-11T04:23:18.8105453Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8105566Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8105572Z 
2025-04-11T04:23:18.8105652Z self = HybridAdam (
2025-04-11T04:23:18.8105735Z Parameter Group 0
2025-04-11T04:23:18.8105814Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8105901Z     bias_correction: True
2025-04-11T04:23:18.8105976Z     eps: 1e-08
2025-04-11T04:23:18.8106048Z     lr: 0.001
2025-04-11T04:23:18.8106138Z     weig...arameter Group 1
2025-04-11T04:23:18.8106214Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8106298Z     bias_correction: True
2025-04-11T04:23:18.8106372Z     eps: 1e-08
2025-04-11T04:23:18.8106444Z     lr: 0.001
2025-04-11T04:23:18.8106526Z     weight_decay: 0.0
2025-04-11T04:23:18.8106598Z )
2025-04-11T04:23:18.8106917Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8107066Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8107222Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8107371Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8107752Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8107759Z 
2025-04-11T04:23:18.8107838Z     def __init__(
2025-04-11T04:23:18.8107911Z         self,
2025-04-11T04:23:18.8107994Z         model_params,
2025-04-11T04:23:18.8108066Z         lr=1e-3,
2025-04-11T04:23:18.8108151Z         bias_correction=True,
2025-04-11T04:23:18.8108230Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8108306Z         eps=1e-8,
2025-04-11T04:23:18.8108389Z         weight_decay=0,
2025-04-11T04:23:18.8108501Z         adamw_mode=True,
2025-04-11T04:23:18.8108600Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8108703Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8108782Z         **defaults: Any,
2025-04-11T04:23:18.8108859Z     ):
2025-04-11T04:23:18.8108939Z         super().__init__(
2025-04-11T04:23:18.8109021Z             model_params,
2025-04-11T04:23:18.8109094Z             lr,
2025-04-11T04:23:18.8109236Z             bias_correction,
2025-04-11T04:23:18.8109312Z             betas,
2025-04-11T04:23:18.8109385Z             eps,
2025-04-11T04:23:18.8109465Z             weight_decay,
2025-04-11T04:23:18.8109544Z             adamw_mode,
2025-04-11T04:23:18.8109632Z             nvme_offload_fraction,
2025-04-11T04:23:18.8109715Z             nvme_offload_dir,
2025-04-11T04:23:18.8109786Z         )
2025-04-11T04:23:18.8109882Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8110066Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8110192Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8110403Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8110511Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8110802Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8110942Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8111105Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8111173Z 
2025-04-11T04:23:18.8111309Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8111491Z _____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] ______
2025-04-11T04:23:18.8111495Z 
2025-04-11T04:23:18.8111660Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8111829Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8111912Z g_dtype = torch.float16
2025-04-11T04:23:18.8111916Z 
2025-04-11T04:23:18.8112080Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8112206Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8112357Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8112451Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8112605Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8112693Z         device: torch.device,
2025-04-11T04:23:18.8112770Z         adamw: bool,
2025-04-11T04:23:18.8112855Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8112940Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8113016Z     ) -> None:
2025-04-11T04:23:18.8113268Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8113365Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8113370Z 
2025-04-11T04:23:18.8113480Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8113594Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8113897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8113994Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8114223Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8114317Z     return self._apply(convert)
2025-04-11T04:23:18.8114550Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8114632Z     module._apply(fn)
2025-04-11T04:23:18.8114861Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8114940Z     module._apply(fn)
2025-04-11T04:23:18.8115174Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8115250Z     module._apply(fn)
2025-04-11T04:23:18.8115478Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8115567Z     param_applied = fn(param)
2025-04-11T04:23:18.8115673Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8115731Z 
2025-04-11T04:23:18.8115819Z t = Parameter containing:
2025-04-11T04:23:18.8115950Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8116051Z         [ 0.0127, -0.0053,...6, -0.0203,  0.0294],
2025-04-11T04:23:18.8116171Z         [ 0.0315,  0.0270, -0.0379,  ...,  0.0044, -0.0077,  0.0209]],
2025-04-11T04:23:18.8116258Z        requires_grad=True)
2025-04-11T04:23:18.8116262Z 
2025-04-11T04:23:18.8116376Z     def convert(t):
2025-04-11T04:23:18.8116505Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8116684Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8116799Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8117009Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8117116Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8117406Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8117589Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8117750Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8117755Z 
2025-04-11T04:23:18.8118002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8118178Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] _______
2025-04-11T04:23:18.8118186Z 
2025-04-11T04:23:18.8118347Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8118513Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8118601Z g_dtype = torch.float16
2025-04-11T04:23:18.8118605Z 
2025-04-11T04:23:18.8118769Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8118893Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8119044Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8119140Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8119292Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8119376Z         device: torch.device,
2025-04-11T04:23:18.8119456Z         adamw: bool,
2025-04-11T04:23:18.8119543Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8119627Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8119700Z     ) -> None:
2025-04-11T04:23:18.8119948Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8120046Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8120101Z 
2025-04-11T04:23:18.8120210Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8120322Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8120568Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8120666Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8120893Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8120986Z     return self._apply(convert)
2025-04-11T04:23:18.8121223Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8121308Z     module._apply(fn)
2025-04-11T04:23:18.8121546Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8121628Z     module._apply(fn)
2025-04-11T04:23:18.8121867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8121952Z     module._apply(fn)
2025-04-11T04:23:18.8122193Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8122336Z     param_applied = fn(param)
2025-04-11T04:23:18.8122446Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8122453Z 
2025-04-11T04:23:18.8122542Z t = Parameter containing:
2025-04-11T04:23:18.8122673Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8122769Z         [ 0.0126,  0.0307,...5,  0.0153,  0.0116],
2025-04-11T04:23:18.8122925Z         [-0.0007,  0.0044, -0.0020,  ..., -0.0033,  0.0164, -0.0073]],
2025-04-11T04:23:18.8123009Z        requires_grad=True)
2025-04-11T04:23:18.8123017Z 
2025-04-11T04:23:18.8123092Z     def convert(t):
2025-04-11T04:23:18.8123217Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8123397Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8123513Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8123721Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8123881Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8124167Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8124301Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8124459Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8124464Z 
2025-04-11T04:23:18.8124719Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8124897Z _______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] ________
2025-04-11T04:23:18.8124903Z 
2025-04-11T04:23:18.8125055Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8125223Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8125312Z g_dtype = torch.float16
2025-04-11T04:23:18.8125316Z 
2025-04-11T04:23:18.8125479Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8125601Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8125754Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8125844Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8126002Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8126086Z         device: torch.device,
2025-04-11T04:23:18.8126188Z         adamw: bool,
2025-04-11T04:23:18.8126274Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8126355Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8126486Z     ) -> None:
2025-04-11T04:23:18.8126740Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8126840Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8126845Z 
2025-04-11T04:23:18.8126953Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8127067Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8127308Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8127400Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8127630Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8127718Z     return self._apply(convert)
2025-04-11T04:23:18.8127960Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8128040Z     module._apply(fn)
2025-04-11T04:23:18.8128281Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8128360Z     module._apply(fn)
2025-04-11T04:23:18.8128645Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8128726Z     module._apply(fn)
2025-04-11T04:23:18.8128954Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8129045Z     param_applied = fn(param)
2025-04-11T04:23:18.8129151Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8129155Z 
2025-04-11T04:23:18.8129302Z t = Parameter containing:
2025-04-11T04:23:18.8129434Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8129527Z         [ 0.0062,  0.0098,...3, -0.0036,  0.0170],
2025-04-11T04:23:18.8129645Z         [ 0.0053,  0.0281, -0.0163,  ..., -0.0098, -0.0364,  0.0040]],
2025-04-11T04:23:18.8129728Z        requires_grad=True)
2025-04-11T04:23:18.8129736Z 
2025-04-11T04:23:18.8129816Z     def convert(t):
2025-04-11T04:23:18.8129944Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8130125Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8130241Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8130495Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8130605Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8130889Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8131030Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8131187Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8131192Z 
2025-04-11T04:23:18.8131448Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8131626Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] ______
2025-04-11T04:23:18.8131633Z 
2025-04-11T04:23:18.8131800Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8131954Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8132036Z g_dtype = torch.float16
2025-04-11T04:23:18.8132044Z 
2025-04-11T04:23:18.8132206Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8132325Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8132482Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8132573Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8132730Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8132816Z         device: torch.device,
2025-04-11T04:23:18.8133037Z         adamw: bool,
2025-04-11T04:23:18.8133126Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8133209Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8133289Z     ) -> None:
2025-04-11T04:23:18.8133540Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8133638Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8133745Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8133821Z         lr = 1e-3
2025-04-11T04:23:18.8133910Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8133990Z         eps = 1e-8
2025-04-11T04:23:18.8134097Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8134328Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8134547Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8134554Z 
2025-04-11T04:23:18.8134663Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8134773Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8134827Z 
2025-04-11T04:23:18.8134912Z self = HybridAdam (
2025-04-11T04:23:18.8134993Z Parameter Group 0
2025-04-11T04:23:18.8135080Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8135164Z     bias_correction: True
2025-04-11T04:23:18.8135242Z     eps: 1e-08
2025-04-11T04:23:18.8135315Z     lr: 0.001
2025-04-11T04:23:18.8135401Z     weig...arameter Group 1
2025-04-11T04:23:18.8135483Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8135637Z     bias_correction: True
2025-04-11T04:23:18.8135715Z     eps: 1e-08
2025-04-11T04:23:18.8135788Z     lr: 0.001
2025-04-11T04:23:18.8135866Z     weight_decay: 0.0
2025-04-11T04:23:18.8135939Z )
2025-04-11T04:23:18.8136251Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8136401Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8136552Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8136649Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8137015Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8137068Z 
2025-04-11T04:23:18.8137142Z     def __init__(
2025-04-11T04:23:18.8137218Z         self,
2025-04-11T04:23:18.8137298Z         model_params,
2025-04-11T04:23:18.8137374Z         lr=1e-3,
2025-04-11T04:23:18.8137456Z         bias_correction=True,
2025-04-11T04:23:18.8137534Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8137612Z         eps=1e-8,
2025-04-11T04:23:18.8137689Z         weight_decay=0,
2025-04-11T04:23:18.8137774Z         adamw_mode=True,
2025-04-11T04:23:18.8137870Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8137976Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8138055Z         **defaults: Any,
2025-04-11T04:23:18.8138126Z     ):
2025-04-11T04:23:18.8138209Z         super().__init__(
2025-04-11T04:23:18.8138288Z             model_params,
2025-04-11T04:23:18.8138362Z             lr,
2025-04-11T04:23:18.8138445Z             bias_correction,
2025-04-11T04:23:18.8138519Z             betas,
2025-04-11T04:23:18.8138594Z             eps,
2025-04-11T04:23:18.8138670Z             weight_decay,
2025-04-11T04:23:18.8138748Z             adamw_mode,
2025-04-11T04:23:18.8138835Z             nvme_offload_fraction,
2025-04-11T04:23:18.8138918Z             nvme_offload_dir,
2025-04-11T04:23:18.8138991Z         )
2025-04-11T04:23:18.8139082Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8139199Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8139319Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8139588Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8139695Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8139980Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8140124Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8140284Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8140289Z 
2025-04-11T04:23:18.8140425Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8140605Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] ______
2025-04-11T04:23:18.8140609Z 
2025-04-11T04:23:18.8140776Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8140937Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8141026Z g_dtype = torch.float16
2025-04-11T04:23:18.8141030Z 
2025-04-11T04:23:18.8141196Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8141371Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8141529Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8141620Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8141776Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8141860Z         device: torch.device,
2025-04-11T04:23:18.8141936Z         adamw: bool,
2025-04-11T04:23:18.8142072Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8142154Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8142230Z     ) -> None:
2025-04-11T04:23:18.8142480Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8142577Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8142584Z 
2025-04-11T04:23:18.8142690Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8142800Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8143049Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8143192Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8143421Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8143510Z     return self._apply(convert)
2025-04-11T04:23:18.8143749Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8143831Z     module._apply(fn)
2025-04-11T04:23:18.8144060Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8144143Z     module._apply(fn)
2025-04-11T04:23:18.8144369Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8144452Z     module._apply(fn)
2025-04-11T04:23:18.8144678Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8144772Z     param_applied = fn(param)
2025-04-11T04:23:18.8144881Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8144887Z 
2025-04-11T04:23:18.8144972Z t = Parameter containing:
2025-04-11T04:23:18.8145108Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8145204Z         [ 0.0145, -0.0268,...4,  0.0235, -0.0067],
2025-04-11T04:23:18.8145328Z         [-0.0276, -0.0061,  0.0080,  ...,  0.0096,  0.0016, -0.0028]],
2025-04-11T04:23:18.8145409Z        requires_grad=True)
2025-04-11T04:23:18.8145414Z 
2025-04-11T04:23:18.8145492Z     def convert(t):
2025-04-11T04:23:18.8145621Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8145849Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8145971Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8146180Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8146293Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8146581Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8146719Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8146880Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8146885Z 
2025-04-11T04:23:18.8147139Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8147318Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] ______
2025-04-11T04:23:18.8147324Z 
2025-04-11T04:23:18.8147486Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8147657Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8147795Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8147800Z 
2025-04-11T04:23:18.8147970Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8148093Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8148250Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8148342Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8148600Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8148689Z         device: torch.device,
2025-04-11T04:23:18.8148766Z         adamw: bool,
2025-04-11T04:23:18.8148853Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8148936Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8149017Z     ) -> None:
2025-04-11T04:23:18.8149265Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8149360Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8149365Z 
2025-04-11T04:23:18.8149479Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8149651Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8149897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8149987Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8150215Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8150302Z     return self._apply(convert)
2025-04-11T04:23:18.8150534Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8150617Z     module._apply(fn)
2025-04-11T04:23:18.8150853Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8150936Z     module._apply(fn)
2025-04-11T04:23:18.8151168Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8151248Z     module._apply(fn)
2025-04-11T04:23:18.8151480Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8151569Z     param_applied = fn(param)
2025-04-11T04:23:18.8151678Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8151682Z 
2025-04-11T04:23:18.8151770Z t = Parameter containing:
2025-04-11T04:23:18.8151903Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8151996Z         [ 0.0360, -0.0060,...2,  0.0336, -0.0315],
2025-04-11T04:23:18.8152116Z         [ 0.0418,  0.0034,  0.0053,  ...,  0.0279, -0.0100,  0.0020]],
2025-04-11T04:23:18.8152198Z        requires_grad=True)
2025-04-11T04:23:18.8152259Z 
2025-04-11T04:23:18.8152337Z     def convert(t):
2025-04-11T04:23:18.8152471Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8152649Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8152769Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8152975Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8153083Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8153365Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8153505Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8153663Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8153667Z 
2025-04-11T04:23:18.8153918Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8154096Z _______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] _______
2025-04-11T04:23:18.8154153Z 
2025-04-11T04:23:18.8154304Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8154481Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8154567Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8154572Z 
2025-04-11T04:23:18.8154739Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8154900Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8155056Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8155149Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8155303Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8155395Z         device: torch.device,
2025-04-11T04:23:18.8155474Z         adamw: bool,
2025-04-11T04:23:18.8155560Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8155644Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8155718Z     ) -> None:
2025-04-11T04:23:18.8155972Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8156117Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8156122Z 
2025-04-11T04:23:18.8156233Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8156342Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8156585Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8156677Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8156896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8156986Z     return self._apply(convert)
2025-04-11T04:23:18.8157218Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8157301Z     module._apply(fn)
2025-04-11T04:23:18.8157529Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8157610Z     module._apply(fn)
2025-04-11T04:23:18.8157839Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8157916Z     module._apply(fn)
2025-04-11T04:23:18.8158144Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8158233Z     param_applied = fn(param)
2025-04-11T04:23:18.8158344Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8158348Z 
2025-04-11T04:23:18.8158433Z t = Parameter containing:
2025-04-11T04:23:18.8158567Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8158715Z         [-0.0029, -0.0003,...8,  0.0132,  0.0134],
2025-04-11T04:23:18.8158832Z         [-0.0017, -0.0011, -0.0088,  ...,  0.0178,  0.0258,  0.0116]],
2025-04-11T04:23:18.8158920Z        requires_grad=True)
2025-04-11T04:23:18.8158925Z 
2025-04-11T04:23:18.8159000Z     def convert(t):
2025-04-11T04:23:18.8159131Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8159306Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8159424Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8159627Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8159735Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8160020Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8160153Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8160318Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8160383Z 
2025-04-11T04:23:18.8160632Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8160821Z _____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] ______
2025-04-11T04:23:18.8160828Z 
2025-04-11T04:23:18.8160991Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8161145Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8161280Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8161284Z 
2025-04-11T04:23:18.8161451Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8161578Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8161731Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8161827Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8161980Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8162069Z         device: torch.device,
2025-04-11T04:23:18.8162146Z         adamw: bool,
2025-04-11T04:23:18.8162230Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8162316Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8162440Z     ) -> None:
2025-04-11T04:23:18.8162695Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8162789Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8162903Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8162977Z         lr = 1e-3
2025-04-11T04:23:18.8163063Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8163145Z         eps = 1e-8
2025-04-11T04:23:18.8163247Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8163478Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8163700Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8163707Z 
2025-04-11T04:23:18.8163819Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8163930Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8163936Z 
2025-04-11T04:23:18.8164016Z self = HybridAdam (
2025-04-11T04:23:18.8164100Z Parameter Group 0
2025-04-11T04:23:18.8164178Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8164264Z     bias_correction: True
2025-04-11T04:23:18.8164340Z     eps: 1e-08
2025-04-11T04:23:18.8164413Z     lr: 0.001
2025-04-11T04:23:18.8164502Z     weig...arameter Group 1
2025-04-11T04:23:18.8164578Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8164664Z     bias_correction: True
2025-04-11T04:23:18.8164738Z     eps: 1e-08
2025-04-11T04:23:18.8164813Z     lr: 0.001
2025-04-11T04:23:18.8164945Z     weight_decay: 0.0
2025-04-11T04:23:18.8165015Z )
2025-04-11T04:23:18.8165331Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8165480Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8165644Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8165738Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8166111Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8166121Z 
2025-04-11T04:23:18.8166193Z     def __init__(
2025-04-11T04:23:18.8166265Z         self,
2025-04-11T04:23:18.8166346Z         model_params,
2025-04-11T04:23:18.8166419Z         lr=1e-3,
2025-04-11T04:23:18.8166503Z         bias_correction=True,
2025-04-11T04:23:18.8166581Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8166655Z         eps=1e-8,
2025-04-11T04:23:18.8166735Z         weight_decay=0,
2025-04-11T04:23:18.8166812Z         adamw_mode=True,
2025-04-11T04:23:18.8166961Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8167066Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8167145Z         **defaults: Any,
2025-04-11T04:23:18.8167217Z     ):
2025-04-11T04:23:18.8167296Z         super().__init__(
2025-04-11T04:23:18.8167379Z             model_params,
2025-04-11T04:23:18.8167451Z             lr,
2025-04-11T04:23:18.8167540Z             bias_correction,
2025-04-11T04:23:18.8167662Z             betas,
2025-04-11T04:23:18.8167735Z             eps,
2025-04-11T04:23:18.8167816Z             weight_decay,
2025-04-11T04:23:18.8167892Z             adamw_mode,
2025-04-11T04:23:18.8167980Z             nvme_offload_fraction,
2025-04-11T04:23:18.8168059Z             nvme_offload_dir,
2025-04-11T04:23:18.8168128Z         )
2025-04-11T04:23:18.8168226Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8168341Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8168465Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8168676Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8168840Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8169129Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8169267Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8169435Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8169439Z 
2025-04-11T04:23:18.8169574Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8169756Z _____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] ______
2025-04-11T04:23:18.8169762Z 
2025-04-11T04:23:18.8169924Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8170093Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8170179Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8170184Z 
2025-04-11T04:23:18.8170353Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8170476Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8170629Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8170722Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8170877Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8170962Z         device: torch.device,
2025-04-11T04:23:18.8171038Z         adamw: bool,
2025-04-11T04:23:18.8171122Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8171209Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8171335Z     ) -> None:
2025-04-11T04:23:18.8171592Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8171688Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8171693Z 
2025-04-11T04:23:18.8171803Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8171916Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8172158Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8172254Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8172480Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8172573Z     return self._apply(convert)
2025-04-11T04:23:18.8172807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8172890Z     module._apply(fn)
2025-04-11T04:23:18.8173124Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8173203Z     module._apply(fn)
2025-04-11T04:23:18.8173498Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8173574Z     module._apply(fn)
2025-04-11T04:23:18.8173803Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8173891Z     param_applied = fn(param)
2025-04-11T04:23:18.8174003Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8174007Z 
2025-04-11T04:23:18.8174143Z t = Parameter containing:
2025-04-11T04:23:18.8174277Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8174375Z         [ 0.0281,  0.0026,...4, -0.0037,  0.0294],
2025-04-11T04:23:18.8174491Z         [ 0.0003,  0.0104, -0.0075,  ...,  0.0078,  0.0005, -0.0179]],
2025-04-11T04:23:18.8174577Z        requires_grad=True)
2025-04-11T04:23:18.8174583Z 
2025-04-11T04:23:18.8174660Z     def convert(t):
2025-04-11T04:23:18.8174790Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8174970Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8175086Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8175346Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8175454Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8175743Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8175880Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8176040Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8176044Z 
2025-04-11T04:23:18.8176295Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8176474Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] _______
2025-04-11T04:23:18.8176480Z 
2025-04-11T04:23:18.8176639Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8176812Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8176901Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8176905Z 
2025-04-11T04:23:18.8177069Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8177196Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8177352Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8177446Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8177597Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8177680Z         device: torch.device,
2025-04-11T04:23:18.8177815Z         adamw: bool,
2025-04-11T04:23:18.8177899Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8177985Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8178061Z     ) -> None:
2025-04-11T04:23:18.8178316Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8178411Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8178416Z 
2025-04-11T04:23:18.8178522Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8178633Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8178874Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8178969Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8179193Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8179282Z     return self._apply(convert)
2025-04-11T04:23:18.8179512Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8179592Z     module._apply(fn)
2025-04-11T04:23:18.8179872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8179951Z     module._apply(fn)
2025-04-11T04:23:18.8180184Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8180262Z     module._apply(fn)
2025-04-11T04:23:18.8180493Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8180631Z     param_applied = fn(param)
2025-04-11T04:23:18.8180739Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8180743Z 
2025-04-11T04:23:18.8180832Z t = Parameter containing:
2025-04-11T04:23:18.8180965Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8181065Z         [ 0.0240, -0.0152,...6, -0.0175, -0.0244],
2025-04-11T04:23:18.8181184Z         [-0.0064, -0.0248,  0.0195,  ..., -0.0030, -0.0263,  0.0248]],
2025-04-11T04:23:18.8181271Z        requires_grad=True)
2025-04-11T04:23:18.8181276Z 
2025-04-11T04:23:18.8181351Z     def convert(t):
2025-04-11T04:23:18.8181481Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8181709Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8181825Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8182032Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8182142Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8182429Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8182562Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8182720Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8182729Z 
2025-04-11T04:23:18.8182982Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8183158Z _______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] ________
2025-04-11T04:23:18.8183164Z 
2025-04-11T04:23:18.8183317Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8183482Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8183574Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8183579Z 
2025-04-11T04:23:18.8183744Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8183864Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8184015Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8184182Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8184339Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8184424Z         device: torch.device,
2025-04-11T04:23:18.8184503Z         adamw: bool,
2025-04-11T04:23:18.8184587Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8184674Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8184748Z     ) -> None:
2025-04-11T04:23:18.8184994Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8185090Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8185096Z 
2025-04-11T04:23:18.8185203Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8185316Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8185555Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8185652Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8185877Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8186016Z     return self._apply(convert)
2025-04-11T04:23:18.8186252Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8186333Z     module._apply(fn)
2025-04-11T04:23:18.8186566Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8186645Z     module._apply(fn)
2025-04-11T04:23:18.8186879Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8187005Z     module._apply(fn)
2025-04-11T04:23:18.8187230Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8187323Z     param_applied = fn(param)
2025-04-11T04:23:18.8187431Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8187437Z 
2025-04-11T04:23:18.8187527Z t = Parameter containing:
2025-04-11T04:23:18.8187658Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8187756Z         [ 0.0105,  0.0235,...3,  0.0204, -0.0137],
2025-04-11T04:23:18.8187874Z         [ 0.0001, -0.0009, -0.0197,  ...,  0.0352, -0.0017,  0.0075]],
2025-04-11T04:23:18.8188009Z        requires_grad=True)
2025-04-11T04:23:18.8188013Z 
2025-04-11T04:23:18.8188095Z     def convert(t):
2025-04-11T04:23:18.8188223Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8188401Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8188548Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8188758Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8188866Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8189152Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8189292Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8189447Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8189454Z 
2025-04-11T04:23:18.8189710Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8189888Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] ______
2025-04-11T04:23:18.8189894Z 
2025-04-11T04:23:18.8190061Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8190213Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8190302Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8190306Z 
2025-04-11T04:23:18.8190470Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8190652Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8190808Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8190899Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8191058Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8191144Z         device: torch.device,
2025-04-11T04:23:18.8191225Z         adamw: bool,
2025-04-11T04:23:18.8191307Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8191389Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8191467Z     ) -> None:
2025-04-11T04:23:18.8191717Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8191813Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8191921Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8191998Z         lr = 1e-3
2025-04-11T04:23:18.8192083Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8192164Z         eps = 1e-8
2025-04-11T04:23:18.8192269Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8192549Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8192765Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8192772Z 
2025-04-11T04:23:18.8192879Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8192993Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8193050Z 
2025-04-11T04:23:18.8193131Z self = HybridAdam (
2025-04-11T04:23:18.8193210Z Parameter Group 0
2025-04-11T04:23:18.8193293Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8193378Z     bias_correction: True
2025-04-11T04:23:18.8193456Z     eps: 1e-08
2025-04-11T04:23:18.8193529Z     lr: 0.001
2025-04-11T04:23:18.8193616Z     weig...arameter Group 1
2025-04-11T04:23:18.8193698Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8193780Z     bias_correction: True
2025-04-11T04:23:18.8193856Z     eps: 1e-08
2025-04-11T04:23:18.8193928Z     lr: 0.001
2025-04-11T04:23:18.8194006Z     weight_decay: 0.0
2025-04-11T04:23:18.8194078Z )
2025-04-11T04:23:18.8194389Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8194593Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8194743Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8194842Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8195217Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8195222Z 
2025-04-11T04:23:18.8195301Z     def __init__(
2025-04-11T04:23:18.8195375Z         self,
2025-04-11T04:23:18.8195453Z         model_params,
2025-04-11T04:23:18.8195532Z         lr=1e-3,
2025-04-11T04:23:18.8195617Z         bias_correction=True,
2025-04-11T04:23:18.8195703Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8195780Z         eps=1e-8,
2025-04-11T04:23:18.8195859Z         weight_decay=0,
2025-04-11T04:23:18.8195944Z         adamw_mode=True,
2025-04-11T04:23:18.8196039Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8196146Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8196226Z         **defaults: Any,
2025-04-11T04:23:18.8196296Z     ):
2025-04-11T04:23:18.8196383Z         super().__init__(
2025-04-11T04:23:18.8196463Z             model_params,
2025-04-11T04:23:18.8196538Z             lr,
2025-04-11T04:23:18.8196620Z             bias_correction,
2025-04-11T04:23:18.8196693Z             betas,
2025-04-11T04:23:18.8196770Z             eps,
2025-04-11T04:23:18.8196849Z             weight_decay,
2025-04-11T04:23:18.8196984Z             adamw_mode,
2025-04-11T04:23:18.8197070Z             nvme_offload_fraction,
2025-04-11T04:23:18.8197154Z             nvme_offload_dir,
2025-04-11T04:23:18.8197228Z         )
2025-04-11T04:23:18.8197322Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8197442Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8197568Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8197781Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8197892Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8198182Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8198325Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8198486Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8198492Z 
2025-04-11T04:23:18.8198633Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8198815Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] ______
2025-04-11T04:23:18.8198869Z 
2025-04-11T04:23:18.8199039Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8199207Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8199296Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8199301Z 
2025-04-11T04:23:18.8199467Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8199586Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8199792Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8199883Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8200044Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8200129Z         device: torch.device,
2025-04-11T04:23:18.8200214Z         adamw: bool,
2025-04-11T04:23:18.8200299Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8200379Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8200460Z     ) -> None:
2025-04-11T04:23:18.8200712Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8200860Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8200865Z 
2025-04-11T04:23:18.8200974Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8201086Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8201335Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8201428Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8201660Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8201748Z     return self._apply(convert)
2025-04-11T04:23:18.8201986Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8202068Z     module._apply(fn)
2025-04-11T04:23:18.8202303Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8202382Z     module._apply(fn)
2025-04-11T04:23:18.8202610Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8202694Z     module._apply(fn)
2025-04-11T04:23:18.8202918Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8203011Z     param_applied = fn(param)
2025-04-11T04:23:18.8203119Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8203123Z 
2025-04-11T04:23:18.8203213Z t = Parameter containing:
2025-04-11T04:23:18.8203346Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8203495Z         [ 0.0140, -0.0115,...1,  0.0094,  0.0310],
2025-04-11T04:23:18.8203619Z         [ 0.0050,  0.0139, -0.0004,  ...,  0.0203, -0.0216, -0.0075]],
2025-04-11T04:23:18.8203709Z        requires_grad=True)
2025-04-11T04:23:18.8203714Z 
2025-04-11T04:23:18.8203795Z     def convert(t):
2025-04-11T04:23:18.8203925Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8204111Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8204229Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8204438Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8204552Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8204835Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8204977Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8205136Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8205192Z 
2025-04-11T04:23:18.8205449Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8205586Z _____________________________ test_dist_adafactor ______________________________
2025-04-11T04:23:18.8205591Z 
2025-04-11T04:23:18.8205686Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8206301Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8206355Z 
2025-04-11T04:23:18.8206461Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8206540Z         try_count = 0
2025-04-11T04:23:18.8206641Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8206725Z             max_try, int
2025-04-11T04:23:18.8206871Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8206946Z     
2025-04-11T04:23:18.8207058Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8207132Z             try:
2025-04-11T04:23:18.8207219Z                 try_count += 1
2025-04-11T04:23:18.8207375Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8207460Z                 return ret
2025-04-11T04:23:18.8207553Z             except exception_type as e:
2025-04-11T04:23:18.8207652Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8207842Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8207960Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8208109Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8208265Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8208351Z                     continue
2025-04-11T04:23:18.8208427Z                 else:
2025-04-11T04:23:18.8208650Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8208729Z >                   raise e
2025-04-11T04:23:18.8208734Z 
2025-04-11T04:23:18.8208826Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8208938Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8209068Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8209158Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8209332Z tests/test_optimizer/test_dist_adafactor.py:468: in test_dist_adafactor
2025-04-11T04:23:18.8209422Z     spawn(run_dist, nprocs=4)
2025-04-11T04:23:18.8209521Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8209620Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8209936Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8210119Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8210408Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8210495Z     while not context.join():
2025-04-11T04:23:18.8210608Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8210613Z 
2025-04-11T04:23:18.8210830Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a8aa10>
2025-04-11T04:23:18.8210909Z timeout = None
2025-04-11T04:23:18.8210914Z 
2025-04-11T04:23:18.8211010Z     def join(self, timeout=None):
2025-04-11T04:23:18.8211134Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8211208Z     
2025-04-11T04:23:18.8211352Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8211501Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8211666Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8211808Z         of the first process exiting.
2025-04-11T04:23:18.8211881Z     
2025-04-11T04:23:18.8212032Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8212172Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8212240Z     
2025-04-11T04:23:18.8212312Z         Args:
2025-04-11T04:23:18.8212461Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8212583Z         """
2025-04-11T04:23:18.8212725Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8212818Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8212901Z             return True
2025-04-11T04:23:18.8212970Z     
2025-04-11T04:23:18.8213108Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8213231Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8213326Z             self.sentinels.keys(),
2025-04-11T04:23:18.8213413Z             timeout=timeout,
2025-04-11T04:23:18.8213485Z         )
2025-04-11T04:23:18.8213555Z     
2025-04-11T04:23:18.8213691Z         error_index = None
2025-04-11T04:23:18.8213775Z         for sentinel in ready:
2025-04-11T04:23:18.8213884Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8213983Z             process = self.processes[index]
2025-04-11T04:23:18.8214069Z             process.join()
2025-04-11T04:23:18.8214164Z             if process.exitcode != 0:
2025-04-11T04:23:18.8214249Z                 error_index = index
2025-04-11T04:23:18.8214327Z                 break
2025-04-11T04:23:18.8214395Z     
2025-04-11T04:23:18.8214488Z         # Return if there was no error.
2025-04-11T04:23:18.8214573Z         if error_index is None:
2025-04-11T04:23:18.8214710Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8214811Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8214885Z     
2025-04-11T04:23:18.8215035Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8215129Z         for process in self.processes:
2025-04-11T04:23:18.8215220Z             if process.is_alive():
2025-04-11T04:23:18.8215313Z                 process.terminate()
2025-04-11T04:23:18.8215397Z             process.join()
2025-04-11T04:23:18.8215471Z     
2025-04-11T04:23:18.8215609Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8215730Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8215834Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8215951Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8216039Z             if exitcode < 0:
2025-04-11T04:23:18.8216197Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8216305Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8216457Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8216556Z                     error_index=error_index,
2025-04-11T04:23:18.8216659Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8216750Z                     exit_code=exitcode,
2025-04-11T04:23:18.8216838Z                     signal_name=name,
2025-04-11T04:23:18.8216911Z                 )
2025-04-11T04:23:18.8216987Z             else:
2025-04-11T04:23:18.8217096Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8217259Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8217355Z                     error_index=error_index,
2025-04-11T04:23:18.8217451Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8217541Z                     exit_code=exitcode,
2025-04-11T04:23:18.8217616Z                 )
2025-04-11T04:23:18.8217686Z     
2025-04-11T04:23:18.8217825Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8218047Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8218136Z         msg += original_trace
2025-04-11T04:23:18.8218311Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8218471Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8218546Z E       
2025-04-11T04:23:18.8218670Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8218825Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8219121Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8219205Z E           fn(i, *args)
2025-04-11T04:23:18.8219454Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
2025-04-11T04:23:18.8219552Z E           exam_dist_adafactor_base()
2025-04-11T04:23:18.8219807Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8219893Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8220202Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8220288Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8220581Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
2025-04-11T04:23:18.8220738Z E           model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
2025-04-11T04:23:18.8221009Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.8221104Z E           return self._apply(convert)
2025-04-11T04:23:18.8221379Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8221480Z E           param_applied = fn(param)
2025-04-11T04:23:18.8221754Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.8221971Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8222079Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8222369Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8222504Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8222665Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8222670Z 
2025-04-11T04:23:18.8222970Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8223174Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8223338Z [04/11/25 04:18:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8223466Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8223577Z                              :75 launch                                         
2025-04-11T04:23:18.8223712Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8223842Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8223926Z Base Test Passed
2025-04-11T04:23:18.8224010Z Base Test Passed
2025-04-11T04:23:18.8224086Z Base Test Passed
2025-04-11T04:23:18.8224286Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8224435Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8225586Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8225814Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8226975Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8227146Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8228258Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8228514Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8229653Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8229821Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8230505Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8230594Z   warnings.warn(
2025-04-11T04:23:18.8231267Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8231357Z   warnings.warn(
2025-04-11T04:23:18.8232023Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8232165Z   warnings.warn(
2025-04-11T04:23:18.8232833Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8232915Z   warnings.warn(
2025-04-11T04:23:18.8233739Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8233820Z   warnings.warn(
2025-04-11T04:23:18.8234621Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8234760Z   warnings.warn(
2025-04-11T04:23:18.8235572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8235720Z   warnings.warn(
2025-04-11T04:23:18.8236514Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8236593Z   warnings.warn(
2025-04-11T04:23:18.8237398Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8237617Z   warnings.warn(
2025-04-11T04:23:18.8238411Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8238490Z   warnings.warn(
2025-04-11T04:23:18.8239283Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8239362Z   warnings.warn(
2025-04-11T04:23:18.8240154Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8240234Z   warnings.warn(
2025-04-11T04:23:18.8241049Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8241196Z   warnings.warn(
2025-04-11T04:23:18.8241981Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8242063Z   warnings.warn(
2025-04-11T04:23:18.8242845Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8242923Z   warnings.warn(
2025-04-11T04:23:18.8243222Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:21639 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8244014Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8244146Z   warnings.warn(
2025-04-11T04:23:18.8244432Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:21639 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8244571Z ________________________________ test_dist_came ________________________________
2025-04-11T04:23:18.8244578Z 
2025-04-11T04:23:18.8244668Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8245330Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8245336Z 
2025-04-11T04:23:18.8245442Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8245522Z         try_count = 0
2025-04-11T04:23:18.8245626Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8245709Z             max_try, int
2025-04-11T04:23:18.8245858Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8245980Z     
2025-04-11T04:23:18.8246093Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8246169Z             try:
2025-04-11T04:23:18.8246252Z                 try_count += 1
2025-04-11T04:23:18.8246345Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8246424Z                 return ret
2025-04-11T04:23:18.8246519Z             except exception_type as e:
2025-04-11T04:23:18.8246622Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8246809Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8246927Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8247077Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8247233Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8247316Z                     continue
2025-04-11T04:23:18.8247392Z                 else:
2025-04-11T04:23:18.8247621Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8247700Z >                   raise e
2025-04-11T04:23:18.8247705Z 
2025-04-11T04:23:18.8247800Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8247914Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8248049Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8248136Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8248283Z tests/test_optimizer/test_dist_came.py:357: in test_dist_came
2025-04-11T04:23:18.8248426Z     spawn(run_dist, nprocs=4)
2025-04-11T04:23:18.8248525Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8248625Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8248883Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8249064Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8249351Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8249437Z     while not context.join():
2025-04-11T04:23:18.8249549Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8249555Z 
2025-04-11T04:23:18.8249750Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8692950>
2025-04-11T04:23:18.8249830Z timeout = None
2025-04-11T04:23:18.8249834Z 
2025-04-11T04:23:18.8249922Z     def join(self, timeout=None):
2025-04-11T04:23:18.8250050Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8250118Z     
2025-04-11T04:23:18.8250261Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8250459Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8250621Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8250716Z         of the first process exiting.
2025-04-11T04:23:18.8250784Z     
2025-04-11T04:23:18.8250936Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8251070Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8251190Z     
2025-04-11T04:23:18.8251267Z         Args:
2025-04-11T04:23:18.8251405Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8251480Z         """
2025-04-11T04:23:18.8251619Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8251712Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8251792Z             return True
2025-04-11T04:23:18.8251860Z     
2025-04-11T04:23:18.8251998Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8252113Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8252204Z             self.sentinels.keys(),
2025-04-11T04:23:18.8252338Z             timeout=timeout,
2025-04-11T04:23:18.8252408Z         )
2025-04-11T04:23:18.8252480Z     
2025-04-11T04:23:18.8252559Z         error_index = None
2025-04-11T04:23:18.8252647Z         for sentinel in ready:
2025-04-11T04:23:18.8252752Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8252850Z             process = self.processes[index]
2025-04-11T04:23:18.8252939Z             process.join()
2025-04-11T04:23:18.8253028Z             if process.exitcode != 0:
2025-04-11T04:23:18.8253116Z                 error_index = index
2025-04-11T04:23:18.8253190Z                 break
2025-04-11T04:23:18.8253261Z     
2025-04-11T04:23:18.8253354Z         # Return if there was no error.
2025-04-11T04:23:18.8253438Z         if error_index is None:
2025-04-11T04:23:18.8253574Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8253669Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8253742Z     
2025-04-11T04:23:18.8253881Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8253975Z         for process in self.processes:
2025-04-11T04:23:18.8254065Z             if process.is_alive():
2025-04-11T04:23:18.8254155Z                 process.terminate()
2025-04-11T04:23:18.8254244Z             process.join()
2025-04-11T04:23:18.8254313Z     
2025-04-11T04:23:18.8254452Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8254571Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8254675Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8254851Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8254934Z             if exitcode < 0:
2025-04-11T04:23:18.8255040Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8255147Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8255294Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8255394Z                     error_index=error_index,
2025-04-11T04:23:18.8255493Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8255582Z                     exit_code=exitcode,
2025-04-11T04:23:18.8255666Z                     signal_name=name,
2025-04-11T04:23:18.8255741Z                 )
2025-04-11T04:23:18.8255818Z             else:
2025-04-11T04:23:18.8255918Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8256084Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8256176Z                     error_index=error_index,
2025-04-11T04:23:18.8256277Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8256362Z                     exit_code=exitcode,
2025-04-11T04:23:18.8256483Z                 )
2025-04-11T04:23:18.8256555Z     
2025-04-11T04:23:18.8256688Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8256865Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8256949Z         msg += original_trace
2025-04-11T04:23:18.8257118Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8257285Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8257406Z E       
2025-04-11T04:23:18.8257536Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8257634Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8257932Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8258015Z E           fn(i, *args)
2025-04-11T04:23:18.8258247Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
2025-04-11T04:23:18.8258394Z E           exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
2025-04-11T04:23:18.8258701Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8258795Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8259100Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
2025-04-11T04:23:18.8259328Z E           ) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
2025-04-11T04:23:18.8259651Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
2025-04-11T04:23:18.8259753Z E           org_model = org_model.cuda()
2025-04-11T04:23:18.8260042Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.8260146Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.8260416Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8260535Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8260812Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8260900Z E           module._apply(fn)
2025-04-11T04:23:18.8261173Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8261257Z E           module._apply(fn)
2025-04-11T04:23:18.8261525Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8261675Z E           param_applied = fn(param)
2025-04-11T04:23:18.8261947Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8262067Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8262174Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8262459Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8262593Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8262757Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8262761Z 
2025-04-11T04:23:18.8263064Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8263214Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8263376Z [04/11/25 04:18:11] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8263575Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8263685Z                              :75 launch                                         
2025-04-11T04:23:18.8263823Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8263949Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8264145Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8264339Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8265487Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8265660Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8266839Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8267008Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8268117Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8268285Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8269431Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8269594Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8270331Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8270419Z   warnings.warn(
2025-04-11T04:23:18.8271088Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8271175Z   warnings.warn(
2025-04-11T04:23:18.8271840Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8271919Z   warnings.warn(
2025-04-11T04:23:18.8272590Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8272727Z   warnings.warn(
2025-04-11T04:23:18.8273571Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8273703Z   warnings.warn(
2025-04-11T04:23:18.8274514Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8274593Z   warnings.warn(
2025-04-11T04:23:18.8275399Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8275534Z   warnings.warn(
2025-04-11T04:23:18.8276326Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8276404Z   warnings.warn(
2025-04-11T04:23:18.8277195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8277274Z   warnings.warn(
2025-04-11T04:23:18.8278063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8278140Z   warnings.warn(
2025-04-11T04:23:18.8278927Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8279006Z   warnings.warn(
2025-04-11T04:23:18.8279841Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8279924Z   warnings.warn(
2025-04-11T04:23:18.8280709Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8280790Z   warnings.warn(
2025-04-11T04:23:18.8281573Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8281657Z   warnings.warn(
2025-04-11T04:23:18.8282440Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8282577Z   warnings.warn(
2025-04-11T04:23:18.8283368Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8283498Z   warnings.warn(
2025-04-11T04:23:18.8283793Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8284078Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8284353Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8284545Z _______________________________ test_dist_galore _______________________________
2025-04-11T04:23:18.8284549Z 
2025-04-11T04:23:18.8284638Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8285242Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8285249Z 
2025-04-11T04:23:18.8285349Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8285431Z         try_count = 0
2025-04-11T04:23:18.8285533Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8285614Z             max_try, int
2025-04-11T04:23:18.8285765Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8285837Z     
2025-04-11T04:23:18.8285950Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8286021Z             try:
2025-04-11T04:23:18.8286105Z                 try_count += 1
2025-04-11T04:23:18.8286200Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8286280Z                 return ret
2025-04-11T04:23:18.8286377Z             except exception_type as e:
2025-04-11T04:23:18.8286476Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8286664Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8286784Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8286929Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8287142Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8287223Z                     continue
2025-04-11T04:23:18.8287304Z                 else:
2025-04-11T04:23:18.8287527Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8287606Z >                   raise e
2025-04-11T04:23:18.8287615Z 
2025-04-11T04:23:18.8287707Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8287818Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8287953Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8288040Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8288200Z tests/test_optimizer/test_dist_galore.py:298: in test_dist_galore
2025-04-11T04:23:18.8288297Z     spawn(check_dist_galore, nprocs=4)
2025-04-11T04:23:18.8288396Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8288500Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8288754Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8288987Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8289268Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8289360Z     while not context.join():
2025-04-11T04:23:18.8289473Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8289477Z 
2025-04-11T04:23:18.8289727Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a8b2e0>
2025-04-11T04:23:18.8289805Z timeout = None
2025-04-11T04:23:18.8289809Z 
2025-04-11T04:23:18.8289898Z     def join(self, timeout=None):
2025-04-11T04:23:18.8290025Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8290096Z     
2025-04-11T04:23:18.8290251Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8290400Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8290568Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8290663Z         of the first process exiting.
2025-04-11T04:23:18.8290779Z     
2025-04-11T04:23:18.8290930Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8291066Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8291139Z     
2025-04-11T04:23:18.8291212Z         Args:
2025-04-11T04:23:18.8291352Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8291428Z         """
2025-04-11T04:23:18.8291564Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8291658Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8291737Z             return True
2025-04-11T04:23:18.8291807Z     
2025-04-11T04:23:18.8291940Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8292056Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8292150Z             self.sentinels.keys(),
2025-04-11T04:23:18.8292232Z             timeout=timeout,
2025-04-11T04:23:18.8292305Z         )
2025-04-11T04:23:18.8292375Z     
2025-04-11T04:23:18.8292454Z         error_index = None
2025-04-11T04:23:18.8292542Z         for sentinel in ready:
2025-04-11T04:23:18.8292646Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8292747Z             process = self.processes[index]
2025-04-11T04:23:18.8292833Z             process.join()
2025-04-11T04:23:18.8292923Z             if process.exitcode != 0:
2025-04-11T04:23:18.8293012Z                 error_index = index
2025-04-11T04:23:18.8293086Z                 break
2025-04-11T04:23:18.8293157Z     
2025-04-11T04:23:18.8293247Z         # Return if there was no error.
2025-04-11T04:23:18.8293398Z         if error_index is None:
2025-04-11T04:23:18.8293533Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8293629Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8293700Z     
2025-04-11T04:23:18.8293839Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8293937Z         for process in self.processes:
2025-04-11T04:23:18.8294023Z             if process.is_alive():
2025-04-11T04:23:18.8294113Z                 process.terminate()
2025-04-11T04:23:18.8294199Z             process.join()
2025-04-11T04:23:18.8294267Z     
2025-04-11T04:23:18.8294410Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8294523Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8294629Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8294752Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8294833Z             if exitcode < 0:
2025-04-11T04:23:18.8294943Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8295046Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8295253Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8295347Z                     error_index=error_index,
2025-04-11T04:23:18.8295450Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8295542Z                     exit_code=exitcode,
2025-04-11T04:23:18.8295625Z                     signal_name=name,
2025-04-11T04:23:18.8295698Z                 )
2025-04-11T04:23:18.8295770Z             else:
2025-04-11T04:23:18.8295924Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8296088Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8296178Z                     error_index=error_index,
2025-04-11T04:23:18.8296277Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8296364Z                     exit_code=exitcode,
2025-04-11T04:23:18.8296436Z                 )
2025-04-11T04:23:18.8296504Z     
2025-04-11T04:23:18.8296636Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8296809Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8296940Z         msg += original_trace
2025-04-11T04:23:18.8297113Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8297270Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8297346Z E       
2025-04-11T04:23:18.8297471Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8297567Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8297874Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8297956Z E           fn(i, *args)
2025-04-11T04:23:18.8298226Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
2025-04-11T04:23:18.8298308Z E           dist.barrier()
2025-04-11T04:23:18.8298611Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.8298705Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.8299023Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:18.8299130Z E           work = default_pg.barrier(opts=opts)
2025-04-11T04:23:18.8299235Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8299523Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8299655Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8299815Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8299869Z 
2025-04-11T04:23:18.8300172Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8300325Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8300483Z [04/11/25 04:18:19] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8300610Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8300727Z                              :75 launch                                         
2025-04-11T04:23:18.8300867Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8300994Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8301129Z Skipping forward-backward tests due to SVD instability
2025-04-11T04:23:18.8301541Z Running bert tests, which are expected to produce minor errors due to instability in SVD convergence.             For example, a 1e-9 grad diff causes drastic difference in SVD output.
2025-04-11T04:23:18.8301685Z CUDA error: out of memory
2025-04-11T04:23:18.8301963Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8302097Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8302254Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8302259Z 
2025-04-11T04:23:18.8302347Z CUDA error: out of memory
2025-04-11T04:23:18.8302617Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8302794Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8302947Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8302952Z 
2025-04-11T04:23:18.8303036Z CUDA error: out of memory
2025-04-11T04:23:18.8303308Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8303433Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8303589Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8303645Z 
2025-04-11T04:23:18.8304012Z [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T04:23:18.8304417Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T04:23:18.8304853Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f10aa7d3d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T04:23:18.8305198Z frame #1: <unknown function> + 0x5522c2e (0x7f10ef0bcc2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8305699Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f10ef0b7440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8306073Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f10ef0b7782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8306418Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f10ef0b85b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8306774Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8307210Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8307617Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8307973Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8308529Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f10ab998a59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8309134Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f10ab99fa4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8309972Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f10ab9b5e4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8310366Z frame #12: <unknown function> + 0x54c7dbd (0x7f10ef061dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8310690Z frame #13: <unknown function> + 0x54d1cb8 (0x7f10ef06bcb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8311060Z frame #14: <unknown function> + 0x4b16e6c (0x7f10ee6b0e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8311377Z frame #15: <unknown function> + 0x1696528 (0x7f10eb230528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8311695Z frame #16: <unknown function> + 0x54d94d3 (0x7f10ef0734d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8312010Z frame #17: <unknown function> + 0x54e48bf (0x7f10ef07e8bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8312956Z frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f10ef0e4f2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8313296Z frame #19: <unknown function> + 0xc55ad1 (0x7f10f6c8fad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8313626Z frame #20: <unknown function> + 0x413ea4 (0x7f10f644dea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8313763Z frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T04:23:18.8313969Z frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8314183Z frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8314387Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8314591Z frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8334458Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8334788Z frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8334938Z frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8335073Z frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T04:23:18.8335400Z frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8335619Z frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8335817Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336036Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336166Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8336354Z frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336564Z frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336754Z frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336957Z frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337147Z frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337415Z frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337601Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337804Z frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337991Z frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8338243Z frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8338432Z frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8338602Z frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8338736Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T04:23:18.8338920Z frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339093Z frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339294Z frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339537Z frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339742Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339932Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8340101Z frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8340227Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T04:23:18.8340416Z frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8340614Z frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8340807Z frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341006Z frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341191Z frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341391Z frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341576Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341773Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341974Z . This may indicate a possible application crash on rank 0 or a network set up issue.
2025-04-11T04:23:18.8342431Z [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T04:23:18.8342837Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T04:23:18.8343237Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f6608535d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T04:23:18.8343575Z frame #1: <unknown function> + 0x5522c2e (0x7f664ce1ec2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8344205Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f664ce19440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8344578Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f664ce19782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8344977Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f664ce1a5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8345332Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8345680Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8346083Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8346428Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8346942Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f66096faa59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8347539Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f6609701a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8348467Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f6609717e4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8348807Z frame #12: <unknown function> + 0x54c7dbd (0x7f664cdc3dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8349132Z frame #13: <unknown function> + 0x54d1cb8 (0x7f664cdcdcb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8349453Z frame #14: <unknown function> + 0x4b16e6c (0x7f664c412e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8349771Z frame #15: <unknown function> + 0x1696528 (0x7f6648f92528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8350091Z frame #16: <unknown function> + 0x54d94d3 (0x7f664cdd54d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8350410Z frame #17: <unknown function> + 0x54e48bf (0x7f664cde08bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8351328Z frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f664ce46f2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8351669Z frame #19: <unknown function> + 0xc55ad1 (0x7f66549f1ad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8351995Z frame #20: <unknown function> + 0x413ea4 (0x7f66541afea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8352132Z frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T04:23:18.8352322Z frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8352533Z frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8352726Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8352988Z frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8353177Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8353391Z frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8353522Z frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8353702Z frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T04:23:18.8353879Z frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8354082Z frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8354273Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8354485Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8354612Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8354799Z frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355061Z frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355253Z frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355451Z frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355643Z frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355840Z frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356031Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356227Z frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356419Z frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356620Z frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356810Z frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356982Z frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8357109Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T04:23:18.8357295Z frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8357462Z frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8357714Z frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8357906Z frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358106Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358297Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358459Z frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358584Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T04:23:18.8358768Z frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358966Z frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359150Z frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359347Z frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359532Z frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359779Z frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359971Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8360167Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8360372Z . This may indicate a possible application crash on rank 0 or a network set up issue.
2025-04-11T04:23:18.8361018Z Failed to replace attention.self.query of type Linear with Linear1D_Col with the exception: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T04:23:18.8361416Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T04:23:18.8361802Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f10aa7d3d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T04:23:18.8362185Z frame #1: <unknown function> + 0x5522c2e (0x7f10ef0bcc2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8362669Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f10ef0b7440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8363024Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f10ef0b7782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8363369Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f10ef0b85b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8363716Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8364073Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8364416Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8364763Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8365252Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f10ab998a59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8365907Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f10ab99fa4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8366733Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f10ab9b5e4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8367068Z frame #12: <unknown function> + 0x54c7dbd (0x7f10ef061dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8367383Z frame #13: <unknown function> + 0x54d1cb8 (0x7f10ef06bcb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8367707Z frame #14: <unknown function> + 0x4b16e6c (0x7f10ee6b0e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8368076Z frame #15: <unknown function> + 0x1696528 (0x7f10eb230528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8368395Z frame #16: <unknown function> + 0x54d94d3 (0x7f10ef0734d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8368710Z frame #17: <unknown function> + 0x54e48bf (0x7f10ef07e8bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8369088Z frame #18: <unknown function> + 0xca3fae (0x7f10f6cddfae in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8369415Z frame #19: <unknown function> + 0x413ea4 (0x7f10f644dea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8369547Z frame #20: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T04:23:18.8369743Z frame #21: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8369872Z frame #22: /opt/conda/envs/pytorch/bin/python() [0x509cbf]
2025-04-11T04:23:18.8370137Z frame #23: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8370327Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8370527Z frame #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8370718Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8370918Z frame #27: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371108Z frame #28: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371308Z frame #29: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371496Z frame #30: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371691Z frame #31: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371882Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8372093Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8372224Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8372348Z frame #35: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T04:23:18.8372522Z frame #36: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8372722Z frame #37: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8372967Z frame #38: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373136Z frame #39: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373334Z frame #40: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373526Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373722Z frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373853Z frame #43: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8374048Z frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8374176Z frame #45: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8374371Z frame #46: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8374496Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8374748Z frame #48: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8374870Z frame #49: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8375071Z frame #50: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8375193Z frame #51: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8375390Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8375630Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8375828Z frame #54: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8375951Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8376148Z frame #56: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8376340Z frame #57: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8376549Z frame #58: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8376723Z frame #59: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8376908Z frame #60: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8377108Z frame #61: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8377299Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8377494Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8377954Z . This may indicate a possible application crash on rank 0 or a network set up issue.. Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
2025-04-11T04:23:18.8378155Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8378316Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8379466Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8379644Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8380756Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8380984Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8382110Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8382278Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8383394Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8383608Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8384311Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8384446Z   warnings.warn(
2025-04-11T04:23:18.8385168Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8385259Z   warnings.warn(
2025-04-11T04:23:18.8385943Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8386088Z   warnings.warn(
2025-04-11T04:23:18.8386796Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8386881Z   warnings.warn(
2025-04-11T04:23:18.8387716Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8387802Z   warnings.warn(
2025-04-11T04:23:18.8388651Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8388737Z   warnings.warn(
2025-04-11T04:23:18.8389531Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8389670Z   warnings.warn(
2025-04-11T04:23:18.8390469Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8390552Z   warnings.warn(
2025-04-11T04:23:18.8391344Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8391424Z   warnings.warn(
2025-04-11T04:23:18.8392227Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8392304Z   warnings.warn(
2025-04-11T04:23:18.8393149Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8393228Z   warnings.warn(
2025-04-11T04:23:18.8394016Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8394147Z   warnings.warn(
2025-04-11T04:23:18.8394931Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8395009Z   warnings.warn(
2025-04-11T04:23:18.8395801Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8395934Z   warnings.warn(
2025-04-11T04:23:18.8396717Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8396795Z   warnings.warn(
2025-04-11T04:23:18.8397580Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8397658Z   warnings.warn(
2025-04-11T04:23:18.8397956Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:36049 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8398240Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:36049 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8398522Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:36049 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8398659Z ________________________________ test_dist_lamb ________________________________
2025-04-11T04:23:18.8398716Z 
2025-04-11T04:23:18.8398812Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8399424Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8399434Z 
2025-04-11T04:23:18.8399540Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8399625Z         try_count = 0
2025-04-11T04:23:18.8399729Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8399817Z             max_try, int
2025-04-11T04:23:18.8399964Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8400037Z     
2025-04-11T04:23:18.8400150Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8400222Z             try:
2025-04-11T04:23:18.8400310Z                 try_count += 1
2025-04-11T04:23:18.8400404Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8400489Z                 return ret
2025-04-11T04:23:18.8400584Z             except exception_type as e:
2025-04-11T04:23:18.8400736Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8400929Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8401049Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8401200Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8401355Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8401510Z                     continue
2025-04-11T04:23:18.8401587Z                 else:
2025-04-11T04:23:18.8401813Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8401896Z >                   raise e
2025-04-11T04:23:18.8401903Z 
2025-04-11T04:23:18.8401996Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8402115Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8402250Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8402339Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8402627Z tests/test_optimizer/test_dist_lamb.py:276: in test_dist_lamb
2025-04-11T04:23:18.8402721Z     spawn(check_dist_lamb, nprocs=4)
2025-04-11T04:23:18.8402826Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8402923Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8403188Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8403368Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8403655Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8403747Z     while not context.join():
2025-04-11T04:23:18.8403855Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8403860Z 
2025-04-11T04:23:18.8404066Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8690c40>
2025-04-11T04:23:18.8404144Z timeout = None
2025-04-11T04:23:18.8404151Z 
2025-04-11T04:23:18.8404243Z     def join(self, timeout=None):
2025-04-11T04:23:18.8404368Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8404440Z     
2025-04-11T04:23:18.8404584Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8404730Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8404897Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8404988Z         of the first process exiting.
2025-04-11T04:23:18.8405060Z     
2025-04-11T04:23:18.8405206Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8405396Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8405467Z     
2025-04-11T04:23:18.8405539Z         Args:
2025-04-11T04:23:18.8405680Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8405751Z         """
2025-04-11T04:23:18.8405894Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8405986Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8406063Z             return True
2025-04-11T04:23:18.8406135Z     
2025-04-11T04:23:18.8406266Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8406388Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8406479Z             self.sentinels.keys(),
2025-04-11T04:23:18.8406562Z             timeout=timeout,
2025-04-11T04:23:18.8406635Z         )
2025-04-11T04:23:18.8406704Z     
2025-04-11T04:23:18.8406787Z         error_index = None
2025-04-11T04:23:18.8406874Z         for sentinel in ready:
2025-04-11T04:23:18.8406981Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8407130Z             process = self.processes[index]
2025-04-11T04:23:18.8407214Z             process.join()
2025-04-11T04:23:18.8407309Z             if process.exitcode != 0:
2025-04-11T04:23:18.8407398Z                 error_index = index
2025-04-11T04:23:18.8407474Z                 break
2025-04-11T04:23:18.8407542Z     
2025-04-11T04:23:18.8407631Z         # Return if there was no error.
2025-04-11T04:23:18.8407718Z         if error_index is None:
2025-04-11T04:23:18.8407850Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8408020Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8408089Z     
2025-04-11T04:23:18.8408225Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8408323Z         for process in self.processes:
2025-04-11T04:23:18.8408411Z             if process.is_alive():
2025-04-11T04:23:18.8408502Z                 process.terminate()
2025-04-11T04:23:18.8408584Z             process.join()
2025-04-11T04:23:18.8408656Z     
2025-04-11T04:23:18.8408796Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8408910Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8409074Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8409195Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8409282Z             if exitcode < 0:
2025-04-11T04:23:18.8409389Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8409497Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8409654Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8409749Z                     error_index=error_index,
2025-04-11T04:23:18.8409853Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8409942Z                     exit_code=exitcode,
2025-04-11T04:23:18.8410029Z                     signal_name=name,
2025-04-11T04:23:18.8410101Z                 )
2025-04-11T04:23:18.8410176Z             else:
2025-04-11T04:23:18.8410281Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8410444Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8410545Z                     error_index=error_index,
2025-04-11T04:23:18.8410642Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8410730Z                     exit_code=exitcode,
2025-04-11T04:23:18.8410802Z                 )
2025-04-11T04:23:18.8410870Z     
2025-04-11T04:23:18.8411005Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8411177Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8411266Z         msg += original_trace
2025-04-11T04:23:18.8411439Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8411657Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8411735Z E       
2025-04-11T04:23:18.8411862Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8411963Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8412266Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8412350Z E           fn(i, *args)
2025-04-11T04:23:18.8412607Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
2025-04-11T04:23:18.8412699Z E           run_dist_lamb_basic()
2025-04-11T04:23:18.8412961Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8413050Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8413303Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8413389Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8413690Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8413773Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8413990Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.8414092Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.8414346Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.8414502Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.8414780Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.8414889Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8414995Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8415276Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8415415Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8415574Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8415626Z 
2025-04-11T04:23:18.8415930Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8416084Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8416246Z [04/11/25 04:18:28] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8416373Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8416489Z                              :75 launch                                         
2025-04-11T04:23:18.8416627Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8416749Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8416951Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8417093Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8418213Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8418380Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8419484Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8419705Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8420801Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8420963Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8422058Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8422273Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8422977Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8423109Z   warnings.warn(
2025-04-11T04:23:18.8423798Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8423883Z   warnings.warn(
2025-04-11T04:23:18.8424563Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8424692Z   warnings.warn(
2025-04-11T04:23:18.8425360Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8425436Z   warnings.warn(
2025-04-11T04:23:18.8426271Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8426351Z   warnings.warn(
2025-04-11T04:23:18.8427146Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8427224Z   warnings.warn(
2025-04-11T04:23:18.8428046Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8428174Z   warnings.warn(
2025-04-11T04:23:18.8429161Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8429241Z   warnings.warn(
2025-04-11T04:23:18.8430027Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8430109Z   warnings.warn(
2025-04-11T04:23:18.8430928Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8431082Z   warnings.warn(
2025-04-11T04:23:18.8431890Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8431972Z   warnings.warn(
2025-04-11T04:23:18.8432771Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8432905Z   warnings.warn(
2025-04-11T04:23:18.8433689Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8433771Z   warnings.warn(
2025-04-11T04:23:18.8434611Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8434694Z   warnings.warn(
2025-04-11T04:23:18.8435481Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8435564Z   warnings.warn(
2025-04-11T04:23:18.8436353Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8436436Z   warnings.warn(
2025-04-11T04:23:18.8436732Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34628 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8437020Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34628 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8437297Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34628 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8437910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8437993Z   warnings.warn(
2025-04-11T04:23:18.8438521Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8438606Z   warnings.warn(
2025-04-11T04:23:18.8439139Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8439222Z   warnings.warn(
2025-04-11T04:23:18.8439748Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8439832Z   warnings.warn(
2025-04-11T04:23:18.8440362Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8440497Z   warnings.warn(
2025-04-11T04:23:18.8441031Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8441159Z   warnings.warn(
2025-04-11T04:23:18.8441697Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8441772Z   warnings.warn(
2025-04-11T04:23:18.8442306Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8442384Z   warnings.warn(
2025-04-11T04:23:18.8442528Z ______________________________ test_pipeline_p2p _______________________________
2025-04-11T04:23:18.8442580Z 
2025-04-11T04:23:18.8442675Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8443281Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8443290Z 
2025-04-11T04:23:18.8443394Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8443477Z         try_count = 0
2025-04-11T04:23:18.8443578Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8443661Z             max_try, int
2025-04-11T04:23:18.8443812Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8443881Z     
2025-04-11T04:23:18.8443995Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8444069Z             try:
2025-04-11T04:23:18.8444151Z                 try_count += 1
2025-04-11T04:23:18.8444244Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8444326Z                 return ret
2025-04-11T04:23:18.8444422Z             except exception_type as e:
2025-04-11T04:23:18.8444520Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8444708Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8444826Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8444969Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8445128Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8445260Z                     continue
2025-04-11T04:23:18.8445339Z                 else:
2025-04-11T04:23:18.8445564Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8445649Z >                   raise e
2025-04-11T04:23:18.8445654Z 
2025-04-11T04:23:18.8445748Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8445859Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8445991Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8446078Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8446258Z tests/test_pipeline/test_p2p_communication.py:79: in test_pipeline_p2p
2025-04-11T04:23:18.8446346Z     spawn(run_dist, WORLD_SIZE)
2025-04-11T04:23:18.8446448Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8446546Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8446805Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8446992Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8447332Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8447424Z     while not context.join():
2025-04-11T04:23:18.8447534Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8447539Z 
2025-04-11T04:23:18.8447737Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f139a0>
2025-04-11T04:23:18.8447815Z timeout = None
2025-04-11T04:23:18.8447868Z 
2025-04-11T04:23:18.8447961Z     def join(self, timeout=None):
2025-04-11T04:23:18.8448088Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8448155Z     
2025-04-11T04:23:18.8448301Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8448440Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8448608Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8448700Z         of the first process exiting.
2025-04-11T04:23:18.8448768Z     
2025-04-11T04:23:18.8448914Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8449097Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8449168Z     
2025-04-11T04:23:18.8449240Z         Args:
2025-04-11T04:23:18.8449376Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8449453Z         """
2025-04-11T04:23:18.8449588Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8449683Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8449760Z             return True
2025-04-11T04:23:18.8449831Z     
2025-04-11T04:23:18.8449960Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8450076Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8450165Z             self.sentinels.keys(),
2025-04-11T04:23:18.8450249Z             timeout=timeout,
2025-04-11T04:23:18.8450321Z         )
2025-04-11T04:23:18.8450389Z     
2025-04-11T04:23:18.8450469Z         error_index = None
2025-04-11T04:23:18.8450559Z         for sentinel in ready:
2025-04-11T04:23:18.8450661Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8450760Z             process = self.processes[index]
2025-04-11T04:23:18.8450844Z             process.join()
2025-04-11T04:23:18.8450936Z             if process.exitcode != 0:
2025-04-11T04:23:18.8451024Z                 error_index = index
2025-04-11T04:23:18.8451099Z                 break
2025-04-11T04:23:18.8451170Z     
2025-04-11T04:23:18.8451260Z         # Return if there was no error.
2025-04-11T04:23:18.8451345Z         if error_index is None:
2025-04-11T04:23:18.8451476Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8451623Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8451696Z     
2025-04-11T04:23:18.8451834Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8451932Z         for process in self.processes:
2025-04-11T04:23:18.8452018Z             if process.is_alive():
2025-04-11T04:23:18.8452110Z                 process.terminate()
2025-04-11T04:23:18.8452193Z             process.join()
2025-04-11T04:23:18.8452262Z     
2025-04-11T04:23:18.8452402Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8452518Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8452630Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8452749Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8452830Z             if exitcode < 0:
2025-04-11T04:23:18.8452940Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8453047Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8453199Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8453345Z                     error_index=error_index,
2025-04-11T04:23:18.8453447Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8453533Z                     exit_code=exitcode,
2025-04-11T04:23:18.8453617Z                     signal_name=name,
2025-04-11T04:23:18.8453692Z                 )
2025-04-11T04:23:18.8453765Z             else:
2025-04-11T04:23:18.8453867Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8454082Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8454175Z                     error_index=error_index,
2025-04-11T04:23:18.8454275Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8454361Z                     exit_code=exitcode,
2025-04-11T04:23:18.8454434Z                 )
2025-04-11T04:23:18.8454504Z     
2025-04-11T04:23:18.8454632Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8454806Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8454892Z         msg += original_trace
2025-04-11T04:23:18.8455069Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8455279Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8455355Z E       
2025-04-11T04:23:18.8455481Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8455577Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8455882Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8455964Z E           fn(i, *args)
2025-04-11T04:23:18.8456221Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
2025-04-11T04:23:18.8456317Z E           check_p2p_communication()
2025-04-11T04:23:18.8456615Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
2025-04-11T04:23:18.8456782Z E           tensor = torch.ones(1, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8456889Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8457179Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8457313Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8457477Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8457482Z 
2025-04-11T04:23:18.8457787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8457943Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8458173Z [04/11/25 04:18:33] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8458303Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8458413Z                              :75 launch                                         
2025-04-11T04:23:18.8458553Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8458681Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8458881Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8459030Z _________________________ test_pipeline_stage_manager __________________________
2025-04-11T04:23:18.8459034Z 
2025-04-11T04:23:18.8459128Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8459734Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8459791Z 
2025-04-11T04:23:18.8459893Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8459973Z         try_count = 0
2025-04-11T04:23:18.8460073Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8460154Z             max_try, int
2025-04-11T04:23:18.8460303Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8460371Z     
2025-04-11T04:23:18.8460486Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8460610Z             try:
2025-04-11T04:23:18.8460697Z                 try_count += 1
2025-04-11T04:23:18.8460788Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8460867Z                 return ret
2025-04-11T04:23:18.8460963Z             except exception_type as e:
2025-04-11T04:23:18.8461062Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8461255Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8461372Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8461514Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8461723Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8461801Z                     continue
2025-04-11T04:23:18.8461881Z                 else:
2025-04-11T04:23:18.8462103Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8462187Z >                   raise e
2025-04-11T04:23:18.8462191Z 
2025-04-11T04:23:18.8462284Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8462397Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8462529Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8462616Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8462801Z tests/test_pipeline/test_stage_manager.py:74: in test_pipeline_stage_manager
2025-04-11T04:23:18.8462884Z     spawn(run_dist, 4)
2025-04-11T04:23:18.8462985Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8463083Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8463343Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8463522Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8463810Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8463900Z     while not context.join():
2025-04-11T04:23:18.8464006Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8464010Z 
2025-04-11T04:23:18.8464209Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f09de0>
2025-04-11T04:23:18.8464341Z timeout = None
2025-04-11T04:23:18.8464345Z 
2025-04-11T04:23:18.8464436Z     def join(self, timeout=None):
2025-04-11T04:23:18.8464565Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8464633Z     
2025-04-11T04:23:18.8464780Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8464924Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8465087Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8465176Z         of the first process exiting.
2025-04-11T04:23:18.8465246Z     
2025-04-11T04:23:18.8465393Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8465528Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8465600Z     
2025-04-11T04:23:18.8465671Z         Args:
2025-04-11T04:23:18.8465814Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8465885Z         """
2025-04-11T04:23:18.8466022Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8466171Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8466248Z             return True
2025-04-11T04:23:18.8466322Z     
2025-04-11T04:23:18.8466452Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8466567Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8466659Z             self.sentinels.keys(),
2025-04-11T04:23:18.8466743Z             timeout=timeout,
2025-04-11T04:23:18.8466869Z         )
2025-04-11T04:23:18.8466936Z     
2025-04-11T04:23:18.8467023Z         error_index = None
2025-04-11T04:23:18.8467108Z         for sentinel in ready:
2025-04-11T04:23:18.8467211Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8467311Z             process = self.processes[index]
2025-04-11T04:23:18.8467394Z             process.join()
2025-04-11T04:23:18.8467487Z             if process.exitcode != 0:
2025-04-11T04:23:18.8467573Z                 error_index = index
2025-04-11T04:23:18.8467648Z                 break
2025-04-11T04:23:18.8467721Z     
2025-04-11T04:23:18.8467809Z         # Return if there was no error.
2025-04-11T04:23:18.8467895Z         if error_index is None:
2025-04-11T04:23:18.8468076Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8468171Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8468243Z     
2025-04-11T04:23:18.8468380Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8468528Z         for process in self.processes:
2025-04-11T04:23:18.8468615Z             if process.is_alive():
2025-04-11T04:23:18.8468709Z                 process.terminate()
2025-04-11T04:23:18.8468790Z             process.join()
2025-04-11T04:23:18.8468857Z     
2025-04-11T04:23:18.8468999Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8469113Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8469222Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8469344Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8469427Z             if exitcode < 0:
2025-04-11T04:23:18.8469538Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8469641Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8469794Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8469889Z                     error_index=error_index,
2025-04-11T04:23:18.8469992Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8470080Z                     exit_code=exitcode,
2025-04-11T04:23:18.8470163Z                     signal_name=name,
2025-04-11T04:23:18.8470237Z                 )
2025-04-11T04:23:18.8470309Z             else:
2025-04-11T04:23:18.8470471Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8470632Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8470725Z                     error_index=error_index,
2025-04-11T04:23:18.8470824Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8470908Z                     exit_code=exitcode,
2025-04-11T04:23:18.8470983Z                 )
2025-04-11T04:23:18.8471052Z     
2025-04-11T04:23:18.8471184Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8471353Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8471439Z         msg += original_trace
2025-04-11T04:23:18.8471616Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8471772Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8471848Z E       
2025-04-11T04:23:18.8471971Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8472070Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8472367Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8472506Z E           fn(i, *args)
2025-04-11T04:23:18.8472747Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
2025-04-11T04:23:18.8472838Z E           check_stage_manager()
2025-04-11T04:23:18.8473104Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
2025-04-11T04:23:18.8473255Z E           dist.barrier(group=group)
2025-04-11T04:23:18.8473558Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.8473652Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.8473970Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
2025-04-11T04:23:18.8474076Z E           work = group.barrier(opts=opts)
2025-04-11T04:23:18.8474182Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8474486Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8474675Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8474838Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8474843Z 
2025-04-11T04:23:18.8475143Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8475295Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8475446Z [04/11/25 04:18:39] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8475573Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8475682Z                              :75 launch                                         
2025-04-11T04:23:18.8475818Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8475945Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8476139Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8476283Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8476579Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26717 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8476864Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26717 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8477001Z _______________________________ test_pp[2-12-4] ________________________________
2025-04-11T04:23:18.8477053Z 
2025-04-11T04:23:18.8477127Z args = ()
2025-04-11T04:23:18.8477292Z kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 2}
2025-04-11T04:23:18.8477369Z try_count = 1
2025-04-11T04:23:18.8477989Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8477997Z 
2025-04-11T04:23:18.8478097Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8478181Z         try_count = 0
2025-04-11T04:23:18.8478279Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8478358Z             max_try, int
2025-04-11T04:23:18.8478508Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8478576Z     
2025-04-11T04:23:18.8478691Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8478765Z             try:
2025-04-11T04:23:18.8478850Z                 try_count += 1
2025-04-11T04:23:18.8478942Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8479089Z                 return ret
2025-04-11T04:23:18.8479189Z             except exception_type as e:
2025-04-11T04:23:18.8479290Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8479486Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8479604Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8479749Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8479956Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8480035Z                     continue
2025-04-11T04:23:18.8480114Z                 else:
2025-04-11T04:23:18.8480337Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8480426Z >                   raise e
2025-04-11T04:23:18.8480430Z 
2025-04-11T04:23:18.8480523Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8480633Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8480768Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8480913Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8481086Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T04:23:18.8481159Z     spawn(
2025-04-11T04:23:18.8481261Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8481360Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8481618Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8481796Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8482080Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8482172Z     while not context.join():
2025-04-11T04:23:18.8482280Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8482284Z 
2025-04-11T04:23:18.8482482Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f09cf0>
2025-04-11T04:23:18.8482560Z timeout = None
2025-04-11T04:23:18.8482564Z 
2025-04-11T04:23:18.8482654Z     def join(self, timeout=None):
2025-04-11T04:23:18.8482779Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8482849Z     
2025-04-11T04:23:18.8482995Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8483136Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8483299Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8483390Z         of the first process exiting.
2025-04-11T04:23:18.8483514Z     
2025-04-11T04:23:18.8483663Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8483799Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8483871Z     
2025-04-11T04:23:18.8483944Z         Args:
2025-04-11T04:23:18.8484082Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8484156Z         """
2025-04-11T04:23:18.8484290Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8484384Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8484463Z             return True
2025-04-11T04:23:18.8484534Z     
2025-04-11T04:23:18.8484663Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8484775Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8484868Z             self.sentinels.keys(),
2025-04-11T04:23:18.8484950Z             timeout=timeout,
2025-04-11T04:23:18.8485024Z         )
2025-04-11T04:23:18.8485092Z     
2025-04-11T04:23:18.8485172Z         error_index = None
2025-04-11T04:23:18.8485259Z         for sentinel in ready:
2025-04-11T04:23:18.8485410Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8485511Z             process = self.processes[index]
2025-04-11T04:23:18.8485597Z             process.join()
2025-04-11T04:23:18.8485692Z             if process.exitcode != 0:
2025-04-11T04:23:18.8485778Z                 error_index = index
2025-04-11T04:23:18.8485853Z                 break
2025-04-11T04:23:18.8485927Z     
2025-04-11T04:23:18.8486015Z         # Return if there was no error.
2025-04-11T04:23:18.8486151Z         if error_index is None:
2025-04-11T04:23:18.8486282Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8486375Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8486447Z     
2025-04-11T04:23:18.8486584Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8486682Z         for process in self.processes:
2025-04-11T04:23:18.8486767Z             if process.is_alive():
2025-04-11T04:23:18.8486859Z                 process.terminate()
2025-04-11T04:23:18.8486943Z             process.join()
2025-04-11T04:23:18.8487011Z     
2025-04-11T04:23:18.8487155Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8487319Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8487428Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8487546Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8487630Z             if exitcode < 0:
2025-04-11T04:23:18.8487737Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8487842Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8487993Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8488086Z                     error_index=error_index,
2025-04-11T04:23:18.8488189Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8488275Z                     exit_code=exitcode,
2025-04-11T04:23:18.8488361Z                     signal_name=name,
2025-04-11T04:23:18.8488437Z                 )
2025-04-11T04:23:18.8488509Z             else:
2025-04-11T04:23:18.8488615Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8488778Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8488870Z                     error_index=error_index,
2025-04-11T04:23:18.8488971Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8489057Z                     exit_code=exitcode,
2025-04-11T04:23:18.8489132Z                 )
2025-04-11T04:23:18.8489201Z     
2025-04-11T04:23:18.8489330Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8489498Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8489640Z         msg += original_trace
2025-04-11T04:23:18.8489815Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8489973Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8490047Z E       
2025-04-11T04:23:18.8490171Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8490271Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8490571Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8490651Z E           fn(i, *args)
2025-04-11T04:23:18.8490926Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:18.8491022Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8491292Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8491411Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8491676Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8491815Z E           module._apply(fn)
2025-04-11T04:23:18.8492083Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8492173Z E           module._apply(fn)
2025-04-11T04:23:18.8492435Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8492581Z E           param_applied = fn(param)
2025-04-11T04:23:18.8492850Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8492967Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8493072Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8493357Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8493494Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8493653Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8493704Z 
2025-04-11T04:23:18.8494012Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8494161Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8494317Z [04/11/25 04:18:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8494444Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8494553Z                              :75 launch                                         
2025-04-11T04:23:18.8494686Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8494809Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8495010Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8495140Z _______________________________ test_pp[2-12-12] _______________________________
2025-04-11T04:23:18.8495146Z 
2025-04-11T04:23:18.8495223Z args = ()
2025-04-11T04:23:18.8495377Z kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 2}
2025-04-11T04:23:18.8495455Z try_count = 1
2025-04-11T04:23:18.8496046Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8496053Z 
2025-04-11T04:23:18.8496157Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8496286Z         try_count = 0
2025-04-11T04:23:18.8496383Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8496467Z             max_try, int
2025-04-11T04:23:18.8496614Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8496685Z     
2025-04-11T04:23:18.8496797Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8496871Z             try:
2025-04-11T04:23:18.8496957Z                 try_count += 1
2025-04-11T04:23:18.8497046Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8497127Z                 return ret
2025-04-11T04:23:18.8497219Z             except exception_type as e:
2025-04-11T04:23:18.8497320Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8497506Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8497619Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8497768Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8497924Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8498055Z                     continue
2025-04-11T04:23:18.8498131Z                 else:
2025-04-11T04:23:18.8498351Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8498433Z >                   raise e
2025-04-11T04:23:18.8498438Z 
2025-04-11T04:23:18.8498529Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8498642Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8498818Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8498907Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8499076Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T04:23:18.8499153Z     spawn(
2025-04-11T04:23:18.8499252Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8499351Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8499607Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8499783Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8500066Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8500203Z     while not context.join():
2025-04-11T04:23:18.8500308Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8500319Z 
2025-04-11T04:23:18.8500512Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5ad28f0>
2025-04-11T04:23:18.8500588Z timeout = None
2025-04-11T04:23:18.8500593Z 
2025-04-11T04:23:18.8500684Z     def join(self, timeout=None):
2025-04-11T04:23:18.8500805Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8500879Z     
2025-04-11T04:23:18.8501022Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8501164Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8501327Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8501418Z         of the first process exiting.
2025-04-11T04:23:18.8501492Z     
2025-04-11T04:23:18.8501636Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8501771Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8501841Z     
2025-04-11T04:23:18.8501914Z         Args:
2025-04-11T04:23:18.8502053Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8502123Z         """
2025-04-11T04:23:18.8502262Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8502352Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8502485Z             return True
2025-04-11T04:23:18.8502555Z     
2025-04-11T04:23:18.8502685Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8502807Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8502896Z             self.sentinels.keys(),
2025-04-11T04:23:18.8502981Z             timeout=timeout,
2025-04-11T04:23:18.8503054Z         )
2025-04-11T04:23:18.8503122Z     
2025-04-11T04:23:18.8503206Z         error_index = None
2025-04-11T04:23:18.8503290Z         for sentinel in ready:
2025-04-11T04:23:18.8503395Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8503492Z             process = self.processes[index]
2025-04-11T04:23:18.8503577Z             process.join()
2025-04-11T04:23:18.8503671Z             if process.exitcode != 0:
2025-04-11T04:23:18.8503756Z                 error_index = index
2025-04-11T04:23:18.8503832Z                 break
2025-04-11T04:23:18.8503899Z     
2025-04-11T04:23:18.8503989Z         # Return if there was no error.
2025-04-11T04:23:18.8504073Z         if error_index is None:
2025-04-11T04:23:18.8504207Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8504359Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8504428Z     
2025-04-11T04:23:18.8504564Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8504659Z         for process in self.processes:
2025-04-11T04:23:18.8504747Z             if process.is_alive():
2025-04-11T04:23:18.8504839Z                 process.terminate()
2025-04-11T04:23:18.8504920Z             process.join()
2025-04-11T04:23:18.8504991Z     
2025-04-11T04:23:18.8505193Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8505306Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8505413Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8505531Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8505618Z             if exitcode < 0:
2025-04-11T04:23:18.8505722Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8505826Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8505977Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8506070Z                     error_index=error_index,
2025-04-11T04:23:18.8506222Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8506308Z                     exit_code=exitcode,
2025-04-11T04:23:18.8506395Z                     signal_name=name,
2025-04-11T04:23:18.8506466Z                 )
2025-04-11T04:23:18.8506542Z             else:
2025-04-11T04:23:18.8506642Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8506803Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8506897Z                     error_index=error_index,
2025-04-11T04:23:18.8506993Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8507082Z                     exit_code=exitcode,
2025-04-11T04:23:18.8507152Z                 )
2025-04-11T04:23:18.8507218Z     
2025-04-11T04:23:18.8507353Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8507522Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8507612Z         msg += original_trace
2025-04-11T04:23:18.8507783Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8507947Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8508018Z E       
2025-04-11T04:23:18.8508146Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8508246Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8508584Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8508669Z E           fn(i, *args)
2025-04-11T04:23:18.8509000Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:18.8509100Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8509370Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8509487Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8509763Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8509848Z E           module._apply(fn)
2025-04-11T04:23:18.8510123Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8510207Z E           module._apply(fn)
2025-04-11T04:23:18.8510475Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8510571Z E           param_applied = fn(param)
2025-04-11T04:23:18.8510847Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8511020Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8511132Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8511429Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8511567Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8511738Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8511883Z 
2025-04-11T04:23:18.8512190Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8512347Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8512503Z [04/11/25 04:18:50] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8512630Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8512743Z                              :75 launch                                         
2025-04-11T04:23:18.8512882Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8513064Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8513259Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8513395Z _______________________________ test_pp[4-12-4] ________________________________
2025-04-11T04:23:18.8513400Z 
2025-04-11T04:23:18.8513474Z args = ()
2025-04-11T04:23:18.8513626Z kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 4}
2025-04-11T04:23:18.8513705Z try_count = 1
2025-04-11T04:23:18.8514311Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8514321Z 
2025-04-11T04:23:18.8514426Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8514506Z         try_count = 0
2025-04-11T04:23:18.8514611Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8514692Z             max_try, int
2025-04-11T04:23:18.8514838Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8514908Z     
2025-04-11T04:23:18.8515022Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8515099Z             try:
2025-04-11T04:23:18.8515180Z                 try_count += 1
2025-04-11T04:23:18.8515274Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8515354Z                 return ret
2025-04-11T04:23:18.8515447Z             except exception_type as e:
2025-04-11T04:23:18.8515604Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8515788Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8515909Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8516054Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8516210Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8516290Z                     continue
2025-04-11T04:23:18.8516364Z                 else:
2025-04-11T04:23:18.8516591Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8516670Z >                   raise e
2025-04-11T04:23:18.8516674Z 
2025-04-11T04:23:18.8516769Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8516878Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8517015Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8517099Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8517320Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T04:23:18.8517397Z     spawn(
2025-04-11T04:23:18.8517499Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8517602Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8517858Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8518034Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8518369Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8518456Z     while not context.join():
2025-04-11T04:23:18.8518569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8518574Z 
2025-04-11T04:23:18.8518773Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3d810>
2025-04-11T04:23:18.8518851Z timeout = None
2025-04-11T04:23:18.8518858Z 
2025-04-11T04:23:18.8518945Z     def join(self, timeout=None):
2025-04-11T04:23:18.8519072Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8519140Z     
2025-04-11T04:23:18.8519334Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8519483Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8519645Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8519742Z         of the first process exiting.
2025-04-11T04:23:18.8519812Z     
2025-04-11T04:23:18.8519963Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8520099Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8520167Z     
2025-04-11T04:23:18.8520244Z         Args:
2025-04-11T04:23:18.8520383Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8520456Z         """
2025-04-11T04:23:18.8520597Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8520686Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8520766Z             return True
2025-04-11T04:23:18.8520837Z     
2025-04-11T04:23:18.8520971Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8521089Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8521183Z             self.sentinels.keys(),
2025-04-11T04:23:18.8521269Z             timeout=timeout,
2025-04-11T04:23:18.8521340Z         )
2025-04-11T04:23:18.8521412Z     
2025-04-11T04:23:18.8521494Z         error_index = None
2025-04-11T04:23:18.8521582Z         for sentinel in ready:
2025-04-11T04:23:18.8521687Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8521785Z             process = self.processes[index]
2025-04-11T04:23:18.8521923Z             process.join()
2025-04-11T04:23:18.8522016Z             if process.exitcode != 0:
2025-04-11T04:23:18.8522109Z                 error_index = index
2025-04-11T04:23:18.8522184Z                 break
2025-04-11T04:23:18.8522252Z     
2025-04-11T04:23:18.8522347Z         # Return if there was no error.
2025-04-11T04:23:18.8522434Z         if error_index is None:
2025-04-11T04:23:18.8522569Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8522663Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8522735Z     
2025-04-11T04:23:18.8522876Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8522968Z         for process in self.processes:
2025-04-11T04:23:18.8523059Z             if process.is_alive():
2025-04-11T04:23:18.8523148Z                 process.terminate()
2025-04-11T04:23:18.8523233Z             process.join()
2025-04-11T04:23:18.8523302Z     
2025-04-11T04:23:18.8523444Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8523559Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8523720Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8523843Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8523926Z             if exitcode < 0:
2025-04-11T04:23:18.8524032Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8524136Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8524283Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8524428Z                     error_index=error_index,
2025-04-11T04:23:18.8524528Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8524620Z                     exit_code=exitcode,
2025-04-11T04:23:18.8524703Z                     signal_name=name,
2025-04-11T04:23:18.8524775Z                 )
2025-04-11T04:23:18.8524853Z             else:
2025-04-11T04:23:18.8524955Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8525120Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8525215Z                     error_index=error_index,
2025-04-11T04:23:18.8525318Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8525455Z                     exit_code=exitcode,
2025-04-11T04:23:18.8525527Z                 )
2025-04-11T04:23:18.8525600Z     
2025-04-11T04:23:18.8525728Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8525898Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8525983Z         msg += original_trace
2025-04-11T04:23:18.8526158Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8526320Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8526392Z E       
2025-04-11T04:23:18.8526521Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8526616Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8526920Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8527000Z E           fn(i, *args)
2025-04-11T04:23:18.8527272Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:18.8527370Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8527641Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8527761Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8528029Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8528117Z E           module._apply(fn)
2025-04-11T04:23:18.8528431Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8528520Z E           module._apply(fn)
2025-04-11T04:23:18.8528788Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8528886Z E           param_applied = fn(param)
2025-04-11T04:23:18.8529167Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8529283Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8529398Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8529685Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8529827Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8529989Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8529996Z 
2025-04-11T04:23:18.8530298Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8530514Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8530668Z [04/11/25 04:18:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8530798Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8530901Z                              :75 launch                                         
2025-04-11T04:23:18.8531076Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8531200Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8531398Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8531529Z _______________________________ test_pp[4-12-12] _______________________________
2025-04-11T04:23:18.8531533Z 
2025-04-11T04:23:18.8531607Z args = ()
2025-04-11T04:23:18.8531768Z kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 4}
2025-04-11T04:23:18.8531842Z try_count = 1
2025-04-11T04:23:18.8532448Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8532498Z 
2025-04-11T04:23:18.8532601Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8532683Z         try_count = 0
2025-04-11T04:23:18.8532783Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8532861Z             max_try, int
2025-04-11T04:23:18.8533009Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8533077Z     
2025-04-11T04:23:18.8533192Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8533266Z             try:
2025-04-11T04:23:18.8533351Z                 try_count += 1
2025-04-11T04:23:18.8533442Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8533520Z                 return ret
2025-04-11T04:23:18.8533613Z             except exception_type as e:
2025-04-11T04:23:18.8533711Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8533899Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8534013Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8534156Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8534313Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8534392Z                     continue
2025-04-11T04:23:18.8534470Z                 else:
2025-04-11T04:23:18.8534743Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8534824Z >                   raise e
2025-04-11T04:23:18.8534831Z 
2025-04-11T04:23:18.8534923Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8535031Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8535166Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8535251Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8535424Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T04:23:18.8535497Z     spawn(
2025-04-11T04:23:18.8535600Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8535698Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8535951Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8536131Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8536413Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8536554Z     while not context.join():
2025-04-11T04:23:18.8536663Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8536667Z 
2025-04-11T04:23:18.8536872Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b42475e0>
2025-04-11T04:23:18.8536948Z timeout = None
2025-04-11T04:23:18.8536953Z 
2025-04-11T04:23:18.8537045Z     def join(self, timeout=None):
2025-04-11T04:23:18.8537168Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8537284Z     
2025-04-11T04:23:18.8537433Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8537573Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8537738Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8537831Z         of the first process exiting.
2025-04-11T04:23:18.8537898Z     
2025-04-11T04:23:18.8538046Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8538184Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8538255Z     
2025-04-11T04:23:18.8538373Z         Args:
2025-04-11T04:23:18.8538517Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8538588Z         """
2025-04-11T04:23:18.8538723Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8538815Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8538894Z             return True
2025-04-11T04:23:18.8538966Z     
2025-04-11T04:23:18.8539095Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8539208Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8539298Z             self.sentinels.keys(),
2025-04-11T04:23:18.8539382Z             timeout=timeout,
2025-04-11T04:23:18.8539455Z         )
2025-04-11T04:23:18.8539522Z     
2025-04-11T04:23:18.8539601Z         error_index = None
2025-04-11T04:23:18.8539690Z         for sentinel in ready:
2025-04-11T04:23:18.8539791Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8539891Z             process = self.processes[index]
2025-04-11T04:23:18.8539975Z             process.join()
2025-04-11T04:23:18.8540065Z             if process.exitcode != 0:
2025-04-11T04:23:18.8540151Z                 error_index = index
2025-04-11T04:23:18.8540225Z                 break
2025-04-11T04:23:18.8540296Z     
2025-04-11T04:23:18.8540385Z         # Return if there was no error.
2025-04-11T04:23:18.8540471Z         if error_index is None:
2025-04-11T04:23:18.8540600Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8540692Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8540763Z     
2025-04-11T04:23:18.8540969Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8541064Z         for process in self.processes:
2025-04-11T04:23:18.8541150Z             if process.is_alive():
2025-04-11T04:23:18.8541245Z                 process.terminate()
2025-04-11T04:23:18.8541326Z             process.join()
2025-04-11T04:23:18.8541393Z     
2025-04-11T04:23:18.8541536Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8541649Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8541757Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8541875Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8541959Z             if exitcode < 0:
2025-04-11T04:23:18.8542066Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8542169Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8542319Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8542414Z                     error_index=error_index,
2025-04-11T04:23:18.8542514Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8542652Z                     exit_code=exitcode,
2025-04-11T04:23:18.8542735Z                     signal_name=name,
2025-04-11T04:23:18.8542811Z                 )
2025-04-11T04:23:18.8542885Z             else:
2025-04-11T04:23:18.8542988Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8543151Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8543242Z                     error_index=error_index,
2025-04-11T04:23:18.8543393Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8543477Z                     exit_code=exitcode,
2025-04-11T04:23:18.8543551Z                 )
2025-04-11T04:23:18.8543619Z     
2025-04-11T04:23:18.8543751Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8543919Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8544005Z         msg += original_trace
2025-04-11T04:23:18.8544178Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8544337Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8544412Z E       
2025-04-11T04:23:18.8544582Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8544679Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8544977Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8545058Z E           fn(i, *args)
2025-04-11T04:23:18.8545329Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:18.8545422Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8545692Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8545807Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8546079Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8546164Z E           module._apply(fn)
2025-04-11T04:23:18.8546431Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8546517Z E           module._apply(fn)
2025-04-11T04:23:18.8546779Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8546878Z E           param_applied = fn(param)
2025-04-11T04:23:18.8547150Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8547265Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8547419Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8547707Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8547847Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8548007Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8548014Z 
2025-04-11T04:23:18.8548318Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8548505Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8548669Z [04/11/25 04:19:00] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8548798Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8548909Z                              :75 launch                                         
2025-04-11T04:23:18.8549049Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8549173Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8549431Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8549578Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8549876Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:47696 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8550010Z _______________________________ test_pp[2-12-4] ________________________________
2025-04-11T04:23:18.8550068Z 
2025-04-11T04:23:18.8550227Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 2}
2025-04-11T04:23:18.8550302Z try_count = 1
2025-04-11T04:23:18.8550901Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8550915Z 
2025-04-11T04:23:18.8551014Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8551091Z         try_count = 0
2025-04-11T04:23:18.8551192Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8551325Z             max_try, int
2025-04-11T04:23:18.8551472Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8551541Z     
2025-04-11T04:23:18.8551653Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8551729Z             try:
2025-04-11T04:23:18.8551811Z                 try_count += 1
2025-04-11T04:23:18.8551903Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8551982Z                 return ret
2025-04-11T04:23:18.8552073Z             except exception_type as e:
2025-04-11T04:23:18.8552172Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8552357Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8552476Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8552618Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8552778Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8552856Z                     continue
2025-04-11T04:23:18.8552933Z                 else:
2025-04-11T04:23:18.8553154Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8553235Z >                   raise e
2025-04-11T04:23:18.8553243Z 
2025-04-11T04:23:18.8553333Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8553440Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8553573Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8553731Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8553897Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T04:23:18.8553972Z     spawn(
2025-04-11T04:23:18.8554070Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8554171Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8554425Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8554604Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8554884Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8554974Z     while not context.join():
2025-04-11T04:23:18.8555080Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8555085Z 
2025-04-11T04:23:18.8555276Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3e200>
2025-04-11T04:23:18.8555357Z timeout = None
2025-04-11T04:23:18.8555361Z 
2025-04-11T04:23:18.8555448Z     def join(self, timeout=None):
2025-04-11T04:23:18.8555624Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8555693Z     
2025-04-11T04:23:18.8555840Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8555985Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8556144Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8556238Z         of the first process exiting.
2025-04-11T04:23:18.8556353Z     
2025-04-11T04:23:18.8556503Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8556639Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8556711Z     
2025-04-11T04:23:18.8556783Z         Args:
2025-04-11T04:23:18.8556920Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8556996Z         """
2025-04-11T04:23:18.8557134Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8557232Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8557309Z             return True
2025-04-11T04:23:18.8557376Z     
2025-04-11T04:23:18.8557557Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8557672Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8557763Z             self.sentinels.keys(),
2025-04-11T04:23:18.8557845Z             timeout=timeout,
2025-04-11T04:23:18.8557921Z         )
2025-04-11T04:23:18.8557988Z     
2025-04-11T04:23:18.8558067Z         error_index = None
2025-04-11T04:23:18.8558155Z         for sentinel in ready:
2025-04-11T04:23:18.8558257Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8558355Z             process = self.processes[index]
2025-04-11T04:23:18.8558438Z             process.join()
2025-04-11T04:23:18.8558529Z             if process.exitcode != 0:
2025-04-11T04:23:18.8558618Z                 error_index = index
2025-04-11T04:23:18.8558694Z                 break
2025-04-11T04:23:18.8558764Z     
2025-04-11T04:23:18.8558853Z         # Return if there was no error.
2025-04-11T04:23:18.8558937Z         if error_index is None:
2025-04-11T04:23:18.8559071Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8559165Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8559235Z     
2025-04-11T04:23:18.8559370Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8559467Z         for process in self.processes:
2025-04-11T04:23:18.8559551Z             if process.is_alive():
2025-04-11T04:23:18.8559640Z                 process.terminate()
2025-04-11T04:23:18.8559725Z             process.join()
2025-04-11T04:23:18.8559793Z     
2025-04-11T04:23:18.8559934Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8560103Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8560208Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8560332Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8560413Z             if exitcode < 0:
2025-04-11T04:23:18.8560524Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8560627Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8560778Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8560872Z                     error_index=error_index,
2025-04-11T04:23:18.8560972Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8561060Z                     exit_code=exitcode,
2025-04-11T04:23:18.8561144Z                     signal_name=name,
2025-04-11T04:23:18.8561218Z                 )
2025-04-11T04:23:18.8561289Z             else:
2025-04-11T04:23:18.8561387Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8561555Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8561700Z                     error_index=error_index,
2025-04-11T04:23:18.8561801Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8561884Z                     exit_code=exitcode,
2025-04-11T04:23:18.8561960Z                 )
2025-04-11T04:23:18.8562029Z     
2025-04-11T04:23:18.8562159Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8562327Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8562465Z         msg += original_trace
2025-04-11T04:23:18.8562639Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8562797Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8562868Z E       
2025-04-11T04:23:18.8562995Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8563093Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8563396Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8563476Z E           fn(i, *args)
2025-04-11T04:23:18.8563752Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:18.8563908Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:18.8564172Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:18.8564270Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8564533Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8564653Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8564918Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8565008Z E           module._apply(fn)
2025-04-11T04:23:18.8565271Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8565355Z E           module._apply(fn)
2025-04-11T04:23:18.8565617Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8565708Z E           param_applied = fn(param)
2025-04-11T04:23:18.8565980Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8566093Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8566208Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8566487Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8566679Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8566840Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8566846Z 
2025-04-11T04:23:18.8567150Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8567305Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8567459Z [04/11/25 04:19:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8567591Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8567699Z                              :75 launch                                         
2025-04-11T04:23:18.8567838Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8567959Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8568159Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8568352Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8568647Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:42298 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8568784Z _______________________________ test_pp[2-12-6] ________________________________
2025-04-11T04:23:18.8568788Z 
2025-04-11T04:23:18.8568941Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 2}
2025-04-11T04:23:18.8569071Z try_count = 1
2025-04-11T04:23:18.8569674Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8569680Z 
2025-04-11T04:23:18.8569785Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8569860Z         try_count = 0
2025-04-11T04:23:18.8569960Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8570041Z             max_try, int
2025-04-11T04:23:18.8570183Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8570303Z     
2025-04-11T04:23:18.8570414Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8570490Z             try:
2025-04-11T04:23:18.8570572Z                 try_count += 1
2025-04-11T04:23:18.8570662Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8570746Z                 return ret
2025-04-11T04:23:18.8570837Z             except exception_type as e:
2025-04-11T04:23:18.8570938Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8571122Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8571241Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8571386Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8571543Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8571626Z                     continue
2025-04-11T04:23:18.8571701Z                 else:
2025-04-11T04:23:18.8571927Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8572005Z >                   raise e
2025-04-11T04:23:18.8572009Z 
2025-04-11T04:23:18.8572103Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8572214Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8572343Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8572433Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8572593Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T04:23:18.8572718Z     spawn(
2025-04-11T04:23:18.8572818Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8572918Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8573178Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8573353Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8573644Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8573736Z     while not context.join():
2025-04-11T04:23:18.8573846Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8573852Z 
2025-04-11T04:23:18.8574050Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b4245e40>
2025-04-11T04:23:18.8574128Z timeout = None
2025-04-11T04:23:18.8574133Z 
2025-04-11T04:23:18.8574218Z     def join(self, timeout=None):
2025-04-11T04:23:18.8574343Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8574417Z     
2025-04-11T04:23:18.8574562Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8574759Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8574922Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8575016Z         of the first process exiting.
2025-04-11T04:23:18.8575086Z     
2025-04-11T04:23:18.8575231Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8575368Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8575488Z     
2025-04-11T04:23:18.8575563Z         Args:
2025-04-11T04:23:18.8575700Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8575774Z         """
2025-04-11T04:23:18.8575910Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8576002Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8576083Z             return True
2025-04-11T04:23:18.8576152Z     
2025-04-11T04:23:18.8576283Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8576400Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8576489Z             self.sentinels.keys(),
2025-04-11T04:23:18.8576638Z             timeout=timeout,
2025-04-11T04:23:18.8576708Z         )
2025-04-11T04:23:18.8576778Z     
2025-04-11T04:23:18.8576858Z         error_index = None
2025-04-11T04:23:18.8576942Z         for sentinel in ready:
2025-04-11T04:23:18.8577052Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8577149Z             process = self.processes[index]
2025-04-11T04:23:18.8577234Z             process.join()
2025-04-11T04:23:18.8577325Z             if process.exitcode != 0:
2025-04-11T04:23:18.8577412Z                 error_index = index
2025-04-11T04:23:18.8577486Z                 break
2025-04-11T04:23:18.8577555Z     
2025-04-11T04:23:18.8577647Z         # Return if there was no error.
2025-04-11T04:23:18.8577730Z         if error_index is None:
2025-04-11T04:23:18.8577866Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8577959Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8578028Z     
2025-04-11T04:23:18.8578168Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8578261Z         for process in self.processes:
2025-04-11T04:23:18.8578352Z             if process.is_alive():
2025-04-11T04:23:18.8578439Z                 process.terminate()
2025-04-11T04:23:18.8578523Z             process.join()
2025-04-11T04:23:18.8578594Z     
2025-04-11T04:23:18.8578734Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8578854Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8578957Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8579130Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8579212Z             if exitcode < 0:
2025-04-11T04:23:18.8579314Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8579423Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8579569Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8579667Z                     error_index=error_index,
2025-04-11T04:23:18.8579762Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8579850Z                     exit_code=exitcode,
2025-04-11T04:23:18.8579935Z                     signal_name=name,
2025-04-11T04:23:18.8580007Z                 )
2025-04-11T04:23:18.8580083Z             else:
2025-04-11T04:23:18.8580180Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8580341Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8580431Z                     error_index=error_index,
2025-04-11T04:23:18.8580530Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8580618Z                     exit_code=exitcode,
2025-04-11T04:23:18.8580737Z                 )
2025-04-11T04:23:18.8580808Z     
2025-04-11T04:23:18.8580940Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8581111Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8581197Z         msg += original_trace
2025-04-11T04:23:18.8581367Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8581580Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8581653Z E       
2025-04-11T04:23:18.8581777Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8581872Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8582165Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8582248Z E           fn(i, *args)
2025-04-11T04:23:18.8582517Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:18.8582630Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:18.8582896Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:18.8583039Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8583306Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8583426Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8583693Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8583776Z E           module._apply(fn)
2025-04-11T04:23:18.8584043Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8584126Z E           module._apply(fn)
2025-04-11T04:23:18.8584389Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8584480Z E           param_applied = fn(param)
2025-04-11T04:23:18.8584755Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8584866Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8584971Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8585261Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8585394Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8585557Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8585610Z 
2025-04-11T04:23:18.8585912Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8586066Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8586222Z [04/11/25 04:19:09] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8586354Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8586459Z                              :75 launch                                         
2025-04-11T04:23:18.8586597Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8586725Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8586919Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8587054Z _______________________________ test_pp[4-12-4] ________________________________
2025-04-11T04:23:18.8587060Z 
2025-04-11T04:23:18.8587215Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 4}
2025-04-11T04:23:18.8587347Z try_count = 1
2025-04-11T04:23:18.8587943Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8587950Z 
2025-04-11T04:23:18.8588051Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8588130Z         try_count = 0
2025-04-11T04:23:18.8588278Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8588359Z             max_try, int
2025-04-11T04:23:18.8588531Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8588603Z     
2025-04-11T04:23:18.8588713Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8588787Z             try:
2025-04-11T04:23:18.8588872Z                 try_count += 1
2025-04-11T04:23:18.8588959Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8589045Z                 return ret
2025-04-11T04:23:18.8589136Z             except exception_type as e:
2025-04-11T04:23:18.8589236Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8589476Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8589590Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8589740Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8589895Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8589979Z                     continue
2025-04-11T04:23:18.8590053Z                 else:
2025-04-11T04:23:18.8590275Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8590354Z >                   raise e
2025-04-11T04:23:18.8590358Z 
2025-04-11T04:23:18.8590450Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8590565Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8590692Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8590783Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8590947Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T04:23:18.8591023Z     spawn(
2025-04-11T04:23:18.8591122Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8591221Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8591480Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8591656Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8591941Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8592088Z     while not context.join():
2025-04-11T04:23:18.8592199Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8592204Z 
2025-04-11T04:23:18.8592397Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3f640>
2025-04-11T04:23:18.8592476Z timeout = None
2025-04-11T04:23:18.8592480Z 
2025-04-11T04:23:18.8592572Z     def join(self, timeout=None):
2025-04-11T04:23:18.8592695Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8592767Z     
2025-04-11T04:23:18.8592912Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8593056Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8593216Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8593305Z         of the first process exiting.
2025-04-11T04:23:18.8593380Z     
2025-04-11T04:23:18.8593524Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8593660Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8593783Z     
2025-04-11T04:23:18.8593854Z         Args:
2025-04-11T04:23:18.8593996Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8594069Z         """
2025-04-11T04:23:18.8594209Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8594299Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8594383Z             return True
2025-04-11T04:23:18.8594504Z     
2025-04-11T04:23:18.8594633Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8594753Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8594842Z             self.sentinels.keys(),
2025-04-11T04:23:18.8594927Z             timeout=timeout,
2025-04-11T04:23:18.8594999Z         )
2025-04-11T04:23:18.8595067Z     
2025-04-11T04:23:18.8595152Z         error_index = None
2025-04-11T04:23:18.8595234Z         for sentinel in ready:
2025-04-11T04:23:18.8595341Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8595437Z             process = self.processes[index]
2025-04-11T04:23:18.8595519Z             process.join()
2025-04-11T04:23:18.8595660Z             if process.exitcode != 0:
2025-04-11T04:23:18.8595744Z                 error_index = index
2025-04-11T04:23:18.8595822Z                 break
2025-04-11T04:23:18.8595892Z     
2025-04-11T04:23:18.8595983Z         # Return if there was no error.
2025-04-11T04:23:18.8596067Z         if error_index is None:
2025-04-11T04:23:18.8596199Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8596295Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8596363Z     
2025-04-11T04:23:18.8596504Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8596599Z         for process in self.processes:
2025-04-11T04:23:18.8596684Z             if process.is_alive():
2025-04-11T04:23:18.8596777Z                 process.terminate()
2025-04-11T04:23:18.8596860Z             process.join()
2025-04-11T04:23:18.8596932Z     
2025-04-11T04:23:18.8597069Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8597188Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8597291Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8597412Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8597498Z             if exitcode < 0:
2025-04-11T04:23:18.8597601Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8597708Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8597855Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8597946Z                     error_index=error_index,
2025-04-11T04:23:18.8598099Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8598186Z                     exit_code=exitcode,
2025-04-11T04:23:18.8598275Z                     signal_name=name,
2025-04-11T04:23:18.8598347Z                 )
2025-04-11T04:23:18.8598421Z             else:
2025-04-11T04:23:18.8598521Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8598685Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8598782Z                     error_index=error_index,
2025-04-11T04:23:18.8598877Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8598969Z                     exit_code=exitcode,
2025-04-11T04:23:18.8599042Z                 )
2025-04-11T04:23:18.8599111Z     
2025-04-11T04:23:18.8599247Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8599417Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8599508Z         msg += original_trace
2025-04-11T04:23:18.8599679Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8599899Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8599971Z E       
2025-04-11T04:23:18.8600094Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8600196Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8600489Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8600571Z E           fn(i, *args)
2025-04-11T04:23:18.8600909Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:18.8601018Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:18.8601286Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:18.8601382Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8601648Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8601763Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8602029Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8602178Z E           module._apply(fn)
2025-04-11T04:23:18.8602448Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8602532Z E           module._apply(fn)
2025-04-11T04:23:18.8602792Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8602888Z E           param_applied = fn(param)
2025-04-11T04:23:18.8603159Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8603278Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8603385Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8603673Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8603809Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8603972Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8603976Z 
2025-04-11T04:23:18.8604273Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8604424Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8604579Z [04/11/25 04:19:15] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8604703Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8604865Z                              :75 launch                                         
2025-04-11T04:23:18.8605006Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8605131Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8605329Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8605474Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8605769Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30189 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8605902Z _______________________________ test_pp[4-12-6] ________________________________
2025-04-11T04:23:18.8605906Z 
2025-04-11T04:23:18.8606063Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 4}
2025-04-11T04:23:18.8606143Z try_count = 1
2025-04-11T04:23:18.8606746Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8606802Z 
2025-04-11T04:23:18.8606907Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8606989Z         try_count = 0
2025-04-11T04:23:18.8607087Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8607168Z             max_try, int
2025-04-11T04:23:18.8607312Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8607414Z     
2025-04-11T04:23:18.8607530Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8607603Z             try:
2025-04-11T04:23:18.8607688Z                 try_count += 1
2025-04-11T04:23:18.8607778Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8607859Z                 return ret
2025-04-11T04:23:18.8607955Z             except exception_type as e:
2025-04-11T04:23:18.8608054Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8608247Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8608362Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8608561Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8608718Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8608798Z                     continue
2025-04-11T04:23:18.8608879Z                 else:
2025-04-11T04:23:18.8609099Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8609177Z >                   raise e
2025-04-11T04:23:18.8609182Z 
2025-04-11T04:23:18.8609272Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8609386Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8609514Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8609601Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8609768Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T04:23:18.8609844Z     spawn(
2025-04-11T04:23:18.8609946Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8610042Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8610295Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8610476Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8610758Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8610850Z     while not context.join():
2025-04-11T04:23:18.8610956Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8611008Z 
2025-04-11T04:23:18.8611207Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b86920e0>
2025-04-11T04:23:18.8611284Z timeout = None
2025-04-11T04:23:18.8611289Z 
2025-04-11T04:23:18.8611377Z     def join(self, timeout=None):
2025-04-11T04:23:18.8611502Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8611570Z     
2025-04-11T04:23:18.8611714Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8611855Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8612017Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8612107Z         of the first process exiting.
2025-04-11T04:23:18.8612178Z     
2025-04-11T04:23:18.8612319Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8612451Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8612525Z     
2025-04-11T04:23:18.8612595Z         Args:
2025-04-11T04:23:18.8612735Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8612950Z         """
2025-04-11T04:23:18.8613088Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8613182Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8613260Z             return True
2025-04-11T04:23:18.8613331Z     
2025-04-11T04:23:18.8613460Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8613580Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8613719Z             self.sentinels.keys(),
2025-04-11T04:23:18.8613800Z             timeout=timeout,
2025-04-11T04:23:18.8613876Z         )
2025-04-11T04:23:18.8613943Z     
2025-04-11T04:23:18.8614025Z         error_index = None
2025-04-11T04:23:18.8614108Z         for sentinel in ready:
2025-04-11T04:23:18.8614212Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8614311Z             process = self.processes[index]
2025-04-11T04:23:18.8614394Z             process.join()
2025-04-11T04:23:18.8614488Z             if process.exitcode != 0:
2025-04-11T04:23:18.8614573Z                 error_index = index
2025-04-11T04:23:18.8614647Z                 break
2025-04-11T04:23:18.8614762Z     
2025-04-11T04:23:18.8614851Z         # Return if there was no error.
2025-04-11T04:23:18.8614937Z         if error_index is None:
2025-04-11T04:23:18.8615065Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8615160Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8615230Z     
2025-04-11T04:23:18.8615364Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8615461Z         for process in self.processes:
2025-04-11T04:23:18.8615547Z             if process.is_alive():
2025-04-11T04:23:18.8615639Z                 process.terminate()
2025-04-11T04:23:18.8615721Z             process.join()
2025-04-11T04:23:18.8615788Z     
2025-04-11T04:23:18.8615929Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8616043Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8616151Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8616271Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8616356Z             if exitcode < 0:
2025-04-11T04:23:18.8616459Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8616559Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8616710Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8616801Z                     error_index=error_index,
2025-04-11T04:23:18.8616900Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8616986Z                     exit_code=exitcode,
2025-04-11T04:23:18.8617070Z                     signal_name=name,
2025-04-11T04:23:18.8617193Z                 )
2025-04-11T04:23:18.8617264Z             else:
2025-04-11T04:23:18.8617365Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8617530Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8617622Z                     error_index=error_index,
2025-04-11T04:23:18.8617721Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8617805Z                     exit_code=exitcode,
2025-04-11T04:23:18.8617881Z                 )
2025-04-11T04:23:18.8617950Z     
2025-04-11T04:23:18.8618083Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8618255Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8618340Z         msg += original_trace
2025-04-11T04:23:18.8618513Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8618667Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8618748Z E       
2025-04-11T04:23:18.8618872Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8619022Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8619318Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8619400Z E           fn(i, *args)
2025-04-11T04:23:18.8619672Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:18.8619777Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:18.8620095Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:18.8620190Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8620458Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8620575Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8620840Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8620929Z E           module._apply(fn)
2025-04-11T04:23:18.8621194Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8621332Z E           module._apply(fn)
2025-04-11T04:23:18.8621592Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8621689Z E           param_applied = fn(param)
2025-04-11T04:23:18.8621958Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8622074Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8622179Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8622464Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8622602Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8622761Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8622767Z 
2025-04-11T04:23:18.8623073Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8623223Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8623380Z [04/11/25 04:19:22] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8623506Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8623610Z                              :75 launch                                         
2025-04-11T04:23:18.8623799Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8623923Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8624124Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8624264Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8624558Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8624839Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8625115Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8625244Z ___________________________________ test_pp ____________________________________
2025-04-11T04:23:18.8625250Z 
2025-04-11T04:23:18.8625343Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8625934Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8626003Z 
2025-04-11T04:23:18.8626107Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8626184Z         try_count = 0
2025-04-11T04:23:18.8626283Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8626365Z             max_try, int
2025-04-11T04:23:18.8626543Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8626614Z     
2025-04-11T04:23:18.8626722Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8626795Z             try:
2025-04-11T04:23:18.8626879Z                 try_count += 1
2025-04-11T04:23:18.8626967Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8627050Z                 return ret
2025-04-11T04:23:18.8627139Z             except exception_type as e:
2025-04-11T04:23:18.8627240Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8627426Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8627592Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8627738Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8627893Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8627977Z                     continue
2025-04-11T04:23:18.8628051Z                 else:
2025-04-11T04:23:18.8628269Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8628351Z >                   raise e
2025-04-11T04:23:18.8628355Z 
2025-04-11T04:23:18.8628489Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8628604Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8628736Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8628825Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8629000Z tests/test_pipeline/test_schedule/test_zerobubble_pp.py:1077: in test_pp
2025-04-11T04:23:18.8629076Z     spawn(
2025-04-11T04:23:18.8629177Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8629274Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8629530Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8629709Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8629994Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8630082Z     while not context.join():
2025-04-11T04:23:18.8630250Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8630257Z 
2025-04-11T04:23:18.8630455Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b4246350>
2025-04-11T04:23:18.8630534Z timeout = None
2025-04-11T04:23:18.8630539Z 
2025-04-11T04:23:18.8630630Z     def join(self, timeout=None):
2025-04-11T04:23:18.8630756Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8630827Z     
2025-04-11T04:23:18.8630971Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8631114Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8631282Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8631370Z         of the first process exiting.
2025-04-11T04:23:18.8631442Z     
2025-04-11T04:23:18.8631586Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8631726Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8631795Z     
2025-04-11T04:23:18.8631866Z         Args:
2025-04-11T04:23:18.8632063Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8632135Z         """
2025-04-11T04:23:18.8632275Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8632368Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8632446Z             return True
2025-04-11T04:23:18.8632517Z     
2025-04-11T04:23:18.8632647Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8632820Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8632911Z             self.sentinels.keys(),
2025-04-11T04:23:18.8632994Z             timeout=timeout,
2025-04-11T04:23:18.8633066Z         )
2025-04-11T04:23:18.8633133Z     
2025-04-11T04:23:18.8633218Z         error_index = None
2025-04-11T04:23:18.8633301Z         for sentinel in ready:
2025-04-11T04:23:18.8633409Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8633506Z             process = self.processes[index]
2025-04-11T04:23:18.8633590Z             process.join()
2025-04-11T04:23:18.8633684Z             if process.exitcode != 0:
2025-04-11T04:23:18.8633768Z                 error_index = index
2025-04-11T04:23:18.8633897Z                 break
2025-04-11T04:23:18.8633965Z     
2025-04-11T04:23:18.8634053Z         # Return if there was no error.
2025-04-11T04:23:18.8634139Z         if error_index is None:
2025-04-11T04:23:18.8634269Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8634368Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8634436Z     
2025-04-11T04:23:18.8634574Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8634668Z         for process in self.processes:
2025-04-11T04:23:18.8634753Z             if process.is_alive():
2025-04-11T04:23:18.8634847Z                 process.terminate()
2025-04-11T04:23:18.8634928Z             process.join()
2025-04-11T04:23:18.8634998Z     
2025-04-11T04:23:18.8635134Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8635250Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8635356Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8635476Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8635560Z             if exitcode < 0:
2025-04-11T04:23:18.8635662Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8635769Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8635912Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8636005Z                     error_index=error_index,
2025-04-11T04:23:18.8636106Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8636192Z                     exit_code=exitcode,
2025-04-11T04:23:18.8636329Z                     signal_name=name,
2025-04-11T04:23:18.8636401Z                 )
2025-04-11T04:23:18.8636472Z             else:
2025-04-11T04:23:18.8636579Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8636742Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8636838Z                     error_index=error_index,
2025-04-11T04:23:18.8636935Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8637025Z                     exit_code=exitcode,
2025-04-11T04:23:18.8637094Z                 )
2025-04-11T04:23:18.8637165Z     
2025-04-11T04:23:18.8637300Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8637465Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8637551Z         msg += original_trace
2025-04-11T04:23:18.8637725Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8637891Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8637964Z E       
2025-04-11T04:23:18.8638137Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8638239Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8638538Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8638623Z E           fn(i, *args)
2025-04-11T04:23:18.8638907Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
2025-04-11T04:23:18.8639062Z E           run_with_booster_moehybridplugin()
2025-04-11T04:23:18.8639318Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8639405Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8639772Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
2025-04-11T04:23:18.8639903Z E           torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T04:23:18.8640197Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.8640347Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.8640620Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8640735Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8641011Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8641100Z E           module._apply(fn)
2025-04-11T04:23:18.8641368Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8641468Z E           param_applied = fn(param)
2025-04-11T04:23:18.8641738Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8641854Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8641960Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8642265Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8642400Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8642559Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8642566Z 
2025-04-11T04:23:18.8642875Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8643024Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8643181Z [04/11/25 04:19:27] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8643365Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8643476Z                              :75 launch                                         
2025-04-11T04:23:18.8643613Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8643738Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8643936Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8644079Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8645209Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.8646361Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.8647506Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.8648594Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.8648783Z _____________________________ test_flash_attn_func _____________________________
2025-04-11T04:23:18.8648787Z 
2025-04-11T04:23:18.8648871Z args = (), kwargs = {}
2025-04-11T04:23:18.8648876Z 
2025-04-11T04:23:18.8648968Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.8649063Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.8649170Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.8649286Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.8649394Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.8649489Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.8649493Z 
2025-04-11T04:23:18.8649587Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.8649700Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8649860Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8649951Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8650060Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8650064Z 
2025-04-11T04:23:18.8650143Z device = None
2025-04-11T04:23:18.8650147Z 
2025-04-11T04:23:18.8650268Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8650425Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8650560Z     
2025-04-11T04:23:18.8650636Z         Args:
2025-04-11T04:23:18.8650806Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8650977Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8651088Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8651160Z         """
2025-04-11T04:23:18.8651241Z         _lazy_init()
2025-04-11T04:23:18.8651332Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8651435Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8651543Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8651832Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8651972Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8652130Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8652137Z 
2025-04-11T04:23:18.8652380Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8652562Z ______________________________ test_release_layer ______________________________
2025-04-11T04:23:18.8652568Z 
2025-04-11T04:23:18.8652657Z     def test_release_layer():
2025-04-11T04:23:18.8652782Z         orig_cuda_allocated = torch.cuda.memory_allocated()
2025-04-11T04:23:18.8652866Z >       model = Net().cuda()
2025-04-11T04:23:18.8652874Z 
2025-04-11T04:23:18.8652986Z tests/test_shardformer/test_shard_utils.py:16: 
2025-04-11T04:23:18.8653124Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8653363Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
2025-04-11T04:23:18.8653472Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8653710Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8653795Z     module._apply(fn)
2025-04-11T04:23:18.8654029Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8654110Z     module._apply(fn)
2025-04-11T04:23:18.8654336Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8654476Z     param_applied = fn(param)
2025-04-11T04:23:18.8654581Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8654585Z 
2025-04-11T04:23:18.8654679Z t = Parameter containing:
2025-04-11T04:23:18.8654757Z tensor([[-0.8151],
2025-04-11T04:23:18.8654844Z         [ 0.1839]], requires_grad=True)
2025-04-11T04:23:18.8654849Z 
2025-04-11T04:23:18.8654956Z >   return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8655056Z E   RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8655344Z E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8655477Z E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8655637Z E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8655641Z 
2025-04-11T04:23:18.8655889Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
2025-04-11T04:23:18.8656018Z __________________________________ test_gpt2 ___________________________________
2025-04-11T04:23:18.8656022Z 
2025-04-11T04:23:18.8656112Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8656118Z 
2025-04-11T04:23:18.8656218Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8656293Z         try_count = 0
2025-04-11T04:23:18.8656390Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8656473Z             max_try, int
2025-04-11T04:23:18.8656617Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8656738Z     
2025-04-11T04:23:18.8656848Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8656922Z             try:
2025-04-11T04:23:18.8657008Z                 try_count += 1
2025-04-11T04:23:18.8657096Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8657103Z 
2025-04-11T04:23:18.8657197Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8657303Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8657417Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8657510Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8657668Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8657765Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8658002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T04:23:18.8658098Z     with torch.cuda.device(device):
2025-04-11T04:23:18.8658205Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8658209Z 
2025-04-11T04:23:18.8658329Z self = <torch.cuda.device object at 0x7fb581d6d5d0>
2025-04-11T04:23:18.8658382Z 
2025-04-11T04:23:18.8658463Z     def __enter__(self):
2025-04-11T04:23:18.8658594Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T04:23:18.8658703Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8658990Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8659180Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8659337Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8659342Z 
2025-04-11T04:23:18.8659575Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T04:23:18.8659705Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T04:23:18.8659712Z 
2025-04-11T04:23:18.8659804Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8659811Z 
2025-04-11T04:23:18.8659907Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8659983Z         try_count = 0
2025-04-11T04:23:18.8660085Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8660212Z             max_try, int
2025-04-11T04:23:18.8660358Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8660426Z     
2025-04-11T04:23:18.8660538Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8660612Z             try:
2025-04-11T04:23:18.8660693Z                 try_count += 1
2025-04-11T04:23:18.8660784Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8660789Z 
2025-04-11T04:23:18.8660880Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8660989Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8661100Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8661190Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8661344Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8661435Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8661544Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8661550Z 
2025-04-11T04:23:18.8661625Z device = None
2025-04-11T04:23:18.8661630Z 
2025-04-11T04:23:18.8661751Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8661902Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8661975Z     
2025-04-11T04:23:18.8662046Z         Args:
2025-04-11T04:23:18.8662210Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8662381Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8662547Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8662622Z         """
2025-04-11T04:23:18.8662699Z         _lazy_init()
2025-04-11T04:23:18.8662790Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8662893Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8663000Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8663289Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8663425Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8663588Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8663592Z 
2025-04-11T04:23:18.8663825Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8663957Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T04:23:18.8663966Z 
2025-04-11T04:23:18.8664055Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8664059Z 
2025-04-11T04:23:18.8664206Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8664286Z         try_count = 0
2025-04-11T04:23:18.8664384Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8664466Z             max_try, int
2025-04-11T04:23:18.8664608Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8664678Z     
2025-04-11T04:23:18.8664785Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8664856Z             try:
2025-04-11T04:23:18.8665006Z                 try_count += 1
2025-04-11T04:23:18.8665094Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8665100Z 
2025-04-11T04:23:18.8665193Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8665301Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8665415Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8665507Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8665659Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8665757Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8665863Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8665916Z 
2025-04-11T04:23:18.8665996Z device = None
2025-04-11T04:23:18.8666001Z 
2025-04-11T04:23:18.8666119Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8666270Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8666344Z     
2025-04-11T04:23:18.8666415Z         Args:
2025-04-11T04:23:18.8666584Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8666750Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8666864Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8666937Z         """
2025-04-11T04:23:18.8667016Z         _lazy_init()
2025-04-11T04:23:18.8667115Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8667215Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8667322Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8667609Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8667747Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8667905Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8667913Z 
2025-04-11T04:23:18.8668145Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8668281Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T04:23:18.8668338Z 
2025-04-11T04:23:18.8668459Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8668464Z 
2025-04-11T04:23:18.8668568Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8668646Z         try_count = 0
2025-04-11T04:23:18.8668746Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8668823Z             max_try, int
2025-04-11T04:23:18.8668967Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8669039Z     
2025-04-11T04:23:18.8669146Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8669222Z             try:
2025-04-11T04:23:18.8669303Z                 try_count += 1
2025-04-11T04:23:18.8669397Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8669402Z 
2025-04-11T04:23:18.8669493Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8669600Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8669714Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8669807Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8669963Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8670118Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8670231Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8670238Z 
2025-04-11T04:23:18.8670312Z device = None
2025-04-11T04:23:18.8670316Z 
2025-04-11T04:23:18.8670437Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8670591Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8670702Z     
2025-04-11T04:23:18.8670778Z         Args:
2025-04-11T04:23:18.8670946Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8671114Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8671225Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8671300Z         """
2025-04-11T04:23:18.8671380Z         _lazy_init()
2025-04-11T04:23:18.8671470Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8671574Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8671680Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8671962Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8672173Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8672330Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8672336Z 
2025-04-11T04:23:18.8672571Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8672708Z ____________________________ test_dist_crossentropy ____________________________
2025-04-11T04:23:18.8672712Z 
2025-04-11T04:23:18.8672805Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8673427Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8673434Z 
2025-04-11T04:23:18.8673537Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8673614Z         try_count = 0
2025-04-11T04:23:18.8673714Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8673791Z             max_try, int
2025-04-11T04:23:18.8673933Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8674008Z     
2025-04-11T04:23:18.8674116Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8674192Z             try:
2025-04-11T04:23:18.8674272Z                 try_count += 1
2025-04-11T04:23:18.8674360Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8674498Z                 return ret
2025-04-11T04:23:18.8674588Z             except exception_type as e:
2025-04-11T04:23:18.8674689Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8674876Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8674993Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8675136Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8675288Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8675371Z                     continue
2025-04-11T04:23:18.8675446Z                 else:
2025-04-11T04:23:18.8675669Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8675747Z >                   raise e
2025-04-11T04:23:18.8675752Z 
2025-04-11T04:23:18.8675843Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8675951Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8676077Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8676217Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8676438Z tests/test_shardformer/test_layer/test_dist_crossentropy.py:51: in test_dist_crossentropy
2025-04-11T04:23:18.8676587Z     spawn(check_dist_crossentropy, 2, ignore_index=ignore_index)
2025-04-11T04:23:18.8676685Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8676785Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8677035Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8677261Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8677551Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8677640Z     while not context.join():
2025-04-11T04:23:18.8677753Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8677757Z 
2025-04-11T04:23:18.8677975Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3e410>
2025-04-11T04:23:18.8678055Z timeout = None
2025-04-11T04:23:18.8678060Z 
2025-04-11T04:23:18.8678201Z     def join(self, timeout=None):
2025-04-11T04:23:18.8678323Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8678394Z     
2025-04-11T04:23:18.8678539Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8678686Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8678851Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8678943Z         of the first process exiting.
2025-04-11T04:23:18.8679011Z     
2025-04-11T04:23:18.8679157Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8679296Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8679364Z     
2025-04-11T04:23:18.8679439Z         Args:
2025-04-11T04:23:18.8679578Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8679652Z         """
2025-04-11T04:23:18.8679789Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8679882Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8679964Z             return True
2025-04-11T04:23:18.8680033Z     
2025-04-11T04:23:18.8680167Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8680286Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8680375Z             self.sentinels.keys(),
2025-04-11T04:23:18.8680460Z             timeout=timeout,
2025-04-11T04:23:18.8680531Z         )
2025-04-11T04:23:18.8680602Z     
2025-04-11T04:23:18.8680683Z         error_index = None
2025-04-11T04:23:18.8680824Z         for sentinel in ready:
2025-04-11T04:23:18.8680934Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8681032Z             process = self.processes[index]
2025-04-11T04:23:18.8681120Z             process.join()
2025-04-11T04:23:18.8681212Z             if process.exitcode != 0:
2025-04-11T04:23:18.8681301Z                 error_index = index
2025-04-11T04:23:18.8681378Z                 break
2025-04-11T04:23:18.8681447Z     
2025-04-11T04:23:18.8681539Z         # Return if there was no error.
2025-04-11T04:23:18.8681624Z         if error_index is None:
2025-04-11T04:23:18.8681760Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8681856Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8681924Z     
2025-04-11T04:23:18.8682067Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8682160Z         for process in self.processes:
2025-04-11T04:23:18.8682250Z             if process.is_alive():
2025-04-11T04:23:18.8682343Z                 process.terminate()
2025-04-11T04:23:18.8682426Z             process.join()
2025-04-11T04:23:18.8682497Z     
2025-04-11T04:23:18.8682688Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8682807Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8682913Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8683036Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8683119Z             if exitcode < 0:
2025-04-11T04:23:18.8683226Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8683379Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8683527Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8683624Z                     error_index=error_index,
2025-04-11T04:23:18.8683724Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8683814Z                     exit_code=exitcode,
2025-04-11T04:23:18.8683899Z                     signal_name=name,
2025-04-11T04:23:18.8683971Z                 )
2025-04-11T04:23:18.8684049Z             else:
2025-04-11T04:23:18.8684152Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8684321Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8684462Z                     error_index=error_index,
2025-04-11T04:23:18.8684561Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8684650Z                     exit_code=exitcode,
2025-04-11T04:23:18.8684721Z                 )
2025-04-11T04:23:18.8684796Z     
2025-04-11T04:23:18.8684926Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8685097Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8685182Z         msg += original_trace
2025-04-11T04:23:18.8685354Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8685518Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8685589Z E       
2025-04-11T04:23:18.8685718Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8685815Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8686116Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8686200Z E           fn(i, *args)
2025-04-11T04:23:18.8686534Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
2025-04-11T04:23:18.8686674Z E           pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
2025-04-11T04:23:18.8686778Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8687063Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8687248Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8687409Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8687416Z 
2025-04-11T04:23:18.8687719Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8687869Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8688028Z [04/11/25 04:19:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8688153Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8688263Z                              :75 launch                                         
2025-04-11T04:23:18.8688401Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8688528Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8688727Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8688871Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8689212Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26698 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8689343Z _________________________________ test_dropout _________________________________
2025-04-11T04:23:18.8689347Z 
2025-04-11T04:23:18.8689441Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8690030Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8690083Z 
2025-04-11T04:23:18.8690187Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8690265Z         try_count = 0
2025-04-11T04:23:18.8690367Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8690448Z             max_try, int
2025-04-11T04:23:18.8690592Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8690663Z     
2025-04-11T04:23:18.8690772Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8690896Z             try:
2025-04-11T04:23:18.8690979Z                 try_count += 1
2025-04-11T04:23:18.8691071Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8691148Z                 return ret
2025-04-11T04:23:18.8691240Z             except exception_type as e:
2025-04-11T04:23:18.8691342Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8691528Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8691644Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8691786Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8691944Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8692024Z                     continue
2025-04-11T04:23:18.8692099Z                 else:
2025-04-11T04:23:18.8692320Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8692399Z >                   raise e
2025-04-11T04:23:18.8692404Z 
2025-04-11T04:23:18.8692497Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8692603Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8692735Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8692818Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8692986Z tests/test_shardformer/test_layer/test_dropout.py:66: in test_dropout
2025-04-11T04:23:18.8693077Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8693174Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8693325Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8693579Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8693760Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8694042Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8694127Z     while not context.join():
2025-04-11T04:23:18.8694237Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8694242Z 
2025-04-11T04:23:18.8694437Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a898d0>
2025-04-11T04:23:18.8694516Z timeout = None
2025-04-11T04:23:18.8694520Z 
2025-04-11T04:23:18.8694607Z     def join(self, timeout=None):
2025-04-11T04:23:18.8694731Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8694800Z     
2025-04-11T04:23:18.8694944Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8695088Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8695298Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8695392Z         of the first process exiting.
2025-04-11T04:23:18.8695464Z     
2025-04-11T04:23:18.8695610Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8695743Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8695810Z     
2025-04-11T04:23:18.8695953Z         Args:
2025-04-11T04:23:18.8696088Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8696161Z         """
2025-04-11T04:23:18.8696296Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8696386Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8696468Z             return True
2025-04-11T04:23:18.8696535Z     
2025-04-11T04:23:18.8696666Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8696785Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8696877Z             self.sentinels.keys(),
2025-04-11T04:23:18.8696960Z             timeout=timeout,
2025-04-11T04:23:18.8697078Z         )
2025-04-11T04:23:18.8697149Z     
2025-04-11T04:23:18.8697230Z         error_index = None
2025-04-11T04:23:18.8697318Z         for sentinel in ready:
2025-04-11T04:23:18.8697420Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8697515Z             process = self.processes[index]
2025-04-11T04:23:18.8697604Z             process.join()
2025-04-11T04:23:18.8697694Z             if process.exitcode != 0:
2025-04-11T04:23:18.8697780Z                 error_index = index
2025-04-11T04:23:18.8697853Z                 break
2025-04-11T04:23:18.8697920Z     
2025-04-11T04:23:18.8698011Z         # Return if there was no error.
2025-04-11T04:23:18.8698096Z         if error_index is None:
2025-04-11T04:23:18.8698228Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8698322Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8698393Z     
2025-04-11T04:23:18.8698530Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8698624Z         for process in self.processes:
2025-04-11T04:23:18.8698713Z             if process.is_alive():
2025-04-11T04:23:18.8698800Z                 process.terminate()
2025-04-11T04:23:18.8698883Z             process.join()
2025-04-11T04:23:18.8698952Z     
2025-04-11T04:23:18.8699089Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8699206Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8699309Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8699430Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8699561Z             if exitcode < 0:
2025-04-11T04:23:18.8699665Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8699770Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8699920Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8700016Z                     error_index=error_index,
2025-04-11T04:23:18.8700116Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8700203Z                     exit_code=exitcode,
2025-04-11T04:23:18.8700286Z                     signal_name=name,
2025-04-11T04:23:18.8700356Z                 )
2025-04-11T04:23:18.8700434Z             else:
2025-04-11T04:23:18.8700534Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8700698Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8700789Z                     error_index=error_index,
2025-04-11T04:23:18.8700888Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8700975Z                     exit_code=exitcode,
2025-04-11T04:23:18.8701045Z                 )
2025-04-11T04:23:18.8701167Z     
2025-04-11T04:23:18.8701298Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8701469Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8701556Z         msg += original_trace
2025-04-11T04:23:18.8701728Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8701890Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8702008Z E       
2025-04-11T04:23:18.8702136Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8702235Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8702540Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8702618Z E           fn(i, *args)
2025-04-11T04:23:18.8702885Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
2025-04-11T04:23:18.8702987Z E           check_dropout_parallel_input()
2025-04-11T04:23:18.8703304Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
2025-04-11T04:23:18.8703580Z E           dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
2025-04-11T04:23:18.8703855Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
2025-04-11T04:23:18.8704074Z E           return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
2025-04-11T04:23:18.8704314Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
2025-04-11T04:23:18.8704518Z E           self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
2025-04-11T04:23:18.8704817Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
2025-04-11T04:23:18.8705005Z E           is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
2025-04-11T04:23:18.8705315Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
2025-04-11T04:23:18.8705555Z E           index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8705665Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8705948Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8706085Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8706243Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8706301Z 
2025-04-11T04:23:18.8706611Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8706763Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8706915Z [04/11/25 04:19:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8707046Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8707151Z                              :75 launch                                         
2025-04-11T04:23:18.8707291Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8707415Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8707615Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8707747Z ______________________________ test_embedding_1d _______________________________
2025-04-11T04:23:18.8707754Z 
2025-04-11T04:23:18.8707847Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8708480Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8708542Z 
2025-04-11T04:23:18.8708646Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8708724Z         try_count = 0
2025-04-11T04:23:18.8708823Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8708948Z             max_try, int
2025-04-11T04:23:18.8709093Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8709166Z     
2025-04-11T04:23:18.8709274Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8709347Z             try:
2025-04-11T04:23:18.8709431Z                 try_count += 1
2025-04-11T04:23:18.8709520Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8709601Z                 return ret
2025-04-11T04:23:18.8709691Z             except exception_type as e:
2025-04-11T04:23:18.8709789Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8709975Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8710141Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8710287Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8710438Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8710523Z                     continue
2025-04-11T04:23:18.8710598Z                 else:
2025-04-11T04:23:18.8710819Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8710900Z >                   raise e
2025-04-11T04:23:18.8710906Z 
2025-04-11T04:23:18.8710996Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8711107Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8711235Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8711321Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8711506Z tests/test_shardformer/test_layer/test_embedding.py:52: in test_embedding_1d
2025-04-11T04:23:18.8711594Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8711694Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8711791Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8712051Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8712224Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8712509Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8712744Z     while not context.join():
2025-04-11T04:23:18.8712849Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8712859Z 
2025-04-11T04:23:18.8713053Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3fe80>
2025-04-11T04:23:18.8713129Z timeout = None
2025-04-11T04:23:18.8713136Z 
2025-04-11T04:23:18.8713225Z     def join(self, timeout=None):
2025-04-11T04:23:18.8713349Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8713420Z     
2025-04-11T04:23:18.8713562Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8713707Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8713867Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8713956Z         of the first process exiting.
2025-04-11T04:23:18.8714028Z     
2025-04-11T04:23:18.8714170Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8714308Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8714427Z     
2025-04-11T04:23:18.8714498Z         Args:
2025-04-11T04:23:18.8714638Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8714713Z         """
2025-04-11T04:23:18.8714851Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8714942Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8715022Z             return True
2025-04-11T04:23:18.8715090Z     
2025-04-11T04:23:18.8715268Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8715387Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8715477Z             self.sentinels.keys(),
2025-04-11T04:23:18.8715563Z             timeout=timeout,
2025-04-11T04:23:18.8715634Z         )
2025-04-11T04:23:18.8715702Z     
2025-04-11T04:23:18.8715789Z         error_index = None
2025-04-11T04:23:18.8715873Z         for sentinel in ready:
2025-04-11T04:23:18.8715979Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8716078Z             process = self.processes[index]
2025-04-11T04:23:18.8716161Z             process.join()
2025-04-11T04:23:18.8716255Z             if process.exitcode != 0:
2025-04-11T04:23:18.8716391Z                 error_index = index
2025-04-11T04:23:18.8716470Z                 break
2025-04-11T04:23:18.8716537Z     
2025-04-11T04:23:18.8716627Z         # Return if there was no error.
2025-04-11T04:23:18.8716710Z         if error_index is None:
2025-04-11T04:23:18.8716847Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8716946Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8717014Z     
2025-04-11T04:23:18.8717154Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8717246Z         for process in self.processes:
2025-04-11T04:23:18.8717334Z             if process.is_alive():
2025-04-11T04:23:18.8717426Z                 process.terminate()
2025-04-11T04:23:18.8717507Z             process.join()
2025-04-11T04:23:18.8717578Z     
2025-04-11T04:23:18.8717714Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8717827Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8717935Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8718053Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8718137Z             if exitcode < 0:
2025-04-11T04:23:18.8718240Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8718348Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8718495Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8718586Z                     error_index=error_index,
2025-04-11T04:23:18.8718688Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8718822Z                     exit_code=exitcode,
2025-04-11T04:23:18.8718911Z                     signal_name=name,
2025-04-11T04:23:18.8718985Z                 )
2025-04-11T04:23:18.8719060Z             else:
2025-04-11T04:23:18.8719163Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8719330Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8719427Z                     error_index=error_index,
2025-04-11T04:23:18.8719522Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8719611Z                     exit_code=exitcode,
2025-04-11T04:23:18.8719684Z                 )
2025-04-11T04:23:18.8719751Z     
2025-04-11T04:23:18.8719884Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8720053Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8720140Z         msg += original_trace
2025-04-11T04:23:18.8720316Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8720484Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8720623Z E       
2025-04-11T04:23:18.8720746Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8720845Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8721145Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8721226Z E           fn(i, *args)
2025-04-11T04:23:18.8721497Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
2025-04-11T04:23:18.8721639Z E           check_embedding_1d()
2025-04-11T04:23:18.8721896Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8721984Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8722283Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
2025-04-11T04:23:18.8722388Z E           embedding = nn.Embedding(32, 128).cuda()
2025-04-11T04:23:18.8722666Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8722832Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8723101Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8723193Z E           param_applied = fn(param)
2025-04-11T04:23:18.8723467Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8723583Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8723687Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8723970Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8724106Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8724269Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8724274Z 
2025-04-11T04:23:18.8724575Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8724728Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8724882Z [04/11/25 04:19:43] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8725008Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8725114Z                              :75 launch                                         
2025-04-11T04:23:18.8725250Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8725427Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8725621Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8725757Z _______________________________ test_linearconv ________________________________
2025-04-11T04:23:18.8725762Z 
2025-04-11T04:23:18.8725855Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8726457Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8726469Z 
2025-04-11T04:23:18.8726570Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8726649Z         try_count = 0
2025-04-11T04:23:18.8726751Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8726829Z             max_try, int
2025-04-11T04:23:18.8726979Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8727049Z     
2025-04-11T04:23:18.8727163Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8727301Z             try:
2025-04-11T04:23:18.8727384Z                 try_count += 1
2025-04-11T04:23:18.8727477Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8727557Z                 return ret
2025-04-11T04:23:18.8727650Z             except exception_type as e:
2025-04-11T04:23:18.8727750Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8727940Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8728107Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8728249Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8728407Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8728490Z                     continue
2025-04-11T04:23:18.8728567Z                 else:
2025-04-11T04:23:18.8728786Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8728867Z >                   raise e
2025-04-11T04:23:18.8728875Z 
2025-04-11T04:23:18.8728967Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8729126Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8729259Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8729342Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8729566Z tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py:209: in test_linearconv
2025-04-11T04:23:18.8729655Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8729755Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8729854Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8730104Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8730282Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8730563Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8730651Z     while not context.join():
2025-04-11T04:23:18.8730757Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8730761Z 
2025-04-11T04:23:18.8730959Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3c910>
2025-04-11T04:23:18.8731035Z timeout = None
2025-04-11T04:23:18.8731042Z 
2025-04-11T04:23:18.8731130Z     def join(self, timeout=None):
2025-04-11T04:23:18.8731254Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8731323Z     
2025-04-11T04:23:18.8731470Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8731611Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8731827Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8731924Z         of the first process exiting.
2025-04-11T04:23:18.8731995Z     
2025-04-11T04:23:18.8732148Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8732291Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8732366Z     
2025-04-11T04:23:18.8732441Z         Args:
2025-04-11T04:23:18.8732582Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8732663Z         """
2025-04-11T04:23:18.8732803Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8732900Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8732980Z             return True
2025-04-11T04:23:18.8733051Z     
2025-04-11T04:23:18.8733187Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8733310Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8733407Z             self.sentinels.keys(),
2025-04-11T04:23:18.8733541Z             timeout=timeout,
2025-04-11T04:23:18.8733616Z         )
2025-04-11T04:23:18.8733685Z     
2025-04-11T04:23:18.8733765Z         error_index = None
2025-04-11T04:23:18.8733855Z         for sentinel in ready:
2025-04-11T04:23:18.8733956Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8734054Z             process = self.processes[index]
2025-04-11T04:23:18.8734137Z             process.join()
2025-04-11T04:23:18.8734226Z             if process.exitcode != 0:
2025-04-11T04:23:18.8734366Z                 error_index = index
2025-04-11T04:23:18.8734440Z                 break
2025-04-11T04:23:18.8734512Z     
2025-04-11T04:23:18.8734601Z         # Return if there was no error.
2025-04-11T04:23:18.8734684Z         if error_index is None:
2025-04-11T04:23:18.8734818Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8734913Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8734983Z     
2025-04-11T04:23:18.8735120Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8735218Z         for process in self.processes:
2025-04-11T04:23:18.8735302Z             if process.is_alive():
2025-04-11T04:23:18.8735442Z                 process.terminate()
2025-04-11T04:23:18.8735527Z             process.join()
2025-04-11T04:23:18.8735595Z     
2025-04-11T04:23:18.8735736Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8735849Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8735956Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8736077Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8736157Z             if exitcode < 0:
2025-04-11T04:23:18.8736263Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8736366Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8736516Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8736611Z                     error_index=error_index,
2025-04-11T04:23:18.8736708Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8736797Z                     exit_code=exitcode,
2025-04-11T04:23:18.8736883Z                     signal_name=name,
2025-04-11T04:23:18.8736957Z                 )
2025-04-11T04:23:18.8737028Z             else:
2025-04-11T04:23:18.8737127Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8737291Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8737384Z                     error_index=error_index,
2025-04-11T04:23:18.8737484Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8737568Z                     exit_code=exitcode,
2025-04-11T04:23:18.8737641Z                 )
2025-04-11T04:23:18.8737762Z     
2025-04-11T04:23:18.8737893Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8738068Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8738153Z         msg += original_trace
2025-04-11T04:23:18.8738328Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8738493Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8738567Z E       
2025-04-11T04:23:18.8738691Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8738787Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8739088Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8739167Z E           fn(i, *args)
2025-04-11T04:23:18.8739487Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
2025-04-11T04:23:18.8739589Z E           check_gpt2_qkv_fused_linear_1d()
2025-04-11T04:23:18.8739846Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8739982Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8740234Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8740326Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8740684Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
2025-04-11T04:23:18.8740874Z E           check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
2025-04-11T04:23:18.8741209Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
2025-04-11T04:23:18.8741306Z E           linear = Conv1D(192, 48).cuda()
2025-04-11T04:23:18.8741571Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8741691Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8741962Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8742100Z E           param_applied = fn(param)
2025-04-11T04:23:18.8742381Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8742495Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8742604Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8742885Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8743021Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8743180Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8743185Z 
2025-04-11T04:23:18.8743486Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8743639Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8743795Z [04/11/25 04:19:48] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8743924Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8744027Z                              :75 launch                                         
2025-04-11T04:23:18.8744169Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8744291Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8744486Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8744698Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8744994Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26338 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8745128Z ________________________________ test_layernorm ________________________________
2025-04-11T04:23:18.8745134Z 
2025-04-11T04:23:18.8745224Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8745818Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8745825Z 
2025-04-11T04:23:18.8745923Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8746004Z         try_count = 0
2025-04-11T04:23:18.8746099Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8746183Z             max_try, int
2025-04-11T04:23:18.8746324Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8746444Z     
2025-04-11T04:23:18.8746559Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8746633Z             try:
2025-04-11T04:23:18.8746717Z                 try_count += 1
2025-04-11T04:23:18.8746808Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8746886Z                 return ret
2025-04-11T04:23:18.8746982Z             except exception_type as e:
2025-04-11T04:23:18.8747079Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8747321Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8747437Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8747584Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8747739Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8747821Z                     continue
2025-04-11T04:23:18.8747897Z                 else:
2025-04-11T04:23:18.8748121Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8748201Z >                   raise e
2025-04-11T04:23:18.8748255Z 
2025-04-11T04:23:18.8748347Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8748488Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8748617Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8748705Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8748884Z tests/test_shardformer/test_layer/test_layernorm.py:50: in test_layernorm
2025-04-11T04:23:18.8748970Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8749071Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8749167Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8749427Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8749601Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8749884Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8749977Z     while not context.join():
2025-04-11T04:23:18.8750083Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8750087Z 
2025-04-11T04:23:18.8750289Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3e9e0>
2025-04-11T04:23:18.8750368Z timeout = None
2025-04-11T04:23:18.8750372Z 
2025-04-11T04:23:18.8750462Z     def join(self, timeout=None):
2025-04-11T04:23:18.8750585Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8750653Z     
2025-04-11T04:23:18.8750798Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8750999Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8751164Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8751256Z         of the first process exiting.
2025-04-11T04:23:18.8751329Z     
2025-04-11T04:23:18.8751474Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8751612Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8751685Z     
2025-04-11T04:23:18.8751757Z         Args:
2025-04-11T04:23:18.8751896Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8751969Z         """
2025-04-11T04:23:18.8752104Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8752198Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8752277Z             return True
2025-04-11T04:23:18.8752348Z     
2025-04-11T04:23:18.8752479Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8752599Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8752741Z             self.sentinels.keys(),
2025-04-11T04:23:18.8752823Z             timeout=timeout,
2025-04-11T04:23:18.8752898Z         )
2025-04-11T04:23:18.8752966Z     
2025-04-11T04:23:18.8753052Z         error_index = None
2025-04-11T04:23:18.8753134Z         for sentinel in ready:
2025-04-11T04:23:18.8753236Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8753335Z             process = self.processes[index]
2025-04-11T04:23:18.8753417Z             process.join()
2025-04-11T04:23:18.8753564Z             if process.exitcode != 0:
2025-04-11T04:23:18.8753651Z                 error_index = index
2025-04-11T04:23:18.8753725Z                 break
2025-04-11T04:23:18.8753796Z     
2025-04-11T04:23:18.8753885Z         # Return if there was no error.
2025-04-11T04:23:18.8753971Z         if error_index is None:
2025-04-11T04:23:18.8754103Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8754199Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8754268Z     
2025-04-11T04:23:18.8754404Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8754505Z         for process in self.processes:
2025-04-11T04:23:18.8754642Z             if process.is_alive():
2025-04-11T04:23:18.8754736Z                 process.terminate()
2025-04-11T04:23:18.8754816Z             process.join()
2025-04-11T04:23:18.8754883Z     
2025-04-11T04:23:18.8755022Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8755137Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8755244Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8755362Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8755444Z             if exitcode < 0:
2025-04-11T04:23:18.8755547Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8755652Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8755802Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8755896Z                     error_index=error_index,
2025-04-11T04:23:18.8755998Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8756084Z                     exit_code=exitcode,
2025-04-11T04:23:18.8756167Z                     signal_name=name,
2025-04-11T04:23:18.8756242Z                 )
2025-04-11T04:23:18.8756312Z             else:
2025-04-11T04:23:18.8756415Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8756578Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8756670Z                     error_index=error_index,
2025-04-11T04:23:18.8756768Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8756852Z                     exit_code=exitcode,
2025-04-11T04:23:18.8756983Z                 )
2025-04-11T04:23:18.8757051Z     
2025-04-11T04:23:18.8757183Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8757351Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8757437Z         msg += original_trace
2025-04-11T04:23:18.8757611Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8757772Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8757847Z E       
2025-04-11T04:23:18.8757968Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8758069Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8758367Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8758444Z E           fn(i, *args)
2025-04-11T04:23:18.8758718Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
2025-04-11T04:23:18.8758803Z E           check_layernorm()
2025-04-11T04:23:18.8759064Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8759199Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8759487Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
2025-04-11T04:23:18.8759592Z E           norm = nn.LayerNorm(128, 0.00001).cuda()
2025-04-11T04:23:18.8759866Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8760034Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8760302Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8760398Z E           param_applied = fn(param)
2025-04-11T04:23:18.8760670Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8760784Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8760891Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8761176Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8761360Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8761520Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8761527Z 
2025-04-11T04:23:18.8761835Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8761985Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8762146Z [04/11/25 04:19:52] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8762275Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8762383Z                              :75 launch                                         
2025-04-11T04:23:18.8762521Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8762645Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8762845Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8762972Z _________________________________ test_linear __________________________________
2025-04-11T04:23:18.8762978Z 
2025-04-11T04:23:18.8763072Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8763680Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8763738Z 
2025-04-11T04:23:18.8763842Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8763922Z         try_count = 0
2025-04-11T04:23:18.8764024Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8764104Z             max_try, int
2025-04-11T04:23:18.8764248Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8764322Z     
2025-04-11T04:23:18.8764431Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8764510Z             try:
2025-04-11T04:23:18.8764592Z                 try_count += 1
2025-04-11T04:23:18.8764692Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8764770Z                 return ret
2025-04-11T04:23:18.8764862Z             except exception_type as e:
2025-04-11T04:23:18.8764966Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8765149Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8765267Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8765409Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8765620Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8765702Z                     continue
2025-04-11T04:23:18.8765776Z                 else:
2025-04-11T04:23:18.8766001Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8766079Z >                   raise e
2025-04-11T04:23:18.8766131Z 
2025-04-11T04:23:18.8766228Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8766339Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8766472Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8766556Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8766728Z tests/test_shardformer/test_layer/test_linear_1d.py:284: in test_linear
2025-04-11T04:23:18.8766826Z     spawn(check_dist_linear, nprocs=2)
2025-04-11T04:23:18.8766929Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8767031Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8767282Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8767525Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8767814Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8767902Z     while not context.join():
2025-04-11T04:23:18.8768013Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8768017Z 
2025-04-11T04:23:18.8768226Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8693070>
2025-04-11T04:23:18.8768307Z timeout = None
2025-04-11T04:23:18.8768315Z 
2025-04-11T04:23:18.8768405Z     def join(self, timeout=None):
2025-04-11T04:23:18.8768530Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8768600Z     
2025-04-11T04:23:18.8768745Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8768892Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8769056Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8769148Z         of the first process exiting.
2025-04-11T04:23:18.8769216Z     
2025-04-11T04:23:18.8769361Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8769500Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8769569Z     
2025-04-11T04:23:18.8769645Z         Args:
2025-04-11T04:23:18.8769779Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8769907Z         """
2025-04-11T04:23:18.8770045Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8770134Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8770218Z             return True
2025-04-11T04:23:18.8770285Z     
2025-04-11T04:23:18.8770418Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8770536Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8770626Z             self.sentinels.keys(),
2025-04-11T04:23:18.8770711Z             timeout=timeout,
2025-04-11T04:23:18.8770780Z         )
2025-04-11T04:23:18.8770855Z     
2025-04-11T04:23:18.8770937Z         error_index = None
2025-04-11T04:23:18.8771023Z         for sentinel in ready:
2025-04-11T04:23:18.8771124Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8771219Z             process = self.processes[index]
2025-04-11T04:23:18.8771304Z             process.join()
2025-04-11T04:23:18.8771394Z             if process.exitcode != 0:
2025-04-11T04:23:18.8771484Z                 error_index = index
2025-04-11T04:23:18.8771559Z                 break
2025-04-11T04:23:18.8771628Z     
2025-04-11T04:23:18.8771780Z         # Return if there was no error.
2025-04-11T04:23:18.8771863Z         if error_index is None:
2025-04-11T04:23:18.8771998Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8772094Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8772163Z     
2025-04-11T04:23:18.8772304Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8772397Z         for process in self.processes:
2025-04-11T04:23:18.8772539Z             if process.is_alive():
2025-04-11T04:23:18.8772630Z                 process.terminate()
2025-04-11T04:23:18.8772714Z             process.join()
2025-04-11T04:23:18.8772781Z     
2025-04-11T04:23:18.8772921Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8773039Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8773146Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8773265Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8773350Z             if exitcode < 0:
2025-04-11T04:23:18.8773454Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8773610Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8773757Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8773853Z                     error_index=error_index,
2025-04-11T04:23:18.8773952Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8774046Z                     exit_code=exitcode,
2025-04-11T04:23:18.8774131Z                     signal_name=name,
2025-04-11T04:23:18.8774203Z                 )
2025-04-11T04:23:18.8774278Z             else:
2025-04-11T04:23:18.8774380Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8774543Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8774636Z                     error_index=error_index,
2025-04-11T04:23:18.8774738Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8774824Z                     exit_code=exitcode,
2025-04-11T04:23:18.8774895Z                 )
2025-04-11T04:23:18.8774969Z     
2025-04-11T04:23:18.8775099Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8775273Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8775357Z         msg += original_trace
2025-04-11T04:23:18.8775532Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8775696Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8775769Z E       
2025-04-11T04:23:18.8775894Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8775992Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8776347Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8776428Z E           fn(i, *args)
2025-04-11T04:23:18.8776725Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
2025-04-11T04:23:18.8776817Z E           run_dist_linear_test()
2025-04-11T04:23:18.8777076Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8777167Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8777422Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8777508Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8777754Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8777836Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8778139Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
2025-04-11T04:23:18.8778333Z E           check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
2025-04-11T04:23:18.8778626Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T04:23:18.8778729Z E           linear = nn.Linear(32, 128).cuda()
2025-04-11T04:23:18.8779000Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8779167Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8779441Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8779535Z E           param_applied = fn(param)
2025-04-11T04:23:18.8779809Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8779927Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8780035Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8780319Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8780501Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8780663Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8780668Z 
2025-04-11T04:23:18.8780978Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8781128Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8781284Z [04/11/25 04:19:56] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8781413Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8781524Z                              :75 launch                                         
2025-04-11T04:23:18.8781662Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8781787Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8781985Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8782120Z _______________________________ test_linearconv ________________________________
2025-04-11T04:23:18.8782126Z 
2025-04-11T04:23:18.8782215Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8782812Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8782870Z 
2025-04-11T04:23:18.8782972Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8783049Z         try_count = 0
2025-04-11T04:23:18.8783154Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8783234Z             max_try, int
2025-04-11T04:23:18.8783381Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8783452Z     
2025-04-11T04:23:18.8783562Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8783638Z             try:
2025-04-11T04:23:18.8783719Z                 try_count += 1
2025-04-11T04:23:18.8783812Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8783892Z                 return ret
2025-04-11T04:23:18.8783987Z             except exception_type as e:
2025-04-11T04:23:18.8784084Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8784268Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8784387Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8784531Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8784738Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8784818Z                     continue
2025-04-11T04:23:18.8784899Z                 else:
2025-04-11T04:23:18.8785117Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8785195Z >                   raise e
2025-04-11T04:23:18.8785199Z 
2025-04-11T04:23:18.8785345Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8785454Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8785588Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8785673Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8785880Z tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py:166: in test_linearconv
2025-04-11T04:23:18.8785969Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8786068Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8786172Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8786425Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8786655Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8786937Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8787024Z     while not context.join():
2025-04-11T04:23:18.8787134Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8787139Z 
2025-04-11T04:23:18.8787336Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3cfd0>
2025-04-11T04:23:18.8787416Z timeout = None
2025-04-11T04:23:18.8787420Z 
2025-04-11T04:23:18.8787510Z     def join(self, timeout=None):
2025-04-11T04:23:18.8787636Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8787704Z     
2025-04-11T04:23:18.8787850Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8787991Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8788154Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8788249Z         of the first process exiting.
2025-04-11T04:23:18.8788317Z     
2025-04-11T04:23:18.8788500Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8788639Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8788710Z     
2025-04-11T04:23:18.8788782Z         Args:
2025-04-11T04:23:18.8788917Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8788992Z         """
2025-04-11T04:23:18.8789209Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8789302Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8789378Z             return True
2025-04-11T04:23:18.8789448Z     
2025-04-11T04:23:18.8789581Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8789698Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8789791Z             self.sentinels.keys(),
2025-04-11T04:23:18.8789874Z             timeout=timeout,
2025-04-11T04:23:18.8789944Z         )
2025-04-11T04:23:18.8790015Z     
2025-04-11T04:23:18.8790095Z         error_index = None
2025-04-11T04:23:18.8790184Z         for sentinel in ready:
2025-04-11T04:23:18.8790285Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8790386Z             process = self.processes[index]
2025-04-11T04:23:18.8790468Z             process.join()
2025-04-11T04:23:18.8790560Z             if process.exitcode != 0:
2025-04-11T04:23:18.8790649Z                 error_index = index
2025-04-11T04:23:18.8790725Z                 break
2025-04-11T04:23:18.8790796Z     
2025-04-11T04:23:18.8790885Z         # Return if there was no error.
2025-04-11T04:23:18.8791025Z         if error_index is None:
2025-04-11T04:23:18.8791159Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8791253Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8791326Z     
2025-04-11T04:23:18.8791462Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8791555Z         for process in self.processes:
2025-04-11T04:23:18.8791643Z             if process.is_alive():
2025-04-11T04:23:18.8791799Z                 process.terminate()
2025-04-11T04:23:18.8791886Z             process.join()
2025-04-11T04:23:18.8791953Z     
2025-04-11T04:23:18.8792091Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8792203Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8792310Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8792433Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8792516Z             if exitcode < 0:
2025-04-11T04:23:18.8792623Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8792725Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8792929Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8793024Z                     error_index=error_index,
2025-04-11T04:23:18.8793122Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8793212Z                     exit_code=exitcode,
2025-04-11T04:23:18.8793298Z                     signal_name=name,
2025-04-11T04:23:18.8793374Z                 )
2025-04-11T04:23:18.8793445Z             else:
2025-04-11T04:23:18.8793545Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8793713Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8793804Z                     error_index=error_index,
2025-04-11T04:23:18.8793904Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8793991Z                     exit_code=exitcode,
2025-04-11T04:23:18.8794063Z                 )
2025-04-11T04:23:18.8794133Z     
2025-04-11T04:23:18.8794262Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8794439Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8794522Z         msg += original_trace
2025-04-11T04:23:18.8794696Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8794860Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8794931Z E       
2025-04-11T04:23:18.8795060Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8795156Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8795454Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8795585Z E           fn(i, *args)
2025-04-11T04:23:18.8795887Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
2025-04-11T04:23:18.8795972Z E           check_linear_1d_col()
2025-04-11T04:23:18.8796228Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8796320Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8796637Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T04:23:18.8796739Z E           linear = nn.Linear(8, 80).cuda()
2025-04-11T04:23:18.8797007Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8797127Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8797395Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8797541Z E           param_applied = fn(param)
2025-04-11T04:23:18.8797815Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8797930Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8798038Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8798319Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8798503Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8798663Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8798668Z 
2025-04-11T04:23:18.8798972Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8799124Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8799282Z [04/11/25 04:20:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8799413Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8799565Z                              :75 launch                                         
2025-04-11T04:23:18.8799708Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8799832Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8800035Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8800165Z ________________________________ test_ring_attn ________________________________
2025-04-11T04:23:18.8800169Z 
2025-04-11T04:23:18.8800264Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8800872Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8800879Z 
2025-04-11T04:23:18.8800982Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8801059Z         try_count = 0
2025-04-11T04:23:18.8801157Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8801238Z             max_try, int
2025-04-11T04:23:18.8801383Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8801456Z     
2025-04-11T04:23:18.8801565Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8801638Z             try:
2025-04-11T04:23:18.8801724Z                 try_count += 1
2025-04-11T04:23:18.8801815Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8801895Z                 return ret
2025-04-11T04:23:18.8802034Z             except exception_type as e:
2025-04-11T04:23:18.8802132Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8802323Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8802440Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8802591Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8802744Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8802827Z                     continue
2025-04-11T04:23:18.8802905Z                 else:
2025-04-11T04:23:18.8803124Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8803204Z >                   raise e
2025-04-11T04:23:18.8803209Z 
2025-04-11T04:23:18.8803299Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8803414Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8803542Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8803681Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8803833Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T04:23:18.8803920Z     partial_func(**kwargs)
2025-04-11T04:23:18.8804100Z tests/test_shardformer/test_layer/test_ring_attn.py:181: in test_ring_attn
2025-04-11T04:23:18.8804212Z     spawn(launch_single_ring, nprocs=world_size)
2025-04-11T04:23:18.8804313Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8804464Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8804724Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8804900Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8805182Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8805272Z     while not context.join():
2025-04-11T04:23:18.8805381Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8805387Z 
2025-04-11T04:23:18.8805590Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8692200>
2025-04-11T04:23:18.8805715Z timeout = None
2025-04-11T04:23:18.8805720Z 
2025-04-11T04:23:18.8805809Z     def join(self, timeout=None):
2025-04-11T04:23:18.8805931Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8806002Z     
2025-04-11T04:23:18.8806151Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8806292Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8806456Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8806545Z         of the first process exiting.
2025-04-11T04:23:18.8806618Z     
2025-04-11T04:23:18.8806761Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8806896Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8806970Z     
2025-04-11T04:23:18.8807042Z         Args:
2025-04-11T04:23:18.8807185Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8807258Z         """
2025-04-11T04:23:18.8807397Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8807486Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8807564Z             return True
2025-04-11T04:23:18.8807637Z     
2025-04-11T04:23:18.8807768Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8807885Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8807974Z             self.sentinels.keys(),
2025-04-11T04:23:18.8808056Z             timeout=timeout,
2025-04-11T04:23:18.8808130Z         )
2025-04-11T04:23:18.8808252Z     
2025-04-11T04:23:18.8808336Z         error_index = None
2025-04-11T04:23:18.8808421Z         for sentinel in ready:
2025-04-11T04:23:18.8808529Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8808631Z             process = self.processes[index]
2025-04-11T04:23:18.8808713Z             process.join()
2025-04-11T04:23:18.8808812Z             if process.exitcode != 0:
2025-04-11T04:23:18.8808898Z                 error_index = index
2025-04-11T04:23:18.8808976Z                 break
2025-04-11T04:23:18.8809045Z     
2025-04-11T04:23:18.8809134Z         # Return if there was no error.
2025-04-11T04:23:18.8809223Z         if error_index is None:
2025-04-11T04:23:18.8809351Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8809449Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8809516Z     
2025-04-11T04:23:18.8809654Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8809755Z         for process in self.processes:
2025-04-11T04:23:18.8809841Z             if process.is_alive():
2025-04-11T04:23:18.8809933Z                 process.terminate()
2025-04-11T04:23:18.8810065Z             process.join()
2025-04-11T04:23:18.8810136Z     
2025-04-11T04:23:18.8810278Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8810394Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8810503Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8810626Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8810711Z             if exitcode < 0:
2025-04-11T04:23:18.8810864Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8810967Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8811117Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8811210Z                     error_index=error_index,
2025-04-11T04:23:18.8811316Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8811401Z                     exit_code=exitcode,
2025-04-11T04:23:18.8811489Z                     signal_name=name,
2025-04-11T04:23:18.8811560Z                 )
2025-04-11T04:23:18.8811631Z             else:
2025-04-11T04:23:18.8811734Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8812041Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8812136Z                     error_index=error_index,
2025-04-11T04:23:18.8812235Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8812321Z                     exit_code=exitcode,
2025-04-11T04:23:18.8812395Z                 )
2025-04-11T04:23:18.8812462Z     
2025-04-11T04:23:18.8812593Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8812761Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8812847Z         msg += original_trace
2025-04-11T04:23:18.8813017Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8813179Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8813255Z E       
2025-04-11T04:23:18.8813375Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8813479Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8813774Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8813856Z E           fn(i, *args)
2025-04-11T04:23:18.8814156Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
2025-04-11T04:23:18.8814239Z E           check_packed_seq()
2025-04-11T04:23:18.8814499Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8814639Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8814890Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8814978Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8815224Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8815309Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8815410Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.8815696Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
2025-04-11T04:23:18.8815872Z E           padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
2025-04-11T04:23:18.8815980Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8816261Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8816401Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8816556Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8816622Z 
2025-04-11T04:23:18.8816932Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8817082Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8817234Z [04/11/25 04:20:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8817364Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8817519Z                              :75 launch                                         
2025-04-11T04:23:18.8817659Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8817786Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8817991Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8818126Z _______________________________ test_double_ring _______________________________
2025-04-11T04:23:18.8818132Z 
2025-04-11T04:23:18.8818228Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8818835Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8818889Z 
2025-04-11T04:23:18.8818990Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8819071Z         try_count = 0
2025-04-11T04:23:18.8819167Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8819249Z             max_try, int
2025-04-11T04:23:18.8819393Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8819468Z     
2025-04-11T04:23:18.8819577Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8819650Z             try:
2025-04-11T04:23:18.8819736Z                 try_count += 1
2025-04-11T04:23:18.8819826Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8819906Z                 return ret
2025-04-11T04:23:18.8819997Z             except exception_type as e:
2025-04-11T04:23:18.8820097Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8820285Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8820400Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8820550Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8820704Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8820787Z                     continue
2025-04-11T04:23:18.8820861Z                 else:
2025-04-11T04:23:18.8821134Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8821218Z >                   raise e
2025-04-11T04:23:18.8821223Z 
2025-04-11T04:23:18.8821314Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8821426Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8821557Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8821645Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8821794Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T04:23:18.8821881Z     partial_func(**kwargs)
2025-04-11T04:23:18.8822069Z tests/test_shardformer/test_layer/test_ring_attn.py:187: in test_double_ring
2025-04-11T04:23:18.8822179Z     spawn(launch_double_ring, nprocs=world_size)
2025-04-11T04:23:18.8822280Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8822376Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8822634Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8822810Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8823147Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8823238Z     while not context.join():
2025-04-11T04:23:18.8823346Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8823350Z 
2025-04-11T04:23:18.8823552Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a88c40>
2025-04-11T04:23:18.8823680Z timeout = None
2025-04-11T04:23:18.8823685Z 
2025-04-11T04:23:18.8823777Z     def join(self, timeout=None):
2025-04-11T04:23:18.8823899Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8823968Z     
2025-04-11T04:23:18.8824116Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8824257Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8824420Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8824512Z         of the first process exiting.
2025-04-11T04:23:18.8824581Z     
2025-04-11T04:23:18.8824724Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8824908Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8824981Z     
2025-04-11T04:23:18.8825052Z         Args:
2025-04-11T04:23:18.8825192Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8825265Z         """
2025-04-11T04:23:18.8825401Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8825495Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8825571Z             return True
2025-04-11T04:23:18.8825643Z     
2025-04-11T04:23:18.8825771Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8825893Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8825981Z             self.sentinels.keys(),
2025-04-11T04:23:18.8826065Z             timeout=timeout,
2025-04-11T04:23:18.8826138Z         )
2025-04-11T04:23:18.8826206Z     
2025-04-11T04:23:18.8826290Z         error_index = None
2025-04-11T04:23:18.8826374Z         for sentinel in ready:
2025-04-11T04:23:18.8826476Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8826576Z             process = self.processes[index]
2025-04-11T04:23:18.8826658Z             process.join()
2025-04-11T04:23:18.8826752Z             if process.exitcode != 0:
2025-04-11T04:23:18.8826838Z                 error_index = index
2025-04-11T04:23:18.8826909Z                 break
2025-04-11T04:23:18.8826980Z     
2025-04-11T04:23:18.8827068Z         # Return if there was no error.
2025-04-11T04:23:18.8827153Z         if error_index is None:
2025-04-11T04:23:18.8827338Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8827435Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8827506Z     
2025-04-11T04:23:18.8827641Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8827739Z         for process in self.processes:
2025-04-11T04:23:18.8827827Z             if process.is_alive():
2025-04-11T04:23:18.8827919Z                 process.terminate()
2025-04-11T04:23:18.8827999Z             process.join()
2025-04-11T04:23:18.8828067Z     
2025-04-11T04:23:18.8828205Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8828321Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8828460Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8828580Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8828666Z             if exitcode < 0:
2025-04-11T04:23:18.8828769Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8828873Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8829024Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8829180Z                     error_index=error_index,
2025-04-11T04:23:18.8829282Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8829373Z                     exit_code=exitcode,
2025-04-11T04:23:18.8829457Z                     signal_name=name,
2025-04-11T04:23:18.8829531Z                 )
2025-04-11T04:23:18.8829602Z             else:
2025-04-11T04:23:18.8829704Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8829922Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8830016Z                     error_index=error_index,
2025-04-11T04:23:18.8830112Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8830199Z                     exit_code=exitcode,
2025-04-11T04:23:18.8830275Z                 )
2025-04-11T04:23:18.8830343Z     
2025-04-11T04:23:18.8830475Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8830646Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8830730Z         msg += original_trace
2025-04-11T04:23:18.8830956Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8831118Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8831197Z E       
2025-04-11T04:23:18.8831319Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8831422Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8831721Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8831799Z E           fn(i, *args)
2025-04-11T04:23:18.8832099Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
2025-04-11T04:23:18.8832202Z E           check_ring_attn(inner_ring_size=2)
2025-04-11T04:23:18.8832462Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8832549Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8832806Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8832892Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8833140Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8833229Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8833333Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.8833615Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
2025-04-11T04:23:18.8833874Z E           qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
2025-04-11T04:23:18.8833982Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8834265Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8834404Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8834565Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8834570Z 
2025-04-11T04:23:18.8834872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8835027Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8835181Z [04/11/25 04:20:10] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8835314Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8835421Z                              :75 launch                                         
2025-04-11T04:23:18.8835561Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8835732Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8835933Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8836071Z __________________________ test_all_to_all_attention ___________________________
2025-04-11T04:23:18.8836075Z 
2025-04-11T04:23:18.8836166Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8836798Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8836804Z 
2025-04-11T04:23:18.8836904Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8836986Z         try_count = 0
2025-04-11T04:23:18.8837085Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8837170Z             max_try, int
2025-04-11T04:23:18.8837314Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8837382Z     
2025-04-11T04:23:18.8837548Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8837621Z             try:
2025-04-11T04:23:18.8837705Z                 try_count += 1
2025-04-11T04:23:18.8837794Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8837875Z                 return ret
2025-04-11T04:23:18.8837969Z             except exception_type as e:
2025-04-11T04:23:18.8838067Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8839089Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8839206Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8839353Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8839506Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8839590Z                     continue
2025-04-11T04:23:18.8839665Z                 else:
2025-04-11T04:23:18.8839890Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8839974Z >                   raise e
2025-04-11T04:23:18.8839978Z 
2025-04-11T04:23:18.8840068Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8840183Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8840312Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8840400Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8840627Z tests/test_shardformer/test_layer/test_sequence_parallel.py:174: in test_all_to_all_attention
2025-04-11T04:23:18.8840797Z     spawn(check_all2all_attn, nprocs=4)
2025-04-11T04:23:18.8840898Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8840994Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8841257Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8841433Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8841721Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8841808Z     while not context.join():
2025-04-11T04:23:18.8841915Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8841920Z 
2025-04-11T04:23:18.8842125Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3c4c0>
2025-04-11T04:23:18.8842201Z timeout = None
2025-04-11T04:23:18.8842206Z 
2025-04-11T04:23:18.8842296Z     def join(self, timeout=None):
2025-04-11T04:23:18.8842420Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8842491Z     
2025-04-11T04:23:18.8842636Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8842828Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8842993Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8843085Z         of the first process exiting.
2025-04-11T04:23:18.8843158Z     
2025-04-11T04:23:18.8843303Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8843444Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8843563Z     
2025-04-11T04:23:18.8843635Z         Args:
2025-04-11T04:23:18.8843775Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8843846Z         """
2025-04-11T04:23:18.8843984Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8844076Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8844153Z             return True
2025-04-11T04:23:18.8844225Z     
2025-04-11T04:23:18.8844355Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8844471Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8844611Z             self.sentinels.keys(),
2025-04-11T04:23:18.8844698Z             timeout=timeout,
2025-04-11T04:23:18.8844768Z         )
2025-04-11T04:23:18.8844835Z     
2025-04-11T04:23:18.8844918Z         error_index = None
2025-04-11T04:23:18.8845001Z         for sentinel in ready:
2025-04-11T04:23:18.8845110Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8845205Z             process = self.processes[index]
2025-04-11T04:23:18.8845286Z             process.join()
2025-04-11T04:23:18.8845379Z             if process.exitcode != 0:
2025-04-11T04:23:18.8845465Z                 error_index = index
2025-04-11T04:23:18.8845541Z                 break
2025-04-11T04:23:18.8845610Z     
2025-04-11T04:23:18.8845697Z         # Return if there was no error.
2025-04-11T04:23:18.8845783Z         if error_index is None:
2025-04-11T04:23:18.8845915Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8846010Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8846079Z     
2025-04-11T04:23:18.8846218Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8846311Z         for process in self.processes:
2025-04-11T04:23:18.8846397Z             if process.is_alive():
2025-04-11T04:23:18.8846488Z                 process.terminate()
2025-04-11T04:23:18.8846570Z             process.join()
2025-04-11T04:23:18.8846641Z     
2025-04-11T04:23:18.8846777Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8846887Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8846994Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8847165Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8847247Z             if exitcode < 0:
2025-04-11T04:23:18.8847353Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8847457Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8847604Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8847698Z                     error_index=error_index,
2025-04-11T04:23:18.8847799Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8847886Z                     exit_code=exitcode,
2025-04-11T04:23:18.8847975Z                     signal_name=name,
2025-04-11T04:23:18.8848046Z                 )
2025-04-11T04:23:18.8848118Z             else:
2025-04-11T04:23:18.8848219Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8848380Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8848475Z                     error_index=error_index,
2025-04-11T04:23:18.8848572Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8848659Z                     exit_code=exitcode,
2025-04-11T04:23:18.8848781Z                 )
2025-04-11T04:23:18.8848848Z     
2025-04-11T04:23:18.8848982Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8849153Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8849239Z         msg += original_trace
2025-04-11T04:23:18.8849406Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8849617Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8849692Z E       
2025-04-11T04:23:18.8849812Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8849911Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8850207Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8850293Z E           fn(i, *args)
2025-04-11T04:23:18.8850616Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
2025-04-11T04:23:18.8850707Z E           run_seq_parallel_attn()
2025-04-11T04:23:18.8851035Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8851121Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8851374Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8851461Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8851711Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8851794Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8851895Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.8852229Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
2025-04-11T04:23:18.8852376Z E           seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
2025-04-11T04:23:18.8852690Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
2025-04-11T04:23:18.8852824Z E           x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
2025-04-11T04:23:18.8852934Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8853217Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8853355Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8853513Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8853518Z 
2025-04-11T04:23:18.8853875Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8854028Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8854183Z [04/11/25 04:20:15] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8854316Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8854419Z                              :75 launch                                         
2025-04-11T04:23:18.8854559Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8854684Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8854881Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8855014Z _____________________________ test_vocab_embedding _____________________________
2025-04-11T04:23:18.8855020Z 
2025-04-11T04:23:18.8855111Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8855713Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8855771Z 
2025-04-11T04:23:18.8855873Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8855958Z         try_count = 0
2025-04-11T04:23:18.8856058Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8856143Z             max_try, int
2025-04-11T04:23:18.8856324Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8856397Z     
2025-04-11T04:23:18.8856508Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8856581Z             try:
2025-04-11T04:23:18.8856667Z                 try_count += 1
2025-04-11T04:23:18.8856756Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8856841Z                 return ret
2025-04-11T04:23:18.8856932Z             except exception_type as e:
2025-04-11T04:23:18.8857033Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8857222Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8857388Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8857541Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8857696Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8857780Z                     continue
2025-04-11T04:23:18.8857855Z                 else:
2025-04-11T04:23:18.8858075Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8858160Z >                   raise e
2025-04-11T04:23:18.8858165Z 
2025-04-11T04:23:18.8858258Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8858369Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8858502Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8858593Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8858831Z tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py:54: in test_vocab_embedding
2025-04-11T04:23:18.8858921Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8859023Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8859120Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8859380Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8859555Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8859840Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8859981Z     while not context.join():
2025-04-11T04:23:18.8860089Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8860095Z 
2025-04-11T04:23:18.8860296Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581d79540>
2025-04-11T04:23:18.8860373Z timeout = None
2025-04-11T04:23:18.8860378Z 
2025-04-11T04:23:18.8860473Z     def join(self, timeout=None):
2025-04-11T04:23:18.8860596Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8860667Z     
2025-04-11T04:23:18.8860810Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8860952Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8861117Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8861206Z         of the first process exiting.
2025-04-11T04:23:18.8861275Z     
2025-04-11T04:23:18.8861416Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8861555Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8861622Z     
2025-04-11T04:23:18.8861750Z         Args:
2025-04-11T04:23:18.8861894Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8861965Z         """
2025-04-11T04:23:18.8862112Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8862201Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8862277Z             return True
2025-04-11T04:23:18.8862349Z     
2025-04-11T04:23:18.8862478Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8862648Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8862739Z             self.sentinels.keys(),
2025-04-11T04:23:18.8862822Z             timeout=timeout,
2025-04-11T04:23:18.8862896Z         )
2025-04-11T04:23:18.8862963Z     
2025-04-11T04:23:18.8863049Z         error_index = None
2025-04-11T04:23:18.8863135Z         for sentinel in ready:
2025-04-11T04:23:18.8863242Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8863341Z             process = self.processes[index]
2025-04-11T04:23:18.8863426Z             process.join()
2025-04-11T04:23:18.8863521Z             if process.exitcode != 0:
2025-04-11T04:23:18.8863674Z                 error_index = index
2025-04-11T04:23:18.8863752Z                 break
2025-04-11T04:23:18.8863822Z     
2025-04-11T04:23:18.8863912Z         # Return if there was no error.
2025-04-11T04:23:18.8864000Z         if error_index is None:
2025-04-11T04:23:18.8864133Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8864231Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8864299Z     
2025-04-11T04:23:18.8864436Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8864537Z         for process in self.processes:
2025-04-11T04:23:18.8864625Z             if process.is_alive():
2025-04-11T04:23:18.8864719Z                 process.terminate()
2025-04-11T04:23:18.8864801Z             process.join()
2025-04-11T04:23:18.8864874Z     
2025-04-11T04:23:18.8865013Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8865125Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8865236Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8865357Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8865441Z             if exitcode < 0:
2025-04-11T04:23:18.8865548Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8865653Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8865806Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8865897Z                     error_index=error_index,
2025-04-11T04:23:18.8866001Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8866146Z                     exit_code=exitcode,
2025-04-11T04:23:18.8866235Z                     signal_name=name,
2025-04-11T04:23:18.8866307Z                 )
2025-04-11T04:23:18.8866380Z             else:
2025-04-11T04:23:18.8866487Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8866652Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8866749Z                     error_index=error_index,
2025-04-11T04:23:18.8866848Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8866936Z                     exit_code=exitcode,
2025-04-11T04:23:18.8867007Z                 )
2025-04-11T04:23:18.8867077Z     
2025-04-11T04:23:18.8867211Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8867379Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8867467Z         msg += original_trace
2025-04-11T04:23:18.8867641Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8867800Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8867925Z E       
2025-04-11T04:23:18.8868050Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8868152Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8868476Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8868561Z E           fn(i, *args)
2025-04-11T04:23:18.8868885Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
2025-04-11T04:23:18.8869036Z E           check_vocab_embedding_1d()
2025-04-11T04:23:18.8869303Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8869390Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8869772Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
2025-04-11T04:23:18.8869890Z E           embedding = nn.Embedding(128, 32).to("cuda")
2025-04-11T04:23:18.8870166Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.8870318Z E           return self._apply(convert)
2025-04-11T04:23:18.8870588Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8870683Z E           param_applied = fn(param)
2025-04-11T04:23:18.8870960Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.8871180Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8871285Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8871590Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8871728Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8871890Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8871896Z 
2025-04-11T04:23:18.8872196Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8872346Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8872504Z [04/11/25 04:20:20] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8872632Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8872741Z                              :75 launch                                         
2025-04-11T04:23:18.8872876Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8873061Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8873262Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8873391Z __________________________________ test_bert ___________________________________
2025-04-11T04:23:18.8873397Z 
2025-04-11T04:23:18.8873487Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8873492Z 
2025-04-11T04:23:18.8873589Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8873671Z         try_count = 0
2025-04-11T04:23:18.8873768Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8873852Z             max_try, int
2025-04-11T04:23:18.8873995Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8874067Z     
2025-04-11T04:23:18.8874174Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8874248Z             try:
2025-04-11T04:23:18.8874335Z                 try_count += 1
2025-04-11T04:23:18.8874426Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8874431Z 
2025-04-11T04:23:18.8874583Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8874692Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8874810Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8874901Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8875054Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8875151Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8875310Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8875315Z 
2025-04-11T04:23:18.8875393Z device = None
2025-04-11T04:23:18.8875398Z 
2025-04-11T04:23:18.8875518Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8875672Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8875742Z     
2025-04-11T04:23:18.8875813Z         Args:
2025-04-11T04:23:18.8875989Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8876158Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8876270Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8876391Z         """
2025-04-11T04:23:18.8876470Z         _lazy_init()
2025-04-11T04:23:18.8876566Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8876668Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8876780Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8877071Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8877213Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8877373Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8877379Z 
2025-04-11T04:23:18.8877619Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8877754Z __________________________________ test_blip2 __________________________________
2025-04-11T04:23:18.8877760Z 
2025-04-11T04:23:18.8877848Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8877852Z 
2025-04-11T04:23:18.8877953Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8878031Z         try_count = 0
2025-04-11T04:23:18.8878129Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8878210Z             max_try, int
2025-04-11T04:23:18.8878357Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8878427Z     
2025-04-11T04:23:18.8878533Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8878610Z             try:
2025-04-11T04:23:18.8878691Z                 try_count += 1
2025-04-11T04:23:18.8878837Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8878842Z 
2025-04-11T04:23:18.8878932Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8879043Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8879159Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8879253Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8879411Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8879504Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8879611Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8879617Z 
2025-04-11T04:23:18.8879691Z device = None
2025-04-11T04:23:18.8879695Z 
2025-04-11T04:23:18.8879812Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8879967Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8880037Z     
2025-04-11T04:23:18.8880113Z         Args:
2025-04-11T04:23:18.8880280Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8880504Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8880609Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8880682Z         """
2025-04-11T04:23:18.8880762Z         _lazy_init()
2025-04-11T04:23:18.8880853Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8880956Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8881059Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8881397Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8881534Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8881689Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8881696Z 
2025-04-11T04:23:18.8881936Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8882066Z __________________________________ test_bloom __________________________________
2025-04-11T04:23:18.8882070Z 
2025-04-11T04:23:18.8882162Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8882215Z 
2025-04-11T04:23:18.8882316Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8882396Z         try_count = 0
2025-04-11T04:23:18.8882493Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8882573Z             max_try, int
2025-04-11T04:23:18.8882718Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8882785Z     
2025-04-11T04:23:18.8882896Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8882968Z             try:
2025-04-11T04:23:18.8883053Z                 try_count += 1
2025-04-11T04:23:18.8883144Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8883148Z 
2025-04-11T04:23:18.8883238Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8883348Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8883462Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8883560Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8883714Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8883811Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8883917Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8883923Z 
2025-04-11T04:23:18.8883996Z device = None
2025-04-11T04:23:18.8884000Z 
2025-04-11T04:23:18.8884118Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8884267Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8884339Z     
2025-04-11T04:23:18.8884409Z         Args:
2025-04-11T04:23:18.8884629Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8884795Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8884901Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8884974Z         """
2025-04-11T04:23:18.8885054Z         _lazy_init()
2025-04-11T04:23:18.8885149Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8885248Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8885353Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8885639Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8885773Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8885933Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8885939Z 
2025-04-11T04:23:18.8886173Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8886306Z _________________________________ test_chatglm _________________________________
2025-04-11T04:23:18.8886360Z 
2025-04-11T04:23:18.8886451Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8886458Z 
2025-04-11T04:23:18.8886556Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8886631Z         try_count = 0
2025-04-11T04:23:18.8886726Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8886806Z             max_try, int
2025-04-11T04:23:18.8886947Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8887087Z     
2025-04-11T04:23:18.8887194Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8887269Z             try:
2025-04-11T04:23:18.8887349Z                 try_count += 1
2025-04-11T04:23:18.8887437Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8887444Z 
2025-04-11T04:23:18.8887538Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8887644Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8887762Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8887852Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8888058Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8888149Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8888252Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8888256Z 
2025-04-11T04:23:18.8888336Z device = None
2025-04-11T04:23:18.8888340Z 
2025-04-11T04:23:18.8888454Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8888604Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8888673Z     
2025-04-11T04:23:18.8888746Z         Args:
2025-04-11T04:23:18.8888911Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8889076Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8889189Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8889258Z         """
2025-04-11T04:23:18.8889336Z         _lazy_init()
2025-04-11T04:23:18.8889428Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8889526Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8889633Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8889910Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8890052Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8890206Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8890211Z 
2025-04-11T04:23:18.8890445Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8890632Z _________________________________ test_command _________________________________
2025-04-11T04:23:18.8890638Z 
2025-04-11T04:23:18.8890732Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8890737Z 
2025-04-11T04:23:18.8890832Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8890912Z         try_count = 0
2025-04-11T04:23:18.8891013Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8891091Z             max_try, int
2025-04-11T04:23:18.8891235Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8891307Z     
2025-04-11T04:23:18.8891415Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8891486Z             try:
2025-04-11T04:23:18.8891565Z                 try_count += 1
2025-04-11T04:23:18.8891657Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8891662Z 
2025-04-11T04:23:18.8891754Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8891860Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8892024Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8892117Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8892268Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8892360Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8892466Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8892470Z 
2025-04-11T04:23:18.8892542Z device = None
2025-04-11T04:23:18.8892596Z 
2025-04-11T04:23:18.8892715Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8892862Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8892933Z     
2025-04-11T04:23:18.8893004Z         Args:
2025-04-11T04:23:18.8893167Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8893337Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8893442Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8893515Z         """
2025-04-11T04:23:18.8893590Z         _lazy_init()
2025-04-11T04:23:18.8893681Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8893833Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8893937Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8894223Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8894359Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8894516Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8894520Z 
2025-04-11T04:23:18.8894750Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8894886Z _______________________________ test_deepseek[4] _______________________________
2025-04-11T04:23:18.8894893Z 
2025-04-11T04:23:18.8895007Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.8895608Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8895619Z 
2025-04-11T04:23:18.8895715Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8895793Z         try_count = 0
2025-04-11T04:23:18.8895893Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8895971Z             max_try, int
2025-04-11T04:23:18.8896115Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8896182Z     
2025-04-11T04:23:18.8896289Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8896416Z             try:
2025-04-11T04:23:18.8896496Z                 try_count += 1
2025-04-11T04:23:18.8896587Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8896667Z                 return ret
2025-04-11T04:23:18.8896759Z             except exception_type as e:
2025-04-11T04:23:18.8896857Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8897043Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8897163Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8897307Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8897466Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8897546Z                     continue
2025-04-11T04:23:18.8897622Z                 else:
2025-04-11T04:23:18.8897844Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8897924Z >                   raise e
2025-04-11T04:23:18.8897929Z 
2025-04-11T04:23:18.8898075Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8898182Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8898316Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8898400Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8898593Z tests/test_shardformer/test_model/test_shard_deepseek.py:228: in test_deepseek
2025-04-11T04:23:18.8898686Z     spawn(check_deepseek, world_size)
2025-04-11T04:23:18.8898836Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8898937Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8899195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8899374Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8899661Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8899755Z     while not context.join():
2025-04-11T04:23:18.8899864Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8899868Z 
2025-04-11T04:23:18.8900118Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581bc8a30>
2025-04-11T04:23:18.8900198Z timeout = None
2025-04-11T04:23:18.8900202Z 
2025-04-11T04:23:18.8900289Z     def join(self, timeout=None):
2025-04-11T04:23:18.8900414Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8900485Z     
2025-04-11T04:23:18.8900630Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8900774Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8900937Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8901031Z         of the first process exiting.
2025-04-11T04:23:18.8901099Z     
2025-04-11T04:23:18.8901247Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8901385Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8901456Z     
2025-04-11T04:23:18.8901528Z         Args:
2025-04-11T04:23:18.8901666Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8901740Z         """
2025-04-11T04:23:18.8901875Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8901968Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8902045Z             return True
2025-04-11T04:23:18.8902113Z     
2025-04-11T04:23:18.8902253Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8902370Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8902462Z             self.sentinels.keys(),
2025-04-11T04:23:18.8902599Z             timeout=timeout,
2025-04-11T04:23:18.8902670Z         )
2025-04-11T04:23:18.8902741Z     
2025-04-11T04:23:18.8902821Z         error_index = None
2025-04-11T04:23:18.8902912Z         for sentinel in ready:
2025-04-11T04:23:18.8903015Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8903115Z             process = self.processes[index]
2025-04-11T04:23:18.8903199Z             process.join()
2025-04-11T04:23:18.8903288Z             if process.exitcode != 0:
2025-04-11T04:23:18.8903378Z                 error_index = index
2025-04-11T04:23:18.8903453Z                 break
2025-04-11T04:23:18.8903524Z     
2025-04-11T04:23:18.8903617Z         # Return if there was no error.
2025-04-11T04:23:18.8903701Z         if error_index is None:
2025-04-11T04:23:18.8903840Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8903933Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8904003Z     
2025-04-11T04:23:18.8904146Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8904238Z         for process in self.processes:
2025-04-11T04:23:18.8904328Z             if process.is_alive():
2025-04-11T04:23:18.8904505Z                 process.terminate()
2025-04-11T04:23:18.8904590Z             process.join()
2025-04-11T04:23:18.8904658Z     
2025-04-11T04:23:18.8904802Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8904915Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8905020Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8905142Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8905276Z             if exitcode < 0:
2025-04-11T04:23:18.8905385Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8905487Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8905636Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8905734Z                     error_index=error_index,
2025-04-11T04:23:18.8905832Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8905925Z                     exit_code=exitcode,
2025-04-11T04:23:18.8906009Z                     signal_name=name,
2025-04-11T04:23:18.8906084Z                 )
2025-04-11T04:23:18.8906206Z             else:
2025-04-11T04:23:18.8906308Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8906480Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8906571Z                     error_index=error_index,
2025-04-11T04:23:18.8906678Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8906763Z                     exit_code=exitcode,
2025-04-11T04:23:18.8906837Z                 )
2025-04-11T04:23:18.8906905Z     
2025-04-11T04:23:18.8907035Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8907206Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8907292Z         msg += original_trace
2025-04-11T04:23:18.8907465Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8907625Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8907696Z E       
2025-04-11T04:23:18.8907826Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8907922Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8908227Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8908308Z E           fn(i, *args)
2025-04-11T04:23:18.8908653Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
2025-04-11T04:23:18.8908740Z E           run_deepseek_test()
2025-04-11T04:23:18.8908993Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8909143Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8909448Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
2025-04-11T04:23:18.8909547Z E           run_deepseek_commom(config)
2025-04-11T04:23:18.8909849Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
2025-04-11T04:23:18.8910054Z E           torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
2025-04-11T04:23:18.8910345Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.8910447Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.8910714Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8910833Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8911105Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8911265Z E           module._apply(fn)
2025-04-11T04:23:18.8911538Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8911632Z E           param_applied = fn(param)
2025-04-11T04:23:18.8911914Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8912189Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8912294Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8912577Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8912710Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8912875Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8912880Z 
2025-04-11T04:23:18.8913185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8913340Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8913548Z [04/11/25 04:20:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8913680Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8913787Z                              :75 launch                                         
2025-04-11T04:23:18.8913922Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8914048Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8914135Z rank 0 testing (0, 1, 4, 1, 1)
2025-04-11T04:23:18.8914335Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8914477Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8915190Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8915277Z   warnings.warn(
2025-04-11T04:23:18.8915952Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8916036Z   warnings.warn(
2025-04-11T04:23:18.8916720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8916856Z   warnings.warn(
2025-04-11T04:23:18.8917543Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8917627Z   warnings.warn(
2025-04-11T04:23:18.8918452Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8918535Z   warnings.warn(
2025-04-11T04:23:18.8919340Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8919474Z   warnings.warn(
2025-04-11T04:23:18.8920276Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8920409Z   warnings.warn(
2025-04-11T04:23:18.8921211Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8921295Z   warnings.warn(
2025-04-11T04:23:18.8922097Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8922226Z   warnings.warn(
2025-04-11T04:23:18.8923025Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8923105Z   warnings.warn(
2025-04-11T04:23:18.8923893Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8923978Z   warnings.warn(
2025-04-11T04:23:18.8924785Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8924866Z   warnings.warn(
2025-04-11T04:23:18.8925668Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8925745Z   warnings.warn(
2025-04-11T04:23:18.8926579Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8926657Z   warnings.warn(
2025-04-11T04:23:18.8926947Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8927041Z - configuration_deepseek.py
2025-04-11T04:23:18.8927393Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8928187Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8928269Z   warnings.warn(
2025-04-11T04:23:18.8928542Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8928685Z - configuration_deepseek.py
2025-04-11T04:23:18.8929030Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8929848Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8929978Z   warnings.warn(
2025-04-11T04:23:18.8930251Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8930345Z - configuration_deepseek.py
2025-04-11T04:23:18.8930684Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8931492Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8931615Z   warnings.warn(
2025-04-11T04:23:18.8931880Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8931969Z - configuration_deepseek.py
2025-04-11T04:23:18.8932303Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8933095Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8933175Z   warnings.warn(
2025-04-11T04:23:18.8933959Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8934040Z   warnings.warn(
2025-04-11T04:23:18.8934821Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8934949Z   warnings.warn(
2025-04-11T04:23:18.8935738Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8935819Z   warnings.warn(
2025-04-11T04:23:18.8936604Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8936681Z   warnings.warn(
2025-04-11T04:23:18.8937468Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8937598Z   warnings.warn(
2025-04-11T04:23:18.8937866Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8937953Z - modeling_deepseek.py
2025-04-11T04:23:18.8938286Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8938550Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8938687Z - modeling_deepseek.py
2025-04-11T04:23:18.8939015Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8939272Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8939360Z - modeling_deepseek.py
2025-04-11T04:23:18.8939690Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8940001Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8940083Z - modeling_deepseek.py
2025-04-11T04:23:18.8940417Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8941213Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8941296Z   warnings.warn(
2025-04-11T04:23:18.8942083Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8942167Z   warnings.warn(
2025-04-11T04:23:18.8942948Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8943031Z   warnings.warn(
2025-04-11T04:23:18.8943166Z _____________________________ test_deepseek_v3[4] ______________________________
2025-04-11T04:23:18.8943170Z 
2025-04-11T04:23:18.8943287Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.8943962Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8943970Z 
2025-04-11T04:23:18.8944074Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8944157Z         try_count = 0
2025-04-11T04:23:18.8944256Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8944339Z             max_try, int
2025-04-11T04:23:18.8944484Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8944556Z     
2025-04-11T04:23:18.8944669Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8944741Z             try:
2025-04-11T04:23:18.8944826Z                 try_count += 1
2025-04-11T04:23:18.8944916Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8945000Z                 return ret
2025-04-11T04:23:18.8945092Z             except exception_type as e:
2025-04-11T04:23:18.8945192Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8945436Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8945554Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8945708Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8945863Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8945997Z                     continue
2025-04-11T04:23:18.8946073Z                 else:
2025-04-11T04:23:18.8946297Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8946379Z >                   raise e
2025-04-11T04:23:18.8946383Z 
2025-04-11T04:23:18.8946477Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8946592Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8946726Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8946815Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8947018Z tests/test_shardformer/test_model/test_shard_deepseek_v3.py:100: in test_deepseek_v3
2025-04-11T04:23:18.8947164Z     spawn(check_deepseek_v3, world_size)
2025-04-11T04:23:18.8947266Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8947365Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8947628Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8947809Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8948094Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8948185Z     while not context.join():
2025-04-11T04:23:18.8948296Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8948300Z 
2025-04-11T04:23:18.8948528Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581e0dde0>
2025-04-11T04:23:18.8948605Z timeout = None
2025-04-11T04:23:18.8948610Z 
2025-04-11T04:23:18.8948702Z     def join(self, timeout=None):
2025-04-11T04:23:18.8948828Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8948899Z     
2025-04-11T04:23:18.8949042Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8949182Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8949351Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8949443Z         of the first process exiting.
2025-04-11T04:23:18.8949514Z     
2025-04-11T04:23:18.8949661Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8949857Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8949930Z     
2025-04-11T04:23:18.8950000Z         Args:
2025-04-11T04:23:18.8950145Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8950215Z         """
2025-04-11T04:23:18.8950353Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8950447Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8950525Z             return True
2025-04-11T04:23:18.8950597Z     
2025-04-11T04:23:18.8950726Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8950847Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8950936Z             self.sentinels.keys(),
2025-04-11T04:23:18.8951019Z             timeout=timeout,
2025-04-11T04:23:18.8951092Z         )
2025-04-11T04:23:18.8951160Z     
2025-04-11T04:23:18.8951245Z         error_index = None
2025-04-11T04:23:18.8951328Z         for sentinel in ready:
2025-04-11T04:23:18.8951436Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8951536Z             process = self.processes[index]
2025-04-11T04:23:18.8951677Z             process.join()
2025-04-11T04:23:18.8951772Z             if process.exitcode != 0:
2025-04-11T04:23:18.8951858Z                 error_index = index
2025-04-11T04:23:18.8951938Z                 break
2025-04-11T04:23:18.8952005Z     
2025-04-11T04:23:18.8952093Z         # Return if there was no error.
2025-04-11T04:23:18.8952179Z         if error_index is None:
2025-04-11T04:23:18.8952313Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8952463Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8952532Z     
2025-04-11T04:23:18.8952671Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8952769Z         for process in self.processes:
2025-04-11T04:23:18.8952856Z             if process.is_alive():
2025-04-11T04:23:18.8952951Z                 process.terminate()
2025-04-11T04:23:18.8953032Z             process.join()
2025-04-11T04:23:18.8953103Z     
2025-04-11T04:23:18.8953244Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8953357Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8953467Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8953638Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8953724Z             if exitcode < 0:
2025-04-11T04:23:18.8953830Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8953937Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8954089Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8954181Z                     error_index=error_index,
2025-04-11T04:23:18.8954283Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8954368Z                     exit_code=exitcode,
2025-04-11T04:23:18.8954456Z                     signal_name=name,
2025-04-11T04:23:18.8954527Z                 )
2025-04-11T04:23:18.8954598Z             else:
2025-04-11T04:23:18.8954705Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8954867Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8954963Z                     error_index=error_index,
2025-04-11T04:23:18.8955063Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8955147Z                     exit_code=exitcode,
2025-04-11T04:23:18.8955220Z                 )
2025-04-11T04:23:18.8955290Z     
2025-04-11T04:23:18.8955422Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8955591Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8955680Z         msg += original_trace
2025-04-11T04:23:18.8955850Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8956065Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8956141Z E       
2025-04-11T04:23:18.8956268Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8956369Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8956666Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8956751Z E           fn(i, *args)
2025-04-11T04:23:18.8957078Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
2025-04-11T04:23:18.8957168Z E           run_deepseek_v3_test()
2025-04-11T04:23:18.8957427Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8957514Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8957832Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
2025-04-11T04:23:18.8957921Z E           check_forward_backward(
2025-04-11T04:23:18.8958295Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
2025-04-11T04:23:18.8958566Z E           org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
2025-04-11T04:23:18.8958864Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
2025-04-11T04:23:18.8959011Z E           org_model = org_model.cuda()
2025-04-11T04:23:18.8959297Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.8959403Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.8959665Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8959789Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8960060Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8960148Z E           module._apply(fn)
2025-04-11T04:23:18.8960462Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8960548Z E           module._apply(fn)
2025-04-11T04:23:18.8960814Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8960910Z E           param_applied = fn(param)
2025-04-11T04:23:18.8961184Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8961298Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8961410Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8961694Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8961831Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8961997Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8962004Z 
2025-04-11T04:23:18.8962306Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8962462Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8962619Z [04/11/25 04:20:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8962750Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8962856Z                              :75 launch                                         
2025-04-11T04:23:18.8963048Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8963173Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8963368Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8963518Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8964666Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8964842Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8965953Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8966173Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8967282Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8967499Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8968592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8968824Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8969505Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8969593Z   warnings.warn(
2025-04-11T04:23:18.8970272Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8970359Z   warnings.warn(
2025-04-11T04:23:18.8971022Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8971106Z   warnings.warn(
2025-04-11T04:23:18.8971777Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8971859Z   warnings.warn(
2025-04-11T04:23:18.8972738Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8972822Z   warnings.warn(
2025-04-11T04:23:18.8973638Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8973722Z   warnings.warn(
2025-04-11T04:23:18.8974528Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8974611Z   warnings.warn(
2025-04-11T04:23:18.8975415Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8975546Z   warnings.warn(
2025-04-11T04:23:18.8976352Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8976480Z   warnings.warn(
2025-04-11T04:23:18.8977281Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8977364Z   warnings.warn(
2025-04-11T04:23:18.8978162Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8978286Z   warnings.warn(
2025-04-11T04:23:18.8979110Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8979184Z   warnings.warn(
2025-04-11T04:23:18.8980008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8980087Z   warnings.warn(
2025-04-11T04:23:18.8980889Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8980967Z   warnings.warn(
2025-04-11T04:23:18.8981787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8981914Z   warnings.warn(
2025-04-11T04:23:18.8982214Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44807 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8983011Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8983099Z   warnings.warn(
2025-04-11T04:23:18.8983891Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8983971Z   warnings.warn(
2025-04-11T04:23:18.8984754Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8984889Z   warnings.warn(
2025-04-11T04:23:18.8985671Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8985804Z   warnings.warn(
2025-04-11T04:23:18.8986590Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8986671Z   warnings.warn(
2025-04-11T04:23:18.8987467Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8987593Z   warnings.warn(
2025-04-11T04:23:18.8988397Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8988520Z   warnings.warn(
2025-04-11T04:23:18.8989310Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8989390Z   warnings.warn(
2025-04-11T04:23:18.8990175Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8990254Z   warnings.warn(
2025-04-11T04:23:18.8991081Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8991215Z   warnings.warn(
2025-04-11T04:23:18.8992005Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8992085Z   warnings.warn(
2025-04-11T04:23:18.8992340Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.8992431Z - configuration_deepseek.py
2025-04-11T04:23:18.8992774Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8993570Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8993647Z   warnings.warn(
2025-04-11T04:23:18.8993966Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.8994057Z - configuration_deepseek.py
2025-04-11T04:23:18.8994406Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8995210Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8995346Z   warnings.warn(
2025-04-11T04:23:18.8995590Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.8995685Z - configuration_deepseek.py
2025-04-11T04:23:18.8996019Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8996261Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.8996406Z - configuration_deepseek.py
2025-04-11T04:23:18.8996740Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8997553Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8997630Z   warnings.warn(
2025-04-11T04:23:18.8998428Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8998507Z   warnings.warn(
2025-04-11T04:23:18.8999299Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8999377Z   warnings.warn(
2025-04-11T04:23:18.9000167Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9000293Z   warnings.warn(
2025-04-11T04:23:18.9001080Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9001161Z   warnings.warn(
2025-04-11T04:23:18.9001952Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9002030Z   warnings.warn(
2025-04-11T04:23:18.9002824Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9002954Z   warnings.warn(
2025-04-11T04:23:18.9003196Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.9003288Z - modeling_deepseek.py
2025-04-11T04:23:18.9003623Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.9003863Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.9004013Z - modeling_deepseek.py
2025-04-11T04:23:18.9004346Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.9004582Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.9004669Z - modeling_deepseek.py
2025-04-11T04:23:18.9005006Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.9005237Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.9005376Z - modeling_deepseek.py
2025-04-11T04:23:18.9005706Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.9005855Z _________________________________ test_falcon __________________________________
2025-04-11T04:23:18.9005860Z 
2025-04-11T04:23:18.9005952Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9005956Z 
2025-04-11T04:23:18.9006063Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9006144Z         try_count = 0
2025-04-11T04:23:18.9006249Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9006328Z             max_try, int
2025-04-11T04:23:18.9006480Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9006558Z     
2025-04-11T04:23:18.9006671Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9006752Z             try:
2025-04-11T04:23:18.9006837Z                 try_count += 1
2025-04-11T04:23:18.9006930Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9006940Z 
2025-04-11T04:23:18.9007032Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9007148Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9007267Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9007360Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9007524Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9007673Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9007783Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9007794Z 
2025-04-11T04:23:18.9007870Z device = None
2025-04-11T04:23:18.9007874Z 
2025-04-11T04:23:18.9007996Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9008157Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9008227Z     
2025-04-11T04:23:18.9008303Z         Args:
2025-04-11T04:23:18.9008477Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9008644Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9008760Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9008832Z         """
2025-04-11T04:23:18.9008914Z         _lazy_init()
2025-04-11T04:23:18.9009008Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9009114Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9009220Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9009516Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9009714Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9009877Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9009882Z 
2025-04-11T04:23:18.9010126Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9010294Z __________________________________ test_gpt2 ___________________________________
2025-04-11T04:23:18.9010298Z 
2025-04-11T04:23:18.9010397Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9010402Z 
2025-04-11T04:23:18.9010501Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9010586Z         try_count = 0
2025-04-11T04:23:18.9010689Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9010769Z             max_try, int
2025-04-11T04:23:18.9010917Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9010988Z     
2025-04-11T04:23:18.9011101Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9011225Z             try:
2025-04-11T04:23:18.9011308Z                 try_count += 1
2025-04-11T04:23:18.9011406Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9011410Z 
2025-04-11T04:23:18.9011502Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9011616Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9011731Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9011829Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9011985Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9012078Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9012192Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9012197Z 
2025-04-11T04:23:18.9012271Z device = None
2025-04-11T04:23:18.9012277Z 
2025-04-11T04:23:18.9012401Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9012552Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9012629Z     
2025-04-11T04:23:18.9012702Z         Args:
2025-04-11T04:23:18.9012869Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9013040Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9013149Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9013226Z         """
2025-04-11T04:23:18.9013305Z         _lazy_init()
2025-04-11T04:23:18.9013405Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9013506Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9013665Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9013958Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9014099Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9014267Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9014272Z 
2025-04-11T04:23:18.9014507Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9014644Z __________________________________ test_llama __________________________________
2025-04-11T04:23:18.9014650Z 
2025-04-11T04:23:18.9014739Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9014743Z 
2025-04-11T04:23:18.9014845Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9014924Z         try_count = 0
2025-04-11T04:23:18.9015023Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9015111Z             max_try, int
2025-04-11T04:23:18.9015258Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9015385Z     
2025-04-11T04:23:18.9015496Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9015571Z             try:
2025-04-11T04:23:18.9015660Z                 try_count += 1
2025-04-11T04:23:18.9015750Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9015755Z 
2025-04-11T04:23:18.9015853Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9015963Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9016138Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9016232Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9016387Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9016484Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9016591Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9016598Z 
2025-04-11T04:23:18.9016682Z device = None
2025-04-11T04:23:18.9016686Z 
2025-04-11T04:23:18.9016809Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9016962Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9017081Z     
2025-04-11T04:23:18.9017156Z         Args:
2025-04-11T04:23:18.9017331Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9017500Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9017617Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9017691Z         """
2025-04-11T04:23:18.9017775Z         _lazy_init()
2025-04-11T04:23:18.9017868Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9017967Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9018080Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9018367Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9018512Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9018672Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9018678Z 
2025-04-11T04:23:18.9018917Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9019050Z _________________________________ test_mistral _________________________________
2025-04-11T04:23:18.9019056Z 
2025-04-11T04:23:18.9019147Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9019156Z 
2025-04-11T04:23:18.9019254Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9019333Z         try_count = 0
2025-04-11T04:23:18.9019435Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9019568Z             max_try, int
2025-04-11T04:23:18.9019718Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9019788Z     
2025-04-11T04:23:18.9019900Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9019981Z             try:
2025-04-11T04:23:18.9020063Z                 try_count += 1
2025-04-11T04:23:18.9020235Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9020240Z 
2025-04-11T04:23:18.9020424Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9020584Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9020821Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9020983Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9021167Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9021310Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9021474Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9021481Z 
2025-04-11T04:23:18.9021632Z device = None
2025-04-11T04:23:18.9021637Z 
2025-04-11T04:23:18.9021782Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9022046Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9022155Z     
2025-04-11T04:23:18.9022245Z         Args:
2025-04-11T04:23:18.9022491Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9022683Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9022850Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9023009Z         """
2025-04-11T04:23:18.9023135Z         _lazy_init()
2025-04-11T04:23:18.9023269Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9023411Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9023576Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9023891Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9024096Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9024267Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9024322Z 
2025-04-11T04:23:18.9024732Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9024896Z _______________________________ test_mixtral[4] ________________________________
2025-04-11T04:23:18.9024901Z 
2025-04-11T04:23:18.9025089Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9025727Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9025736Z 
2025-04-11T04:23:18.9025882Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9025997Z         try_count = 0
2025-04-11T04:23:18.9026137Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9026288Z             max_try, int
2025-04-11T04:23:18.9026465Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9026597Z     
2025-04-11T04:23:18.9026721Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9026828Z             try:
2025-04-11T04:23:18.9026985Z                 try_count += 1
2025-04-11T04:23:18.9027113Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9027251Z                 return ret
2025-04-11T04:23:18.9027373Z             except exception_type as e:
2025-04-11T04:23:18.9027523Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9027752Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9027957Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9028161Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9028349Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9028610Z                     continue
2025-04-11T04:23:18.9028728Z                 else:
2025-04-11T04:23:18.9029037Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9029148Z >                   raise e
2025-04-11T04:23:18.9029154Z 
2025-04-11T04:23:18.9029279Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9029443Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9029590Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9029768Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9029983Z tests/test_shardformer/test_model/test_shard_mixtral.py:222: in test_mixtral
2025-04-11T04:23:18.9030144Z     spawn(check_mixtral, world_size)
2025-04-11T04:23:18.9030278Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9030464Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9030784Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9031007Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9031349Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9031471Z     while not context.join():
2025-04-11T04:23:18.9031690Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9031695Z 
2025-04-11T04:23:18.9031918Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a214b0>
2025-04-11T04:23:18.9032082Z timeout = None
2025-04-11T04:23:18.9032087Z 
2025-04-11T04:23:18.9032210Z     def join(self, timeout=None):
2025-04-11T04:23:18.9032366Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9032575Z     
2025-04-11T04:23:18.9032764Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9032969Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9033226Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9033384Z         of the first process exiting.
2025-04-11T04:23:18.9033485Z     
2025-04-11T04:23:18.9033669Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9033858Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9033965Z     
2025-04-11T04:23:18.9034112Z         Args:
2025-04-11T04:23:18.9034277Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9034415Z         """
2025-04-11T04:23:18.9034585Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9034695Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9034856Z             return True
2025-04-11T04:23:18.9034956Z     
2025-04-11T04:23:18.9035156Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9035306Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9035429Z             self.sentinels.keys(),
2025-04-11T04:23:18.9035568Z             timeout=timeout,
2025-04-11T04:23:18.9035682Z         )
2025-04-11T04:23:18.9035811Z     
2025-04-11T04:23:18.9036019Z         error_index = None
2025-04-11T04:23:18.9036169Z         for sentinel in ready:
2025-04-11T04:23:18.9036290Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9036425Z             process = self.processes[index]
2025-04-11T04:23:18.9036582Z             process.join()
2025-04-11T04:23:18.9036712Z             if process.exitcode != 0:
2025-04-11T04:23:18.9036867Z                 error_index = index
2025-04-11T04:23:18.9037030Z                 break
2025-04-11T04:23:18.9037152Z     
2025-04-11T04:23:18.9037283Z         # Return if there was no error.
2025-04-11T04:23:18.9037409Z         if error_index is None:
2025-04-11T04:23:18.9037601Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9037727Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9037841Z     
2025-04-11T04:23:18.9038018Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9038153Z         for process in self.processes:
2025-04-11T04:23:18.9038312Z             if process.is_alive():
2025-04-11T04:23:18.9038436Z                 process.terminate()
2025-04-11T04:23:18.9038583Z             process.join()
2025-04-11T04:23:18.9038669Z     
2025-04-11T04:23:18.9038897Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9039041Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9039259Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9039442Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9039610Z             if exitcode < 0:
2025-04-11T04:23:18.9039780Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9039937Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9040124Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9040281Z                     error_index=error_index,
2025-04-11T04:23:18.9040411Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9040548Z                     exit_code=exitcode,
2025-04-11T04:23:18.9040727Z                     signal_name=name,
2025-04-11T04:23:18.9040870Z                 )
2025-04-11T04:23:18.9040972Z             else:
2025-04-11T04:23:18.9041105Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9041332Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9041451Z                     error_index=error_index,
2025-04-11T04:23:18.9041632Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9041752Z                     exit_code=exitcode,
2025-04-11T04:23:18.9041880Z                 )
2025-04-11T04:23:18.9041977Z     
2025-04-11T04:23:18.9042149Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9042422Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9042630Z         msg += original_trace
2025-04-11T04:23:18.9042863Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9043054Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9043198Z E       
2025-04-11T04:23:18.9043344Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9043480Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9043854Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9043966Z E           fn(i, *args)
2025-04-11T04:23:18.9044340Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
2025-04-11T04:23:18.9044456Z E           run_mixtral_test()
2025-04-11T04:23:18.9044773Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9044904Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9045229Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
2025-04-11T04:23:18.9045393Z E           run_mixtral_commom(config)
2025-04-11T04:23:18.9045724Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
2025-04-11T04:23:18.9045900Z E           torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T04:23:18.9046279Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9046454Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9046757Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9046936Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9047231Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9047335Z E           module._apply(fn)
2025-04-11T04:23:18.9047752Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9047876Z E           param_applied = fn(param)
2025-04-11T04:23:18.9048223Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9048375Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9048527Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9048898Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9049117Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9049305Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9049311Z 
2025-04-11T04:23:18.9049641Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9049901Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9050073Z [04/11/25 04:20:47] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9050292Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9050433Z                              :75 launch                                         
2025-04-11T04:23:18.9050632Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9050785Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9051078Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9051280Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9052424Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.9053578Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.9054720Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.9055954Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.9056182Z ________________________________ test_OPTModel _________________________________
2025-04-11T04:23:18.9056187Z 
2025-04-11T04:23:18.9056321Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9056359Z 
2025-04-11T04:23:18.9056489Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9056597Z         try_count = 0
2025-04-11T04:23:18.9056767Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9056867Z             max_try, int
2025-04-11T04:23:18.9057095Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9057197Z     
2025-04-11T04:23:18.9057338Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9057539Z             try:
2025-04-11T04:23:18.9057650Z                 try_count += 1
2025-04-11T04:23:18.9057790Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9057798Z 
2025-04-11T04:23:18.9057929Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9058098Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9058242Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9058421Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9058625Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9058757Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9058931Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9058936Z 
2025-04-11T04:23:18.9059039Z device = None
2025-04-11T04:23:18.9059044Z 
2025-04-11T04:23:18.9059315Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9059499Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9059623Z     
2025-04-11T04:23:18.9059742Z         Args:
2025-04-11T04:23:18.9059946Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9060240Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9060377Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9060493Z         """
2025-04-11T04:23:18.9060611Z         _lazy_init()
2025-04-11T04:23:18.9060751Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9060926Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9061064Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9061418Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9061573Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9061813Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9061818Z 
2025-04-11T04:23:18.9062099Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9062289Z __________________________________ test_qwen2 __________________________________
2025-04-11T04:23:18.9062293Z 
2025-04-11T04:23:18.9062412Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9062420Z 
2025-04-11T04:23:18.9062562Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9062679Z         try_count = 0
2025-04-11T04:23:18.9062827Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9063050Z             max_try, int
2025-04-11T04:23:18.9063224Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9063403Z     
2025-04-11T04:23:18.9063531Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9070349Z             try:
2025-04-11T04:23:18.9070464Z                 try_count += 1
2025-04-11T04:23:18.9070602Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9070608Z 
2025-04-11T04:23:18.9070748Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9070900Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9071033Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9071134Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9071312Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9071415Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9071537Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9071542Z 
2025-04-11T04:23:18.9071623Z device = None
2025-04-11T04:23:18.9071631Z 
2025-04-11T04:23:18.9071766Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9071930Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9072107Z     
2025-04-11T04:23:18.9072190Z         Args:
2025-04-11T04:23:18.9072367Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9072548Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9072663Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9072741Z         """
2025-04-11T04:23:18.9072827Z         _lazy_init()
2025-04-11T04:23:18.9073006Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9073119Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9073239Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9073550Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9073703Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9073869Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9073880Z 
2025-04-11T04:23:18.9074134Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9074334Z ___________________________________ test_sam ___________________________________
2025-04-11T04:23:18.9074339Z 
2025-04-11T04:23:18.9074440Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9074445Z 
2025-04-11T04:23:18.9074554Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9074640Z         try_count = 0
2025-04-11T04:23:18.9074742Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9074831Z             max_try, int
2025-04-11T04:23:18.9074984Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9075062Z     
2025-04-11T04:23:18.9075184Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9075264Z             try:
2025-04-11T04:23:18.9075357Z                 try_count += 1
2025-04-11T04:23:18.9075458Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9075463Z 
2025-04-11T04:23:18.9075566Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9075687Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9075810Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9075914Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9076083Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9076191Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9076309Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9076313Z 
2025-04-11T04:23:18.9076399Z device = None
2025-04-11T04:23:18.9076403Z 
2025-04-11T04:23:18.9076530Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9076775Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9076858Z     
2025-04-11T04:23:18.9076933Z         Args:
2025-04-11T04:23:18.9077107Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9077282Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9077393Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9077473Z         """
2025-04-11T04:23:18.9077553Z         _lazy_init()
2025-04-11T04:23:18.9077656Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9077763Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9077880Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9078169Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9078311Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9078479Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9078535Z 
2025-04-11T04:23:18.9078782Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9078924Z ___________________________________ test_t5 ____________________________________
2025-04-11T04:23:18.9078928Z 
2025-04-11T04:23:18.9079021Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9079026Z 
2025-04-11T04:23:18.9079131Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9079256Z         try_count = 0
2025-04-11T04:23:18.9079363Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9079447Z             max_try, int
2025-04-11T04:23:18.9079593Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9079671Z     
2025-04-11T04:23:18.9079785Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9079867Z             try:
2025-04-11T04:23:18.9079955Z                 try_count += 1
2025-04-11T04:23:18.9080051Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9080061Z 
2025-04-11T04:23:18.9080156Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9080269Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9080442Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9080540Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9080708Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9080810Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9080924Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9080933Z 
2025-04-11T04:23:18.9081015Z device = None
2025-04-11T04:23:18.9081019Z 
2025-04-11T04:23:18.9081142Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9081308Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9081383Z     
2025-04-11T04:23:18.9081465Z         Args:
2025-04-11T04:23:18.9081639Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9081816Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9081935Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9082013Z         """
2025-04-11T04:23:18.9082101Z         _lazy_init()
2025-04-11T04:23:18.9082203Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9082316Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9082425Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9082719Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9082871Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9103651Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9103667Z 
2025-04-11T04:23:18.9103942Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9104074Z ___________________________________ test_vit ___________________________________
2025-04-11T04:23:18.9104081Z 
2025-04-11T04:23:18.9104179Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9104183Z 
2025-04-11T04:23:18.9104283Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9104367Z         try_count = 0
2025-04-11T04:23:18.9104471Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9104552Z             max_try, int
2025-04-11T04:23:18.9104703Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9104774Z     
2025-04-11T04:23:18.9104890Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9104967Z             try:
2025-04-11T04:23:18.9105051Z                 try_count += 1
2025-04-11T04:23:18.9105148Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9105213Z 
2025-04-11T04:23:18.9105309Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9105429Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9105547Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9105646Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9105814Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9105917Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9106120Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9106125Z 
2025-04-11T04:23:18.9106202Z device = None
2025-04-11T04:23:18.9106207Z 
2025-04-11T04:23:18.9106333Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9106485Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9106560Z     
2025-04-11T04:23:18.9106633Z         Args:
2025-04-11T04:23:18.9106802Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9106970Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9107157Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9107231Z         """
2025-04-11T04:23:18.9107310Z         _lazy_init()
2025-04-11T04:23:18.9107407Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9107507Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9107613Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9107907Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9108044Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9108205Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9108209Z 
2025-04-11T04:23:18.9108483Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9108625Z _________________________________ test_whisper _________________________________
2025-04-11T04:23:18.9108631Z 
2025-04-11T04:23:18.9108721Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9108726Z 
2025-04-11T04:23:18.9108828Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9108905Z         try_count = 0
2025-04-11T04:23:18.9109003Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9109086Z             max_try, int
2025-04-11T04:23:18.9109230Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9109303Z     
2025-04-11T04:23:18.9109413Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9109486Z             try:
2025-04-11T04:23:18.9109636Z                 try_count += 1
2025-04-11T04:23:18.9109725Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9109730Z 
2025-04-11T04:23:18.9109826Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9109936Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9110055Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9110146Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9110297Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9110394Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9110501Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9110507Z 
2025-04-11T04:23:18.9110585Z device = None
2025-04-11T04:23:18.9110589Z 
2025-04-11T04:23:18.9110706Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9110857Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9110927Z     
2025-04-11T04:23:18.9110997Z         Args:
2025-04-11T04:23:18.9111167Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9111387Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9111496Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9111569Z         """
2025-04-11T04:23:18.9111648Z         _lazy_init()
2025-04-11T04:23:18.9111739Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9111838Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9112000Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9112289Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9112431Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9112588Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9112594Z 
2025-04-11T04:23:18.9112837Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9112970Z ________________________________ test_comm_spec ________________________________
2025-04-11T04:23:18.9112975Z 
2025-04-11T04:23:18.9113137Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9113763Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9113771Z 
2025-04-11T04:23:18.9113871Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9113952Z         try_count = 0
2025-04-11T04:23:18.9114049Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9114131Z             max_try, int
2025-04-11T04:23:18.9114276Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9114347Z     
2025-04-11T04:23:18.9114455Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9114530Z             try:
2025-04-11T04:23:18.9114615Z                 try_count += 1
2025-04-11T04:23:18.9114705Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9114789Z                 return ret
2025-04-11T04:23:18.9114883Z             except exception_type as e:
2025-04-11T04:23:18.9114980Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9115172Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9115291Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9115438Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9115593Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9115729Z                     continue
2025-04-11T04:23:18.9115804Z                 else:
2025-04-11T04:23:18.9116030Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9116119Z >                   raise e
2025-04-11T04:23:18.9116124Z 
2025-04-11T04:23:18.9116219Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9116330Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9116465Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9116552Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9116718Z tests/test_tensor/test_comm_spec_apply.py:211: in test_comm_spec
2025-04-11T04:23:18.9116809Z     spawn(check_comm, world_size)
2025-04-11T04:23:18.9116909Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9117012Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9117270Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9117456Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9117795Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9117889Z     while not context.join():
2025-04-11T04:23:18.9118000Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9118004Z 
2025-04-11T04:23:18.9118203Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5ad0f40>
2025-04-11T04:23:18.9118285Z timeout = None
2025-04-11T04:23:18.9118337Z 
2025-04-11T04:23:18.9118429Z     def join(self, timeout=None):
2025-04-11T04:23:18.9118558Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9118626Z     
2025-04-11T04:23:18.9118776Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9118919Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9119087Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9119187Z         of the first process exiting.
2025-04-11T04:23:18.9119255Z     
2025-04-11T04:23:18.9119408Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9119593Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9119665Z     
2025-04-11T04:23:18.9119738Z         Args:
2025-04-11T04:23:18.9119874Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9119951Z         """
2025-04-11T04:23:18.9120090Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9120187Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9120266Z             return True
2025-04-11T04:23:18.9120336Z     
2025-04-11T04:23:18.9120470Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9120592Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9120690Z             self.sentinels.keys(),
2025-04-11T04:23:18.9120775Z             timeout=timeout,
2025-04-11T04:23:18.9120855Z         )
2025-04-11T04:23:18.9120924Z     
2025-04-11T04:23:18.9121007Z         error_index = None
2025-04-11T04:23:18.9121097Z         for sentinel in ready:
2025-04-11T04:23:18.9121206Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9121309Z             process = self.processes[index]
2025-04-11T04:23:18.9121395Z             process.join()
2025-04-11T04:23:18.9121488Z             if process.exitcode != 0:
2025-04-11T04:23:18.9121584Z                 error_index = index
2025-04-11T04:23:18.9121660Z                 break
2025-04-11T04:23:18.9121732Z     
2025-04-11T04:23:18.9121823Z         # Return if there was no error.
2025-04-11T04:23:18.9121907Z         if error_index is None:
2025-04-11T04:23:18.9122047Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9122196Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9122272Z     
2025-04-11T04:23:18.9122412Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9122515Z         for process in self.processes:
2025-04-11T04:23:18.9122603Z             if process.is_alive():
2025-04-11T04:23:18.9122699Z                 process.terminate()
2025-04-11T04:23:18.9122788Z             process.join()
2025-04-11T04:23:18.9122858Z     
2025-04-11T04:23:18.9123001Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9123118Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9123227Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9123353Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9123437Z             if exitcode < 0:
2025-04-11T04:23:18.9123552Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9123658Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9123814Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9123959Z                     error_index=error_index,
2025-04-11T04:23:18.9124062Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9124155Z                     exit_code=exitcode,
2025-04-11T04:23:18.9124244Z                     signal_name=name,
2025-04-11T04:23:18.9124323Z                 )
2025-04-11T04:23:18.9124397Z             else:
2025-04-11T04:23:18.9124500Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9124669Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9124812Z                     error_index=error_index,
2025-04-11T04:23:18.9124923Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9125012Z                     exit_code=exitcode,
2025-04-11T04:23:18.9125092Z                 )
2025-04-11T04:23:18.9125168Z     
2025-04-11T04:23:18.9125304Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9125484Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9125578Z         msg += original_trace
2025-04-11T04:23:18.9125762Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9125979Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9126054Z E       
2025-04-11T04:23:18.9126191Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9126291Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9126606Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9126690Z E           fn(i, *args)
2025-04-11T04:23:18.9126956Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
2025-04-11T04:23:18.9127065Z E           check_all_gather(device_mesh, rank)
2025-04-11T04:23:18.9127324Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
2025-04-11T04:23:18.9127457Z E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T04:23:18.9127563Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9127857Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9127995Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9128160Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9128167Z 
2025-04-11T04:23:18.9128477Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9128641Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9128856Z [04/11/25 04:20:53] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9128990Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9129106Z                              :75 launch                                         
2025-04-11T04:23:18.9129246Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9129381Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9129576Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9129730Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9130026Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9130316Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9130454Z ______________________________ test_padded_tensor ______________________________
2025-04-11T04:23:18.9130510Z 
2025-04-11T04:23:18.9130605Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9131214Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9131222Z 
2025-04-11T04:23:18.9131325Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9131461Z         try_count = 0
2025-04-11T04:23:18.9131560Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9131647Z             max_try, int
2025-04-11T04:23:18.9131790Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9131866Z     
2025-04-11T04:23:18.9131976Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9132051Z             try:
2025-04-11T04:23:18.9132139Z                 try_count += 1
2025-04-11T04:23:18.9132230Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9132315Z                 return ret
2025-04-11T04:23:18.9132409Z             except exception_type as e:
2025-04-11T04:23:18.9132508Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9132754Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9132871Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9133023Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9133180Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9133268Z                     continue
2025-04-11T04:23:18.9133345Z                 else:
2025-04-11T04:23:18.9133568Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9133658Z >                   raise e
2025-04-11T04:23:18.9133663Z 
2025-04-11T04:23:18.9133757Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9133874Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9134007Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9134098Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9134263Z tests/test_tensor/test_padded_tensor.py:42: in test_padded_tensor
2025-04-11T04:23:18.9134363Z     spawn(check_padded_tensor, world_size)
2025-04-11T04:23:18.9134470Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9134571Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9134836Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9135015Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9135359Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9135451Z     while not context.join():
2025-04-11T04:23:18.9135560Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9135564Z 
2025-04-11T04:23:18.9135770Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581eca8f0>
2025-04-11T04:23:18.9135849Z timeout = None
2025-04-11T04:23:18.9135853Z 
2025-04-11T04:23:18.9135950Z     def join(self, timeout=None):
2025-04-11T04:23:18.9136075Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9136152Z     
2025-04-11T04:23:18.9136299Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9136444Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9136616Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9136710Z         of the first process exiting.
2025-04-11T04:23:18.9136787Z     
2025-04-11T04:23:18.9136934Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9137136Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9137207Z     
2025-04-11T04:23:18.9137281Z         Args:
2025-04-11T04:23:18.9137431Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9137506Z         """
2025-04-11T04:23:18.9137651Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9137746Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9137891Z             return True
2025-04-11T04:23:18.9137976Z     
2025-04-11T04:23:18.9138108Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9138234Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9138325Z             self.sentinels.keys(),
2025-04-11T04:23:18.9138420Z             timeout=timeout,
2025-04-11T04:23:18.9138493Z         )
2025-04-11T04:23:18.9138565Z     
2025-04-11T04:23:18.9138655Z         error_index = None
2025-04-11T04:23:18.9138745Z         for sentinel in ready:
2025-04-11T04:23:18.9138856Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9138956Z             process = self.processes[index]
2025-04-11T04:23:18.9139092Z             process.join()
2025-04-11T04:23:18.9139196Z             if process.exitcode != 0:
2025-04-11T04:23:18.9139286Z                 error_index = index
2025-04-11T04:23:18.9139370Z                 break
2025-04-11T04:23:18.9139444Z     
2025-04-11T04:23:18.9139539Z         # Return if there was no error.
2025-04-11T04:23:18.9139627Z         if error_index is None:
2025-04-11T04:23:18.9139761Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9139862Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9139932Z     
2025-04-11T04:23:18.9140074Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9140177Z         for process in self.processes:
2025-04-11T04:23:18.9140266Z             if process.is_alive():
2025-04-11T04:23:18.9140367Z                 process.terminate()
2025-04-11T04:23:18.9140451Z             process.join()
2025-04-11T04:23:18.9140526Z     
2025-04-11T04:23:18.9140673Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9140787Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9140901Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9141023Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9141115Z             if exitcode < 0:
2025-04-11T04:23:18.9141220Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9141329Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9141480Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9141630Z                     error_index=error_index,
2025-04-11T04:23:18.9141739Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9141830Z                     exit_code=exitcode,
2025-04-11T04:23:18.9141924Z                     signal_name=name,
2025-04-11T04:23:18.9141997Z                 )
2025-04-11T04:23:18.9142076Z             else:
2025-04-11T04:23:18.9142189Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9142363Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9142466Z                     error_index=error_index,
2025-04-11T04:23:18.9142573Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9142669Z                     exit_code=exitcode,
2025-04-11T04:23:18.9142744Z                 )
2025-04-11T04:23:18.9142818Z     
2025-04-11T04:23:18.9142961Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9143136Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9143233Z         msg += original_trace
2025-04-11T04:23:18.9143410Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9143628Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9143708Z E       
2025-04-11T04:23:18.9143840Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9143943Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9144241Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9144382Z E           fn(i, *args)
2025-04-11T04:23:18.9144652Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
2025-04-11T04:23:18.9144771Z E           original_tensor = torch.rand(32, 64).to("cuda")
2025-04-11T04:23:18.9144881Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9145168Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9145312Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9145474Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9145527Z 
2025-04-11T04:23:18.9145842Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9145995Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9146158Z [04/11/25 04:20:59] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9146289Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9146396Z                              :75 launch                                         
2025-04-11T04:23:18.9146540Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9146667Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9146873Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9147018Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9147316Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:39213 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9147448Z __________________________________ test_apply __________________________________
2025-04-11T04:23:18.9147455Z 
2025-04-11T04:23:18.9147553Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9148154Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9148219Z 
2025-04-11T04:23:18.9148329Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9148406Z         try_count = 0
2025-04-11T04:23:18.9148536Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9148621Z             max_try, int
2025-04-11T04:23:18.9148765Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9148845Z     
2025-04-11T04:23:18.9148959Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9149038Z             try:
2025-04-11T04:23:18.9149132Z                 try_count += 1
2025-04-11T04:23:18.9149226Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9149314Z                 return ret
2025-04-11T04:23:18.9149411Z             except exception_type as e:
2025-04-11T04:23:18.9149513Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9149708Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9149831Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9149984Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9150198Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9150289Z                     continue
2025-04-11T04:23:18.9150366Z                 else:
2025-04-11T04:23:18.9150586Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9150667Z >                   raise e
2025-04-11T04:23:18.9150724Z 
2025-04-11T04:23:18.9150817Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9150930Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9151060Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9151152Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9151317Z tests/test_tensor/test_shape_consistency_apply.py:72: in test_apply
2025-04-11T04:23:18.9151410Z     spawn(check_apply, world_size)
2025-04-11T04:23:18.9151511Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9151611Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9151870Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9152102Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9152387Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9152477Z     while not context.join():
2025-04-11T04:23:18.9152586Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9152594Z 
2025-04-11T04:23:18.9152789Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a6f6a0>
2025-04-11T04:23:18.9152866Z timeout = None
2025-04-11T04:23:18.9152871Z 
2025-04-11T04:23:18.9152964Z     def join(self, timeout=None):
2025-04-11T04:23:18.9153087Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9153159Z     
2025-04-11T04:23:18.9153302Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9153442Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9153606Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9153696Z         of the first process exiting.
2025-04-11T04:23:18.9153767Z     
2025-04-11T04:23:18.9153910Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9154048Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9154116Z     
2025-04-11T04:23:18.9154187Z         Args:
2025-04-11T04:23:18.9154325Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9154396Z         """
2025-04-11T04:23:18.9154591Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9154683Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9154764Z             return True
2025-04-11T04:23:18.9154834Z     
2025-04-11T04:23:18.9154963Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9155081Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9155171Z             self.sentinels.keys(),
2025-04-11T04:23:18.9155256Z             timeout=timeout,
2025-04-11T04:23:18.9155326Z         )
2025-04-11T04:23:18.9155393Z     
2025-04-11T04:23:18.9155475Z         error_index = None
2025-04-11T04:23:18.9155560Z         for sentinel in ready:
2025-04-11T04:23:18.9155667Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9155765Z             process = self.processes[index]
2025-04-11T04:23:18.9155848Z             process.join()
2025-04-11T04:23:18.9155940Z             if process.exitcode != 0:
2025-04-11T04:23:18.9156024Z                 error_index = index
2025-04-11T04:23:18.9156104Z                 break
2025-04-11T04:23:18.9156172Z     
2025-04-11T04:23:18.9156259Z         # Return if there was no error.
2025-04-11T04:23:18.9156398Z         if error_index is None:
2025-04-11T04:23:18.9156529Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9156628Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9156697Z     
2025-04-11T04:23:18.9156836Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9156933Z         for process in self.processes:
2025-04-11T04:23:18.9157018Z             if process.is_alive():
2025-04-11T04:23:18.9157162Z                 process.terminate()
2025-04-11T04:23:18.9157243Z             process.join()
2025-04-11T04:23:18.9157315Z     
2025-04-11T04:23:18.9157456Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9157570Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9157680Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9157799Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9157885Z             if exitcode < 0:
2025-04-11T04:23:18.9157989Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9158093Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9158308Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9158401Z                     error_index=error_index,
2025-04-11T04:23:18.9158505Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9158593Z                     exit_code=exitcode,
2025-04-11T04:23:18.9158679Z                     signal_name=name,
2025-04-11T04:23:18.9158749Z                 )
2025-04-11T04:23:18.9158824Z             else:
2025-04-11T04:23:18.9158922Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9159084Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9159180Z                     error_index=error_index,
2025-04-11T04:23:18.9159277Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9159366Z                     exit_code=exitcode,
2025-04-11T04:23:18.9159437Z                 )
2025-04-11T04:23:18.9159505Z     
2025-04-11T04:23:18.9159639Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9159805Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9159894Z         msg += original_trace
2025-04-11T04:23:18.9160065Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9160232Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9160303Z E       
2025-04-11T04:23:18.9160431Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9160531Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9160878Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9160963Z E           fn(i, *args)
2025-04-11T04:23:18.9161236Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
2025-04-11T04:23:18.9161411Z E           tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
2025-04-11T04:23:18.9161517Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9161800Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9161939Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9162096Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9162101Z 
2025-04-11T04:23:18.9162406Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9162558Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9162713Z [04/11/25 04:21:03] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9162906Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9163016Z                              :75 launch                                         
2025-04-11T04:23:18.9163150Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9163272Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9163505Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9163639Z ________________________________ test_comm_spec ________________________________
2025-04-11T04:23:18.9163643Z 
2025-04-11T04:23:18.9163736Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9164341Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9164349Z 
2025-04-11T04:23:18.9164452Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9164580Z         try_count = 0
2025-04-11T04:23:18.9164681Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9164759Z             max_try, int
2025-04-11T04:23:18.9164902Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9164976Z     
2025-04-11T04:23:18.9165088Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9165163Z             try:
2025-04-11T04:23:18.9165244Z                 try_count += 1
2025-04-11T04:23:18.9165332Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9165414Z                 return ret
2025-04-11T04:23:18.9165507Z             except exception_type as e:
2025-04-11T04:23:18.9165608Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9165792Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9165909Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9166054Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9166205Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9166288Z                     continue
2025-04-11T04:23:18.9166364Z                 else:
2025-04-11T04:23:18.9166588Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9166666Z >                   raise e
2025-04-11T04:23:18.9166671Z 
2025-04-11T04:23:18.9166763Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9166874Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9167053Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9167140Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9167314Z tests/test_tensor/test_dtensor/test_comm_spec.py:157: in test_comm_spec
2025-04-11T04:23:18.9167406Z     spawn(check_comm, world_size)
2025-04-11T04:23:18.9167507Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9167608Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9167865Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9168038Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9168324Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9168413Z     while not context.join():
2025-04-11T04:23:18.9168526Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9168532Z 
2025-04-11T04:23:18.9168732Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a6f700>
2025-04-11T04:23:18.9168861Z timeout = None
2025-04-11T04:23:18.9168866Z 
2025-04-11T04:23:18.9168953Z     def join(self, timeout=None):
2025-04-11T04:23:18.9169075Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9169148Z     
2025-04-11T04:23:18.9169292Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9169437Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9169601Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9169740Z         of the first process exiting.
2025-04-11T04:23:18.9169809Z     
2025-04-11T04:23:18.9169954Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9170093Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9170163Z     
2025-04-11T04:23:18.9170237Z         Args:
2025-04-11T04:23:18.9170372Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9170445Z         """
2025-04-11T04:23:18.9170587Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9170677Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9170807Z             return True
2025-04-11T04:23:18.9170875Z     
2025-04-11T04:23:18.9171011Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9171127Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9171218Z             self.sentinels.keys(),
2025-04-11T04:23:18.9171302Z             timeout=timeout,
2025-04-11T04:23:18.9171373Z         )
2025-04-11T04:23:18.9171443Z     
2025-04-11T04:23:18.9171524Z         error_index = None
2025-04-11T04:23:18.9171607Z         for sentinel in ready:
2025-04-11T04:23:18.9171714Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9171812Z             process = self.processes[index]
2025-04-11T04:23:18.9171897Z             process.join()
2025-04-11T04:23:18.9171986Z             if process.exitcode != 0:
2025-04-11T04:23:18.9172073Z                 error_index = index
2025-04-11T04:23:18.9172150Z                 break
2025-04-11T04:23:18.9172217Z     
2025-04-11T04:23:18.9172311Z         # Return if there was no error.
2025-04-11T04:23:18.9172394Z         if error_index is None:
2025-04-11T04:23:18.9172527Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9172621Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9172690Z     
2025-04-11T04:23:18.9172832Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9172926Z         for process in self.processes:
2025-04-11T04:23:18.9173014Z             if process.is_alive():
2025-04-11T04:23:18.9173102Z                 process.terminate()
2025-04-11T04:23:18.9173183Z             process.join()
2025-04-11T04:23:18.9173304Z     
2025-04-11T04:23:18.9173443Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9173561Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9173667Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9173789Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9173872Z             if exitcode < 0:
2025-04-11T04:23:18.9173975Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9174081Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9174231Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9174327Z                     error_index=error_index,
2025-04-11T04:23:18.9174427Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9174511Z                     exit_code=exitcode,
2025-04-11T04:23:18.9174597Z                     signal_name=name,
2025-04-11T04:23:18.9174670Z                 )
2025-04-11T04:23:18.9174746Z             else:
2025-04-11T04:23:18.9174845Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9175063Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9175154Z                     error_index=error_index,
2025-04-11T04:23:18.9175253Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9175341Z                     exit_code=exitcode,
2025-04-11T04:23:18.9175412Z                 )
2025-04-11T04:23:18.9175483Z     
2025-04-11T04:23:18.9175613Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9175829Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9175916Z         msg += original_trace
2025-04-11T04:23:18.9176085Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9176247Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9176320Z E       
2025-04-11T04:23:18.9176447Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9176545Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9176839Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9177043Z E           fn(i, *args)
2025-04-11T04:23:18.9177310Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
2025-04-11T04:23:18.9177426Z E           check_all_gather(process_group_dict, rank)
2025-04-11T04:23:18.9177699Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
2025-04-11T04:23:18.9177820Z E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T04:23:18.9177924Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9178206Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9178343Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9178504Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9178508Z 
2025-04-11T04:23:18.9178815Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9178966Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9179121Z [04/11/25 04:21:09] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9179251Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9179358Z                              :75 launch                                         
2025-04-11T04:23:18.9179493Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9179664Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9179865Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9180010Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9180315Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9180605Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9180743Z _________________________________ test_dtensor _________________________________
2025-04-11T04:23:18.9180747Z 
2025-04-11T04:23:18.9180836Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9181466Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9181475Z 
2025-04-11T04:23:18.9181621Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9181703Z         try_count = 0
2025-04-11T04:23:18.9181802Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9181884Z             max_try, int
2025-04-11T04:23:18.9182033Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9182102Z     
2025-04-11T04:23:18.9182216Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9182288Z             try:
2025-04-11T04:23:18.9182419Z                 try_count += 1
2025-04-11T04:23:18.9182514Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9182593Z                 return ret
2025-04-11T04:23:18.9182688Z             except exception_type as e:
2025-04-11T04:23:18.9182785Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9182976Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9183090Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9183237Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9183395Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9183519Z                     continue
2025-04-11T04:23:18.9183597Z                 else:
2025-04-11T04:23:18.9183822Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9183905Z >                   raise e
2025-04-11T04:23:18.9183910Z 
2025-04-11T04:23:18.9184001Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9184111Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9184248Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9184334Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9184500Z tests/test_tensor/test_dtensor/test_dtensor.py:83: in test_dtensor
2025-04-11T04:23:18.9184595Z     spawn(check_dtensor, world_size)
2025-04-11T04:23:18.9184697Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9184793Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9185050Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9185229Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9185515Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9185605Z     while not context.join():
2025-04-11T04:23:18.9185712Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9185716Z 
2025-04-11T04:23:18.9185914Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581db6ec0>
2025-04-11T04:23:18.9186054Z timeout = None
2025-04-11T04:23:18.9186059Z 
2025-04-11T04:23:18.9186145Z     def join(self, timeout=None):
2025-04-11T04:23:18.9186275Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9186343Z     
2025-04-11T04:23:18.9186492Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9186636Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9186800Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9186889Z         of the first process exiting.
2025-04-11T04:23:18.9186958Z     
2025-04-11T04:23:18.9187107Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9187244Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9187313Z     
2025-04-11T04:23:18.9187385Z         Args:
2025-04-11T04:23:18.9187519Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9187595Z         """
2025-04-11T04:23:18.9187732Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9187874Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9187951Z             return True
2025-04-11T04:23:18.9188025Z     
2025-04-11T04:23:18.9188154Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9188269Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9188363Z             self.sentinels.keys(),
2025-04-11T04:23:18.9188472Z             timeout=timeout,
2025-04-11T04:23:18.9188600Z         )
2025-04-11T04:23:18.9188668Z     
2025-04-11T04:23:18.9188751Z         error_index = None
2025-04-11T04:23:18.9188840Z         for sentinel in ready:
2025-04-11T04:23:18.9188945Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9189047Z             process = self.processes[index]
2025-04-11T04:23:18.9189130Z             process.join()
2025-04-11T04:23:18.9189222Z             if process.exitcode != 0:
2025-04-11T04:23:18.9189312Z                 error_index = index
2025-04-11T04:23:18.9189385Z                 break
2025-04-11T04:23:18.9189457Z     
2025-04-11T04:23:18.9189548Z         # Return if there was no error.
2025-04-11T04:23:18.9189633Z         if error_index is None:
2025-04-11T04:23:18.9189816Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9189910Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9189981Z     
2025-04-11T04:23:18.9190118Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9190219Z         for process in self.processes:
2025-04-11T04:23:18.9190304Z             if process.is_alive():
2025-04-11T04:23:18.9190392Z                 process.terminate()
2025-04-11T04:23:18.9190476Z             process.join()
2025-04-11T04:23:18.9190543Z     
2025-04-11T04:23:18.9190684Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9190799Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9190907Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9191026Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9191107Z             if exitcode < 0:
2025-04-11T04:23:18.9191215Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9191318Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9191469Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9191561Z                     error_index=error_index,
2025-04-11T04:23:18.9191661Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9191750Z                     exit_code=exitcode,
2025-04-11T04:23:18.9191833Z                     signal_name=name,
2025-04-11T04:23:18.9191909Z                 )
2025-04-11T04:23:18.9191980Z             else:
2025-04-11T04:23:18.9192082Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9192299Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9192392Z                     error_index=error_index,
2025-04-11T04:23:18.9192495Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9192579Z                     exit_code=exitcode,
2025-04-11T04:23:18.9192655Z                 )
2025-04-11T04:23:18.9192722Z     
2025-04-11T04:23:18.9192852Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9193024Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9193112Z         msg += original_trace
2025-04-11T04:23:18.9193287Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9193446Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9193521Z E       
2025-04-11T04:23:18.9193646Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9193743Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9194040Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9194172Z E           fn(i, *args)
2025-04-11T04:23:18.9194440Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
2025-04-11T04:23:18.9194547Z E           test_model = TestModel(8, 8).to("cuda")
2025-04-11T04:23:18.9194813Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.9194959Z E           return self._apply(convert)
2025-04-11T04:23:18.9195226Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9195314Z E           module._apply(fn)
2025-04-11T04:23:18.9195576Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9195674Z E           param_applied = fn(param)
2025-04-11T04:23:18.9195949Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.9196171Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.9196324Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9196608Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9196748Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9196909Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9196914Z 
2025-04-11T04:23:18.9197222Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9197374Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9197532Z [04/11/25 04:21:13] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9197662Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9197771Z                              :75 launch                                         
2025-04-11T04:23:18.9197908Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9198030Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9198229Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9198364Z ____________________________ test_layout_converter _____________________________
2025-04-11T04:23:18.9198368Z 
2025-04-11T04:23:18.9198462Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9199049Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9199102Z 
2025-04-11T04:23:18.9199206Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9199284Z         try_count = 0
2025-04-11T04:23:18.9199386Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9199466Z             max_try, int
2025-04-11T04:23:18.9199611Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9199685Z     
2025-04-11T04:23:18.9199797Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9199874Z             try:
2025-04-11T04:23:18.9199959Z                 try_count += 1
2025-04-11T04:23:18.9200050Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9200128Z                 return ret
2025-04-11T04:23:18.9200218Z             except exception_type as e:
2025-04-11T04:23:18.9200322Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9200505Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9200674Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9200820Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9200979Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9201058Z                     continue
2025-04-11T04:23:18.9201132Z                 else:
2025-04-11T04:23:18.9201391Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9201469Z >                   raise e
2025-04-11T04:23:18.9201475Z 
2025-04-11T04:23:18.9201568Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9201679Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9201814Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9201897Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9202104Z tests/test_tensor/test_dtensor/test_layout_converter.py:180: in test_layout_converter
2025-04-11T04:23:18.9202226Z     spawn(check_layout_converting_apply, world_size)
2025-04-11T04:23:18.9202371Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9202471Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9202725Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9202901Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9203192Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9203277Z     while not context.join():
2025-04-11T04:23:18.9203389Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9203396Z 
2025-04-11T04:23:18.9203593Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b85701f0>
2025-04-11T04:23:18.9203675Z timeout = None
2025-04-11T04:23:18.9203680Z 
2025-04-11T04:23:18.9203767Z     def join(self, timeout=None):
2025-04-11T04:23:18.9203896Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9203967Z     
2025-04-11T04:23:18.9204111Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9204258Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9204420Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9204516Z         of the first process exiting.
2025-04-11T04:23:18.9204583Z     
2025-04-11T04:23:18.9204727Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9204868Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9204984Z     
2025-04-11T04:23:18.9205060Z         Args:
2025-04-11T04:23:18.9205197Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9205275Z         """
2025-04-11T04:23:18.9205412Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9205503Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9205587Z             return True
2025-04-11T04:23:18.9205655Z     
2025-04-11T04:23:18.9205788Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9205903Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9205993Z             self.sentinels.keys(),
2025-04-11T04:23:18.9206079Z             timeout=timeout,
2025-04-11T04:23:18.9206151Z         )
2025-04-11T04:23:18.9206222Z     
2025-04-11T04:23:18.9206302Z         error_index = None
2025-04-11T04:23:18.9206389Z         for sentinel in ready:
2025-04-11T04:23:18.9206493Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9206592Z             process = self.processes[index]
2025-04-11T04:23:18.9206677Z             process.join()
2025-04-11T04:23:18.9206767Z             if process.exitcode != 0:
2025-04-11T04:23:18.9206909Z                 error_index = index
2025-04-11T04:23:18.9206983Z                 break
2025-04-11T04:23:18.9207051Z     
2025-04-11T04:23:18.9207147Z         # Return if there was no error.
2025-04-11T04:23:18.9207230Z         if error_index is None:
2025-04-11T04:23:18.9207363Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9207455Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9207569Z     
2025-04-11T04:23:18.9207711Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9207807Z         for process in self.processes:
2025-04-11T04:23:18.9207897Z             if process.is_alive():
2025-04-11T04:23:18.9207984Z                 process.terminate()
2025-04-11T04:23:18.9208067Z             process.join()
2025-04-11T04:23:18.9208137Z     
2025-04-11T04:23:18.9208274Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9208393Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9208499Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9208621Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9208758Z             if exitcode < 0:
2025-04-11T04:23:18.9208864Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9208968Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9209114Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9209213Z                     error_index=error_index,
2025-04-11T04:23:18.9209312Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9209401Z                     exit_code=exitcode,
2025-04-11T04:23:18.9209484Z                     signal_name=name,
2025-04-11T04:23:18.9209558Z                 )
2025-04-11T04:23:18.9209632Z             else:
2025-04-11T04:23:18.9209731Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9209899Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9209989Z                     error_index=error_index,
2025-04-11T04:23:18.9210093Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9210177Z                     exit_code=exitcode,
2025-04-11T04:23:18.9210246Z                 )
2025-04-11T04:23:18.9210317Z     
2025-04-11T04:23:18.9210446Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9210616Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9210699Z         msg += original_trace
2025-04-11T04:23:18.9210867Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9211025Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9211148Z E       
2025-04-11T04:23:18.9211276Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9211374Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9211685Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9211768Z E           fn(i, *args)
2025-04-11T04:23:18.9212106Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
2025-04-11T04:23:18.9212234Z E           original_tensor = torch.rand(global_shape).cuda()
2025-04-11T04:23:18.9212337Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9212630Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9212765Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9212928Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9212933Z 
2025-04-11T04:23:18.9213243Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9213444Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9213600Z [04/11/25 04:21:19] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9213728Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9213837Z                              :75 launch                                         
2025-04-11T04:23:18.9214006Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9214132Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9214275Z [04/11/25 04:21:24] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9214405Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9214508Z                              :75 launch                                         
2025-04-11T04:23:18.9214640Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9214812Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9214951Z [04/11/25 04:21:28] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9215076Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9215179Z                              :75 launch                                         
2025-04-11T04:23:18.9215312Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9215432Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9215629Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9215776Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9216071Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9216362Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9216636Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:32601 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9216917Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:32601 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9217188Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24558 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9217513Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24558 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9217648Z ____________________________ test_chunk_manager[2] _____________________________
2025-04-11T04:23:18.9217652Z 
2025-04-11T04:23:18.9217769Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.9218377Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9218384Z 
2025-04-11T04:23:18.9218487Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9218564Z         try_count = 0
2025-04-11T04:23:18.9218663Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9218746Z             max_try, int
2025-04-11T04:23:18.9218890Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9218962Z     
2025-04-11T04:23:18.9219072Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9219145Z             try:
2025-04-11T04:23:18.9219281Z                 try_count += 1
2025-04-11T04:23:18.9219371Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9219455Z                 return ret
2025-04-11T04:23:18.9219551Z             except exception_type as e:
2025-04-11T04:23:18.9219653Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9219838Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9220016Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9220164Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9220318Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9220402Z                     continue
2025-04-11T04:23:18.9220479Z                 else:
2025-04-11T04:23:18.9220705Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9220786Z >                   raise e
2025-04-11T04:23:18.9220790Z 
2025-04-11T04:23:18.9220880Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9221039Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9221167Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9221255Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9221431Z tests/test_zero/test_gemini/test_chunk_mgrv2.py:60: in test_chunk_manager
2025-04-11T04:23:18.9221523Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9221624Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9221724Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9221986Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9222164Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9222451Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9222540Z     while not context.join():
2025-04-11T04:23:18.9222652Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9222658Z 
2025-04-11T04:23:18.9222852Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581d6cc70>
2025-04-11T04:23:18.9222927Z timeout = None
2025-04-11T04:23:18.9222937Z 
2025-04-11T04:23:18.9223029Z     def join(self, timeout=None):
2025-04-11T04:23:18.9223150Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9223221Z     
2025-04-11T04:23:18.9223367Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9223514Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9223726Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9223815Z         of the first process exiting.
2025-04-11T04:23:18.9223890Z     
2025-04-11T04:23:18.9224036Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9224172Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9224242Z     
2025-04-11T04:23:18.9224315Z         Args:
2025-04-11T04:23:18.9224452Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9224526Z         """
2025-04-11T04:23:18.9224668Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9224758Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9224837Z             return True
2025-04-11T04:23:18.9224906Z     
2025-04-11T04:23:18.9225036Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9225153Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9225245Z             self.sentinels.keys(),
2025-04-11T04:23:18.9225330Z             timeout=timeout,
2025-04-11T04:23:18.9225401Z         )
2025-04-11T04:23:18.9225517Z     
2025-04-11T04:23:18.9225602Z         error_index = None
2025-04-11T04:23:18.9225687Z         for sentinel in ready:
2025-04-11T04:23:18.9225795Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9225896Z             process = self.processes[index]
2025-04-11T04:23:18.9225981Z             process.join()
2025-04-11T04:23:18.9226072Z             if process.exitcode != 0:
2025-04-11T04:23:18.9226157Z                 error_index = index
2025-04-11T04:23:18.9226283Z                 break
2025-04-11T04:23:18.9226352Z     
2025-04-11T04:23:18.9226443Z         # Return if there was no error.
2025-04-11T04:23:18.9226525Z         if error_index is None:
2025-04-11T04:23:18.9226655Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9226751Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9226820Z     
2025-04-11T04:23:18.9226959Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9227054Z         for process in self.processes:
2025-04-11T04:23:18.9227140Z             if process.is_alive():
2025-04-11T04:23:18.9227231Z                 process.terminate()
2025-04-11T04:23:18.9227358Z             process.join()
2025-04-11T04:23:18.9227431Z     
2025-04-11T04:23:18.9227571Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9227687Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9227794Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9227914Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9228001Z             if exitcode < 0:
2025-04-11T04:23:18.9228105Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9228210Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9228359Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9228482Z                     error_index=error_index,
2025-04-11T04:23:18.9228585Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9228670Z                     exit_code=exitcode,
2025-04-11T04:23:18.9228757Z                     signal_name=name,
2025-04-11T04:23:18.9228830Z                 )
2025-04-11T04:23:18.9228905Z             else:
2025-04-11T04:23:18.9229006Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9229166Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9229261Z                     error_index=error_index,
2025-04-11T04:23:18.9229359Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9229447Z                     exit_code=exitcode,
2025-04-11T04:23:18.9229517Z                 )
2025-04-11T04:23:18.9229590Z     
2025-04-11T04:23:18.9229720Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9229946Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9230035Z         msg += original_trace
2025-04-11T04:23:18.9230204Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9230367Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9230440Z E       
2025-04-11T04:23:18.9230562Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9230663Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9230952Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9231037Z E           fn(i, *args)
2025-04-11T04:23:18.9231285Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
2025-04-11T04:23:18.9231371Z E           exam_chunk_memory()
2025-04-11T04:23:18.9231628Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9231716Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9232022Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9232110Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9232387Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
2025-04-11T04:23:18.9232490Z E           chunk_manager = ChunkManager(config)
2025-04-11T04:23:18.9232795Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T04:23:18.9233048Z E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.9233155Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9233440Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9233574Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9233739Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9233744Z 
2025-04-11T04:23:18.9234095Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9234249Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9234405Z [04/11/25 04:21:33] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9234543Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9234652Z                              :75 launch                                         
2025-04-11T04:23:18.9234791Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9234923Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9235123Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9235274Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9236159Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:18.9236269Z   return tensor.storage().size() == 0
2025-04-11T04:23:18.9236407Z ____________________________ test_chunk_function[1] ____________________________
2025-04-11T04:23:18.9236411Z 
2025-04-11T04:23:18.9236533Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T04:23:18.9237184Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9237191Z 
2025-04-11T04:23:18.9237296Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9237376Z         try_count = 0
2025-04-11T04:23:18.9237476Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9237555Z             max_try, int
2025-04-11T04:23:18.9237700Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9237774Z     
2025-04-11T04:23:18.9237885Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9237961Z             try:
2025-04-11T04:23:18.9238042Z                 try_count += 1
2025-04-11T04:23:18.9238132Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9238215Z                 return ret
2025-04-11T04:23:18.9238309Z             except exception_type as e:
2025-04-11T04:23:18.9238410Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9238646Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9238763Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9238907Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9239059Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9239143Z                     continue
2025-04-11T04:23:18.9239265Z                 else:
2025-04-11T04:23:18.9239487Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9239565Z >                   raise e
2025-04-11T04:23:18.9239569Z 
2025-04-11T04:23:18.9239663Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9239777Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9239907Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9239998Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9240166Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T04:23:18.9240254Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9240396Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9240493Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9240756Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9240936Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9241227Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9241315Z     while not context.join():
2025-04-11T04:23:18.9241425Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9241432Z 
2025-04-11T04:23:18.9241627Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c77700>
2025-04-11T04:23:18.9241709Z timeout = None
2025-04-11T04:23:18.9241714Z 
2025-04-11T04:23:18.9241800Z     def join(self, timeout=None):
2025-04-11T04:23:18.9241924Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9241999Z     
2025-04-11T04:23:18.9242142Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9242286Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9242450Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9242544Z         of the first process exiting.
2025-04-11T04:23:18.9242612Z     
2025-04-11T04:23:18.9242756Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9242895Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9243013Z     
2025-04-11T04:23:18.9243088Z         Args:
2025-04-11T04:23:18.9243226Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9243299Z         """
2025-04-11T04:23:18.9243441Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9243534Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9243620Z             return True
2025-04-11T04:23:18.9243688Z     
2025-04-11T04:23:18.9243816Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9243935Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9244026Z             self.sentinels.keys(),
2025-04-11T04:23:18.9244111Z             timeout=timeout,
2025-04-11T04:23:18.9244182Z         )
2025-04-11T04:23:18.9244254Z     
2025-04-11T04:23:18.9244335Z         error_index = None
2025-04-11T04:23:18.9244416Z         for sentinel in ready:
2025-04-11T04:23:18.9244524Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9244623Z             process = self.processes[index]
2025-04-11T04:23:18.9244710Z             process.join()
2025-04-11T04:23:18.9244851Z             if process.exitcode != 0:
2025-04-11T04:23:18.9244935Z                 error_index = index
2025-04-11T04:23:18.9245012Z                 break
2025-04-11T04:23:18.9245081Z     
2025-04-11T04:23:18.9245171Z         # Return if there was no error.
2025-04-11T04:23:18.9245252Z         if error_index is None:
2025-04-11T04:23:18.9245382Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9245479Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9245595Z     
2025-04-11T04:23:18.9245739Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9245832Z         for process in self.processes:
2025-04-11T04:23:18.9245921Z             if process.is_alive():
2025-04-11T04:23:18.9246009Z                 process.terminate()
2025-04-11T04:23:18.9246093Z             process.join()
2025-04-11T04:23:18.9246165Z     
2025-04-11T04:23:18.9246304Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9246424Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9246530Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9246693Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9246777Z             if exitcode < 0:
2025-04-11T04:23:18.9246878Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9246984Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9247133Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9247229Z                     error_index=error_index,
2025-04-11T04:23:18.9247326Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9247411Z                     exit_code=exitcode,
2025-04-11T04:23:18.9247497Z                     signal_name=name,
2025-04-11T04:23:18.9247570Z                 )
2025-04-11T04:23:18.9247646Z             else:
2025-04-11T04:23:18.9247744Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9247910Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9248001Z                     error_index=error_index,
2025-04-11T04:23:18.9248098Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9248185Z                     exit_code=exitcode,
2025-04-11T04:23:18.9248255Z                 )
2025-04-11T04:23:18.9248326Z     
2025-04-11T04:23:18.9248457Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9248627Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9248714Z         msg += original_trace
2025-04-11T04:23:18.9248885Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9249046Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9249170Z E       
2025-04-11T04:23:18.9249298Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9249395Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9249696Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9249781Z E           fn(i, *args)
2025-04-11T04:23:18.9250029Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:18.9250117Z E           exam_chunk_basic()
2025-04-11T04:23:18.9250373Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9250462Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9250711Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9250799Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9251047Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9251177Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9251282Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.9251539Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:18.9251626Z E           my_chunk = Chunk(
2025-04-11T04:23:18.9251859Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:18.9252109Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:18.9252218Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9252502Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9252642Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9252802Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9252808Z 
2025-04-11T04:23:18.9253118Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9253314Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9253471Z [04/11/25 04:21:37] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9253599Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9253707Z                              :75 launch                                         
2025-04-11T04:23:18.9253845Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9253967Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.9254166Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9254299Z ____________________________ test_chunk_function[2] ____________________________
2025-04-11T04:23:18.9254305Z 
2025-04-11T04:23:18.9254422Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.9255022Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9255027Z 
2025-04-11T04:23:18.9255134Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9255211Z         try_count = 0
2025-04-11T04:23:18.9255308Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9255390Z             max_try, int
2025-04-11T04:23:18.9255534Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9255654Z     
2025-04-11T04:23:18.9255765Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9255842Z             try:
2025-04-11T04:23:18.9255926Z                 try_count += 1
2025-04-11T04:23:18.9256015Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9256099Z                 return ret
2025-04-11T04:23:18.9256191Z             except exception_type as e:
2025-04-11T04:23:18.9256291Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9256473Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9256587Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9256734Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9256888Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9256971Z                     continue
2025-04-11T04:23:18.9257046Z                 else:
2025-04-11T04:23:18.9257274Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9257415Z >                   raise e
2025-04-11T04:23:18.9257420Z 
2025-04-11T04:23:18.9257509Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9257623Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9257753Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9257840Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9258009Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T04:23:18.9258147Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9258244Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9258341Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9258601Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9258775Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9259060Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9259150Z     while not context.join():
2025-04-11T04:23:18.9259261Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9259308Z 
2025-04-11T04:23:18.9259512Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5be2230>
2025-04-11T04:23:18.9259590Z timeout = None
2025-04-11T04:23:18.9259598Z 
2025-04-11T04:23:18.9259686Z     def join(self, timeout=None):
2025-04-11T04:23:18.9259815Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9259887Z     
2025-04-11T04:23:18.9260031Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9260175Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9260336Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9260428Z         of the first process exiting.
2025-04-11T04:23:18.9260499Z     
2025-04-11T04:23:18.9260648Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9260788Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9260858Z     
2025-04-11T04:23:18.9260931Z         Args:
2025-04-11T04:23:18.9261069Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9261141Z         """
2025-04-11T04:23:18.9261282Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9261374Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9261454Z             return True
2025-04-11T04:23:18.9261522Z     
2025-04-11T04:23:18.9261653Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9261773Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9261913Z             self.sentinels.keys(),
2025-04-11T04:23:18.9262001Z             timeout=timeout,
2025-04-11T04:23:18.9262073Z         )
2025-04-11T04:23:18.9262145Z     
2025-04-11T04:23:18.9262228Z         error_index = None
2025-04-11T04:23:18.9262312Z         for sentinel in ready:
2025-04-11T04:23:18.9262421Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9262521Z             process = self.processes[index]
2025-04-11T04:23:18.9262604Z             process.join()
2025-04-11T04:23:18.9262695Z             if process.exitcode != 0:
2025-04-11T04:23:18.9262780Z                 error_index = index
2025-04-11T04:23:18.9262861Z                 break
2025-04-11T04:23:18.9262928Z     
2025-04-11T04:23:18.9263022Z         # Return if there was no error.
2025-04-11T04:23:18.9263104Z         if error_index is None:
2025-04-11T04:23:18.9263236Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9263332Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9263403Z     
2025-04-11T04:23:18.9263545Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9263640Z         for process in self.processes:
2025-04-11T04:23:18.9263779Z             if process.is_alive():
2025-04-11T04:23:18.9263869Z                 process.terminate()
2025-04-11T04:23:18.9263949Z             process.join()
2025-04-11T04:23:18.9264024Z     
2025-04-11T04:23:18.9264161Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9264279Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9264382Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9264549Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9264633Z             if exitcode < 0:
2025-04-11T04:23:18.9264738Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9264844Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9264993Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9265093Z                     error_index=error_index,
2025-04-11T04:23:18.9265192Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9265282Z                     exit_code=exitcode,
2025-04-11T04:23:18.9265372Z                     signal_name=name,
2025-04-11T04:23:18.9265490Z                 )
2025-04-11T04:23:18.9265566Z             else:
2025-04-11T04:23:18.9265669Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9265831Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9265927Z                     error_index=error_index,
2025-04-11T04:23:18.9266022Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9266113Z                     exit_code=exitcode,
2025-04-11T04:23:18.9266182Z                 )
2025-04-11T04:23:18.9266254Z     
2025-04-11T04:23:18.9266387Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9266557Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9266645Z         msg += original_trace
2025-04-11T04:23:18.9266815Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9266974Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9267047Z E       
2025-04-11T04:23:18.9267170Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9267269Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9267563Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9267648Z E           fn(i, *args)
2025-04-11T04:23:18.9267898Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:18.9267985Z E           exam_chunk_basic()
2025-04-11T04:23:18.9268237Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9268370Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9268667Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9268752Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9269006Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9269089Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9269194Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.9269453Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:18.9269539Z E           my_chunk = Chunk(
2025-04-11T04:23:18.9269771Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:18.9269971Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:18.9270083Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9270420Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9270557Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9270716Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9270721Z 
2025-04-11T04:23:18.9271024Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9271224Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9271376Z [04/11/25 04:21:42] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9271506Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9271614Z                              :75 launch                                         
2025-04-11T04:23:18.9271753Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9271878Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9272075Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9272267Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9272565Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34935 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9272701Z ____________________________ test_chunk_function[4] ____________________________
2025-04-11T04:23:18.9272706Z 
2025-04-11T04:23:18.9272823Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9273426Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9273435Z 
2025-04-11T04:23:18.9273536Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9273618Z         try_count = 0
2025-04-11T04:23:18.9273717Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9273800Z             max_try, int
2025-04-11T04:23:18.9273944Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9274016Z     
2025-04-11T04:23:18.9274126Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9274200Z             try:
2025-04-11T04:23:18.9274286Z                 try_count += 1
2025-04-11T04:23:18.9274374Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9274456Z                 return ret
2025-04-11T04:23:18.9274548Z             except exception_type as e:
2025-04-11T04:23:18.9274698Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9274885Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9275001Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9275149Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9275305Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9275388Z                     continue
2025-04-11T04:23:18.9275464Z                 else:
2025-04-11T04:23:18.9275686Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9275771Z >                   raise e
2025-04-11T04:23:18.9275776Z 
2025-04-11T04:23:18.9275866Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9275980Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9276113Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9276200Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9276372Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T04:23:18.9276506Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9276607Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9276709Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9276966Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9277139Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9277544Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9277631Z     while not context.join():
2025-04-11T04:23:18.9277741Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9277745Z 
2025-04-11T04:23:18.9277948Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c77700>
2025-04-11T04:23:18.9278026Z timeout = None
2025-04-11T04:23:18.9278031Z 
2025-04-11T04:23:18.9278123Z     def join(self, timeout=None):
2025-04-11T04:23:18.9278245Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9278317Z     
2025-04-11T04:23:18.9278504Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9278644Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9278810Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9278905Z         of the first process exiting.
2025-04-11T04:23:18.9278978Z     
2025-04-11T04:23:18.9279122Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9279262Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9279330Z     
2025-04-11T04:23:18.9279404Z         Args:
2025-04-11T04:23:18.9279544Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9279615Z         """
2025-04-11T04:23:18.9279757Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9279847Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9279923Z             return True
2025-04-11T04:23:18.9279996Z     
2025-04-11T04:23:18.9280124Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9280244Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9280331Z             self.sentinels.keys(),
2025-04-11T04:23:18.9280417Z             timeout=timeout,
2025-04-11T04:23:18.9280487Z         )
2025-04-11T04:23:18.9280555Z     
2025-04-11T04:23:18.9280638Z         error_index = None
2025-04-11T04:23:18.9280720Z         for sentinel in ready:
2025-04-11T04:23:18.9280827Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9280923Z             process = self.processes[index]
2025-04-11T04:23:18.9281067Z             process.join()
2025-04-11T04:23:18.9281162Z             if process.exitcode != 0:
2025-04-11T04:23:18.9281249Z                 error_index = index
2025-04-11T04:23:18.9281324Z                 break
2025-04-11T04:23:18.9281391Z     
2025-04-11T04:23:18.9281478Z         # Return if there was no error.
2025-04-11T04:23:18.9281565Z         if error_index is None:
2025-04-11T04:23:18.9281695Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9281791Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9281858Z     
2025-04-11T04:23:18.9281998Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9282095Z         for process in self.processes:
2025-04-11T04:23:18.9282181Z             if process.is_alive():
2025-04-11T04:23:18.9282272Z                 process.terminate()
2025-04-11T04:23:18.9282353Z             process.join()
2025-04-11T04:23:18.9282423Z     
2025-04-11T04:23:18.9282561Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9282674Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9282846Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9282964Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9283050Z             if exitcode < 0:
2025-04-11T04:23:18.9283153Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9283261Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9283406Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9283544Z                     error_index=error_index,
2025-04-11T04:23:18.9283645Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9283731Z                     exit_code=exitcode,
2025-04-11T04:23:18.9283818Z                     signal_name=name,
2025-04-11T04:23:18.9283890Z                 )
2025-04-11T04:23:18.9283964Z             else:
2025-04-11T04:23:18.9284067Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9284225Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9284322Z                     error_index=error_index,
2025-04-11T04:23:18.9284418Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9284553Z                     exit_code=exitcode,
2025-04-11T04:23:18.9284623Z                 )
2025-04-11T04:23:18.9284692Z     
2025-04-11T04:23:18.9284824Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9284992Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9285082Z         msg += original_trace
2025-04-11T04:23:18.9285250Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9285405Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9285479Z E       
2025-04-11T04:23:18.9285606Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9285705Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9286003Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9286085Z E           fn(i, *args)
2025-04-11T04:23:18.9286336Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:18.9286419Z E           exam_chunk_basic()
2025-04-11T04:23:18.9286676Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9286764Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9287015Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9287099Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9287350Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9287487Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9287590Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.9287853Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:18.9287938Z E           my_chunk = Chunk(
2025-04-11T04:23:18.9288175Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:18.9288377Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:18.9288490Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9288772Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9288910Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9289070Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9289075Z 
2025-04-11T04:23:18.9289376Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9289576Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9289735Z [04/11/25 04:21:48] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9289867Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9289972Z                              :75 launch                                         
2025-04-11T04:23:18.9290141Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9290263Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9290461Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9290605Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9290899Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38965 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9291193Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38965 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9291378Z ____________________________ test_grad_accumulation ____________________________
2025-04-11T04:23:18.9291383Z 
2025-04-11T04:23:18.9291477Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9292076Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9292082Z 
2025-04-11T04:23:18.9292184Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9292265Z         try_count = 0
2025-04-11T04:23:18.9292365Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9292446Z             max_try, int
2025-04-11T04:23:18.9292589Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9292661Z     
2025-04-11T04:23:18.9292773Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9292851Z             try:
2025-04-11T04:23:18.9292932Z                 try_count += 1
2025-04-11T04:23:18.9293020Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9293100Z                 return ret
2025-04-11T04:23:18.9293192Z             except exception_type as e:
2025-04-11T04:23:18.9293293Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9293478Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9293600Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9293794Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9293949Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9294034Z                     continue
2025-04-11T04:23:18.9294108Z                 else:
2025-04-11T04:23:18.9294334Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9294412Z >                   raise e
2025-04-11T04:23:18.9294417Z 
2025-04-11T04:23:18.9294510Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9294622Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9294753Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9294840Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9295026Z tests/test_zero/test_gemini/test_grad_accum.py:158: in test_grad_accumulation
2025-04-11T04:23:18.9295114Z     spawn(run_dist, 2)
2025-04-11T04:23:18.9295213Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9295313Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9295625Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9295800Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9296092Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9296179Z     while not context.join():
2025-04-11T04:23:18.9296289Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9296337Z 
2025-04-11T04:23:18.9296541Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76290>
2025-04-11T04:23:18.9296622Z timeout = None
2025-04-11T04:23:18.9296627Z 
2025-04-11T04:23:18.9296714Z     def join(self, timeout=None):
2025-04-11T04:23:18.9296837Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9296913Z     
2025-04-11T04:23:18.9297055Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9297202Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9297363Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9297499Z         of the first process exiting.
2025-04-11T04:23:18.9297568Z     
2025-04-11T04:23:18.9297712Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9297850Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9297921Z     
2025-04-11T04:23:18.9297996Z         Args:
2025-04-11T04:23:18.9298134Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9298209Z         """
2025-04-11T04:23:18.9298346Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9298438Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9298520Z             return True
2025-04-11T04:23:18.9298588Z     
2025-04-11T04:23:18.9298721Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9298838Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9298927Z             self.sentinels.keys(),
2025-04-11T04:23:18.9299015Z             timeout=timeout,
2025-04-11T04:23:18.9299085Z         )
2025-04-11T04:23:18.9299158Z     
2025-04-11T04:23:18.9299238Z         error_index = None
2025-04-11T04:23:18.9299321Z         for sentinel in ready:
2025-04-11T04:23:18.9299427Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9299525Z             process = self.processes[index]
2025-04-11T04:23:18.9299610Z             process.join()
2025-04-11T04:23:18.9299700Z             if process.exitcode != 0:
2025-04-11T04:23:18.9299792Z                 error_index = index
2025-04-11T04:23:18.9299866Z                 break
2025-04-11T04:23:18.9299984Z     
2025-04-11T04:23:18.9300081Z         # Return if there was no error.
2025-04-11T04:23:18.9300165Z         if error_index is None:
2025-04-11T04:23:18.9300304Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9300399Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9300469Z     
2025-04-11T04:23:18.9300617Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9300713Z         for process in self.processes:
2025-04-11T04:23:18.9300804Z             if process.is_alive():
2025-04-11T04:23:18.9300895Z                 process.terminate()
2025-04-11T04:23:18.9300980Z             process.join()
2025-04-11T04:23:18.9301051Z     
2025-04-11T04:23:18.9301189Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9301307Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9301413Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9301538Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9301618Z             if exitcode < 0:
2025-04-11T04:23:18.9301723Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9301879Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9302030Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9302129Z                     error_index=error_index,
2025-04-11T04:23:18.9302227Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9302316Z                     exit_code=exitcode,
2025-04-11T04:23:18.9302399Z                     signal_name=name,
2025-04-11T04:23:18.9302518Z                 )
2025-04-11T04:23:18.9302596Z             else:
2025-04-11T04:23:18.9302698Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9302864Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9302957Z                     error_index=error_index,
2025-04-11T04:23:18.9303056Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9303143Z                     exit_code=exitcode,
2025-04-11T04:23:18.9303215Z                 )
2025-04-11T04:23:18.9303286Z     
2025-04-11T04:23:18.9303420Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9303655Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9303740Z         msg += original_trace
2025-04-11T04:23:18.9303913Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9304076Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9304148Z E       
2025-04-11T04:23:18.9304274Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9304371Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9304671Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9304755Z E           fn(i, *args)
2025-04-11T04:23:18.9305008Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
2025-04-11T04:23:18.9305099Z E           exam_gemini_grad_acc()
2025-04-11T04:23:18.9305350Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9305444Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9305691Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9305778Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9306027Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9306110Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9306218Z E         [Previous line repeated 4 more times]
2025-04-11T04:23:18.9306499Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
2025-04-11T04:23:18.9306651Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9306945Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9307050Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9307314Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9307434Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9307712Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9307797Z E           module._apply(fn)
2025-04-11T04:23:18.9308067Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9308154Z E           module._apply(fn)
2025-04-11T04:23:18.9308455Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9308605Z E           param_applied = fn(param)
2025-04-11T04:23:18.9308878Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9308999Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9309103Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9309390Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9309576Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9309738Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9309743Z 
2025-04-11T04:23:18.9310041Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9310197Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9310355Z [04/11/25 04:21:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9310482Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9310642Z                              :75 launch                                         
2025-04-11T04:23:18.9310780Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9310909Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9311109Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9311259Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9312414Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9312600Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9313724Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9313901Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9314651Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9314740Z   warnings.warn(
2025-04-11T04:23:18.9315404Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9315488Z   warnings.warn(
2025-04-11T04:23:18.9316301Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9316387Z   warnings.warn(
2025-04-11T04:23:18.9317187Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9317314Z   warnings.warn(
2025-04-11T04:23:18.9318117Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9318242Z   warnings.warn(
2025-04-11T04:23:18.9319055Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9319137Z   warnings.warn(
2025-04-11T04:23:18.9319958Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9320085Z   warnings.warn(
2025-04-11T04:23:18.9320869Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9320951Z   warnings.warn(
2025-04-11T04:23:18.9321833Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:18.9321939Z   return tensor.storage().size() == 0
2025-04-11T04:23:18.9322889Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9323060Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9323198Z ______________________________ test_grad_clip[1] _______________________________
2025-04-11T04:23:18.9323250Z 
2025-04-11T04:23:18.9323368Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T04:23:18.9323974Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9323984Z 
2025-04-11T04:23:18.9324085Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9324167Z         try_count = 0
2025-04-11T04:23:18.9324266Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9324344Z             max_try, int
2025-04-11T04:23:18.9324493Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9324563Z     
2025-04-11T04:23:18.9324675Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9324747Z             try:
2025-04-11T04:23:18.9324828Z                 try_count += 1
2025-04-11T04:23:18.9324925Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9325005Z                 return ret
2025-04-11T04:23:18.9325101Z             except exception_type as e:
2025-04-11T04:23:18.9325249Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9325441Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9325559Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9325702Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9325858Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9325986Z                     continue
2025-04-11T04:23:18.9326065Z                 else:
2025-04-11T04:23:18.9326291Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9326371Z >                   raise e
2025-04-11T04:23:18.9326378Z 
2025-04-11T04:23:18.9326469Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9326578Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9326715Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9326801Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9327012Z tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
2025-04-11T04:23:18.9327099Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9327199Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9327295Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9327551Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9327730Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9328012Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9328104Z     while not context.join():
2025-04-11T04:23:18.9328211Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9328217Z 
2025-04-11T04:23:18.9328419Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c77b20>
2025-04-11T04:23:18.9328495Z timeout = None
2025-04-11T04:23:18.9328501Z 
2025-04-11T04:23:18.9328588Z     def join(self, timeout=None):
2025-04-11T04:23:18.9328715Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9328784Z     
2025-04-11T04:23:18.9328929Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9329072Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9329238Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9329328Z         of the first process exiting.
2025-04-11T04:23:18.9329396Z     
2025-04-11T04:23:18.9329545Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9329726Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9329798Z     
2025-04-11T04:23:18.9329872Z         Args:
2025-04-11T04:23:18.9330010Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9330085Z         """
2025-04-11T04:23:18.9330224Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9330320Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9330399Z             return True
2025-04-11T04:23:18.9330471Z     
2025-04-11T04:23:18.9330600Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9330720Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9330814Z             self.sentinels.keys(),
2025-04-11T04:23:18.9330895Z             timeout=timeout,
2025-04-11T04:23:18.9330968Z         )
2025-04-11T04:23:18.9331038Z     
2025-04-11T04:23:18.9331118Z         error_index = None
2025-04-11T04:23:18.9331208Z         for sentinel in ready:
2025-04-11T04:23:18.9331315Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9331463Z             process = self.processes[index]
2025-04-11T04:23:18.9331545Z             process.join()
2025-04-11T04:23:18.9331635Z             if process.exitcode != 0:
2025-04-11T04:23:18.9331726Z                 error_index = index
2025-04-11T04:23:18.9331801Z                 break
2025-04-11T04:23:18.9331872Z     
2025-04-11T04:23:18.9331960Z         # Return if there was no error.
2025-04-11T04:23:18.9332046Z         if error_index is None:
2025-04-11T04:23:18.9332176Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9332326Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9332398Z     
2025-04-11T04:23:18.9332538Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9332637Z         for process in self.processes:
2025-04-11T04:23:18.9332724Z             if process.is_alive():
2025-04-11T04:23:18.9332816Z                 process.terminate()
2025-04-11T04:23:18.9332900Z             process.join()
2025-04-11T04:23:18.9332969Z     
2025-04-11T04:23:18.9333111Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9333227Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9333383Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9333503Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9333583Z             if exitcode < 0:
2025-04-11T04:23:18.9333690Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9333797Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9333949Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9334044Z                     error_index=error_index,
2025-04-11T04:23:18.9334142Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9334234Z                     exit_code=exitcode,
2025-04-11T04:23:18.9334318Z                     signal_name=name,
2025-04-11T04:23:18.9334392Z                 )
2025-04-11T04:23:18.9334466Z             else:
2025-04-11T04:23:18.9334569Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9334734Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9334828Z                     error_index=error_index,
2025-04-11T04:23:18.9334929Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9335014Z                     exit_code=exitcode,
2025-04-11T04:23:18.9335088Z                 )
2025-04-11T04:23:18.9335157Z     
2025-04-11T04:23:18.9335286Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9335461Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9335544Z         msg += original_trace
2025-04-11T04:23:18.9335721Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9335933Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9336013Z E       
2025-04-11T04:23:18.9336142Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9336243Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9336551Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9336634Z E           fn(i, *args)
2025-04-11T04:23:18.9336897Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T04:23:18.9336991Z E           exam_grad_clipping()
2025-04-11T04:23:18.9337254Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9337347Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9337599Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9337694Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9337989Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9338078Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9338185Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.9338459Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T04:23:18.9338561Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9338892Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9338994Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9339260Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9339382Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9339651Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9339741Z E           module._apply(fn)
2025-04-11T04:23:18.9340006Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9340137Z E           module._apply(fn)
2025-04-11T04:23:18.9340400Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9340495Z E           param_applied = fn(param)
2025-04-11T04:23:18.9340768Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9340883Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9340992Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9341283Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9341422Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9341582Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9341589Z 
2025-04-11T04:23:18.9341890Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9342042Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9342197Z [04/11/25 04:22:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9342328Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9342433Z                              :75 launch                                         
2025-04-11T04:23:18.9342570Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9342742Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.9342942Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9343084Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9344209Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9344383Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9345063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9345197Z   warnings.warn(
2025-04-11T04:23:18.9346002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9346088Z   warnings.warn(
2025-04-11T04:23:18.9346925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9347009Z   warnings.warn(
2025-04-11T04:23:18.9347790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9347919Z   warnings.warn(
2025-04-11T04:23:18.9348055Z ______________________________ test_grad_clip[2] _______________________________
2025-04-11T04:23:18.9348059Z 
2025-04-11T04:23:18.9348178Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.9348823Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9348831Z 
2025-04-11T04:23:18.9348933Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9349014Z         try_count = 0
2025-04-11T04:23:18.9349116Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9349195Z             max_try, int
2025-04-11T04:23:18.9349343Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9349415Z     
2025-04-11T04:23:18.9349525Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9349605Z             try:
2025-04-11T04:23:18.9349687Z                 try_count += 1
2025-04-11T04:23:18.9349776Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9349865Z                 return ret
2025-04-11T04:23:18.9349958Z             except exception_type as e:
2025-04-11T04:23:18.9350061Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9350249Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9350368Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9350611Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9350767Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9350855Z                     continue
2025-04-11T04:23:18.9350930Z                 else:
2025-04-11T04:23:18.9351154Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9351233Z >                   raise e
2025-04-11T04:23:18.9351238Z 
2025-04-11T04:23:18.9351332Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9351445Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9351579Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9351667Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9351829Z tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
2025-04-11T04:23:18.9351920Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9352020Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9352118Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9352376Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9352607Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9352894Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9352981Z     while not context.join():
2025-04-11T04:23:18.9353092Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9353146Z 
2025-04-11T04:23:18.9353345Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b841a9b0>
2025-04-11T04:23:18.9353423Z timeout = None
2025-04-11T04:23:18.9353428Z 
2025-04-11T04:23:18.9353514Z     def join(self, timeout=None):
2025-04-11T04:23:18.9353636Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9353710Z     
2025-04-11T04:23:18.9353850Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9353995Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9354156Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9354297Z         of the first process exiting.
2025-04-11T04:23:18.9354366Z     
2025-04-11T04:23:18.9354510Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9354646Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9354716Z     
2025-04-11T04:23:18.9354791Z         Args:
2025-04-11T04:23:18.9354928Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9354998Z         """
2025-04-11T04:23:18.9355137Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9355226Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9355310Z             return True
2025-04-11T04:23:18.9355377Z     
2025-04-11T04:23:18.9355505Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9355628Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9355717Z             self.sentinels.keys(),
2025-04-11T04:23:18.9355803Z             timeout=timeout,
2025-04-11T04:23:18.9355874Z         )
2025-04-11T04:23:18.9355944Z     
2025-04-11T04:23:18.9356023Z         error_index = None
2025-04-11T04:23:18.9356106Z         for sentinel in ready:
2025-04-11T04:23:18.9356214Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9356312Z             process = self.processes[index]
2025-04-11T04:23:18.9356398Z             process.join()
2025-04-11T04:23:18.9356489Z             if process.exitcode != 0:
2025-04-11T04:23:18.9356574Z                 error_index = index
2025-04-11T04:23:18.9356651Z                 break
2025-04-11T04:23:18.9356718Z     
2025-04-11T04:23:18.9356861Z         # Return if there was no error.
2025-04-11T04:23:18.9356943Z         if error_index is None:
2025-04-11T04:23:18.9357075Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9357175Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9357243Z     
2025-04-11T04:23:18.9357386Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9357481Z         for process in self.processes:
2025-04-11T04:23:18.9357569Z             if process.is_alive():
2025-04-11T04:23:18.9357658Z                 process.terminate()
2025-04-11T04:23:18.9357738Z             process.join()
2025-04-11T04:23:18.9357812Z     
2025-04-11T04:23:18.9357950Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9358066Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9358170Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9358288Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9358373Z             if exitcode < 0:
2025-04-11T04:23:18.9358476Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9358648Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9358797Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9358897Z                     error_index=error_index,
2025-04-11T04:23:18.9358997Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9359081Z                     exit_code=exitcode,
2025-04-11T04:23:18.9359169Z                     signal_name=name,
2025-04-11T04:23:18.9359287Z                 )
2025-04-11T04:23:18.9359362Z             else:
2025-04-11T04:23:18.9359461Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9359624Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9359716Z                     error_index=error_index,
2025-04-11T04:23:18.9359815Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9359903Z                     exit_code=exitcode,
2025-04-11T04:23:18.9359974Z                 )
2025-04-11T04:23:18.9360049Z     
2025-04-11T04:23:18.9360180Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9360350Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9360484Z         msg += original_trace
2025-04-11T04:23:18.9360655Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9360814Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9360888Z E       
2025-04-11T04:23:18.9361015Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9361114Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9361414Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9361498Z E           fn(i, *args)
2025-04-11T04:23:18.9361751Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T04:23:18.9361846Z E           exam_grad_clipping()
2025-04-11T04:23:18.9362098Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9362189Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9362437Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9362523Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9362773Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9362856Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9362962Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.9363237Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T04:23:18.9363391Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9363677Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9363776Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9364044Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9364158Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9364428Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9364515Z E           module._apply(fn)
2025-04-11T04:23:18.9364780Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9364862Z E           module._apply(fn)
2025-04-11T04:23:18.9365125Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9365219Z E           param_applied = fn(param)
2025-04-11T04:23:18.9365538Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9365658Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9365761Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9366045Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9366227Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9366390Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9366395Z 
2025-04-11T04:23:18.9366692Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9366841Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9366998Z [04/11/25 04:22:07] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9367126Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9367284Z                              :75 launch                                         
2025-04-11T04:23:18.9367419Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9367543Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9367738Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9367882Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9369000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9369176Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9370274Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9370443Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9371183Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9371271Z   warnings.warn(
2025-04-11T04:23:18.9371957Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9372040Z   warnings.warn(
2025-04-11T04:23:18.9372862Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9372942Z   warnings.warn(
2025-04-11T04:23:18.9373748Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9373875Z   warnings.warn(
2025-04-11T04:23:18.9374677Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9374799Z   warnings.warn(
2025-04-11T04:23:18.9375606Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9375684Z   warnings.warn(
2025-04-11T04:23:18.9376495Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9376618Z   warnings.warn(
2025-04-11T04:23:18.9377413Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9377493Z   warnings.warn(
2025-04-11T04:23:18.9378444Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9378617Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9378758Z ______________________________ test_inference[1] _______________________________
2025-04-11T04:23:18.9378762Z 
2025-04-11T04:23:18.9378879Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T04:23:18.9379477Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9379483Z 
2025-04-11T04:23:18.9379584Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9379713Z         try_count = 0
2025-04-11T04:23:18.9379816Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9379895Z             max_try, int
2025-04-11T04:23:18.9380045Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9380115Z     
2025-04-11T04:23:18.9380229Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9380303Z             try:
2025-04-11T04:23:18.9380385Z                 try_count += 1
2025-04-11T04:23:18.9380478Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9380556Z                 return ret
2025-04-11T04:23:18.9380654Z             except exception_type as e:
2025-04-11T04:23:18.9380750Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9380933Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9381051Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9381197Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9381355Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9381485Z                     continue
2025-04-11T04:23:18.9381564Z                 else:
2025-04-11T04:23:18.9381790Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9381867Z >                   raise e
2025-04-11T04:23:18.9381876Z 
2025-04-11T04:23:18.9381965Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9382075Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9382256Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9382342Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9382508Z tests/test_zero/test_gemini/test_inference.py:118: in test_inference
2025-04-11T04:23:18.9382595Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9382695Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9382795Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9383050Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9383230Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9383558Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9383647Z     while not context.join():
2025-04-11T04:23:18.9383758Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9383764Z 
2025-04-11T04:23:18.9383964Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c74af0>
2025-04-11T04:23:18.9384040Z timeout = None
2025-04-11T04:23:18.9384045Z 
2025-04-11T04:23:18.9384132Z     def join(self, timeout=None):
2025-04-11T04:23:18.9384260Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9384330Z     
2025-04-11T04:23:18.9384473Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9384617Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9384780Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9384876Z         of the first process exiting.
2025-04-11T04:23:18.9384944Z     
2025-04-11T04:23:18.9385091Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9385227Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9385300Z     
2025-04-11T04:23:18.9385371Z         Args:
2025-04-11T04:23:18.9385508Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9385582Z         """
2025-04-11T04:23:18.9385721Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9385815Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9385952Z             return True
2025-04-11T04:23:18.9386021Z     
2025-04-11T04:23:18.9386156Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9386274Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9386368Z             self.sentinels.keys(),
2025-04-11T04:23:18.9386452Z             timeout=timeout,
2025-04-11T04:23:18.9386526Z         )
2025-04-11T04:23:18.9386594Z     
2025-04-11T04:23:18.9386674Z         error_index = None
2025-04-11T04:23:18.9386761Z         for sentinel in ready:
2025-04-11T04:23:18.9386865Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9386968Z             process = self.processes[index]
2025-04-11T04:23:18.9387051Z             process.join()
2025-04-11T04:23:18.9387141Z             if process.exitcode != 0:
2025-04-11T04:23:18.9387229Z                 error_index = index
2025-04-11T04:23:18.9387301Z                 break
2025-04-11T04:23:18.9387372Z     
2025-04-11T04:23:18.9387462Z         # Return if there was no error.
2025-04-11T04:23:18.9387545Z         if error_index is None:
2025-04-11T04:23:18.9387678Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9387890Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9387962Z     
2025-04-11T04:23:18.9388102Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9388198Z         for process in self.processes:
2025-04-11T04:23:18.9388284Z             if process.is_alive():
2025-04-11T04:23:18.9388373Z                 process.terminate()
2025-04-11T04:23:18.9388491Z             process.join()
2025-04-11T04:23:18.9388610Z     
2025-04-11T04:23:18.9388751Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9388865Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9388970Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9389093Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9389177Z             if exitcode < 0:
2025-04-11T04:23:18.9389284Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9389390Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9389541Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9389686Z                     error_index=error_index,
2025-04-11T04:23:18.9389785Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9389876Z                     exit_code=exitcode,
2025-04-11T04:23:18.9389960Z                     signal_name=name,
2025-04-11T04:23:18.9390039Z                 )
2025-04-11T04:23:18.9390111Z             else:
2025-04-11T04:23:18.9390210Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9390376Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9390467Z                     error_index=error_index,
2025-04-11T04:23:18.9390568Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9390655Z                     exit_code=exitcode,
2025-04-11T04:23:18.9390730Z                 )
2025-04-11T04:23:18.9390798Z     
2025-04-11T04:23:18.9390928Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9391103Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9391189Z         msg += original_trace
2025-04-11T04:23:18.9391362Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9391523Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9391599Z E       
2025-04-11T04:23:18.9391723Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9391819Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9392117Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9392250Z E           fn(i, *args)
2025-04-11T04:23:18.9392503Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T04:23:18.9392588Z E           exam_inference()
2025-04-11T04:23:18.9392841Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9392934Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9393178Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9393270Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9393515Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9393601Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9393855Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T04:23:18.9393961Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9394244Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9394392Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9394660Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9394779Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9395049Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9395180Z E           module._apply(fn)
2025-04-11T04:23:18.9395449Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9395531Z E           module._apply(fn)
2025-04-11T04:23:18.9395802Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9395899Z E           param_applied = fn(param)
2025-04-11T04:23:18.9396174Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9396295Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9396446Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9396730Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9396861Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9397025Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9397029Z 
2025-04-11T04:23:18.9397328Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9397475Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9397630Z [04/11/25 04:22:14] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9397756Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9397863Z                              :75 launch                                         
2025-04-11T04:23:18.9398000Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9398124Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.9398320Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9398463Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9399577Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9399800Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9400485Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9400568Z   warnings.warn(
2025-04-11T04:23:18.9401388Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9401471Z   warnings.warn(
2025-04-11T04:23:18.9402282Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9402408Z   warnings.warn(
2025-04-11T04:23:18.9403214Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9403337Z   warnings.warn(
2025-04-11T04:23:18.9403475Z ______________________________ test_inference[4] _______________________________
2025-04-11T04:23:18.9403479Z 
2025-04-11T04:23:18.9403597Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9404195Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9404249Z 
2025-04-11T04:23:18.9404351Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9404433Z         try_count = 0
2025-04-11T04:23:18.9404532Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9404611Z             max_try, int
2025-04-11T04:23:18.9404758Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9404830Z     
2025-04-11T04:23:18.9404941Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9405013Z             try:
2025-04-11T04:23:18.9405097Z                 try_count += 1
2025-04-11T04:23:18.9405186Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9405266Z                 return ret
2025-04-11T04:23:18.9405361Z             except exception_type as e:
2025-04-11T04:23:18.9405456Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9405645Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9405758Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9405904Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9406062Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9406144Z                     continue
2025-04-11T04:23:18.9406221Z                 else:
2025-04-11T04:23:18.9406441Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9406523Z >                   raise e
2025-04-11T04:23:18.9406527Z 
2025-04-11T04:23:18.9406618Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9406777Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9406911Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9406999Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9407167Z tests/test_zero/test_gemini/test_inference.py:118: in test_inference
2025-04-11T04:23:18.9407254Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9407355Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9407453Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9407701Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9407880Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9408159Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9408249Z     while not context.join():
2025-04-11T04:23:18.9408357Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9408361Z 
2025-04-11T04:23:18.9408563Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8490bb0>
2025-04-11T04:23:18.9408687Z timeout = None
2025-04-11T04:23:18.9408691Z 
2025-04-11T04:23:18.9408785Z     def join(self, timeout=None):
2025-04-11T04:23:18.9408911Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9408979Z     
2025-04-11T04:23:18.9409124Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9409266Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9409479Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9409571Z         of the first process exiting.
2025-04-11T04:23:18.9409640Z     
2025-04-11T04:23:18.9409788Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9409926Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9409998Z     
2025-04-11T04:23:18.9410071Z         Args:
2025-04-11T04:23:18.9410213Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9410284Z         """
2025-04-11T04:23:18.9410420Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9410577Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9410654Z             return True
2025-04-11T04:23:18.9410725Z     
2025-04-11T04:23:18.9410854Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9410971Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9411063Z             self.sentinels.keys(),
2025-04-11T04:23:18.9411144Z             timeout=timeout,
2025-04-11T04:23:18.9411217Z         )
2025-04-11T04:23:18.9411284Z     
2025-04-11T04:23:18.9411364Z         error_index = None
2025-04-11T04:23:18.9411452Z         for sentinel in ready:
2025-04-11T04:23:18.9411556Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9411657Z             process = self.processes[index]
2025-04-11T04:23:18.9411741Z             process.join()
2025-04-11T04:23:18.9411832Z             if process.exitcode != 0:
2025-04-11T04:23:18.9411917Z                 error_index = index
2025-04-11T04:23:18.9411992Z                 break
2025-04-11T04:23:18.9412063Z     
2025-04-11T04:23:18.9412151Z         # Return if there was no error.
2025-04-11T04:23:18.9412236Z         if error_index is None:
2025-04-11T04:23:18.9412366Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9412462Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9412532Z     
2025-04-11T04:23:18.9412667Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9412763Z         for process in self.processes:
2025-04-11T04:23:18.9412847Z             if process.is_alive():
2025-04-11T04:23:18.9412992Z                 process.terminate()
2025-04-11T04:23:18.9413074Z             process.join()
2025-04-11T04:23:18.9413141Z     
2025-04-11T04:23:18.9413283Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9413398Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9413505Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9413626Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9413708Z             if exitcode < 0:
2025-04-11T04:23:18.9413816Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9413919Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9414071Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9414164Z                     error_index=error_index,
2025-04-11T04:23:18.9414266Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9414352Z                     exit_code=exitcode,
2025-04-11T04:23:18.9414437Z                     signal_name=name,
2025-04-11T04:23:18.9414512Z                 )
2025-04-11T04:23:18.9414583Z             else:
2025-04-11T04:23:18.9414739Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9414905Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9415002Z                     error_index=error_index,
2025-04-11T04:23:18.9415106Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9415194Z                     exit_code=exitcode,
2025-04-11T04:23:18.9415271Z                 )
2025-04-11T04:23:18.9415343Z     
2025-04-11T04:23:18.9415529Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9415696Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9415779Z         msg += original_trace
2025-04-11T04:23:18.9415955Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9416120Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9416195Z E       
2025-04-11T04:23:18.9416318Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9416417Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9416716Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9416863Z E           fn(i, *args)
2025-04-11T04:23:18.9417119Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T04:23:18.9417205Z E           exam_inference()
2025-04-11T04:23:18.9417466Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9417553Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9417800Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9417893Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9418139Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9418228Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9418484Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T04:23:18.9418589Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9418872Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9418975Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9419239Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9419353Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9419627Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9419769Z E           module._apply(fn)
2025-04-11T04:23:18.9420038Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9420119Z E           module._apply(fn)
2025-04-11T04:23:18.9420384Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9420476Z E           param_applied = fn(param)
2025-04-11T04:23:18.9420744Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9420860Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9420966Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9421251Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9421386Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9421551Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9421610Z 
2025-04-11T04:23:18.9421911Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9422065Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9422217Z [04/11/25 04:22:24] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9422342Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9422500Z                              :75 launch                                         
2025-04-11T04:23:18.9422636Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9422761Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9422959Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9423104Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9424248Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9424467Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9425564Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9425733Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9426843Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9427008Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9428101Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9428319Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9429042Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9429130Z   warnings.warn(
2025-04-11T04:23:18.9429810Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9429897Z   warnings.warn(
2025-04-11T04:23:18.9430569Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9430711Z   warnings.warn(
2025-04-11T04:23:18.9431401Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9431535Z   warnings.warn(
2025-04-11T04:23:18.9432377Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9432456Z   warnings.warn(
2025-04-11T04:23:18.9433282Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9433415Z   warnings.warn(
2025-04-11T04:23:18.9434219Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9434297Z   warnings.warn(
2025-04-11T04:23:18.9435092Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9435171Z   warnings.warn(
2025-04-11T04:23:18.9435962Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9436041Z   warnings.warn(
2025-04-11T04:23:18.9436835Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9436971Z   warnings.warn(
2025-04-11T04:23:18.9437768Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9437846Z   warnings.warn(
2025-04-11T04:23:18.9438649Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9438727Z   warnings.warn(
2025-04-11T04:23:18.9439529Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9439607Z   warnings.warn(
2025-04-11T04:23:18.9440455Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9440536Z   warnings.warn(
2025-04-11T04:23:18.9440829Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9441175Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9441448Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9442250Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9442378Z   warnings.warn(
2025-04-11T04:23:18.9443182Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9443261Z   warnings.warn(
2025-04-11T04:23:18.9443546Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9443818Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9444760Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9444933Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9445843Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9446067Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9446991Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9447157Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9447296Z ________________________________ test_optim[4] _________________________________
2025-04-11T04:23:18.9447302Z 
2025-04-11T04:23:18.9447419Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9448022Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9448030Z 
2025-04-11T04:23:18.9448130Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9448266Z         try_count = 0
2025-04-11T04:23:18.9448367Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9448447Z             max_try, int
2025-04-11T04:23:18.9448598Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9448667Z     
2025-04-11T04:23:18.9448782Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9448855Z             try:
2025-04-11T04:23:18.9448988Z                 try_count += 1
2025-04-11T04:23:18.9449085Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9449163Z                 return ret
2025-04-11T04:23:18.9449260Z             except exception_type as e:
2025-04-11T04:23:18.9449357Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9449547Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9449666Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9449812Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9449971Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9450099Z                     continue
2025-04-11T04:23:18.9450177Z                 else:
2025-04-11T04:23:18.9450399Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9450480Z >                   raise e
2025-04-11T04:23:18.9450485Z 
2025-04-11T04:23:18.9450575Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9450685Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9450819Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9450907Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9451055Z tests/test_zero/test_gemini/test_optim.py:193: in test_optim
2025-04-11T04:23:18.9451142Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9451245Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9451342Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9451592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9451772Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9452053Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9452144Z     while not context.join():
2025-04-11T04:23:18.9452251Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9452256Z 
2025-04-11T04:23:18.9452454Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76260>
2025-04-11T04:23:18.9452586Z timeout = None
2025-04-11T04:23:18.9452591Z 
2025-04-11T04:23:18.9452679Z     def join(self, timeout=None):
2025-04-11T04:23:18.9452804Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9452875Z     
2025-04-11T04:23:18.9453022Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9453167Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9453331Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9453420Z         of the first process exiting.
2025-04-11T04:23:18.9453488Z     
2025-04-11T04:23:18.9453636Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9453771Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9453841Z     
2025-04-11T04:23:18.9453912Z         Args:
2025-04-11T04:23:18.9454048Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9454123Z         """
2025-04-11T04:23:18.9454260Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9454354Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9454485Z             return True
2025-04-11T04:23:18.9454557Z     
2025-04-11T04:23:18.9454687Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9454804Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9454896Z             self.sentinels.keys(),
2025-04-11T04:23:18.9454979Z             timeout=timeout,
2025-04-11T04:23:18.9455050Z         )
2025-04-11T04:23:18.9455118Z     
2025-04-11T04:23:18.9455250Z         error_index = None
2025-04-11T04:23:18.9455337Z         for sentinel in ready:
2025-04-11T04:23:18.9455440Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9455540Z             process = self.processes[index]
2025-04-11T04:23:18.9455623Z             process.join()
2025-04-11T04:23:18.9455718Z             if process.exitcode != 0:
2025-04-11T04:23:18.9455806Z                 error_index = index
2025-04-11T04:23:18.9455878Z                 break
2025-04-11T04:23:18.9455949Z     
2025-04-11T04:23:18.9456039Z         # Return if there was no error.
2025-04-11T04:23:18.9456124Z         if error_index is None:
2025-04-11T04:23:18.9456255Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9456398Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9456470Z     
2025-04-11T04:23:18.9456611Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9456709Z         for process in self.processes:
2025-04-11T04:23:18.9456797Z             if process.is_alive():
2025-04-11T04:23:18.9456887Z                 process.terminate()
2025-04-11T04:23:18.9456972Z             process.join()
2025-04-11T04:23:18.9457039Z     
2025-04-11T04:23:18.9457178Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9457294Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9457401Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9457519Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9457602Z             if exitcode < 0:
2025-04-11T04:23:18.9457711Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9457817Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9457968Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9458061Z                     error_index=error_index,
2025-04-11T04:23:18.9458160Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9458250Z                     exit_code=exitcode,
2025-04-11T04:23:18.9458333Z                     signal_name=name,
2025-04-11T04:23:18.9458408Z                 )
2025-04-11T04:23:18.9458479Z             else:
2025-04-11T04:23:18.9458582Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9458797Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9458888Z                     error_index=error_index,
2025-04-11T04:23:18.9458991Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9459076Z                     exit_code=exitcode,
2025-04-11T04:23:18.9459149Z                 )
2025-04-11T04:23:18.9459219Z     
2025-04-11T04:23:18.9459351Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9459523Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9459606Z         msg += original_trace
2025-04-11T04:23:18.9459786Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9459949Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9460023Z E       
2025-04-11T04:23:18.9460145Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9460243Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9460545Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9460675Z E           fn(i, *args)
2025-04-11T04:23:18.9460923Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
2025-04-11T04:23:18.9461009Z E           exam_model_step()
2025-04-11T04:23:18.9461268Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9461356Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9461656Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9461745Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9461990Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9462080Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9462186Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.9462437Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
2025-04-11T04:23:18.9462539Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9462824Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9462976Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9463239Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9463361Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9463628Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9463713Z E           module._apply(fn)
2025-04-11T04:23:18.9463978Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9464063Z E           module._apply(fn)
2025-04-11T04:23:18.9464326Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9464417Z E           param_applied = fn(param)
2025-04-11T04:23:18.9464693Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9464807Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9464915Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9465201Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9465337Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9465497Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9465553Z 
2025-04-11T04:23:18.9465854Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9466008Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9466162Z [04/11/25 04:22:32] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9466296Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9466400Z                              :75 launch                                         
2025-04-11T04:23:18.9466542Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9466665Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9466862Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9467004Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9468130Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9468371Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9469510Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9469736Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9470827Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9471047Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9472148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9472308Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9472999Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9473082Z   warnings.warn(
2025-04-11T04:23:18.9473750Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9473836Z   warnings.warn(
2025-04-11T04:23:18.9474502Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9474643Z   warnings.warn(
2025-04-11T04:23:18.9475302Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9475383Z   warnings.warn(
2025-04-11T04:23:18.9476195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9476277Z   warnings.warn(
2025-04-11T04:23:18.9477067Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9477206Z   warnings.warn(
2025-04-11T04:23:18.9477997Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9478129Z   warnings.warn(
2025-04-11T04:23:18.9478907Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9478988Z   warnings.warn(
2025-04-11T04:23:18.9479764Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9479893Z   warnings.warn(
2025-04-11T04:23:18.9480680Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9480762Z   warnings.warn(
2025-04-11T04:23:18.9481546Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9481629Z   warnings.warn(
2025-04-11T04:23:18.9482412Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9482493Z   warnings.warn(
2025-04-11T04:23:18.9483278Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9483354Z   warnings.warn(
2025-04-11T04:23:18.9484195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9484274Z   warnings.warn(
2025-04-11T04:23:18.9485076Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9485154Z   warnings.warn(
2025-04-11T04:23:18.9485947Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9486026Z   warnings.warn(
2025-04-11T04:23:18.9486971Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9487193Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9488150Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9488319Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9489226Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9489445Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9489588Z ________________________________ test_search[1] ________________________________
2025-04-11T04:23:18.9489592Z 
2025-04-11T04:23:18.9489710Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T04:23:18.9490313Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9490321Z 
2025-04-11T04:23:18.9490426Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9490507Z         try_count = 0
2025-04-11T04:23:18.9490609Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9490691Z             max_try, int
2025-04-11T04:23:18.9490842Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9490910Z     
2025-04-11T04:23:18.9491027Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9491099Z             try:
2025-04-11T04:23:18.9491187Z                 try_count += 1
2025-04-11T04:23:18.9491276Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9491355Z                 return ret
2025-04-11T04:23:18.9491454Z             except exception_type as e:
2025-04-11T04:23:18.9491552Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9491792Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9491908Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9492055Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9492213Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9492296Z                     continue
2025-04-11T04:23:18.9492375Z                 else:
2025-04-11T04:23:18.9492594Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9492678Z >                   raise e
2025-04-11T04:23:18.9492683Z 
2025-04-11T04:23:18.9492775Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9492889Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9493019Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9493106Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9493252Z tests/test_zero/test_gemini/test_search.py:68: in test_search
2025-04-11T04:23:18.9493386Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9493487Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9493584Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9493842Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9494025Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9494310Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9494465Z     while not context.join():
2025-04-11T04:23:18.9494577Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9494581Z 
2025-04-11T04:23:18.9494785Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a6dcf0>
2025-04-11T04:23:18.9494865Z timeout = None
2025-04-11T04:23:18.9494870Z 
2025-04-11T04:23:18.9494960Z     def join(self, timeout=None):
2025-04-11T04:23:18.9495085Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9495155Z     
2025-04-11T04:23:18.9495303Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9495494Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9495660Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9495751Z         of the first process exiting.
2025-04-11T04:23:18.9495821Z     
2025-04-11T04:23:18.9495969Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9496103Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9496175Z     
2025-04-11T04:23:18.9496248Z         Args:
2025-04-11T04:23:18.9496386Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9496460Z         """
2025-04-11T04:23:18.9496597Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9496694Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9496773Z             return True
2025-04-11T04:23:18.9496844Z     
2025-04-11T04:23:18.9496973Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9497092Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9497184Z             self.sentinels.keys(),
2025-04-11T04:23:18.9497266Z             timeout=timeout,
2025-04-11T04:23:18.9497342Z         )
2025-04-11T04:23:18.9497411Z     
2025-04-11T04:23:18.9497494Z         error_index = None
2025-04-11T04:23:18.9497581Z         for sentinel in ready:
2025-04-11T04:23:18.9497683Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9497785Z             process = self.processes[index]
2025-04-11T04:23:18.9497871Z             process.join()
2025-04-11T04:23:18.9498028Z             if process.exitcode != 0:
2025-04-11T04:23:18.9498115Z                 error_index = index
2025-04-11T04:23:18.9498190Z                 break
2025-04-11T04:23:18.9498266Z     
2025-04-11T04:23:18.9498355Z         # Return if there was no error.
2025-04-11T04:23:18.9498441Z         if error_index is None:
2025-04-11T04:23:18.9498575Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9498671Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9498742Z     
2025-04-11T04:23:18.9498880Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9498981Z         for process in self.processes:
2025-04-11T04:23:18.9499067Z             if process.is_alive():
2025-04-11T04:23:18.9499161Z                 process.terminate()
2025-04-11T04:23:18.9499244Z             process.join()
2025-04-11T04:23:18.9499312Z     
2025-04-11T04:23:18.9499455Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9499571Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9499679Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9499850Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9499932Z             if exitcode < 0:
2025-04-11T04:23:18.9500040Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9500145Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9500298Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9500392Z                     error_index=error_index,
2025-04-11T04:23:18.9500546Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9500633Z                     exit_code=exitcode,
2025-04-11T04:23:18.9500717Z                     signal_name=name,
2025-04-11T04:23:18.9500792Z                 )
2025-04-11T04:23:18.9500865Z             else:
2025-04-11T04:23:18.9500968Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9501130Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9501221Z                     error_index=error_index,
2025-04-11T04:23:18.9501324Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9501408Z                     exit_code=exitcode,
2025-04-11T04:23:18.9501529Z                 )
2025-04-11T04:23:18.9501597Z     
2025-04-11T04:23:18.9501731Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9501901Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9501985Z         msg += original_trace
2025-04-11T04:23:18.9502160Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9502321Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9502395Z E       
2025-04-11T04:23:18.9502522Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9502625Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9502928Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9503010Z E           fn(i, *args)
2025-04-11T04:23:18.9503255Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T04:23:18.9503343Z E           exam_chunk_manager()
2025-04-11T04:23:18.9503608Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T04:23:18.9503711Z E           chunk_manager = init_chunk_manager(
2025-04-11T04:23:18.9503981Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T04:23:18.9504063Z E           dist.barrier()
2025-04-11T04:23:18.9504365Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.9504587Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.9504907Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:18.9505016Z E           work = default_pg.barrier(opts=opts)
2025-04-11T04:23:18.9505119Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9505406Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9505540Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9505703Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9505712Z 
2025-04-11T04:23:18.9506013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9506167Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9506329Z [04/11/25 04:22:36] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9506455Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9506623Z                              :75 launch                                         
2025-04-11T04:23:18.9506762Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9506887Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.9507081Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9507244Z ________________________________ test_search[4] ________________________________
2025-04-11T04:23:18.9507252Z 
2025-04-11T04:23:18.9507366Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9507959Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9507969Z 
2025-04-11T04:23:18.9508073Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9508149Z         try_count = 0
2025-04-11T04:23:18.9508251Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9508381Z             max_try, int
2025-04-11T04:23:18.9508556Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9508625Z     
2025-04-11T04:23:18.9508736Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9508814Z             try:
2025-04-11T04:23:18.9508894Z                 try_count += 1
2025-04-11T04:23:18.9508987Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9509065Z                 return ret
2025-04-11T04:23:18.9509157Z             except exception_type as e:
2025-04-11T04:23:18.9509257Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9509441Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9509563Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9509705Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9509862Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9509942Z                     continue
2025-04-11T04:23:18.9510017Z                 else:
2025-04-11T04:23:18.9510240Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9510321Z >                   raise e
2025-04-11T04:23:18.9510325Z 
2025-04-11T04:23:18.9510419Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9510528Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9510660Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9510807Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9510952Z tests/test_zero/test_gemini/test_search.py:68: in test_search
2025-04-11T04:23:18.9511043Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9511141Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9511242Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9511503Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9511680Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9511964Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9512052Z     while not context.join():
2025-04-11T04:23:18.9512165Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9512169Z 
2025-04-11T04:23:18.9512363Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c75d50>
2025-04-11T04:23:18.9512445Z timeout = None
2025-04-11T04:23:18.9512450Z 
2025-04-11T04:23:18.9512538Z     def join(self, timeout=None):
2025-04-11T04:23:18.9512720Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9512790Z     
2025-04-11T04:23:18.9512933Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9513082Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9513245Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9513339Z         of the first process exiting.
2025-04-11T04:23:18.9513460Z     
2025-04-11T04:23:18.9513605Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9513737Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9513805Z     
2025-04-11T04:23:18.9513881Z         Args:
2025-04-11T04:23:18.9514018Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9514092Z         """
2025-04-11T04:23:18.9514228Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9514321Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9514402Z             return True
2025-04-11T04:23:18.9514469Z     
2025-04-11T04:23:18.9514651Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9514768Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9514862Z             self.sentinels.keys(),
2025-04-11T04:23:18.9514943Z             timeout=timeout,
2025-04-11T04:23:18.9515016Z         )
2025-04-11T04:23:18.9515087Z     
2025-04-11T04:23:18.9515168Z         error_index = None
2025-04-11T04:23:18.9515254Z         for sentinel in ready:
2025-04-11T04:23:18.9515356Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9515452Z             process = self.processes[index]
2025-04-11T04:23:18.9515544Z             process.join()
2025-04-11T04:23:18.9515635Z             if process.exitcode != 0:
2025-04-11T04:23:18.9515725Z                 error_index = index
2025-04-11T04:23:18.9515800Z                 break
2025-04-11T04:23:18.9515868Z     
2025-04-11T04:23:18.9515958Z         # Return if there was no error.
2025-04-11T04:23:18.9516040Z         if error_index is None:
2025-04-11T04:23:18.9516177Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9516271Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9516342Z     
2025-04-11T04:23:18.9516479Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9516575Z         for process in self.processes:
2025-04-11T04:23:18.9516665Z             if process.is_alive():
2025-04-11T04:23:18.9516755Z                 process.terminate()
2025-04-11T04:23:18.9516839Z             process.join()
2025-04-11T04:23:18.9516906Z     
2025-04-11T04:23:18.9517043Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9517215Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9517322Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9517449Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9517535Z             if exitcode < 0:
2025-04-11T04:23:18.9517649Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9517756Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9517908Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9518010Z                     error_index=error_index,
2025-04-11T04:23:18.9518114Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9518207Z                     exit_code=exitcode,
2025-04-11T04:23:18.9518294Z                     signal_name=name,
2025-04-11T04:23:18.9518368Z                 )
2025-04-11T04:23:18.9518445Z             else:
2025-04-11T04:23:18.9518549Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9518719Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9518862Z                     error_index=error_index,
2025-04-11T04:23:18.9518963Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9519048Z                     exit_code=exitcode,
2025-04-11T04:23:18.9519121Z                 )
2025-04-11T04:23:18.9519192Z     
2025-04-11T04:23:18.9519320Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9519493Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9519631Z         msg += original_trace
2025-04-11T04:23:18.9519801Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9519959Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9520032Z E       
2025-04-11T04:23:18.9520158Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9520258Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9520557Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9520638Z E           fn(i, *args)
2025-04-11T04:23:18.9520874Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T04:23:18.9521030Z E           exam_chunk_manager()
2025-04-11T04:23:18.9521291Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T04:23:18.9521400Z E           chunk_manager = init_chunk_manager(
2025-04-11T04:23:18.9521664Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T04:23:18.9521748Z E           dist.barrier()
2025-04-11T04:23:18.9522042Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.9522136Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.9522460Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:18.9522564Z E           work = default_pg.barrier(opts=opts)
2025-04-11T04:23:18.9522670Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9522953Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9523087Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9523247Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9523252Z 
2025-04-11T04:23:18.9523559Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9523709Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9523914Z [04/11/25 04:22:42] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9524049Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9524152Z                              :75 launch                                         
2025-04-11T04:23:18.9524295Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9524416Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9524613Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9524760Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9525078Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43909 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9525210Z _______________________________ test_zero_ddp[4] _______________________________
2025-04-11T04:23:18.9525215Z 
2025-04-11T04:23:18.9525328Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9525986Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9525994Z 
2025-04-11T04:23:18.9526094Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9526176Z         try_count = 0
2025-04-11T04:23:18.9526323Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9526409Z             max_try, int
2025-04-11T04:23:18.9526561Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9526635Z     
2025-04-11T04:23:18.9526749Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9526824Z             try:
2025-04-11T04:23:18.9526917Z                 try_count += 1
2025-04-11T04:23:18.9527012Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9527097Z                 return ret
2025-04-11T04:23:18.9527195Z             except exception_type as e:
2025-04-11T04:23:18.9527298Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9527535Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9527649Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9527796Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9527952Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9528035Z                     continue
2025-04-11T04:23:18.9528109Z                 else:
2025-04-11T04:23:18.9528325Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9528409Z >                   raise e
2025-04-11T04:23:18.9528414Z 
2025-04-11T04:23:18.9528505Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9528619Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9528748Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9528838Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9529015Z tests/test_zero/test_gemini/test_zeroddp_state_dict.py:85: in test_zero_ddp
2025-04-11T04:23:18.9529101Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9529204Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9529303Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9529559Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9529733Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9530017Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9530153Z     while not context.join():
2025-04-11T04:23:18.9530263Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9530269Z 
2025-04-11T04:23:18.9530466Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840b160>
2025-04-11T04:23:18.9530547Z timeout = None
2025-04-11T04:23:18.9530552Z 
2025-04-11T04:23:18.9530642Z     def join(self, timeout=None):
2025-04-11T04:23:18.9530765Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9530837Z     
2025-04-11T04:23:18.9530986Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9531128Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9531293Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9531383Z         of the first process exiting.
2025-04-11T04:23:18.9531455Z     
2025-04-11T04:23:18.9531599Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9531737Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9531855Z     
2025-04-11T04:23:18.9531928Z         Args:
2025-04-11T04:23:18.9532073Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9532148Z         """
2025-04-11T04:23:18.9532290Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9532381Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9532459Z             return True
2025-04-11T04:23:18.9532580Z     
2025-04-11T04:23:18.9532712Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9532828Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9532918Z             self.sentinels.keys(),
2025-04-11T04:23:18.9533001Z             timeout=timeout,
2025-04-11T04:23:18.9533076Z         )
2025-04-11T04:23:18.9533146Z     
2025-04-11T04:23:18.9533232Z         error_index = None
2025-04-11T04:23:18.9533316Z         for sentinel in ready:
2025-04-11T04:23:18.9533427Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9533525Z             process = self.processes[index]
2025-04-11T04:23:18.9533608Z             process.join()
2025-04-11T04:23:18.9533753Z             if process.exitcode != 0:
2025-04-11T04:23:18.9533838Z                 error_index = index
2025-04-11T04:23:18.9533914Z                 break
2025-04-11T04:23:18.9533982Z     
2025-04-11T04:23:18.9534071Z         # Return if there was no error.
2025-04-11T04:23:18.9534160Z         if error_index is None:
2025-04-11T04:23:18.9534294Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9534392Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9534460Z     
2025-04-11T04:23:18.9534598Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9534699Z         for process in self.processes:
2025-04-11T04:23:18.9534787Z             if process.is_alive():
2025-04-11T04:23:18.9534879Z                 process.terminate()
2025-04-11T04:23:18.9534968Z             process.join()
2025-04-11T04:23:18.9535038Z     
2025-04-11T04:23:18.9535174Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9535291Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9535400Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9535518Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9535604Z             if exitcode < 0:
2025-04-11T04:23:18.9535711Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9535814Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9535964Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9536057Z                     error_index=error_index,
2025-04-11T04:23:18.9536212Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9536298Z                     exit_code=exitcode,
2025-04-11T04:23:18.9536389Z                     signal_name=name,
2025-04-11T04:23:18.9536460Z                 )
2025-04-11T04:23:18.9536531Z             else:
2025-04-11T04:23:18.9536635Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9536798Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9536893Z                     error_index=error_index,
2025-04-11T04:23:18.9536990Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9537080Z                     exit_code=exitcode,
2025-04-11T04:23:18.9537151Z                 )
2025-04-11T04:23:18.9537220Z     
2025-04-11T04:23:18.9537352Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9537520Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9537608Z         msg += original_trace
2025-04-11T04:23:18.9537778Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9537989Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9538064Z E       
2025-04-11T04:23:18.9538189Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9538293Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9538591Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9538672Z E           fn(i, *args)
2025-04-11T04:23:18.9538997Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
2025-04-11T04:23:18.9539081Z E           exam_state_dict()
2025-04-11T04:23:18.9539341Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9539429Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9539684Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9539772Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9540020Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9540152Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9540253Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.9540541Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
2025-04-11T04:23:18.9540796Z E           model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
2025-04-11T04:23:18.9541034Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
2025-04-11T04:23:18.9541135Z E           self.chunk_manager = ChunkManager(
2025-04-11T04:23:18.9541377Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T04:23:18.9541627Z E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.9541736Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9542028Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9542161Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9542324Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9542331Z 
2025-04-11T04:23:18.9542636Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9542789Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9542995Z [04/11/25 04:22:49] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9543124Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9543233Z                              :75 launch                                         
2025-04-11T04:23:18.9543373Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9543496Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9543691Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9543837Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9544975Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9545198Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9546300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9546530Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9547669Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9547838Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9549017Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9549182Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9549857Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9549946Z   warnings.warn(
2025-04-11T04:23:18.9550615Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9550701Z   warnings.warn(
2025-04-11T04:23:18.9551360Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9551444Z   warnings.warn(
2025-04-11T04:23:18.9552098Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9552237Z   warnings.warn(
2025-04-11T04:23:18.9553080Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9553164Z   warnings.warn(
2025-04-11T04:23:18.9553971Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9554052Z   warnings.warn(
2025-04-11T04:23:18.9554852Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9554986Z   warnings.warn(
2025-04-11T04:23:18.9555790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9555920Z   warnings.warn(
2025-04-11T04:23:18.9556705Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9556786Z   warnings.warn(
2025-04-11T04:23:18.9557567Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9557695Z   warnings.warn(
2025-04-11T04:23:18.9558514Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9558592Z   warnings.warn(
2025-04-11T04:23:18.9559393Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9559472Z   warnings.warn(
2025-04-11T04:23:18.9560274Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9560352Z   warnings.warn(
2025-04-11T04:23:18.9561149Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9561274Z   warnings.warn(
2025-04-11T04:23:18.9562064Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9562143Z   warnings.warn(
2025-04-11T04:23:18.9562923Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9562999Z   warnings.warn(
2025-04-11T04:23:18.9563166Z Exception ignored in: <function GeminiDDP.__del__ at 0x7ff5876a5fc0>
2025-04-11T04:23:18.9563259Z Traceback (most recent call last):
2025-04-11T04:23:18.9563495Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.9563586Z     self.remove_hooks()
2025-04-11T04:23:18.9563829Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.9563995Z     for p in self.module.parameters():
2025-04-11T04:23:18.9564298Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.9564492Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.9564642Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.9565582Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:18.9565685Z   return tensor.storage().size() == 0
2025-04-11T04:23:18.9566560Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:18.9566703Z   return tensor.storage().size() == 0
2025-04-11T04:23:18.9566839Z _________________________________ test_comm_nd _________________________________
2025-04-11T04:23:18.9566849Z 
2025-04-11T04:23:18.9566937Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9567539Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9567546Z 
2025-04-11T04:23:18.9567652Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9567730Z         try_count = 0
2025-04-11T04:23:18.9567835Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9567913Z             max_try, int
2025-04-11T04:23:18.9568061Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9568132Z     
2025-04-11T04:23:18.9568242Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9568318Z             try:
2025-04-11T04:23:18.9568401Z                 try_count += 1
2025-04-11T04:23:18.9568494Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9568571Z                 return ret
2025-04-11T04:23:18.9568665Z             except exception_type as e:
2025-04-11T04:23:18.9568768Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9568953Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9569125Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9569270Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9569430Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9569511Z                     continue
2025-04-11T04:23:18.9569585Z                 else:
2025-04-11T04:23:18.9569811Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9569888Z >                   raise e
2025-04-11T04:23:18.9569894Z 
2025-04-11T04:23:18.9569988Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9570099Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9570232Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9570317Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9570474Z tests/test_zero/test_low_level/test_coll_nd.py:38: in test_comm_nd
2025-04-11T04:23:18.9570560Z     spawn(run_dist, 4)
2025-04-11T04:23:18.9570660Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9570809Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9571064Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9571247Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9571534Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9571669Z     while not context.join():
2025-04-11T04:23:18.9571783Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9571787Z 
2025-04-11T04:23:18.9571981Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76dd0>
2025-04-11T04:23:18.9572060Z timeout = None
2025-04-11T04:23:18.9572065Z 
2025-04-11T04:23:18.9572155Z     def join(self, timeout=None):
2025-04-11T04:23:18.9572279Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9572349Z     
2025-04-11T04:23:18.9572493Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9572642Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9572853Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9572948Z         of the first process exiting.
2025-04-11T04:23:18.9573017Z     
2025-04-11T04:23:18.9573164Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9573301Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9573370Z     
2025-04-11T04:23:18.9573447Z         Args:
2025-04-11T04:23:18.9573582Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9573656Z         """
2025-04-11T04:23:18.9573797Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9573888Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9573970Z             return True
2025-04-11T04:23:18.9574040Z     
2025-04-11T04:23:18.9574171Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9574289Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9574383Z             self.sentinels.keys(),
2025-04-11T04:23:18.9574465Z             timeout=timeout,
2025-04-11T04:23:18.9574536Z         )
2025-04-11T04:23:18.9574609Z     
2025-04-11T04:23:18.9574690Z         error_index = None
2025-04-11T04:23:18.9574778Z         for sentinel in ready:
2025-04-11T04:23:18.9574883Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9574980Z             process = self.processes[index]
2025-04-11T04:23:18.9575069Z             process.join()
2025-04-11T04:23:18.9575160Z             if process.exitcode != 0:
2025-04-11T04:23:18.9575248Z                 error_index = index
2025-04-11T04:23:18.9575371Z                 break
2025-04-11T04:23:18.9575438Z     
2025-04-11T04:23:18.9575531Z         # Return if there was no error.
2025-04-11T04:23:18.9575618Z         if error_index is None:
2025-04-11T04:23:18.9575754Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9575851Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9575921Z     
2025-04-11T04:23:18.9576057Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9576151Z         for process in self.processes:
2025-04-11T04:23:18.9576242Z             if process.is_alive():
2025-04-11T04:23:18.9576334Z                 process.terminate()
2025-04-11T04:23:18.9576418Z             process.join()
2025-04-11T04:23:18.9576485Z     
2025-04-11T04:23:18.9576623Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9576740Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9576847Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9576966Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9577183Z             if exitcode < 0:
2025-04-11T04:23:18.9577292Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9577399Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9577548Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9577647Z                     error_index=error_index,
2025-04-11T04:23:18.9577747Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9577885Z                     exit_code=exitcode,
2025-04-11T04:23:18.9577971Z                     signal_name=name,
2025-04-11T04:23:18.9578042Z                 )
2025-04-11T04:23:18.9578116Z             else:
2025-04-11T04:23:18.9578217Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9578384Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9578478Z                     error_index=error_index,
2025-04-11T04:23:18.9578580Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9578667Z                     exit_code=exitcode,
2025-04-11T04:23:18.9578737Z                 )
2025-04-11T04:23:18.9578807Z     
2025-04-11T04:23:18.9578987Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9579159Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9579244Z         msg += original_trace
2025-04-11T04:23:18.9579415Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9579582Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9579653Z E       
2025-04-11T04:23:18.9579781Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9579876Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9580177Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9580255Z E           fn(i, *args)
2025-04-11T04:23:18.9580505Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
2025-04-11T04:23:18.9580593Z E           check_all_gather_2d()
2025-04-11T04:23:18.9580874Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
2025-04-11T04:23:18.9581009Z E           tensor = torch.rand(128, device=get_current_device())
2025-04-11T04:23:18.9581114Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9581398Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9581530Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9581690Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9581746Z 
2025-04-11T04:23:18.9582059Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9582210Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9582367Z [04/11/25 04:22:56] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9582497Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9582605Z                              :75 launch                                         
2025-04-11T04:23:18.9582743Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9582868Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9583065Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9583206Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9583501Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44260 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9583694Z ____________________________ test_grad_accumulation ____________________________
2025-04-11T04:23:18.9583700Z 
2025-04-11T04:23:18.9583786Z     @pytest.mark.dist
2025-04-11T04:23:18.9583876Z     def test_grad_accumulation():
2025-04-11T04:23:18.9583962Z >       spawn(run_dist, 2)
2025-04-11T04:23:18.9583967Z 
2025-04-11T04:23:18.9584096Z tests/test_zero/test_low_level/test_grad_acc.py:146: 
2025-04-11T04:23:18.9584241Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9584339Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9584435Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9584691Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9584868Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9585148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9585236Z     while not context.join():
2025-04-11T04:23:18.9585343Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9585394Z 
2025-04-11T04:23:18.9585596Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5be2fe0>
2025-04-11T04:23:18.9585674Z timeout = None
2025-04-11T04:23:18.9585682Z 
2025-04-11T04:23:18.9585772Z     def join(self, timeout=None):
2025-04-11T04:23:18.9585892Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9585964Z     
2025-04-11T04:23:18.9586107Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9586249Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9586414Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9586502Z         of the first process exiting.
2025-04-11T04:23:18.9586576Z     
2025-04-11T04:23:18.9586722Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9586858Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9586927Z     
2025-04-11T04:23:18.9587001Z         Args:
2025-04-11T04:23:18.9587139Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9587209Z         """
2025-04-11T04:23:18.9587351Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9587443Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9587524Z             return True
2025-04-11T04:23:18.9587593Z     
2025-04-11T04:23:18.9587721Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9587840Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9587980Z             self.sentinels.keys(),
2025-04-11T04:23:18.9588068Z             timeout=timeout,
2025-04-11T04:23:18.9588139Z         )
2025-04-11T04:23:18.9588210Z     
2025-04-11T04:23:18.9588294Z         error_index = None
2025-04-11T04:23:18.9588378Z         for sentinel in ready:
2025-04-11T04:23:18.9588508Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9588609Z             process = self.processes[index]
2025-04-11T04:23:18.9588694Z             process.join()
2025-04-11T04:23:18.9588785Z             if process.exitcode != 0:
2025-04-11T04:23:18.9588871Z                 error_index = index
2025-04-11T04:23:18.9588950Z                 break
2025-04-11T04:23:18.9589019Z     
2025-04-11T04:23:18.9589111Z         # Return if there was no error.
2025-04-11T04:23:18.9589193Z         if error_index is None:
2025-04-11T04:23:18.9589325Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9589423Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9589491Z     
2025-04-11T04:23:18.9589631Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9589780Z         for process in self.processes:
2025-04-11T04:23:18.9589866Z             if process.is_alive():
2025-04-11T04:23:18.9589960Z                 process.terminate()
2025-04-11T04:23:18.9590043Z             process.join()
2025-04-11T04:23:18.9590115Z     
2025-04-11T04:23:18.9590252Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9590368Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9590528Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9590649Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9590736Z             if exitcode < 0:
2025-04-11T04:23:18.9590841Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9590948Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9591099Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9591196Z                     error_index=error_index,
2025-04-11T04:23:18.9591299Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9591385Z                     exit_code=exitcode,
2025-04-11T04:23:18.9591474Z                     signal_name=name,
2025-04-11T04:23:18.9591600Z                 )
2025-04-11T04:23:18.9591675Z             else:
2025-04-11T04:23:18.9591780Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9591941Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9592037Z                     error_index=error_index,
2025-04-11T04:23:18.9592135Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9592223Z                     exit_code=exitcode,
2025-04-11T04:23:18.9592294Z                 )
2025-04-11T04:23:18.9592366Z     
2025-04-11T04:23:18.9592498Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9592667Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9592759Z         msg += original_trace
2025-04-11T04:23:18.9592932Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9593094Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9593168Z E       
2025-04-11T04:23:18.9593290Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9593390Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9593685Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9593772Z E           fn(i, *args)
2025-04-11T04:23:18.9594025Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
2025-04-11T04:23:18.9594121Z E           exam_zero_1_grad_acc(sync=True)
2025-04-11T04:23:18.9594457Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
2025-04-11T04:23:18.9594558Z E           zero_model = zero_model.to(device)
2025-04-11T04:23:18.9594828Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.9594923Z E           return self._apply(convert)
2025-04-11T04:23:18.9595199Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9595283Z E           module._apply(fn)
2025-04-11T04:23:18.9595558Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9595648Z E           param_applied = fn(param)
2025-04-11T04:23:18.9595921Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.9596134Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.9596238Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9596573Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9596712Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9596874Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9596879Z 
2025-04-11T04:23:18.9597186Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9597384Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9597537Z [04/11/25 04:23:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9597665Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9597777Z                              :75 launch                                         
2025-04-11T04:23:18.9597916Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9598044Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9598232Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9598526Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43283 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9598654Z ________________________________ test_zero_1_2 _________________________________
2025-04-11T04:23:18.9598659Z 
2025-04-11T04:23:18.9598749Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9599348Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9599356Z 
2025-04-11T04:23:18.9599459Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9599537Z         try_count = 0
2025-04-11T04:23:18.9599635Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9599719Z             max_try, int
2025-04-11T04:23:18.9599861Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9599934Z     
2025-04-11T04:23:18.9600044Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9600120Z             try:
2025-04-11T04:23:18.9600203Z                 try_count += 1
2025-04-11T04:23:18.9600292Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9600374Z                 return ret
2025-04-11T04:23:18.9600465Z             except exception_type as e:
2025-04-11T04:23:18.9600568Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9600824Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9600939Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9601087Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9601240Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9601325Z                     continue
2025-04-11T04:23:18.9601401Z                 else:
2025-04-11T04:23:18.9601624Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9601705Z >                   raise e
2025-04-11T04:23:18.9601709Z 
2025-04-11T04:23:18.9601799Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9601910Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9602040Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9602131Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9602294Z tests/test_zero/test_low_level/test_mem_leak.py:57: in test_zero_1_2
2025-04-11T04:23:18.9602427Z     spawn(run_dist, 2)
2025-04-11T04:23:18.9602526Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9602623Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9602886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9603066Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9603352Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9603486Z     while not context.join():
2025-04-11T04:23:18.9603599Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9603603Z 
2025-04-11T04:23:18.9603798Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c74e50>
2025-04-11T04:23:18.9603878Z timeout = None
2025-04-11T04:23:18.9603886Z 
2025-04-11T04:23:18.9603973Z     def join(self, timeout=None):
2025-04-11T04:23:18.9604096Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9604173Z     
2025-04-11T04:23:18.9604316Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9604508Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9604671Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9604759Z         of the first process exiting.
2025-04-11T04:23:18.9604832Z     
2025-04-11T04:23:18.9604975Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9605113Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9605181Z     
2025-04-11T04:23:18.9605255Z         Args:
2025-04-11T04:23:18.9605390Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9605462Z         """
2025-04-11T04:23:18.9605605Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9605699Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9605780Z             return True
2025-04-11T04:23:18.9605848Z     
2025-04-11T04:23:18.9605976Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9606100Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9606188Z             self.sentinels.keys(),
2025-04-11T04:23:18.9606272Z             timeout=timeout,
2025-04-11T04:23:18.9606343Z         )
2025-04-11T04:23:18.9606412Z     
2025-04-11T04:23:18.9606497Z         error_index = None
2025-04-11T04:23:18.9606581Z         for sentinel in ready:
2025-04-11T04:23:18.9606688Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9606783Z             process = self.processes[index]
2025-04-11T04:23:18.9606870Z             process.join()
2025-04-11T04:23:18.9607011Z             if process.exitcode != 0:
2025-04-11T04:23:18.9607098Z                 error_index = index
2025-04-11T04:23:18.9607176Z                 break
2025-04-11T04:23:18.9607246Z     
2025-04-11T04:23:18.9607337Z         # Return if there was no error.
2025-04-11T04:23:18.9607422Z         if error_index is None:
2025-04-11T04:23:18.9607552Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9607653Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9607722Z     
2025-04-11T04:23:18.9607861Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9607956Z         for process in self.processes:
2025-04-11T04:23:18.9608047Z             if process.is_alive():
2025-04-11T04:23:18.9608136Z                 process.terminate()
2025-04-11T04:23:18.9608218Z             process.join()
2025-04-11T04:23:18.9608290Z     
2025-04-11T04:23:18.9608427Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9608545Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9608650Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9608816Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9608900Z             if exitcode < 0:
2025-04-11T04:23:18.9609005Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9609115Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9609262Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9609357Z                     error_index=error_index,
2025-04-11T04:23:18.9609505Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9609591Z                     exit_code=exitcode,
2025-04-11T04:23:18.9609679Z                     signal_name=name,
2025-04-11T04:23:18.9609750Z                 )
2025-04-11T04:23:18.9609824Z             else:
2025-04-11T04:23:18.9609922Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9610087Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9610183Z                     error_index=error_index,
2025-04-11T04:23:18.9610283Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9610371Z                     exit_code=exitcode,
2025-04-11T04:23:18.9610564Z                 )
2025-04-11T04:23:18.9610634Z     
2025-04-11T04:23:18.9610766Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9610934Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9611026Z         msg += original_trace
2025-04-11T04:23:18.9611199Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9611367Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9611440Z E       
2025-04-11T04:23:18.9611562Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9611664Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9611963Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9612050Z E           fn(i, *args)
2025-04-11T04:23:18.9612297Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
2025-04-11T04:23:18.9612406Z E           exam_mem_leak(world_size=world_size)
2025-04-11T04:23:18.9612665Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
2025-04-11T04:23:18.9612758Z E           zero_model = MlpModel().cuda()
2025-04-11T04:23:18.9613039Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9613158Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9613433Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9613568Z E           module._apply(fn)
2025-04-11T04:23:18.9613835Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9613929Z E           param_applied = fn(param)
2025-04-11T04:23:18.9614202Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9614318Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9614423Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9614712Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9614845Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9615008Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9615015Z 
2025-04-11T04:23:18.9615314Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9615465Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9615673Z [04/11/25 04:23:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9615807Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9615912Z                              :75 launch                                         
2025-04-11T04:23:18.9616046Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9616207Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9616401Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9616532Z ________________________________ test_zero_1_2 _________________________________
2025-04-11T04:23:18.9616537Z 
2025-04-11T04:23:18.9616627Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9617219Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9617276Z 
2025-04-11T04:23:18.9617379Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9617458Z         try_count = 0
2025-04-11T04:23:18.9617556Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9617634Z             max_try, int
2025-04-11T04:23:18.9617783Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9617852Z     
2025-04-11T04:23:18.9617965Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9618038Z             try:
2025-04-11T04:23:18.9618119Z                 try_count += 1
2025-04-11T04:23:18.9618213Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9618290Z                 return ret
2025-04-11T04:23:18.9618385Z             except exception_type as e:
2025-04-11T04:23:18.9618485Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9618674Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9618792Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9618937Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9619094Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9619175Z                     continue
2025-04-11T04:23:18.9619254Z                 else:
2025-04-11T04:23:18.9619471Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9619550Z >                   raise e
2025-04-11T04:23:18.9619598Z 
2025-04-11T04:23:18.9619692Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9619803Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9619939Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9620023Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9620187Z tests/test_zero/test_low_level/test_zero1_2.py:224: in test_zero_1_2
2025-04-11T04:23:18.9620269Z     spawn(run_dist, 4)
2025-04-11T04:23:18.9620370Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9620467Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9620724Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9620902Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9621182Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9621269Z     while not context.join():
2025-04-11T04:23:18.9621378Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9621382Z 
2025-04-11T04:23:18.9621627Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581d6cd00>
2025-04-11T04:23:18.9621703Z timeout = None
2025-04-11T04:23:18.9621708Z 
2025-04-11T04:23:18.9621798Z     def join(self, timeout=None):
2025-04-11T04:23:18.9621926Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9621994Z     
2025-04-11T04:23:18.9622137Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9622280Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9622494Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9622584Z         of the first process exiting.
2025-04-11T04:23:18.9622652Z     
2025-04-11T04:23:18.9622797Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9622932Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9623003Z     
2025-04-11T04:23:18.9623077Z         Args:
2025-04-11T04:23:18.9623209Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9623283Z         """
2025-04-11T04:23:18.9623483Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9623577Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9623653Z             return True
2025-04-11T04:23:18.9623725Z     
2025-04-11T04:23:18.9623854Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9623972Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9624067Z             self.sentinels.keys(),
2025-04-11T04:23:18.9624149Z             timeout=timeout,
2025-04-11T04:23:18.9624222Z         )
2025-04-11T04:23:18.9624289Z     
2025-04-11T04:23:18.9624368Z         error_index = None
2025-04-11T04:23:18.9624457Z         for sentinel in ready:
2025-04-11T04:23:18.9624561Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9624661Z             process = self.processes[index]
2025-04-11T04:23:18.9624745Z             process.join()
2025-04-11T04:23:18.9624835Z             if process.exitcode != 0:
2025-04-11T04:23:18.9624927Z                 error_index = index
2025-04-11T04:23:18.9625003Z                 break
2025-04-11T04:23:18.9625077Z     
2025-04-11T04:23:18.9625166Z         # Return if there was no error.
2025-04-11T04:23:18.9625253Z         if error_index is None:
2025-04-11T04:23:18.9625384Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9625480Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9625551Z     
2025-04-11T04:23:18.9625688Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9625785Z         for process in self.processes:
2025-04-11T04:23:18.9625871Z             if process.is_alive():
2025-04-11T04:23:18.9626027Z                 process.terminate()
2025-04-11T04:23:18.9626113Z             process.join()
2025-04-11T04:23:18.9626182Z     
2025-04-11T04:23:18.9626324Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9626437Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9626547Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9626667Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9626750Z             if exitcode < 0:
2025-04-11T04:23:18.9626860Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9626967Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9627117Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9627210Z                     error_index=error_index,
2025-04-11T04:23:18.9627309Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9627402Z                     exit_code=exitcode,
2025-04-11T04:23:18.9627488Z                     signal_name=name,
2025-04-11T04:23:18.9627564Z                 )
2025-04-11T04:23:18.9627682Z             else:
2025-04-11T04:23:18.9627786Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9627947Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9628039Z                     error_index=error_index,
2025-04-11T04:23:18.9628140Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9628226Z                     exit_code=exitcode,
2025-04-11T04:23:18.9628301Z                 )
2025-04-11T04:23:18.9628445Z     
2025-04-11T04:23:18.9628580Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9628752Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9628836Z         msg += original_trace
2025-04-11T04:23:18.9629014Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9629175Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9629249Z E       
2025-04-11T04:23:18.9629377Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9629472Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9629767Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9629902Z E           fn(i, *args)
2025-04-11T04:23:18.9630163Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
2025-04-11T04:23:18.9630252Z E           exam_zero_1_torch_ddp()
2025-04-11T04:23:18.9630510Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9630597Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9630849Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9630940Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9631190Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9631279Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9631562Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
2025-04-11T04:23:18.9631676Z E           torch_model = MlpModel().cuda().to(dtype)
2025-04-11T04:23:18.9631944Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9632062Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9632341Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9632428Z E           module._apply(fn)
2025-04-11T04:23:18.9632751Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9632843Z E           param_applied = fn(param)
2025-04-11T04:23:18.9633116Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9633233Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9633342Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9633626Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9633762Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9633925Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9633930Z 
2025-04-11T04:23:18.9634228Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9634382Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9634534Z [04/11/25 04:23:11] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9634718Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9634825Z                              :75 launch                                         
2025-04-11T04:23:18.9634961Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9635086Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9635323Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9635470Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9635764Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:63920 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9635903Z ________________________________ test_zero_ckpt ________________________________
2025-04-11T04:23:18.9635907Z 
2025-04-11T04:23:18.9635999Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9636607Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9636657Z 
2025-04-11T04:23:18.9636759Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9636839Z         try_count = 0
2025-04-11T04:23:18.9636939Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9637018Z             max_try, int
2025-04-11T04:23:18.9637164Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9637232Z     
2025-04-11T04:23:18.9637345Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9637421Z             try:
2025-04-11T04:23:18.9637503Z                 try_count += 1
2025-04-11T04:23:18.9637596Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9637674Z                 return ret
2025-04-11T04:23:18.9637770Z             except exception_type as e:
2025-04-11T04:23:18.9637868Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9638055Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9638167Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9638308Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9638468Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9638547Z                     continue
2025-04-11T04:23:18.9638625Z                 else:
2025-04-11T04:23:18.9638842Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9638972Z >                   raise e
2025-04-11T04:23:18.9638977Z 
2025-04-11T04:23:18.9639070Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9639179Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9639313Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9639399Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9639572Z tests/test_zero/test_low_level/test_zero_ckpt.py:129: in test_zero_ckpt
2025-04-11T04:23:18.9639651Z     spawn(run_dist, 4)
2025-04-11T04:23:18.9639752Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9639851Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9640109Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9640288Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9640571Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9640660Z     while not context.join():
2025-04-11T04:23:18.9640816Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9640820Z 
2025-04-11T04:23:18.9641022Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76dd0>
2025-04-11T04:23:18.9641104Z timeout = None
2025-04-11T04:23:18.9641109Z 
2025-04-11T04:23:18.9641204Z     def join(self, timeout=None):
2025-04-11T04:23:18.9641335Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9641452Z     
2025-04-11T04:23:18.9641600Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9641743Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9641908Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9641998Z         of the first process exiting.
2025-04-11T04:23:18.9642068Z     
2025-04-11T04:23:18.9642218Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9642355Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9642426Z     
2025-04-11T04:23:18.9642498Z         Args:
2025-04-11T04:23:18.9642681Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9642756Z         """
2025-04-11T04:23:18.9642892Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9642986Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9643065Z             return True
2025-04-11T04:23:18.9643136Z     
2025-04-11T04:23:18.9643266Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9643382Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9643475Z             self.sentinels.keys(),
2025-04-11T04:23:18.9643558Z             timeout=timeout,
2025-04-11T04:23:18.9643634Z         )
2025-04-11T04:23:18.9643702Z     
2025-04-11T04:23:18.9643783Z         error_index = None
2025-04-11T04:23:18.9643872Z         for sentinel in ready:
2025-04-11T04:23:18.9643977Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9644077Z             process = self.processes[index]
2025-04-11T04:23:18.9644161Z             process.join()
2025-04-11T04:23:18.9644253Z             if process.exitcode != 0:
2025-04-11T04:23:18.9644343Z                 error_index = index
2025-04-11T04:23:18.9644417Z                 break
2025-04-11T04:23:18.9644492Z     
2025-04-11T04:23:18.9644580Z         # Return if there was no error.
2025-04-11T04:23:18.9644670Z         if error_index is None:
2025-04-11T04:23:18.9644801Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9644894Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9644965Z     
2025-04-11T04:23:18.9645103Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9645251Z         for process in self.processes:
2025-04-11T04:23:18.9645337Z             if process.is_alive():
2025-04-11T04:23:18.9645428Z                 process.terminate()
2025-04-11T04:23:18.9645515Z             process.join()
2025-04-11T04:23:18.9645583Z     
2025-04-11T04:23:18.9645724Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9645841Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9645950Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9646070Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9646153Z             if exitcode < 0:
2025-04-11T04:23:18.9646259Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9646361Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9646511Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9646603Z                     error_index=error_index,
2025-04-11T04:23:18.9646702Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9646791Z                     exit_code=exitcode,
2025-04-11T04:23:18.9646928Z                     signal_name=name,
2025-04-11T04:23:18.9647003Z                 )
2025-04-11T04:23:18.9647076Z             else:
2025-04-11T04:23:18.9647182Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9647344Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9647435Z                     error_index=error_index,
2025-04-11T04:23:18.9647535Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9647694Z                     exit_code=exitcode,
2025-04-11T04:23:18.9647769Z                 )
2025-04-11T04:23:18.9647837Z     
2025-04-11T04:23:18.9647966Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9648139Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9648224Z         msg += original_trace
2025-04-11T04:23:18.9648400Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9648563Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9648637Z E       
2025-04-11T04:23:18.9648762Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9648905Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9649209Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9649288Z E           fn(i, *args)
2025-04-11T04:23:18.9649555Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
2025-04-11T04:23:18.9649650Z E           exam_zero_1_torch_ddp_ckpt()
2025-04-11T04:23:18.9649910Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9649997Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9650299Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
2025-04-11T04:23:18.9650401Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.9650669Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9650793Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9651064Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9651155Z E           module._apply(fn)
2025-04-11T04:23:18.9651424Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9651518Z E           param_applied = fn(param)
2025-04-11T04:23:18.9651793Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9651954Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9652068Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9652353Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9652495Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9652657Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9652662Z 
2025-04-11T04:23:18.9652971Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9653124Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9653278Z [04/11/25 04:23:17] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9653412Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9653520Z                              :75 launch                                         
2025-04-11T04:23:18.9653711Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9653838Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9654040Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9654184Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9654478Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9654807Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9654916Z =============================== warnings summary ===============================
2025-04-11T04:23:18.9655016Z colossalai/interface/model.py:45
2025-04-11T04:23:18.9655305Z   /__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\S'
2025-04-11T04:23:18.9655525Z     to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T04:23:18.9655575Z 
2025-04-11T04:23:18.9655670Z colossalai/interface/model.py:45
2025-04-11T04:23:18.9655957Z   /__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T04:23:18.9656159Z     to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T04:23:18.9656166Z 
2025-04-11T04:23:18.9656265Z colossalai/checkpoint_io/utils.py:862
2025-04-11T04:23:18.9656565Z   /__w/ColossalAI/ColossalAI/colossalai/checkpoint_io/utils.py:862: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T04:23:18.9656673Z     reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T04:23:18.9656683Z 
2025-04-11T04:23:18.9656784Z colossalai/nn/optimizer/cpu_adam.py:12
2025-04-11T04:23:18.9657085Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/cpu_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T04:23:18.9657164Z     """
2025-04-11T04:23:18.9657168Z 
2025-04-11T04:23:18.9657269Z colossalai/nn/optimizer/fused_adam.py:15
2025-04-11T04:23:18.9657573Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/fused_adam.py:15: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T04:23:18.9657662Z     """Implements Adam algorithm.
2025-04-11T04:23:18.9657669Z 
2025-04-11T04:23:18.9657771Z colossalai/nn/optimizer/hybrid_adam.py:12
2025-04-11T04:23:18.9658078Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/hybrid_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T04:23:18.9658166Z     """Implements Adam algorithm.
2025-04-11T04:23:18.9658175Z 
2025-04-11T04:23:18.9658455Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34
2025-04-11T04:23:18.9659649Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9659827Z     deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9659834Z 
2025-04-11T04:23:18.9660069Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896
2025-04-11T04:23:18.9660215Z tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T04:23:18.9660379Z tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T04:23:18.9660542Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9661244Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9661381Z     warnings.warn(
2025-04-11T04:23:18.9661386Z 
2025-04-11T04:23:18.9661603Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9661818Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9662064Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9662274Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9662486Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9662606Z tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T04:23:18.9662769Z tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T04:23:18.9662928Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9663765Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9663899Z     warnings.warn(
2025-04-11T04:23:18.9663903Z 
2025-04-11T04:23:18.9663998Z <frozen importlib._bootstrap>:283
2025-04-11T04:23:18.9664134Z tests/test_config/test_load_config.py::test_load_config
2025-04-11T04:23:18.9664290Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9664447Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9664599Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9664752Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9665134Z   <frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead
2025-04-11T04:23:18.9665142Z 
2025-04-11T04:23:18.9665242Z colossalai/fx/profiler/dataflow.py:20
2025-04-11T04:23:18.9665554Z   /__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:20: DeprecationWarning: invalid escape sequence '\_'
2025-04-11T04:23:18.9665633Z     """
2025-04-11T04:23:18.9665637Z 
2025-04-11T04:23:18.9665731Z colossalai/fx/profiler/dataflow.py:77
2025-04-11T04:23:18.9666026Z   /__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:77: DeprecationWarning: invalid escape sequence '\_'
2025-04-11T04:23:18.9666202Z     """Analyze the autograd node dependencies and find out the memory usage.
2025-04-11T04:23:18.9666260Z 
2025-04-11T04:23:18.9666412Z colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31
2025-04-11T04:23:19.1713720Z   /__w/ColossalAI/ColossalAI/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T04:23:19.1714439Z     """A wrapper for optimizer. ``ShardedOptimizerV2`` and ``ShardedModelV2`` implement Zero Redundancy Optimizer (ZeRO).
2025-04-11T04:23:19.1714788Z 
2025-04-11T04:23:19.1714896Z colossalai/inference/utils.py:80
2025-04-11T04:23:19.1715352Z   /__w/ColossalAI/ColossalAI/colossalai/inference/utils.py:80: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T04:23:19.1715816Z     reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T04:23:19.1715992Z 
2025-04-11T04:23:19.1716119Z colossalai/inference/executor/rpc_worker.py:188
2025-04-11T04:23:19.1716657Z   /__w/ColossalAI/ColossalAI/colossalai/inference/executor/rpc_worker.py:188: SyntaxWarning: "is" with a literal. Did you mean "=="?
2025-04-11T04:23:19.1717178Z     if arch is "BaichuanForCausalLM":
2025-04-11T04:23:19.1717344Z 
2025-04-11T04:23:19.1717486Z tests/test_infer/test_async_engine/test_async_engine.py:49
2025-04-11T04:23:19.1718506Z   /__w/ColossalAI/ColossalAI/tests/test_infer/test_async_engine/test_async_engine.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
2025-04-11T04:23:19.1719376Z     @pytest.mark.asyncio
2025-04-11T04:23:19.1719515Z 
2025-04-11T04:23:19.1719633Z tests/test_tensor/test_dtensor/test_dtensor.py:10
2025-04-11T04:23:19.1720465Z   /__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py:10: PytestCollectionWarning: cannot collect test class 'TestModel' because it has a __init__ constructor (from: tests/test_tensor/test_dtensor/test_dtensor.py)
2025-04-11T04:23:19.1721230Z     class TestModel(torch.nn.Module):
2025-04-11T04:23:19.1721482Z 
2025-04-11T04:23:19.1721606Z tests/test_zero/test_low_level/test_mem_leak.py:23
2025-04-11T04:23:19.1722407Z   /__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py:23: PytestCollectionWarning: cannot collect test class 'TestLowLevelZeroOptimizer' because it has a __init__ constructor (from: tests/test_zero/test_low_level/test_mem_leak.py)
2025-04-11T04:23:19.1723310Z     class TestLowLevelZeroOptimizer(LowLevelZeroOptimizer):
2025-04-11T04:23:19.1723523Z 
2025-04-11T04:23:19.1723648Z tests/test_booster/test_accelerator.py: 1 warning
2025-04-11T04:23:19.1723987Z tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
2025-04-11T04:23:19.1724372Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
2025-04-11T04:23:19.1724760Z tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
2025-04-11T04:23:19.1725076Z tests/test_fp8/test_fp8_cast.py: 1 warning
2025-04-11T04:23:19.1725434Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
2025-04-11T04:23:19.1725834Z tests/test_shardformer/test_flash_attention.py: 1 warning
2025-04-11T04:23:19.1726160Z tests/test_shardformer/test_with_torch_ddp.py: 1 warning
2025-04-11T04:23:19.1726569Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
2025-04-11T04:23:19.1727083Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
2025-04-11T04:23:19.1727588Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
2025-04-11T04:23:19.1728018Z tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
2025-04-11T04:23:19.1728391Z tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
2025-04-11T04:23:19.1728749Z tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
2025-04-11T04:23:19.1729118Z tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
2025-04-11T04:23:19.1729489Z tests/test_shardformer/test_model/test_shard_command.py: 1 warning
2025-04-11T04:23:19.1729939Z tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
2025-04-11T04:23:19.1730300Z tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
2025-04-11T04:23:19.1730657Z tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
2025-04-11T04:23:19.1731018Z tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
2025-04-11T04:23:19.1731393Z tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
2025-04-11T04:23:19.1731763Z tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
2025-04-11T04:23:19.1732109Z tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
2025-04-11T04:23:19.1732456Z tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
2025-04-11T04:23:19.1732830Z tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
2025-04-11T04:23:19.1733187Z tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
2025-04-11T04:23:19.1733970Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:19.1734783Z     warnings.warn(
2025-04-11T04:23:19.1734903Z 
2025-04-11T04:23:19.1735016Z tests/test_booster/test_accelerator.py: 1 warning
2025-04-11T04:23:19.1735348Z tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
2025-04-11T04:23:19.1735752Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
2025-04-11T04:23:19.1736144Z tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
2025-04-11T04:23:19.1736456Z tests/test_fp8/test_fp8_cast.py: 1 warning
2025-04-11T04:23:19.1736859Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
2025-04-11T04:23:19.1737265Z tests/test_shardformer/test_flash_attention.py: 1 warning
2025-04-11T04:23:19.1737608Z tests/test_shardformer/test_with_torch_ddp.py: 1 warning
2025-04-11T04:23:19.1738023Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
2025-04-11T04:23:19.1738533Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
2025-04-11T04:23:19.1739081Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
2025-04-11T04:23:19.1739522Z tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
2025-04-11T04:23:19.1739925Z tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
2025-04-11T04:23:19.1740304Z tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
2025-04-11T04:23:19.1740669Z tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
2025-04-11T04:23:19.1741048Z tests/test_shardformer/test_model/test_shard_command.py: 1 warning
2025-04-11T04:23:19.1741413Z tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
2025-04-11T04:23:19.1741790Z tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
2025-04-11T04:23:19.1742144Z tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
2025-04-11T04:23:19.1742509Z tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
2025-04-11T04:23:19.1742872Z tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
2025-04-11T04:23:19.1743228Z tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
2025-04-11T04:23:19.1743577Z tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
2025-04-11T04:23:19.1743919Z tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
2025-04-11T04:23:19.1744260Z tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
2025-04-11T04:23:19.1744616Z tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
2025-04-11T04:23:19.1745383Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:19.1746066Z     warnings.warn(
2025-04-11T04:23:19.1746242Z 
2025-04-11T04:23:19.1746447Z tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
2025-04-11T04:23:19.1747184Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/_pytest/python.py:183: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
2025-04-11T04:23:19.1747899Z   You need to install a suitable plugin for your async framework, for example:
2025-04-11T04:23:19.1748225Z     - anyio
2025-04-11T04:23:19.1748476Z     - pytest-asyncio
2025-04-11T04:23:19.1748689Z     - pytest-tornasync
2025-04-11T04:23:19.1748893Z     - pytest-trio
2025-04-11T04:23:19.1749081Z     - pytest-twisted
2025-04-11T04:23:19.1749386Z     warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))
2025-04-11T04:23:19.1749629Z 
2025-04-11T04:23:19.1749914Z tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T04:23:19.1751149Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:19.1752243Z     numel += p.storage().size()
2025-04-11T04:23:19.1752390Z 
2025-04-11T04:23:19.1752564Z tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T04:23:19.1753948Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
2025-04-11T04:23:19.1755372Z     warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2025-04-11T04:23:19.1755636Z 
2025-04-11T04:23:19.1755804Z tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T04:23:19.1756493Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
2025-04-11T04:23:19.1757235Z     warnings.warn("To get the last learning rate computed by the scheduler, "
2025-04-11T04:23:19.1757474Z 
2025-04-11T04:23:19.1757650Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2025-04-11T04:23:19.1758014Z ============================== slowest durations ===============================
2025-04-11T04:23:19.1758393Z 17.15s call     tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:19.1758822Z 15.35s call     tests/test_infer/test_continuous_batching.py::test_continuous_batching
2025-04-11T04:23:19.1759283Z 15.10s call     tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
2025-04-11T04:23:19.1759762Z 10.96s call     tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
2025-04-11T04:23:19.1760213Z 9.67s call     tests/test_zero/test_gemini/test_inference.py::test_inference[4]
2025-04-11T04:23:19.1760622Z 8.90s call     tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
2025-04-11T04:23:19.1761005Z 8.86s call     tests/test_optimizer/test_dist_came.py::test_dist_came
2025-04-11T04:23:19.1761407Z 8.72s call     tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
2025-04-11T04:23:19.1761831Z 8.72s call     tests/test_optimizer/test_dist_galore.py::test_dist_galore
2025-04-11T04:23:19.1762290Z 8.67s call     tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
2025-04-11T04:23:19.1762783Z 8.57s call     tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
2025-04-11T04:23:19.1763265Z 8.45s call     tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
2025-04-11T04:23:19.1763683Z 8.35s call     tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
2025-04-11T04:23:19.1764080Z 8.16s call     tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
2025-04-11T04:23:19.1764510Z 7.92s call     tests/test_zero/test_gemini/test_optim.py::test_optim[4]
2025-04-11T04:23:19.1764996Z 7.53s call     tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
2025-04-11T04:23:19.1765539Z 7.41s call     tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
2025-04-11T04:23:19.1765953Z 7.38s call     tests/test_lora/test_lora.py::test_torch_ddp_lora
2025-04-11T04:23:19.1766338Z 7.25s call     tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
2025-04-11T04:23:19.1766731Z 7.08s call     tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
2025-04-11T04:23:19.1767143Z 6.86s call     tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
2025-04-11T04:23:19.1767578Z 6.67s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
2025-04-11T04:23:19.1768070Z 6.46s call     tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
2025-04-11T04:23:19.1768532Z 6.42s call     tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
2025-04-11T04:23:19.1769008Z 6.40s call     tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
2025-04-11T04:23:19.1769460Z 6.31s call     tests/test_zero/test_gemini/test_search.py::test_search[4]
2025-04-11T04:23:19.1769897Z 6.29s call     tests/test_zero/test_gemini/test_inference.py::test_inference[1]
2025-04-11T04:23:19.1770296Z 6.25s call     tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
2025-04-11T04:23:19.1770708Z 6.22s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
2025-04-11T04:23:19.1771146Z 6.21s call     tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
2025-04-11T04:23:19.1771568Z 6.11s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
2025-04-11T04:23:19.1771971Z 6.02s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
2025-04-11T04:23:19.1772451Z 6.01s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
2025-04-11T04:23:19.1772861Z 5.96s call     tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
2025-04-11T04:23:19.1773227Z 5.89s call     tests/test_infer/test_streamingllm.py::test_engine
2025-04-11T04:23:19.1773595Z 5.81s call     tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
2025-04-11T04:23:19.1773978Z 5.79s call     tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
2025-04-11T04:23:19.1774359Z 5.73s call     tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
2025-04-11T04:23:19.1774809Z 5.53s call     tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
2025-04-11T04:23:19.1775250Z 5.40s call     tests/test_fp8/test_fp8_allgather.py::test_all_gather
2025-04-11T04:23:19.1775625Z 5.40s call     tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
2025-04-11T04:23:19.1776010Z 5.37s call     tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
2025-04-11T04:23:19.1776426Z 5.35s call     tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
2025-04-11T04:23:19.1776900Z 5.31s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
2025-04-11T04:23:19.1777297Z 5.28s call     tests/test_infer/test_drafter.py::test_spec_dec
2025-04-11T04:23:19.1777667Z 5.22s call     tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
2025-04-11T04:23:19.1778084Z 5.21s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
2025-04-11T04:23:19.1778479Z 5.15s call     tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
2025-04-11T04:23:19.1778965Z 5.15s call     tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
2025-04-11T04:23:19.1779469Z 5.14s call     tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T04:23:19.1779972Z 5.14s call     tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
2025-04-11T04:23:19.1780445Z 5.09s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
2025-04-11T04:23:19.1780853Z 5.08s call     tests/test_device/test_init_logical_pg.py::test_logical_pg
2025-04-11T04:23:19.1781242Z 5.06s call     tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
2025-04-11T04:23:19.1781650Z 5.04s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
2025-04-11T04:23:19.1782074Z 5.04s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
2025-04-11T04:23:19.1782484Z 5.03s call     tests/test_tensor/test_padded_tensor.py::test_padded_tensor
2025-04-11T04:23:19.1782891Z 5.02s call     tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
2025-04-11T04:23:19.1783367Z 4.99s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
2025-04-11T04:23:19.1783811Z 4.96s call     tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
2025-04-11T04:23:19.1784253Z 4.72s call     tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T04:23:19.1784634Z 4.68s call     tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T04:23:19.1785029Z 4.66s call     tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
2025-04-11T04:23:19.1785538Z 4.48s call     tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
2025-04-11T04:23:19.1785951Z 4.41s call     tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
2025-04-11T04:23:19.1786317Z 4.38s call     tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
2025-04-11T04:23:19.1786714Z 4.31s call     tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
2025-04-11T04:23:19.1787110Z 4.29s call     tests/test_tensor/test_shape_consistency_apply.py::test_apply
2025-04-11T04:23:19.1787507Z 4.18s call     tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
2025-04-11T04:23:19.1787902Z 4.16s call     tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
2025-04-11T04:23:19.1788363Z 4.11s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
2025-04-11T04:23:19.1788862Z 4.10s call     tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
2025-04-11T04:23:19.1789375Z 4.10s call     tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
2025-04-11T04:23:19.1789865Z 4.10s call     tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
2025-04-11T04:23:19.1790305Z 4.09s call     tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
2025-04-11T04:23:19.1790759Z 4.08s call     tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T04:23:19.1791206Z 4.07s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
2025-04-11T04:23:19.1791638Z 4.07s call     tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
2025-04-11T04:23:19.1792060Z 4.05s call     tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
2025-04-11T04:23:19.1792473Z 4.04s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
2025-04-11T04:23:19.1792887Z 4.03s call     tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
2025-04-11T04:23:19.1793292Z 4.03s call     tests/test_shardformer/test_layer/test_dropout.py::test_dropout
2025-04-11T04:23:19.1793683Z 3.82s call     tests/test_infer/test_kvcache_manager.py::test_cache_manager
2025-04-11T04:23:19.1794112Z 3.80s call     tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
2025-04-11T04:23:19.1794622Z 3.79s call     tests/test_zero/test_gemini/test_search.py::test_search[1]
2025-04-11T04:23:19.1795016Z 3.69s call     tests/test_infer/test_config_and_struct.py::test_config_and_inference
2025-04-11T04:23:19.1795432Z 3.48s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
2025-04-11T04:23:19.1795922Z 0.66s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
2025-04-11T04:23:19.1796488Z 0.57s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
2025-04-11T04:23:19.1797057Z 0.56s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
2025-04-11T04:23:19.1797525Z 0.55s setup    tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T04:23:19.1798002Z 0.47s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
2025-04-11T04:23:19.1798591Z 0.36s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
2025-04-11T04:23:19.1799166Z 0.35s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
2025-04-11T04:23:19.1799803Z 0.33s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
2025-04-11T04:23:19.1800365Z 0.32s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
2025-04-11T04:23:19.1800883Z 0.27s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
2025-04-11T04:23:19.1801410Z 0.27s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
2025-04-11T04:23:19.1801828Z 0.22s call     tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
2025-04-11T04:23:19.1802226Z 0.21s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
2025-04-11T04:23:19.1802614Z 0.20s call     tests/test_fp8/test_fp8_cast.py::test_fp8_cast
2025-04-11T04:23:19.1802993Z 0.19s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
2025-04-11T04:23:19.1803415Z 0.19s setup    tests/test_infer/test_kvcache_manager.py::test_logical_blocks
2025-04-11T04:23:19.1803926Z 0.18s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_distribution
2025-04-11T04:23:19.1804472Z 0.17s setup    tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
2025-04-11T04:23:19.1804843Z 0.17s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
2025-04-11T04:23:19.1805317Z 0.17s call     tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
2025-04-11T04:23:19.1805775Z 0.16s setup    tests/test_optimizer/test_dist_came.py::test_dist_came
2025-04-11T04:23:19.1806143Z 0.16s setup    tests/test_optimizer/test_dist_galore.py::test_dist_galore
2025-04-11T04:23:19.1806614Z 0.16s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T04:23:19.1807106Z 0.16s setup    tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
2025-04-11T04:23:19.1807550Z 0.16s setup    tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T04:23:19.1808054Z 0.16s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_distribution
2025-04-11T04:23:19.1808648Z 0.16s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T04:23:19.1809253Z 0.16s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
2025-04-11T04:23:19.1809754Z 0.15s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
2025-04-11T04:23:19.1810250Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
2025-04-11T04:23:19.1810864Z 0.15s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T04:23:19.1811438Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
2025-04-11T04:23:19.1812048Z 0.15s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
2025-04-11T04:23:19.1812658Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
2025-04-11T04:23:19.1813255Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
2025-04-11T04:23:19.1813817Z 0.15s setup    tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T04:23:19.1814376Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
2025-04-11T04:23:19.1814976Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
2025-04-11T04:23:19.1815631Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
2025-04-11T04:23:19.1816147Z 0.15s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
2025-04-11T04:23:19.1816670Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
2025-04-11T04:23:19.1817194Z 0.14s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
2025-04-11T04:23:19.1817685Z 0.14s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
2025-04-11T04:23:19.1818103Z 0.14s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
2025-04-11T04:23:19.1818626Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
2025-04-11T04:23:19.1819164Z 0.14s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
2025-04-11T04:23:19.1819613Z 0.14s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
2025-04-11T04:23:19.1820142Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
2025-04-11T04:23:19.1820809Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
2025-04-11T04:23:19.1821307Z 0.14s setup    tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
2025-04-11T04:23:19.1821800Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
2025-04-11T04:23:19.1822323Z 0.14s setup    tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
2025-04-11T04:23:19.1822842Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
2025-04-11T04:23:19.1823439Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
2025-04-11T04:23:19.1824034Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
2025-04-11T04:23:19.1824631Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
2025-04-11T04:23:19.1825190Z 0.14s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_batch_size
2025-04-11T04:23:19.1825660Z 0.14s setup    tests/test_tensor/test_shape_consistency.py::test_one_step_transform
2025-04-11T04:23:19.1826091Z 0.14s setup    tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
2025-04-11T04:23:19.1826609Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
2025-04-11T04:23:19.1827180Z 0.14s setup    tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
2025-04-11T04:23:19.1827590Z 0.14s setup    tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
2025-04-11T04:23:19.1827981Z 0.14s setup    tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
2025-04-11T04:23:19.1828518Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
2025-04-11T04:23:19.1828997Z 0.14s setup    tests/test_infer/test_drafter.py::test_spec_dec
2025-04-11T04:23:19.1829392Z 0.14s setup    tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
2025-04-11T04:23:19.1829906Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
2025-04-11T04:23:19.1830529Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
2025-04-11T04:23:19.1831095Z 0.14s setup    tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
2025-04-11T04:23:19.1831573Z 0.14s setup    tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
2025-04-11T04:23:19.1832054Z 0.14s setup    tests/test_tensor/test_padded_tensor.py::test_padded_tensor
2025-04-11T04:23:19.1832454Z 0.14s setup    tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
2025-04-11T04:23:19.1832860Z 0.14s setup    tests/test_shardformer/test_model/test_shard_bert.py::test_bert
2025-04-11T04:23:19.1833270Z 0.14s setup    tests/test_shardformer/test_layer/test_dropout.py::test_dropout
2025-04-11T04:23:19.1833940Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
2025-04-11T04:23:19.1834446Z 0.14s setup    tests/test_infer/test_kvcache_manager.py::test_cache_manager
2025-04-11T04:23:19.1834837Z 0.14s setup    tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
2025-04-11T04:23:19.1835348Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
2025-04-11T04:23:19.1835865Z 0.14s setup    tests/test_tensor/test_sharding_spec.py::test_sharding_spec
2025-04-11T04:23:19.1836331Z 0.14s setup    tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
2025-04-11T04:23:19.1836912Z 0.14s setup    tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py::test_dtensor_sharding_spec
2025-04-11T04:23:19.1837496Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
2025-04-11T04:23:19.1838026Z 0.14s setup    tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
2025-04-11T04:23:19.1838466Z 0.14s setup    tests/test_zero/test_gemini/test_search.py::test_search[4]
2025-04-11T04:23:19.1838878Z 0.14s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
2025-04-11T04:23:19.1839362Z 0.14s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
2025-04-11T04:23:19.1839801Z 0.14s setup    tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
2025-04-11T04:23:19.1840229Z 0.14s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
2025-04-11T04:23:19.1840720Z 0.14s setup    tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
2025-04-11T04:23:19.1841222Z 0.14s setup    tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
2025-04-11T04:23:19.1841634Z 0.14s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
2025-04-11T04:23:19.1842052Z 0.14s setup    tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
2025-04-11T04:23:19.1842505Z 0.14s setup    tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T04:23:19.1842959Z 0.14s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
2025-04-11T04:23:19.1843383Z 0.14s setup    tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
2025-04-11T04:23:19.1843870Z 0.14s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
2025-04-11T04:23:19.1844297Z 0.14s setup    tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
2025-04-11T04:23:19.1844725Z 0.14s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[1]
2025-04-11T04:23:19.1845145Z 0.14s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
2025-04-11T04:23:19.1845535Z 0.14s setup    tests/test_zero/test_gemini/test_search.py::test_search[1]
2025-04-11T04:23:19.1845937Z 0.14s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[4]
2025-04-11T04:23:19.1846358Z 0.14s setup    tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
2025-04-11T04:23:19.1846771Z 0.14s setup    tests/test_zero/test_gemini/test_optim.py::test_optim[4]
2025-04-11T04:23:19.1847216Z 0.14s setup    tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
2025-04-11T04:23:19.1847815Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
2025-04-11T04:23:19.1848536Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
2025-04-11T04:23:19.1849157Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
2025-04-11T04:23:19.1849773Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
2025-04-11T04:23:19.1850446Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
2025-04-11T04:23:19.1851076Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
2025-04-11T04:23:19.1851695Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
2025-04-11T04:23:19.1852304Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
2025-04-11T04:23:19.1852861Z 0.14s setup    tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
2025-04-11T04:23:19.1853497Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
2025-04-11T04:23:19.1854114Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
2025-04-11T04:23:19.1854693Z 0.13s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T04:23:19.1855309Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
2025-04-11T04:23:19.1855931Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
2025-04-11T04:23:19.1856531Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
2025-04-11T04:23:19.1857124Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
2025-04-11T04:23:19.1857747Z 0.13s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
2025-04-11T04:23:19.1858362Z 0.13s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
2025-04-11T04:23:19.1858958Z 0.13s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
2025-04-11T04:23:19.1859419Z 0.13s setup    tests/test_lazy/test_ops.py::test_lazy_ops
2025-04-11T04:23:19.1859889Z 0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
2025-04-11T04:23:19.1860568Z 0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
2025-04-11T04:23:19.1861148Z 0.12s setup    tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
2025-04-11T04:23:19.1861736Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
2025-04-11T04:23:19.1862345Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
2025-04-11T04:23:19.1862955Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
2025-04-11T04:23:19.1863553Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
2025-04-11T04:23:19.1864146Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
2025-04-11T04:23:19.1864728Z 0.12s setup    tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
2025-04-11T04:23:19.1865301Z 0.12s setup    tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
2025-04-11T04:23:19.1865766Z 0.12s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
2025-04-11T04:23:19.1866311Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
2025-04-11T04:23:19.1866785Z 0.12s setup    tests/test_fp8/test_fp8_cast.py::test_fp8_cast
2025-04-11T04:23:19.1867301Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
2025-04-11T04:23:19.1867778Z 0.12s setup    tests/test_lora/test_lora.py::test_torch_ddp_lora
2025-04-11T04:23:19.1868238Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
2025-04-11T04:23:19.1868872Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
2025-04-11T04:23:19.1869354Z 0.12s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
2025-04-11T04:23:19.1869746Z 0.12s setup    tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
2025-04-11T04:23:19.1870312Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
2025-04-11T04:23:19.1870927Z 0.11s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
2025-04-11T04:23:19.1871512Z 0.11s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T04:23:19.1872037Z 0.11s setup    tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
2025-04-11T04:23:19.1872563Z 0.11s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T04:23:19.1873076Z 0.11s setup    tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
2025-04-11T04:23:19.1873558Z 0.11s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
2025-04-11T04:23:19.1874064Z 0.11s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T04:23:19.1874559Z 0.11s setup    tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
2025-04-11T04:23:19.1874969Z 0.11s setup    tests/test_config/test_load_config.py::test_load_config
2025-04-11T04:23:19.1875464Z 0.11s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
2025-04-11T04:23:19.1876080Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
2025-04-11T04:23:19.1876751Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
2025-04-11T04:23:19.1877401Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
2025-04-11T04:23:19.1877930Z 0.11s setup    tests/test_infer/test_streamingllm.py::test_engine
2025-04-11T04:23:19.1878358Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
2025-04-11T04:23:19.1878855Z 0.11s setup    tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
2025-04-11T04:23:19.1879347Z 0.11s setup    tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
2025-04-11T04:23:19.1879778Z 0.11s setup    tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
2025-04-11T04:23:19.1880202Z 0.11s setup    tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
2025-04-11T04:23:19.1880752Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
2025-04-11T04:23:19.1881362Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
2025-04-11T04:23:19.1882032Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
2025-04-11T04:23:19.1882644Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
2025-04-11T04:23:19.1883232Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
2025-04-11T04:23:19.1883761Z 0.11s setup    tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
2025-04-11T04:23:19.1884151Z 0.11s setup    tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
2025-04-11T04:23:19.1884582Z 0.11s setup    tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
2025-04-11T04:23:19.1884976Z 0.11s setup    tests/test_infer/test_batch_bucket.py::test_bucket
2025-04-11T04:23:19.1885440Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
2025-04-11T04:23:19.1885949Z 0.11s setup    tests/test_infer/test_continuous_batching.py::test_continuous_batching
2025-04-11T04:23:19.1886395Z 0.11s setup    tests/test_fp8/test_fp8_hook.py::test_fp8_hook
2025-04-11T04:23:19.1886743Z 0.11s setup    tests/test_fp8/test_fp8_allgather.py::test_all_gather
2025-04-11T04:23:19.1887119Z 0.11s setup    tests/test_device/test_init_logical_pg.py::test_logical_pg
2025-04-11T04:23:19.1887556Z 0.11s setup    tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
2025-04-11T04:23:19.1888083Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
2025-04-11T04:23:19.1888571Z 0.11s setup    tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
2025-04-11T04:23:19.1889001Z 0.11s setup    tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
2025-04-11T04:23:19.1889514Z 0.11s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.1890116Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T04:23:19.1890625Z 0.11s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
2025-04-11T04:23:19.1891038Z 0.11s setup    tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
2025-04-11T04:23:19.1891541Z 0.11s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
2025-04-11T04:23:19.1892105Z 0.11s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_layers
2025-04-11T04:23:19.1892688Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
2025-04-11T04:23:19.1893271Z 0.11s setup    tests/test_shardformer/test_shard_utils.py::test_release_layer
2025-04-11T04:23:19.1893716Z 0.11s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
2025-04-11T04:23:19.1908690Z 0.11s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.1909432Z 0.10s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.1910016Z 0.10s setup    tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
2025-04-11T04:23:19.1910607Z 0.10s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
2025-04-11T04:23:19.1911141Z 0.10s setup    tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
2025-04-11T04:23:19.1911628Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T04:23:19.1912129Z 0.10s setup    tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
2025-04-11T04:23:19.1912693Z 0.10s setup    tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
2025-04-11T04:23:19.1913164Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
2025-04-11T04:23:19.1913656Z 0.10s setup    tests/test_shardformer/test_model/test_shard_command.py::test_command
2025-04-11T04:23:19.1914081Z 0.10s setup    tests/test_shardformer/test_model/test_shard_llama.py::test_llama
2025-04-11T04:23:19.1914563Z 0.10s setup    tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
2025-04-11T04:23:19.1914964Z 0.10s setup    tests/test_shardformer/test_model/test_shard_sam.py::test_sam
2025-04-11T04:23:19.1915386Z 0.10s setup    tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
2025-04-11T04:23:19.1915809Z 0.10s setup    tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
2025-04-11T04:23:19.1916219Z 0.10s setup    tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
2025-04-11T04:23:19.1916661Z 0.10s setup    tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
2025-04-11T04:23:19.1917075Z 0.10s setup    tests/test_shardformer/test_model/test_shard_t5.py::test_t5
2025-04-11T04:23:19.1917531Z 0.10s setup    tests/test_shardformer/test_model/test_shard_vit.py::test_vit
2025-04-11T04:23:19.1918023Z 0.10s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
2025-04-11T04:23:19.1918549Z 0.10s setup    tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
2025-04-11T04:23:19.1919073Z 0.10s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_layers
2025-04-11T04:23:19.1919587Z 0.10s setup    tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
2025-04-11T04:23:19.1920046Z 0.10s setup    tests/test_infer/test_async_engine/test_request_tracer.py::test_request_tracer
2025-04-11T04:23:19.1920603Z 0.10s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
2025-04-11T04:23:19.1921186Z 0.10s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_micro_batch
2025-04-11T04:23:19.1921734Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
2025-04-11T04:23:19.1922339Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
2025-04-11T04:23:19.1922920Z 0.10s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_merge_batch
2025-04-11T04:23:19.1923497Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
2025-04-11T04:23:19.1924081Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T04:23:19.1924617Z 0.10s setup    tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
2025-04-11T04:23:19.1925049Z 0.10s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
2025-04-11T04:23:19.1925538Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
2025-04-11T04:23:19.1926043Z 0.10s setup    tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
2025-04-11T04:23:19.1926563Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
2025-04-11T04:23:19.1927050Z 0.10s setup    tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
2025-04-11T04:23:19.1927466Z 0.10s setup    tests/test_tensor/test_shape_consistency.py::test_shape_consistency
2025-04-11T04:23:19.1927987Z 0.10s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
2025-04-11T04:23:19.1928496Z 0.10s setup    tests/test_tensor/test_shape_consistency_apply.py::test_apply
2025-04-11T04:23:19.1928937Z 0.10s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
2025-04-11T04:23:19.1929459Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-7]
2025-04-11T04:23:19.1930050Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T04:23:19.1930573Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T04:23:19.1931205Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
2025-04-11T04:23:19.1931649Z 0.10s call     tests/test_lazy/test_ops.py::test_lazy_ops
2025-04-11T04:23:19.1932143Z 0.10s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
2025-04-11T04:23:19.1932809Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
2025-04-11T04:23:19.1933397Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
2025-04-11T04:23:19.1933921Z 0.10s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
2025-04-11T04:23:19.1934443Z 0.10s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
2025-04-11T04:23:19.1934936Z 0.10s call     tests/test_fp8/test_fp8_hook.py::test_fp8_hook
2025-04-11T04:23:19.1935312Z 0.10s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:19.1935796Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T04:23:19.1936334Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T04:23:19.1936867Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T04:23:19.1937456Z 0.10s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
2025-04-11T04:23:19.1938125Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
2025-04-11T04:23:19.1938721Z 0.10s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
2025-04-11T04:23:19.1939242Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
2025-04-11T04:23:19.1939775Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
2025-04-11T04:23:19.1940305Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T04:23:19.1940918Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
2025-04-11T04:23:19.1941463Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
2025-04-11T04:23:19.1941959Z 0.09s call     tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
2025-04-11T04:23:19.1942454Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
2025-04-11T04:23:19.1942908Z 0.09s setup    tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
2025-04-11T04:23:19.1944050Z 0.09s call     tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
2025-04-11T04:23:19.1944482Z 0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
2025-04-11T04:23:19.1944968Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
2025-04-11T04:23:19.1945519Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
2025-04-11T04:23:19.1946138Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
2025-04-11T04:23:19.1946676Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T04:23:19.1947229Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
2025-04-11T04:23:19.1947773Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T04:23:19.1948383Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
2025-04-11T04:23:19.1948971Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
2025-04-11T04:23:19.1949504Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T04:23:19.1949958Z 0.09s setup    tests/test_booster/test_accelerator.py::test_accelerator
2025-04-11T04:23:19.1950325Z 0.09s call     tests/test_infer/test_batch_bucket.py::test_bucket
2025-04-11T04:23:19.1950686Z 0.09s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
2025-04-11T04:23:19.1951284Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
2025-04-11T04:23:19.1951877Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
2025-04-11T04:23:19.1952419Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
2025-04-11T04:23:19.1952957Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T04:23:19.1953525Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
2025-04-11T04:23:19.1954107Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
2025-04-11T04:23:19.1954649Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
2025-04-11T04:23:19.1955178Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T04:23:19.1955651Z 0.09s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
2025-04-11T04:23:19.1956113Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T04:23:19.1956571Z 0.09s call     tests/test_shardformer/test_shard_utils.py::test_release_layer
2025-04-11T04:23:19.1957032Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T04:23:19.1957641Z 0.09s setup    tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
2025-04-11T04:23:19.1958180Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
2025-04-11T04:23:19.1958720Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T04:23:19.1959278Z 0.09s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
2025-04-11T04:23:19.1959843Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
2025-04-11T04:23:19.1960397Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
2025-04-11T04:23:19.1960941Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
2025-04-11T04:23:19.1961466Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
2025-04-11T04:23:19.1961992Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T04:23:19.1962598Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
2025-04-11T04:23:19.1963149Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T04:23:19.1963689Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
2025-04-11T04:23:19.1964359Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
2025-04-11T04:23:19.1964972Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
2025-04-11T04:23:19.1965528Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
2025-04-11T04:23:19.1966086Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
2025-04-11T04:23:19.1966578Z 0.09s call     tests/test_shardformer/test_model/test_shard_bert.py::test_bert
2025-04-11T04:23:19.1967054Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T04:23:19.1967666Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
2025-04-11T04:23:19.1968230Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
2025-04-11T04:23:19.1968789Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
2025-04-11T04:23:19.1969346Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
2025-04-11T04:23:19.1969903Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
2025-04-11T04:23:19.1970489Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
2025-04-11T04:23:19.1971071Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
2025-04-11T04:23:19.1971622Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
2025-04-11T04:23:19.1972107Z 0.09s call     tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
2025-04-11T04:23:19.1972593Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
2025-04-11T04:23:19.1973215Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
2025-04-11T04:23:19.1973897Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
2025-04-11T04:23:19.1974457Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
2025-04-11T04:23:19.1975021Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
2025-04-11T04:23:19.1975613Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
2025-04-11T04:23:19.1976193Z 0.09s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
2025-04-11T04:23:19.1976775Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
2025-04-11T04:23:19.1977347Z 0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
2025-04-11T04:23:19.1977952Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
2025-04-11T04:23:19.1978597Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
2025-04-11T04:23:19.1979265Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
2025-04-11T04:23:19.1979539Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
2025-04-11T04:23:19.1979778Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T04:23:19.1980135Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
2025-04-11T04:23:19.1980379Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
2025-04-11T04:23:19.1980653Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
2025-04-11T04:23:19.1980927Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
2025-04-11T04:23:19.1981166Z 0.09s call     tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
2025-04-11T04:23:19.1981495Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
2025-04-11T04:23:19.1981764Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
2025-04-11T04:23:19.1982061Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
2025-04-11T04:23:19.1982339Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
2025-04-11T04:23:19.1982611Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
2025-04-11T04:23:19.1982886Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
2025-04-11T04:23:19.1983154Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
2025-04-11T04:23:19.1983453Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
2025-04-11T04:23:19.1983697Z 0.09s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
2025-04-11T04:23:19.1983988Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
2025-04-11T04:23:19.1984235Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
2025-04-11T04:23:19.1984599Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
2025-04-11T04:23:19.1984890Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
2025-04-11T04:23:19.1985144Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
2025-04-11T04:23:19.1985387Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
2025-04-11T04:23:19.1985635Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
2025-04-11T04:23:19.1985883Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
2025-04-11T04:23:19.1986123Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
2025-04-11T04:23:19.1986373Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
2025-04-11T04:23:19.1986686Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
2025-04-11T04:23:19.1986980Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
2025-04-11T04:23:19.1987275Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
2025-04-11T04:23:19.1987636Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
2025-04-11T04:23:19.1987927Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
2025-04-11T04:23:19.1988225Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
2025-04-11T04:23:19.1988573Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
2025-04-11T04:23:19.1988867Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
2025-04-11T04:23:19.1989223Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
2025-04-11T04:23:19.1989507Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
2025-04-11T04:23:19.1989802Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
2025-04-11T04:23:19.1990097Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
2025-04-11T04:23:19.1990375Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
2025-04-11T04:23:19.1990630Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
2025-04-11T04:23:19.1990906Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
2025-04-11T04:23:19.1991203Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
2025-04-11T04:23:19.1991500Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
2025-04-11T04:23:19.1991790Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
2025-04-11T04:23:19.1992060Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
2025-04-11T04:23:19.1992421Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
2025-04-11T04:23:19.1992695Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
2025-04-11T04:23:19.1992975Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
2025-04-11T04:23:19.1993243Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
2025-04-11T04:23:19.1993521Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
2025-04-11T04:23:19.1993809Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
2025-04-11T04:23:19.1994098Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
2025-04-11T04:23:19.1994368Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
2025-04-11T04:23:19.1994698Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
2025-04-11T04:23:19.1994967Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
2025-04-11T04:23:19.1995202Z 0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
2025-04-11T04:23:19.1995530Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
2025-04-11T04:23:19.1995816Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
2025-04-11T04:23:19.1996068Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
2025-04-11T04:23:19.1996341Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
2025-04-11T04:23:19.1996621Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
2025-04-11T04:23:19.1996942Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
2025-04-11T04:23:19.1997214Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
2025-04-11T04:23:19.1997382Z 0.09s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
2025-04-11T04:23:19.1997657Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
2025-04-11T04:23:19.1997910Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
2025-04-11T04:23:19.1998223Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
2025-04-11T04:23:19.1998517Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
2025-04-11T04:23:19.1998802Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
2025-04-11T04:23:19.1999091Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
2025-04-11T04:23:19.1999324Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T04:23:19.1999563Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T04:23:19.1999843Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T04:23:19.2000075Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T04:23:19.2000303Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T04:23:19.2000533Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
2025-04-11T04:23:19.2000813Z 0.09s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
2025-04-11T04:23:19.2001094Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2001326Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T04:23:19.2001620Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2001853Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T04:23:19.2002132Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T04:23:19.2002428Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2002653Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T04:23:19.2002946Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T04:23:19.2003257Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
2025-04-11T04:23:19.2003558Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2003850Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2004121Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
2025-04-11T04:23:19.2004468Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2004764Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2005059Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2005349Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2005649Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2005938Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2006232Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2006518Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2006813Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2007100Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2007387Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2007730Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2008017Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2008311Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2008532Z 0.09s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
2025-04-11T04:23:19.2008822Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2009110Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2009404Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2009743Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2010032Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2010320Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2010610Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2010942Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2011221Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2011515Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2011802Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2012136Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2012435Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
2025-04-11T04:23:19.2012729Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2013019Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2013310Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2013598Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2013889Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2014179Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2014463Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2014758Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2015044Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2015396Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2015704Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
2025-04-11T04:23:19.2016000Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2016280Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2016574Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2016856Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2017027Z 0.09s setup    tests/test_infer/test_models/test_custom_model.py::test_model
2025-04-11T04:23:19.2017308Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2017647Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2017948Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2018228Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2018565Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2018848Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2019140Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2019425Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2019714Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2020045Z 0.09s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
2025-04-11T04:23:19.2020338Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2020618Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
2025-04-11T04:23:19.2020897Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
2025-04-11T04:23:19.2021188Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2021470Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
2025-04-11T04:23:19.2021750Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
2025-04-11T04:23:19.2022033Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2022309Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
2025-04-11T04:23:19.2022593Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2022866Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
2025-04-11T04:23:19.2023206Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2023499Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2023772Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
2025-04-11T04:23:19.2024045Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
2025-04-11T04:23:19.2024323Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
2025-04-11T04:23:19.2024602Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
2025-04-11T04:23:19.2024873Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
2025-04-11T04:23:19.2025198Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
2025-04-11T04:23:19.2025470Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
2025-04-11T04:23:19.2025758Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2026051Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2026372Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
2025-04-11T04:23:19.2026646Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
2025-04-11T04:23:19.2026914Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
2025-04-11T04:23:19.2027208Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2027544Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2027817Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
2025-04-11T04:23:19.2028105Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2028386Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2028729Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
2025-04-11T04:23:19.2029011Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2029284Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
2025-04-11T04:23:19.2029569Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2029749Z 0.09s setup    tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
2025-04-11T04:23:19.2030037Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2030326Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2030674Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2030883Z 0.09s setup    tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
2025-04-11T04:23:19.2031167Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2031452Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2031736Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2032019Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2032302Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2032585Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2032929Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2033213Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2033501Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2033804Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
2025-04-11T04:23:19.2034092Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2034379Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2034671Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2034961Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2035299Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2035589Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2035875Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2036162Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2036446Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2036754Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
2025-04-11T04:23:19.2037038Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2037332Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2037617Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2037911Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2038200Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2038548Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2038834Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2039057Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
2025-04-11T04:23:19.2039344Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2039633Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2039920Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2040207Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2040481Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
2025-04-11T04:23:19.2040957Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2041241Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2041522Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2041856Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2042144Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2042430Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2042721Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2043070Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2043353Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2043635Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2043917Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2044205Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2044492Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2044781Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2045070Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2045353Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2045634Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2045920Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2046256Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2046546Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2046826Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2047109Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2047420Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
2025-04-11T04:23:19.2047704Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2048010Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
2025-04-11T04:23:19.2048318Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
2025-04-11T04:23:19.2048675Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
2025-04-11T04:23:19.2048975Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
2025-04-11T04:23:19.2049270Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2049619Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-7]
2025-04-11T04:23:19.2049922Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
2025-04-11T04:23:19.2050227Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
2025-04-11T04:23:19.2050536Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
2025-04-11T04:23:19.2050885Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
2025-04-11T04:23:19.2051193Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
2025-04-11T04:23:19.2051498Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
2025-04-11T04:23:19.2051663Z 0.09s call     tests/test_booster/test_accelerator.py::test_accelerator
2025-04-11T04:23:19.2051965Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
2025-04-11T04:23:19.2052279Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
2025-04-11T04:23:19.2052586Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
2025-04-11T04:23:19.2052896Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
2025-04-11T04:23:19.2053205Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
2025-04-11T04:23:19.2053509Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
2025-04-11T04:23:19.2053822Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
2025-04-11T04:23:19.2054183Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
2025-04-11T04:23:19.2054488Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
2025-04-11T04:23:19.2054795Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
2025-04-11T04:23:19.2055103Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
2025-04-11T04:23:19.2055412Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
2025-04-11T04:23:19.2055721Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
2025-04-11T04:23:19.2056025Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
2025-04-11T04:23:19.2056392Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
2025-04-11T04:23:19.2056696Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
2025-04-11T04:23:19.2056995Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
2025-04-11T04:23:19.2057321Z 0.09s setup    tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
2025-04-11T04:23:19.2057623Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
2025-04-11T04:23:19.2057929Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
2025-04-11T04:23:19.2058230Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
2025-04-11T04:23:19.2058479Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
2025-04-11T04:23:19.2058836Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
2025-04-11T04:23:19.2059124Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2059427Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
2025-04-11T04:23:19.2059732Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
2025-04-11T04:23:19.2060035Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
2025-04-11T04:23:19.2060341Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
2025-04-11T04:23:19.2060642Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
2025-04-11T04:23:19.2060927Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2061236Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
2025-04-11T04:23:19.2061535Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
2025-04-11T04:23:19.2061892Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
2025-04-11T04:23:19.2062189Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
2025-04-11T04:23:19.2062488Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
2025-04-11T04:23:19.2062793Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
2025-04-11T04:23:19.2063098Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
2025-04-11T04:23:19.2063398Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
2025-04-11T04:23:19.2063698Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
2025-04-11T04:23:19.2063998Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
2025-04-11T04:23:19.2064352Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
2025-04-11T04:23:19.2064652Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
2025-04-11T04:23:19.2064955Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
2025-04-11T04:23:19.2065302Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2065606Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
2025-04-11T04:23:19.2065913Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
2025-04-11T04:23:19.2066194Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2066545Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
2025-04-11T04:23:19.2066845Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
2025-04-11T04:23:19.2067148Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
2025-04-11T04:23:19.2067444Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
2025-04-11T04:23:19.2067750Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
2025-04-11T04:23:19.2068050Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
2025-04-11T04:23:19.2068351Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
2025-04-11T04:23:19.2068695Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
2025-04-11T04:23:19.2068994Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
2025-04-11T04:23:19.2069300Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
2025-04-11T04:23:19.2069593Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
2025-04-11T04:23:19.2069975Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
2025-04-11T04:23:19.2070276Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
2025-04-11T04:23:19.2070581Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
2025-04-11T04:23:19.2070879Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
2025-04-11T04:23:19.2071177Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2071483Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
2025-04-11T04:23:19.2071773Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2072130Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
2025-04-11T04:23:19.2072438Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
2025-04-11T04:23:19.2072735Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
2025-04-11T04:23:19.2073095Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
2025-04-11T04:23:19.2073371Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
2025-04-11T04:23:19.2073671Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
2025-04-11T04:23:19.2073968Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2074250Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2074603Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
2025-04-11T04:23:19.2074909Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
2025-04-11T04:23:19.2075211Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
2025-04-11T04:23:19.2075509Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
2025-04-11T04:23:19.2075817Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
2025-04-11T04:23:19.2076113Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
2025-04-11T04:23:19.2076403Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2076706Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
2025-04-11T04:23:19.2076997Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2077283Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2077618Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2077902Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
2025-04-11T04:23:19.2078192Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2078480Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2078767Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2079069Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
2025-04-11T04:23:19.2079352Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2079637Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2079971Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2080254Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2080539Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2080846Z 0.09s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
2025-04-11T04:23:19.2081141Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2081423Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2081711Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2081995Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2082333Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2082603Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
2025-04-11T04:23:19.2082893Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2083178Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2083459Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2083745Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2084047Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
2025-04-11T04:23:19.2084336Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2084623Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2084907Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2085238Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2085526Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2085815Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2086102Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2086382Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2086675Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2086962Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2087245Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2087515Z 0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
2025-04-11T04:23:19.2087798Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2088075Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
2025-04-11T04:23:19.2088409Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2088693Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2088980Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2089265Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2089545Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2089880Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2090145Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
2025-04-11T04:23:19.2090426Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2090709Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2090991Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2091276Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2091558Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2091811Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
2025-04-11T04:23:19.2092089Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2092359Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
2025-04-11T04:23:19.2092639Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2092976Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2093257Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2093549Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2093828Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2094114Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2094341Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
2025-04-11T04:23:19.2094626Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2094914Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2095182Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
2025-04-11T04:23:19.2095470Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2095755Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2096090Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2096339Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
2025-04-11T04:23:19.2096623Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2096903Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2097183Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2097520Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2097802Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2098089Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2098383Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2098665Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2098949Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2099175Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
2025-04-11T04:23:19.2099457Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2099741Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2100022Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2100275Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
2025-04-11T04:23:19.2100577Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
2025-04-11T04:23:19.2100857Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2101144Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2101427Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2101716Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2101998Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2102279Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2102453Z 0.09s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
2025-04-11T04:23:19.2102806Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2103085Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2103305Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
2025-04-11T04:23:19.2103608Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
2025-04-11T04:23:19.2103892Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2104178Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2104390Z 0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
2025-04-11T04:23:19.2104680Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2105006Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2105287Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2105568Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2105849Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2106097Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
2025-04-11T04:23:19.2106344Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
2025-04-11T04:23:19.2106630Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2106914Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2107136Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
2025-04-11T04:23:19.2107418Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2107665Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
2025-04-11T04:23:19.2107942Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2108255Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
2025-04-11T04:23:19.2108574Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2108857Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2109139Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2109390Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
2025-04-11T04:23:19.2109638Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
2025-04-11T04:23:19.2109923Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2110147Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
2025-04-11T04:23:19.2110489Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2110778Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2111056Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2111359Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
2025-04-11T04:23:19.2111580Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
2025-04-11T04:23:19.2111888Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
2025-04-11T04:23:19.2112140Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
2025-04-11T04:23:19.2112432Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2112735Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
2025-04-11T04:23:19.2113018Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2113301Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2113519Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
2025-04-11T04:23:19.2113807Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2114055Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
2025-04-11T04:23:19.2114304Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
2025-04-11T04:23:19.2114591Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2114842Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
2025-04-11T04:23:19.2115131Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2115414Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2115787Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2116041Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
2025-04-11T04:23:19.2116331Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2116551Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
2025-04-11T04:23:19.2116735Z 0.09s call     tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
2025-04-11T04:23:19.2117020Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2117237Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
2025-04-11T04:23:19.2117520Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2117808Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2118141Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2118328Z 0.09s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
2025-04-11T04:23:19.2118550Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
2025-04-11T04:23:19.2118883Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2119098Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
2025-04-11T04:23:19.2119346Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
2025-04-11T04:23:19.2119602Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
2025-04-11T04:23:19.2119911Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
2025-04-11T04:23:19.2120230Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
2025-04-11T04:23:19.2120538Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
2025-04-11T04:23:19.2120846Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
2025-04-11T04:23:19.2121066Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
2025-04-11T04:23:19.2121311Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
2025-04-11T04:23:19.2121619Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
2025-04-11T04:23:19.2121838Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
2025-04-11T04:23:19.2122057Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
2025-04-11T04:23:19.2122363Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
2025-04-11T04:23:19.2122649Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2122861Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
2025-04-11T04:23:19.2123166Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
2025-04-11T04:23:19.2123525Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
2025-04-11T04:23:19.2123833Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
2025-04-11T04:23:19.2124118Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2124395Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
2025-04-11T04:23:19.2124705Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
2025-04-11T04:23:19.2124977Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
2025-04-11T04:23:19.2125285Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
2025-04-11T04:23:19.2125634Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
2025-04-11T04:23:19.2125933Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
2025-04-11T04:23:19.2126227Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
2025-04-11T04:23:19.2126454Z 0.09s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
2025-04-11T04:23:19.2126736Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2127018Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2127318Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
2025-04-11T04:23:19.2127622Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
2025-04-11T04:23:19.2127968Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
2025-04-11T04:23:19.2128253Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2128552Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
2025-04-11T04:23:19.2128714Z 0.09s call     tests/test_shardformer/test_model/test_shard_sam.py::test_sam
2025-04-11T04:23:19.2128970Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
2025-04-11T04:23:19.2129253Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2129557Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
2025-04-11T04:23:19.2129856Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
2025-04-11T04:23:19.2130112Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
2025-04-11T04:23:19.2130409Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
2025-04-11T04:23:19.2130598Z 0.09s setup    tests/test_infer/test_config_and_struct.py::test_config_and_inference
2025-04-11T04:23:19.2130910Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
2025-04-11T04:23:19.2131200Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2131455Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
2025-04-11T04:23:19.2131759Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
2025-04-11T04:23:19.2132017Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
2025-04-11T04:23:19.2132269Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
2025-04-11T04:23:19.2132521Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
2025-04-11T04:23:19.2132811Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2133111Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
2025-04-11T04:23:19.2133395Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2133678Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2133862Z 0.09s setup    tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T04:23:19.2134161Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
2025-04-11T04:23:19.2134451Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2134701Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
2025-04-11T04:23:19.2134952Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
2025-04-11T04:23:19.2135134Z 0.09s call     tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
2025-04-11T04:23:19.2135456Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
2025-04-11T04:23:19.2135755Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
2025-04-11T04:23:19.2136057Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
2025-04-11T04:23:19.2136342Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2136626Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2136915Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2137087Z 0.09s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
2025-04-11T04:23:19.2137341Z 0.08s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
2025-04-11T04:23:19.2137622Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2137908Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2138156Z 0.08s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
2025-04-11T04:23:19.2138516Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
2025-04-11T04:23:19.2138736Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
2025-04-11T04:23:19.2138993Z 0.08s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
2025-04-11T04:23:19.2139266Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
2025-04-11T04:23:19.2139567Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
2025-04-11T04:23:19.2139840Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
2025-04-11T04:23:19.2140106Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
2025-04-11T04:23:19.2140410Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
2025-04-11T04:23:19.2140747Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2141034Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2141313Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2141658Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2141943Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2142240Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
2025-04-11T04:23:19.2142509Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
2025-04-11T04:23:19.2142798Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2143111Z 0.08s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.2143298Z 0.08s call     tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
2025-04-11T04:23:19.2143529Z 0.08s setup    tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
2025-04-11T04:23:19.2143797Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
2025-04-11T04:23:19.2144067Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
2025-04-11T04:23:19.2144354Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2144671Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
2025-04-11T04:23:19.2144973Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
2025-04-11T04:23:19.2145241Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
2025-04-11T04:23:19.2145519Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2145778Z 0.08s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
2025-04-11T04:23:19.2146094Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
2025-04-11T04:23:19.2146365Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
2025-04-11T04:23:19.2146655Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2146956Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-32]
2025-04-11T04:23:19.2147243Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2147416Z 0.08s call     tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
2025-04-11T04:23:19.2147706Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2147984Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
2025-04-11T04:23:19.2148307Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
2025-04-11T04:23:19.2148625Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2148896Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
2025-04-11T04:23:19.2149256Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
2025-04-11T04:23:19.2149426Z 0.08s setup    tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
2025-04-11T04:23:19.2149699Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
2025-04-11T04:23:19.2149998Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
2025-04-11T04:23:19.2150271Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
2025-04-11T04:23:19.2150584Z 0.08s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.2150760Z 0.08s call     tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
2025-04-11T04:23:19.2151043Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2151335Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2151604Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
2025-04-11T04:23:19.2151908Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
2025-04-11T04:23:19.2152174Z 0.08s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.2152458Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2152676Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
2025-04-11T04:23:19.2152943Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
2025-04-11T04:23:19.2153210Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
2025-04-11T04:23:19.2153503Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2153837Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
2025-04-11T04:23:19.2154125Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2154397Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
2025-04-11T04:23:19.2154581Z 0.08s call     tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
2025-04-11T04:23:19.2154875Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2155144Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
2025-04-11T04:23:19.2155426Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2155711Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
2025-04-11T04:23:19.2156035Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
2025-04-11T04:23:19.2156317Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2156602Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2156943Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2157226Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2157526Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-7]
2025-04-11T04:23:19.2157798Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
2025-04-11T04:23:19.2158099Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
2025-04-11T04:23:19.2158427Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
2025-04-11T04:23:19.2158707Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2158983Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
2025-04-11T04:23:19.2159269Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2159575Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
2025-04-11T04:23:19.2159877Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-32]
2025-04-11T04:23:19.2160152Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
2025-04-11T04:23:19.2160418Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
2025-04-11T04:23:19.2160691Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
2025-04-11T04:23:19.2160997Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
2025-04-11T04:23:19.2161435Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
2025-04-11T04:23:19.2161743Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
2025-04-11T04:23:19.2162049Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
2025-04-11T04:23:19.2162336Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2162605Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
2025-04-11T04:23:19.2162896Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2163202Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
2025-04-11T04:23:19.2163513Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-7]
2025-04-11T04:23:19.2163862Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-32]
2025-04-11T04:23:19.2164151Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2164452Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-7]
2025-04-11T04:23:19.2164800Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-7]
2025-04-11T04:23:19.2165093Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2165383Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2165668Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2165973Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
2025-04-11T04:23:19.2166323Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-32]
2025-04-11T04:23:19.2166540Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
2025-04-11T04:23:19.2166828Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2167102Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
2025-04-11T04:23:19.2167418Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
2025-04-11T04:23:19.2167718Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
2025-04-11T04:23:19.2168025Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-32]
2025-04-11T04:23:19.2168239Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
2025-04-11T04:23:19.2168518Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
2025-04-11T04:23:19.2168821Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-32]
2025-04-11T04:23:19.2169163Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2169450Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2169759Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
2025-04-11T04:23:19.2170070Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
2025-04-11T04:23:19.2170377Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-32]
2025-04-11T04:23:19.2170687Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-32]
2025-04-11T04:23:19.2170972Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2171264Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2171622Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
2025-04-11T04:23:19.2171916Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2172216Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
2025-04-11T04:23:19.2172554Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2172835Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2173118Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2173422Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-32]
2025-04-11T04:23:19.2173721Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-32]
2025-04-11T04:23:19.2174069Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2174345Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
2025-04-11T04:23:19.2174511Z 0.08s call     tests/test_shardformer/test_model/test_shard_vit.py::test_vit
2025-04-11T04:23:19.2174808Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
2025-04-11T04:23:19.2175100Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2175320Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
2025-04-11T04:23:19.2175627Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
2025-04-11T04:23:19.2175929Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
2025-04-11T04:23:19.2176239Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-32]
2025-04-11T04:23:19.2176516Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2176821Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-32]
2025-04-11T04:23:19.2177166Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2177450Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2177735Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2178028Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-7]
2025-04-11T04:23:19.2178314Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2178580Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
2025-04-11T04:23:19.2178768Z 0.08s call     tests/test_shardformer/test_model/test_shard_command.py::test_command
2025-04-11T04:23:19.2179062Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-7]
2025-04-11T04:23:19.2179396Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2179683Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2179984Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-7]
2025-04-11T04:23:19.2180339Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-7]
2025-04-11T04:23:19.2180635Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-7]
2025-04-11T04:23:19.2180924Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2181228Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-32]
2025-04-11T04:23:19.2181578Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-32]
2025-04-11T04:23:19.2181873Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-7]
2025-04-11T04:23:19.2182165Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2182458Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-7]
2025-04-11T04:23:19.2182747Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2183028Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2183326Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
2025-04-11T04:23:19.2183606Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2183902Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
2025-04-11T04:23:19.2184200Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
2025-04-11T04:23:19.2184499Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
2025-04-11T04:23:19.2184858Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-32]
2025-04-11T04:23:19.2185073Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
2025-04-11T04:23:19.2185377Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-7]
2025-04-11T04:23:19.2185662Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2185948Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2186252Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-32]
2025-04-11T04:23:19.2186560Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
2025-04-11T04:23:19.2186912Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
2025-04-11T04:23:19.2187213Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
2025-04-11T04:23:19.2187514Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-7]
2025-04-11T04:23:19.2187869Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2188170Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
2025-04-11T04:23:19.2188503Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
2025-04-11T04:23:19.2188807Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-7]
2025-04-11T04:23:19.2189094Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2189471Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
2025-04-11T04:23:19.2189755Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2189978Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
2025-04-11T04:23:19.2190260Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2190531Z 0.08s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
2025-04-11T04:23:19.2190807Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
2025-04-11T04:23:19.2191090Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2191390Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
2025-04-11T04:23:19.2191674Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2191961Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2192261Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
2025-04-11T04:23:19.2192620Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-7]
2025-04-11T04:23:19.2192918Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
2025-04-11T04:23:19.2193220Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
2025-04-11T04:23:19.2193511Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
2025-04-11T04:23:19.2193794Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2194012Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
2025-04-11T04:23:19.2194306Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2194637Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
2025-04-11T04:23:19.2194921Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2195216Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2195378Z 0.08s call     tests/test_shardformer/test_model/test_shard_t5.py::test_t5
2025-04-11T04:23:19.2195739Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
2025-04-11T04:23:19.2196039Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
2025-04-11T04:23:19.2196327Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2196610Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2196973Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
2025-04-11T04:23:19.2197270Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
2025-04-11T04:23:19.2197562Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2197856Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
2025-04-11T04:23:19.2198150Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
2025-04-11T04:23:19.2198434Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2198733Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
2025-04-11T04:23:19.2199031Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
2025-04-11T04:23:19.2199315Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2199615Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
2025-04-11T04:23:19.2199913Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
2025-04-11T04:23:19.2200252Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2200535Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2200833Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2201120Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2201423Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
2025-04-11T04:23:19.2201703Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2201999Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
2025-04-11T04:23:19.2202335Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2202617Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2202921Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
2025-04-11T04:23:19.2203201Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2203427Z 0.08s call     tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
2025-04-11T04:23:19.2203711Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2204018Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
2025-04-11T04:23:19.2204301Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2204639Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2204919Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2205209Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2205391Z 0.08s call     tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
2025-04-11T04:23:19.2205673Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2205959Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2206239Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2206525Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2206827Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
2025-04-11T04:23:19.2207113Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2207413Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
2025-04-11T04:23:19.2207755Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2208036Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2208320Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2208603Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2208902Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
2025-04-11T04:23:19.2209206Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
2025-04-11T04:23:19.2209502Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
2025-04-11T04:23:19.2209680Z 0.08s call     tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
2025-04-11T04:23:19.2210025Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2210310Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2210526Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
2025-04-11T04:23:19.2210867Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2211169Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
2025-04-11T04:23:19.2211457Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2211675Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
2025-04-11T04:23:19.2211955Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2212310Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
2025-04-11T04:23:19.2212592Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2212809Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
2025-04-11T04:23:19.2213091Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2213377Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2213664Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2213967Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
2025-04-11T04:23:19.2214247Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2214531Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2214813Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2215095Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2215458Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
2025-04-11T04:23:19.2215741Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2216024Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2216322Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
2025-04-11T04:23:19.2216502Z 0.08s call     tests/test_shardformer/test_model/test_shard_llama.py::test_llama
2025-04-11T04:23:19.2216783Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2217083Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
2025-04-11T04:23:19.2217367Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2217712Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
2025-04-11T04:23:19.2217995Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2218300Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
2025-04-11T04:23:19.2218633Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2218912Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2219202Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2219474Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
2025-04-11T04:23:19.2219763Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2220116Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
2025-04-11T04:23:19.2220402Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2220624Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
2025-04-11T04:23:19.2220909Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2221195Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2221508Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
2025-04-11T04:23:19.2221794Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2222082Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2222370Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2222669Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
2025-04-11T04:23:19.2223021Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
2025-04-11T04:23:19.2223237Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
2025-04-11T04:23:19.2223528Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2223813Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2224118Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
2025-04-11T04:23:19.2224403Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2224693Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2224991Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
2025-04-11T04:23:19.2225327Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2225604Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2225882Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2226221Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2226509Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2226808Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2227098Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2227382Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2227714Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2227935Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
2025-04-11T04:23:19.2228218Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2228548Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2228833Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2229111Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2229402Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2229679Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2229974Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2230252Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2230555Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
2025-04-11T04:23:19.2230924Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
2025-04-11T04:23:19.2231213Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2231501Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2231807Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
2025-04-11T04:23:19.2232091Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2232374Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2232653Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2232990Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2233277Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2233560Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2233901Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2234181Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2234464Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2234745Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2235028Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2235371Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2235654Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2235933Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2236216Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2236375Z 0.08s setup    tests/test_device/test_device_mesh.py::test_device_mesh
2025-04-11T04:23:19.2236574Z 0.08s setup    tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
2025-04-11T04:23:19.2236582Z 
2025-04-11T04:23:19.2236750Z (1097 durations < 0.005s hidden.  Use -vv to show these durations.)
2025-04-11T04:23:19.2236877Z =========================== short test summary info ============================
2025-04-11T04:23:19.2237138Z FAILED tests/test_booster/test_accelerator.py::test_accelerator - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2237426Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2237566Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2237732Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2238058Z FAILED tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2238119Z 
2025-04-11T04:23:19.2238242Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2238347Z Traceback (most recent call last):
2025-04-11T04:23:19.2238656Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2238737Z     fn(i, *args)
2025-04-11T04:23:19.2238999Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
2025-04-11T04:23:19.2239102Z     check_3d_plugin(early_stop=early_stop)
2025-04-11T04:23:19.2239360Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2239455Z     partial_func(**kwargs)
2025-04-11T04:23:19.2239723Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
2025-04-11T04:23:19.2239882Z     err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:19.2240098Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2240251Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2240503Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2240607Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2240881Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2240978Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2241080Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2241412Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2241550Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2241710Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2242087Z FAILED tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2242094Z 
2025-04-11T04:23:19.2242212Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2242312Z Traceback (most recent call last):
2025-04-11T04:23:19.2242647Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2242724Z     fn(i, *args)
2025-04-11T04:23:19.2242997Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
2025-04-11T04:23:19.2243094Z     check_dataloader_sharding()
2025-04-11T04:23:19.2243403Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
2025-04-11T04:23:19.2243513Z     batch = next(iter(train_dataloader))[0].cuda()
2025-04-11T04:23:19.2243614Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2243891Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2244022Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2244183Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2244532Z FAILED tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2244536Z 
2025-04-11T04:23:19.2244656Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2244747Z Traceback (most recent call last):
2025-04-11T04:23:19.2245034Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2245113Z     fn(i, *args)
2025-04-11T04:23:19.2245373Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
2025-04-11T04:23:19.2245551Z     check_gemini_plugin(early_stop=early_stop)
2025-04-11T04:23:19.2245803Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2245898Z     partial_func(**kwargs)
2025-04-11T04:23:19.2246145Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2246235Z     partial_func(**kwargs)
2025-04-11T04:23:19.2246479Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2246564Z     partial_func(**kwargs)
2025-04-11T04:23:19.2246671Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2246969Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
2025-04-11T04:23:19.2247166Z     err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
2025-04-11T04:23:19.2247381Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2247481Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2247731Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2247889Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2248171Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2248268Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2248368Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2248645Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2248830Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2248984Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2249374Z FAILED tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2249382Z 
2025-04-11T04:23:19.2249499Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2249593Z Traceback (most recent call last):
2025-04-11T04:23:19.2249882Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2250007Z     fn(i, *args)
2025-04-11T04:23:19.2250297Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
2025-04-11T04:23:19.2250418Z     check_low_level_zero_plugin(early_stop=early_stop)
2025-04-11T04:23:19.2250671Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2250754Z     partial_func(**kwargs)
2025-04-11T04:23:19.2251089Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
2025-04-11T04:23:19.2251231Z     err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:19.2251443Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2251546Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2251794Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2251894Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2252167Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2252264Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2252357Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2252630Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2252760Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2252967Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2253338Z FAILED tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2253344Z 
2025-04-11T04:23:19.2253458Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2253558Z Traceback (most recent call last):
2025-04-11T04:23:19.2253837Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2253911Z     fn(i, *args)
2025-04-11T04:23:19.2254185Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
2025-04-11T04:23:19.2254278Z     check_torch_ddp_plugin()
2025-04-11T04:23:19.2254587Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
2025-04-11T04:23:19.2254708Z     run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:19.2254927Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2255072Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2255321Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2255418Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2255686Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2255784Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2255878Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2256207Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2256335Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2256489Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2256880Z FAILED tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2256886Z 
2025-04-11T04:23:19.2257002Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2257096Z Traceback (most recent call last):
2025-04-11T04:23:19.2257426Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2257504Z     fn(i, *args)
2025-04-11T04:23:19.2257779Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
2025-04-11T04:23:19.2257875Z     check_torch_fsdp_plugin()
2025-04-11T04:23:19.2258185Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
2025-04-11T04:23:19.2258302Z     run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:19.2258518Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2258615Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2258872Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2258966Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2259238Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2259331Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2259426Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2259705Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2259833Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2259994Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2260363Z FAILED tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2260420Z 
2025-04-11T04:23:19.2260540Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2260635Z Traceback (most recent call last):
2025-04-11T04:23:19.2260912Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2260994Z     fn(i, *args)
2025-04-11T04:23:19.2261256Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
2025-04-11T04:23:19.2261340Z     exam_state_dict()
2025-04-11T04:23:19.2261551Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2261646Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2261895Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2261988Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2262251Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2262344Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2262491Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2262766Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2262901Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2263054Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2263424Z FAILED tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2263494Z 
2025-04-11T04:23:19.2263608Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2263698Z Traceback (most recent call last):
2025-04-11T04:23:19.2263978Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2264054Z     fn(i, *args)
2025-04-11T04:23:19.2264340Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
2025-04-11T04:23:19.2264431Z     exam_torch_load_from_gemini()
2025-04-11T04:23:19.2264643Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2264787Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2265036Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2265133Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2265399Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2265495Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2265588Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2265864Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2265992Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2266148Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2266465Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2266739Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2266867Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2267019Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2267372Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2267643Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2267820Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2267971Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2268318Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2268617Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2268741Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2268895Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2269243Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2269517Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2269640Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2269846Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2270189Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2270457Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2270580Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2270780Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2271153Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2271421Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2271547Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2271695Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2272063Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2272402Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2272522Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2272673Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2273062Z FAILED tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2273068Z 
2025-04-11T04:23:19.2273184Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2273278Z Traceback (most recent call last):
2025-04-11T04:23:19.2273568Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2273644Z     fn(i, *args)
2025-04-11T04:23:19.2273958Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
2025-04-11T04:23:19.2274040Z     exam_state_dict()
2025-04-11T04:23:19.2274293Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2274384Z     partial_func(**kwargs)
2025-04-11T04:23:19.2274634Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2274722Z     partial_func(**kwargs)
2025-04-11T04:23:19.2274967Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2275109Z     partial_func(**kwargs)
2025-04-11T04:23:19.2275203Z   [Previous line repeated 3 more times]
2025-04-11T04:23:19.2275414Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2275512Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2275758Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2275855Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2276122Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2276217Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2276316Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2276585Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2276713Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2276864Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2277206Z FAILED tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2277528Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2277658Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2277805Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2278228Z FAILED tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2278289Z 
2025-04-11T04:23:19.2278405Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2278495Z Traceback (most recent call last):
2025-04-11T04:23:19.2278783Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2278860Z     fn(i, *args)
2025-04-11T04:23:19.2279167Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
2025-04-11T04:23:19.2279255Z     exam_from_pretrained()
2025-04-11T04:23:19.2279470Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2279700Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2279950Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2280045Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2280316Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2280412Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2280504Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2280777Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2280906Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2281058Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2281365Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2281630Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2281757Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2281907Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2282208Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2282477Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2282652Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2282803Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2283097Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2283374Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2283496Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2283644Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2283941Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2284211Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2284335Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2284480Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2284809Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2285076Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2285204Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2285352Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2285784Z FAILED tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2285789Z 
2025-04-11T04:23:19.2285904Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2286002Z Traceback (most recent call last):
2025-04-11T04:23:19.2286291Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2286368Z     fn(i, *args)
2025-04-11T04:23:19.2286646Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
2025-04-11T04:23:19.2286741Z     check_torch_ddp_checkpointIO()
2025-04-11T04:23:19.2287043Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2287126Z     partial_func(**kwargs)
2025-04-11T04:23:19.2287376Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2287461Z     partial_func(**kwargs)
2025-04-11T04:23:19.2287704Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2287789Z     partial_func(**kwargs)
2025-04-11T04:23:19.2287887Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2288216Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
2025-04-11T04:23:19.2288454Z     model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
2025-04-11T04:23:19.2288657Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T04:23:19.2288846Z     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T04:23:19.2289110Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
2025-04-11T04:23:19.2289214Z     model = model.to(get_current_device())
2025-04-11T04:23:19.2289477Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.2289571Z     return self._apply(convert)
2025-04-11T04:23:19.2289835Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2289974Z     module._apply(fn)
2025-04-11T04:23:19.2290239Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2290333Z     param_applied = fn(param)
2025-04-11T04:23:19.2290600Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.2290812Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.2290912Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2291185Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2291318Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2291469Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2291837Z FAILED tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2291844Z 
2025-04-11T04:23:19.2291956Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2292099Z Traceback (most recent call last):
2025-04-11T04:23:19.2292383Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2292460Z     fn(i, *args)
2025-04-11T04:23:19.2292736Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
2025-04-11T04:23:19.2292822Z     check_torch_fsdp_ckpt()
2025-04-11T04:23:19.2293126Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2293209Z     partial_func(**kwargs)
2025-04-11T04:23:19.2293513Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
2025-04-11T04:23:19.2293712Z     fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
2025-04-11T04:23:19.2293911Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T04:23:19.2294103Z     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T04:23:19.2294369Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
2025-04-11T04:23:19.2294646Z     fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
2025-04-11T04:23:19.2294905Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
2025-04-11T04:23:19.2295020Z     self.module = FSDP(module, *args, **kwargs)
2025-04-11T04:23:19.2295383Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
2025-04-11T04:23:19.2295477Z     _init_param_handle_from_module(
2025-04-11T04:23:19.2295852Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
2025-04-11T04:23:19.2295941Z     _move_module_to_device(
2025-04-11T04:23:19.2296289Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
2025-04-11T04:23:19.2296466Z     _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
2025-04-11T04:23:19.2296812Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
2025-04-11T04:23:19.2296927Z     param.data = param.to(device_from_device_id)
2025-04-11T04:23:19.2297028Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2297309Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2297438Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2297647Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2297958Z FAILED tests/test_device/test_init_logical_pg.py::test_logical_pg - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2297965Z 
2025-04-11T04:23:19.2298082Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2298176Z Traceback (most recent call last):
2025-04-11T04:23:19.2298458Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2298534Z     fn(i, *args)
2025-04-11T04:23:19.2298771Z   File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
2025-04-11T04:23:19.2298894Z     tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
2025-04-11T04:23:19.2298989Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2299263Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2299392Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2299549Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2299914Z FAILED tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2299922Z 
2025-04-11T04:23:19.2300036Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2300128Z Traceback (most recent call last):
2025-04-11T04:23:19.2300407Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2300551Z     fn(i, *args)
2025-04-11T04:23:19.2300776Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
2025-04-11T04:23:19.2300859Z     check_all2all()
2025-04-11T04:23:19.2301074Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2301169Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2301422Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2301515Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2301791Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2301935Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2302030Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2302300Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2302432Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2302585Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2302873Z FAILED tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2302880Z 
2025-04-11T04:23:19.2302995Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2303084Z Traceback (most recent call last):
2025-04-11T04:23:19.2303363Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2303438Z     fn(i, *args)
2025-04-11T04:23:19.2303664Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
2025-04-11T04:23:19.2303742Z     check_4gpu()
2025-04-11T04:23:19.2303952Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2304050Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2304302Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2304397Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2304669Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2304825Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2304917Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2305191Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2305321Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2305476Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2305801Z FAILED tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2305808Z 
2025-04-11T04:23:19.2305921Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2306012Z Traceback (most recent call last):
2025-04-11T04:23:19.2306288Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2306364Z     fn(i, *args)
2025-04-11T04:23:19.2306598Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
2025-04-11T04:23:19.2306674Z     check_4gpu()
2025-04-11T04:23:19.2306942Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2307033Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2307292Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2307382Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2307649Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2307793Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2307885Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2308161Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2308289Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2308477Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2308773Z FAILED tests/test_fp8/test_fp8_allgather.py::test_all_gather - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2308778Z 
2025-04-11T04:23:19.2308891Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2309038Z Traceback (most recent call last):
2025-04-11T04:23:19.2309317Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2309395Z     fn(i, *args)
2025-04-11T04:23:19.2309619Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
2025-04-11T04:23:19.2309699Z     check_4gpu()
2025-04-11T04:23:19.2309911Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2310003Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2310255Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2310343Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2310623Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2310714Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2310813Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2311087Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2311217Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2311374Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2311668Z FAILED tests/test_fp8/test_fp8_allreduce.py::test_all_reduce - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2311675Z 
2025-04-11T04:23:19.2311787Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2311934Z Traceback (most recent call last):
2025-04-11T04:23:19.2312218Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2312294Z     fn(i, *args)
2025-04-11T04:23:19.2312516Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
2025-04-11T04:23:19.2312593Z     check_4gpu()
2025-04-11T04:23:19.2312841Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2312931Z     partial_func(**kwargs)
2025-04-11T04:23:19.2313144Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2313237Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2313485Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2313579Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2313849Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2313940Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2314092Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2314364Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2314496Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2314650Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2314869Z FAILED tests/test_fp8/test_fp8_cast.py::test_fp8_cast - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2315195Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2315321Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2315472Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2315761Z FAILED tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2315768Z 
2025-04-11T04:23:19.2315883Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2315971Z Traceback (most recent call last):
2025-04-11T04:23:19.2316301Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2316373Z     fn(i, *args)
2025-04-11T04:23:19.2316608Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
2025-04-11T04:23:19.2316685Z     run_model()
2025-04-11T04:23:19.2316895Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2316989Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2317235Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2317331Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2317595Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2317690Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2317782Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2318061Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2318188Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2318339Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2318557Z FAILED tests/test_fp8/test_fp8_hook.py::test_fp8_hook - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2318821Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2318949Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2319152Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2319406Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2319680Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2319806Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2319958Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2320213Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2320482Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2320604Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2320756Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2321007Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2321326Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2321451Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2321601Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2321861Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2322180Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2322304Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2322451Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2322771Z FAILED tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2322782Z 
2025-04-11T04:23:19.2322896Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2322989Z Traceback (most recent call last):
2025-04-11T04:23:19.2323277Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2323400Z     fn(i, *args)
2025-04-11T04:23:19.2323633Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
2025-04-11T04:23:19.2323710Z     check_4gpu()
2025-04-11T04:23:19.2323923Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2324016Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2324268Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2324362Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2324626Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2324721Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2324817Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2325089Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2325215Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2325364Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2325599Z FAILED tests/test_infer/test_batch_bucket.py::test_bucket - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2325866Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2325992Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2326139Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2326541Z FAILED tests/test_infer/test_continuous_batching.py::test_continuous_batching - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2326548Z 
2025-04-11T04:23:19.2326662Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2326752Z Traceback (most recent call last):
2025-04-11T04:23:19.2327034Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2327107Z     fn(i, *args)
2025-04-11T04:23:19.2327351Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
2025-04-11T04:23:19.2327443Z     check_inference_engine()
2025-04-11T04:23:19.2327696Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2327781Z     partial_func(**kwargs)
2025-04-11T04:23:19.2328027Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2328116Z     partial_func(**kwargs)
2025-04-11T04:23:19.2328362Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2328514Z     partial_func(**kwargs)
2025-04-11T04:23:19.2328608Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2328893Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
2025-04-11T04:23:19.2329052Z     model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
2025-04-11T04:23:19.2329329Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2329476Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2329734Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2329850Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2330117Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2330201Z     module._apply(fn)
2025-04-11T04:23:19.2330465Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2330547Z     module._apply(fn)
2025-04-11T04:23:19.2330855Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2330947Z     param_applied = fn(param)
2025-04-11T04:23:19.2331217Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2331330Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2331431Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2331711Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2331844Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2331998Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2332232Z FAILED tests/test_infer/test_drafter.py::test_drafter[5] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2332504Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2332630Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2332783Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2333008Z FAILED tests/test_infer/test_drafter.py::test_spec_dec - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2333275Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2333398Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2333544Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2333920Z FAILED tests/test_infer/test_kvcache_manager.py::test_cache_manager - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2333927Z 
2025-04-11T04:23:19.2334039Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2334134Z Traceback (most recent call last):
2025-04-11T04:23:19.2334415Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2334492Z     fn(i, *args)
2025-04-11T04:23:19.2334721Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
2025-04-11T04:23:19.2334810Z     check_cache_manager()
2025-04-11T04:23:19.2335059Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2335141Z     partial_func(**kwargs)
2025-04-11T04:23:19.2335397Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
2025-04-11T04:23:19.2335550Z     cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T04:23:19.2335875Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T04:23:19.2336032Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:19.2336329Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T04:23:19.2336543Z     k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:19.2336688Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2336972Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2337096Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2337251Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2337620Z FAILED tests/test_infer/test_request_handler.py::test_running_list_and_request_handler - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2337626Z 
2025-04-11T04:23:19.2337742Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2337832Z Traceback (most recent call last):
2025-04-11T04:23:19.2338170Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2338243Z     fn(i, *args)
2025-04-11T04:23:19.2338470Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
2025-04-11T04:23:19.2338562Z     check_request_handler()
2025-04-11T04:23:19.2338822Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
2025-04-11T04:23:19.2338981Z     request_handler = RequestHandler(inference_config, model_config)
2025-04-11T04:23:19.2339233Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
2025-04-11T04:23:19.2339322Z     self._init_cache(model_config)
2025-04-11T04:23:19.2339584Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
2025-04-11T04:23:19.2339758Z     self.cache_manager = KVCacheManager(self.inference_config, model_config)
2025-04-11T04:23:19.2340017Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T04:23:19.2340169Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:19.2340465Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T04:23:19.2340672Z     k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:19.2340773Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2341101Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2341225Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2341382Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2341671Z FAILED tests/test_infer/test_streamingllm.py::test_engine - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2341678Z 
2025-04-11T04:23:19.2341794Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2341884Z Traceback (most recent call last):
2025-04-11T04:23:19.2342167Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2342243Z     fn(i, *args)
2025-04-11T04:23:19.2342468Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
2025-04-11T04:23:19.2342560Z     ret[rank] = func_to_run(**kwargs)
2025-04-11T04:23:19.2342807Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
2025-04-11T04:23:19.2342885Z     ).cuda()
2025-04-11T04:23:19.2343219Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2343319Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2343580Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2343691Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2343954Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2344088Z     module._apply(fn)
2025-04-11T04:23:19.2344352Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2344434Z     module._apply(fn)
2025-04-11T04:23:19.2344697Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2344790Z     param_applied = fn(param)
2025-04-11T04:23:19.2345059Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2345168Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2345315Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2345593Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2345720Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2345879Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2346237Z FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2346507Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2346633Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2346782Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2347155Z FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2347423Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2347551Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2347702Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2348037Z FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2348303Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2348530Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2348678Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2349004Z FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2349276Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2349398Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2349549Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2349886Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2350154Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2350279Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2350428Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2350823Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2351088Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2351217Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2351365Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2351764Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2352036Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2352165Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2352311Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2352649Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2352972Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2353094Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2353243Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2353573Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2353844Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2353970Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2354120Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2354463Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2354735Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2354859Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2355004Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2355337Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2355603Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2355808Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2355953Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2356289Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2356559Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2356685Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2356830Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2357165Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2357435Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2357558Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2357710Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2358096Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2358369Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2358490Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2358639Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2359026Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2359294Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2359420Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2359566Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2359910Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2360179Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2360358Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2360505Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2360844Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2361111Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2361233Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2361386Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2361719Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2361989Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2362114Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2362262Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2362602Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2362875Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2362998Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2363203Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2363542Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2363811Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2363938Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2364084Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2364420Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2364689Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2364814Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2364963Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2365299Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2365626Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2365749Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2365901Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2366271Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2366542Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2366664Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2366814Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2367146Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2367413Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2367590Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2367735Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2368070Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2368338Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2368462Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2368609Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2368944Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2369211Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2369334Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2369481Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2369810Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2370082Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2370203Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2370407Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2370743Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2371014Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2371139Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2371285Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2371625Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2371894Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2372020Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2372166Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2372502Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2372825Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2372952Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2373098Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2373430Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2373752Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2373874Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2374026Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2374361Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2374632Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2374803Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2374952Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2375288Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2375558Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2375685Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2375832Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2376175Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2376445Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2376570Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2376716Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2377053Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2377321Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2377442Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2377592Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2377974Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2378249Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2378371Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2378521Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2378859Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2379128Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2379248Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2379393Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2379731Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2380049Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2380176Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2380320Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2380657Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2380976Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2381101Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2381247Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2381586Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2381856Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2382037Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2382186Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2382497Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2382768Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2382888Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2383037Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2383348Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2383616Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2383744Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2383891Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2384198Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2384468Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2384594Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2384738Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2385103Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2385376Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2385498Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2385650Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2385958Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2386231Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2386351Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2386500Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2386806Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2387075Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2387248Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2387397Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2387704Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2387968Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2388145Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2388291Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2388650Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2388917Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2389040Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2389190Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2389556Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2389829Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2389954Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2390103Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2390409Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2390682Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2390805Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2390951Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2391264Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2391531Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2391657Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2391803Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2392117Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2392441Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2392566Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2392716Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2393025Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2393296Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2393416Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2393568Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2393875Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2394145Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2394268Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2394481Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2394784Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2395051Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2395174Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2395464Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2395779Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2396049Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2396177Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2396325Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2396686Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2397005Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2397126Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2397278Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2397632Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2397897Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2398019Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2398169Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2398519Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2398786Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2398910Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2399056Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2399409Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2399671Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2399849Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2399996Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2400325Z FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2400591Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2400715Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2400861Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2401180Z FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2401448Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2401570Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2401718Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2402181Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2402459Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2402579Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2402776Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2403179Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2403446Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2403569Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2403715Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2404114Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2404430Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2404555Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2404702Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2405097Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2405366Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2405489Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2405641Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2406034Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2406308Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2406429Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2406578Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2406977Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2407315Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2407437Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2407584Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2407979Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2408246Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2408374Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2408520Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2408918Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2409184Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2409366Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2409514Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2409909Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2410180Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2410367Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2410517Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2410908Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2411178Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2411300Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2411451Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2411896Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2412165Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2412295Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2412440Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2412841Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2413110Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2413238Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2413386Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2413783Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2414051Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2414175Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2414324Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2414722Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2415044Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2415168Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2415315Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2415707Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2415977Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2416100Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2416246Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2416645Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2416914Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2417087Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2417236Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2417634Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2417950Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2418077Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2418222Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2418620Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2418890Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2419009Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2419209Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2419604Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2419877Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2419999Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2420147Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2420547Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2420819Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2420939Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2421089Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2421494Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2421764Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2421889Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2422036Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2422493Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2422761Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2422887Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2423033Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2423429Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2423699Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2423821Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2423969Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2424371Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2424696Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2424817Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2424967Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2425366Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2425686Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2425810Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2425957Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2426357Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2426625Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2426802Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2426948Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2427345Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2427611Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2427732Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2427883Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2428274Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2428568Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2428693Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2428840Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2429237Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2429508Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2429630Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2429836Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2430239Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2430505Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2430632Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2430777Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2431177Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2431442Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2431569Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2431714Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2432172Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2432443Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2432564Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2432715Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2433167Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2433440Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2433563Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2433714Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2434113Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2434434Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2434555Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2434701Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2435112Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2435376Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2435503Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2435647Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2436048Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2436314Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2436438Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2436586Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2436976Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2437244Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2437416Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2437569Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2437963Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2438233Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2438354Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2438504Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2438892Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2439154Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2439279Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2439484Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2439882Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2440150Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2440275Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2440470Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2440868Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2441135Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2441256Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2441407Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2441798Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2442118Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2442242Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2442391Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2442780Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2443048Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2443173Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2443318Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2443715Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2443983Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2444110Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2444256Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2444652Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2444978Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2445104Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2445250Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2445651Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2445924Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2446047Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2446198Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2446594Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2446867Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2447042Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2447192Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2447591Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2447859Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2448030Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2448175Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2448572Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2448837Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2448964Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2449110Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2449560Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2449825Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2449950Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2450096Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2450485Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2450757Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2450881Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2451032Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2451429Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2451700Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2451820Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2451972Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2452367Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2452692Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2452821Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2452970Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2453374Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2453642Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2453767Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2453913Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2454314Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2454631Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2454750Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2454901Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2455296Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2455622Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2455742Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2455892Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2456287Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2456563Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2456734Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2456880Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2457281Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2457550Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2457677Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2457823Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2458225Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2458495Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2458620Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2458767Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2459167Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2459438Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2459557Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2459705Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2460156Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2460426Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2460548Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2460699Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2461098Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2461374Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2461494Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2461642Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2462042Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2462356Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2462481Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2462627Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2463027Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2463343Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2463470Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2463619Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2464013Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2464286Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2464456Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2464605Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2465008Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2465279Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2465401Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2465547Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2465942Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2466207Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2466330Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2466475Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2466879Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2467147Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2467337Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2467483Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2467886Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2468155Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2468276Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2468464Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2468870Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2469142Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2469264Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2469413Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2469866Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2470143Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2470265Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2470465Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2470866Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2471131Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2471256Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2471404Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2471799Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2472117Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2472240Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2472387Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2472784Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2473055Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2473177Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2473328Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2473722Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2473993Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2474115Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2474264Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2474662Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2474994Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2475116Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2475263Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2475664Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2475930Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2476056Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2476200Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2476599Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2476868Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2477046Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2477192Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2477588Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2477861Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2478031Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2478180Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2478581Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2478853Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2478977Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2479127Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2479589Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2479856Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2479983Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2480127Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2480534Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2480804Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2480933Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2481078Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2481485Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2481753Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2481876Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2482025Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2482427Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2482750Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2482872Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2483020Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2483427Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2483697Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2483820Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2483966Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2484381Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2484652Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2484827Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2484976Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2485379Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2485695Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2485819Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2485965Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2486366Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2486642Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2486762Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2486961Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2487355Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2487628Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2487750Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2487899Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2488299Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2488570Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2488693Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2488839Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2489241Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2489507Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2489631Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2489776Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2490234Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2490501Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2490627Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2490773Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2491169Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2491441Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2491563Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2491710Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2492115Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2492436Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2492558Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2492707Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2493107Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2493428Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2493552Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2493698Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2494103Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2494372Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2494561Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2494704Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2495097Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2495366Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2495485Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2495635Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2496020Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2496293Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2496415Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2496563Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2496945Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2497216Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2497336Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2497536Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2497922Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2498190Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2498317Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2498463Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2498851Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2499117Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2499241Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2499388Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2499824Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2500099Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2500221Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2500373Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2500806Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2501081Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2501204Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2501353Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2501743Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2502056Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2502178Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2502321Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2502700Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2502961Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2503086Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2503230Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2503608Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2503875Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2503998Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2504140Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2504514Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2504782Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2504955Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2505104Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2505488Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2505761Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2505880Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2506026Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2506411Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2506678Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2506804Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2506950Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2507389Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2507653Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2507777Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2507975Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2508352Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2508671Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2508795Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2508944Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2509327Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2509658Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2509778Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2509927Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2510308Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2510579Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2510701Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2510849Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2511236Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2511504Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2511630Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2511777Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2512162Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2512429Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2512609Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2512757Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2513140Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2513414Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2513535Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2513685Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2514070Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2514340Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2514462Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2514758Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2515149Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2515416Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2515542Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2515746Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2516132Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2516401Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2516524Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2516671Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2517063Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2517378Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2517503Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2517653Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2518033Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2518307Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2518428Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2518577Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2518961Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2519234Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2519357Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2519503Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2519886Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2520201Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2520327Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2520470Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2520857Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2521120Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2521245Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2521388Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2521768Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2522039Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2522213Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2522361Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2522751Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2523018Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2523203Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2523353Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2523734Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2524001Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2524127Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2524272Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2524717Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2524982Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2525107Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2525252Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2525638Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2525906Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2526034Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2526180Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2526565Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2526839Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2526962Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2527111Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2527489Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2527816Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2527938Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2528088Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2528471Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2528739Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2528865Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2529013Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2529396Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2529666Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2529842Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2529989Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2530370Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2530642Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2530811Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2530960Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2531334Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2531606Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2531727Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2531926Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2532308Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2532578Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2532701Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2532845Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2533223Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2533492Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2533619Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2533766Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2534145Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2534412Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2534534Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2534679Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2535053Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2535378Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2535501Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2535654Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2536030Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2536302Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2536423Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2536571Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2536944Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2537260Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2537385Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2537531Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2537917Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2538231Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2538356Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2538502Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2538889Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2539157Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2539276Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2539472Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2539850Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2540123Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2540242Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2540391Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2540771Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2541040Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2541163Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2541311Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2541695Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2541960Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2542083Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2542228Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2542664Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2542935Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2543062Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2543208Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2543592Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2543866Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2543988Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2544138Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2544527Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2544851Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2544974Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2545120Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2545504Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2545821Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2545946Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2546090Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2546482Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2546748Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2546921Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2547067Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2547454Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2547720Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2547842Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2547988Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2548368Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2548713Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2548835Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2548985Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2549365Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2549636Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2549754Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2549962Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2550343Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2550607Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2550733Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2550879Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2551263Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2551528Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2551652Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2551800Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2552181Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2552522Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2552644Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2552792Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2553184Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2553522Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2553645Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2553794Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2554173Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2554444Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2554617Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2554762Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2555155Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2555418Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2555543Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2555688Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2556068Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2556336Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2556459Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2556604Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2556983Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2557253Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2557424Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2557575Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2557955Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2558230Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2558352Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2558501Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2558882Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2559151Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2559277Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2559422Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2559855Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2560127Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2560250Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2560395Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2560832Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2561101Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2561223Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2561373Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2561756Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2562075Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2562194Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2562345Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2562728Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2562999Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2563121Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2563266Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2563648Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2563918Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2564040Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2564186Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2564571Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2564838Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2565015Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2565161Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2565539Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2565813Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2565933Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2566088Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2566467Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2566739Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2566862Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2567011Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2567440Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2567708Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2567832Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2568029Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2568412Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2568673Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2568799Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2568946Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2569319Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2569631Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2569750Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2569901Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2570277Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2570543Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2570665Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2570814Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2571188Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2571461Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2571581Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2571730Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2572116Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2572381Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2572558Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2572706Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2573089Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2573357Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2573480Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2573628Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2574008Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2574279Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2574398Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2574600Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2574984Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2575260Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2575435Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2575584Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2575971Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2576244Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2576365Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2576513Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2576893Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2577210Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2577335Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2577481Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2577866Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2578134Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2578260Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2578410Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2578791Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2579060Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2579182Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2579330Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2579708Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2580049Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2580171Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2580320Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2580700Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2580969Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2581093Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2581238Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2581622Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2581888Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2582063Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2582207Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2582592Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2582857Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2583025Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2583172Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2583553Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2583824Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2583945Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2584092Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2584525Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2584794Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2584916Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2585061Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2585449Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2585717Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2585841Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2585986Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2586370Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2586633Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2586756Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2586900Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2587278Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2587598Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2587722Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2587873Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2588248Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2588554Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2588678Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2588828Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2589204Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2589470Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2589652Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2589801Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2590181Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2590501Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2590623Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2590768Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2591151Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2591418Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2591543Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2591743Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2592122Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2592395Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2592515Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2592665Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2593047Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2593322Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2593443Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2593591Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2593972Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2594240Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2594363Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2594510Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2594953Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2595222Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2595346Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2595493Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2595868Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2596143Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2596262Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2596414Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2596797Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2597120Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2597242Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2597396Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2597774Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2598098Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2598219Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2598364Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2598755Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2599029Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2599203Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2599350Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2599727Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2599996Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2600120Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2600268Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2600647Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2600916Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2601039Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2601189Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2601567Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2601836Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2601958Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2602107Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2602540Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2602808Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2602934Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2603081Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2603459Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2603728Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2603851Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2603998Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2604384Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2604705Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2604828Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2604977Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2605357Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2605679Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2605800Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2605951Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2606335Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2606608Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2606793Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2606939Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2607324Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2607594Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2607718Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2607867Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2608252Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2608521Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2608647Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2608796Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2609174Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2609447Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2609568Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2609769Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2610148Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2610423Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2610544Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2610693Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2611070Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2611333Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2611456Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2611604Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2611985Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2612306Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2612434Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2612580Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2613007Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2613276Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2613402Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2613553Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2613946Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2614265Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2614386Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2614537Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2614922Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2615191Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2615313Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2615459Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2615847Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2616113Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2616237Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2616381Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2616764Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2617034Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2617210Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2617355Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2617738Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2618014Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2618136Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2618284Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2618666Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2618935Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2619058Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2619207Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2619636Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2619907Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2620028Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2620173Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2620609Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2620878Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2621005Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2621150Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2621544Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2621862Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2621985Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2622133Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2622514Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2622784Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2622905Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2623053Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2623437Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2623707Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2623827Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2623975Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2624354Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2624620Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2624800Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2624947Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2625333Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2625600Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2625724Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2625871Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2626248Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2626512Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2626633Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2626847Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2627231Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2627506Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2627627Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2627825Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2628205Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2628507Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2628631Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2628778Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2629162Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2629485Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2629609Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2629759Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2630141Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2630408Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2630533Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2630681Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2631066Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2631338Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2631462Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2631610Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2631996Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2632412Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2632535Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2632685Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2633073Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2633339Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2633466Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2633611Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2633999Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2634266Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2634445Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2634590Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2634987Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2635255Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2635446Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2635594Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2635984Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2636263Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2636386Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2636537Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2636979Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2637249Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2637373Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2637521Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2637907Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2638175Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2638302Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2638447Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2638843Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2639110Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2639237Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2639382Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2639771Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2640090Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2640213Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2640362Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2640745Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2641013Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2641136Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2641284Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2641670Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2641946Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2642117Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2642263Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2642653Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2642917Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2643095Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2643242Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2643632Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2643901Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2644027Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2644224Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2644615Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2644885Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2645006Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2645156Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2645541Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2645814Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2645937Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2646088Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2646476Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2646740Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2646866Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2647011Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2647400Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2647721Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2647845Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2647993Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2648375Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2648643Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2648762Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2648911Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2649291Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2649615Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2649736Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2649888Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2650276Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2650595Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2650716Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2650863Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2651250Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2651519Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2651692Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2651837Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2652220Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2652485Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2652609Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2652754Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2653134Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2653403Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2653526Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2653676Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2654054Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2654325Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2654447Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2654596Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2655029Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2655302Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2655424Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2655570Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2655954Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2656220Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2656345Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2656490Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2656873Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2657193Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2657320Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2657465Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2657844Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2658167Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2658287Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2658439Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2658816Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2659088Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2659261Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2659410Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2659794Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2660062Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2660184Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2660329Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2660712Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2660979Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2661105Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2661250Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2661635Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2661906Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2662025Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2662226Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2662616Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2662892Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2663013Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2663161Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2663547Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2663818Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2663938Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2664087Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2664477Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2664806Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2664935Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2665080Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2665511Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2665885Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2666025Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2666174Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2666564Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2666889Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2667010Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2667163Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2667557Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2667829Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2667951Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2668099Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2668520Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2668793Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2668918Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2669066Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2669460Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2669724Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2669907Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2670050Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2670440Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2670709Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2670831Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2670980Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2671369Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2671643Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2671766Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2671915Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2672356Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2672629Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2672750Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2672952Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2673341Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2673609Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2673736Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2673882Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2674270Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2674592Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2674716Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2674864Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2675254Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2675524Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2675648Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2675798Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2676180Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2676448Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2676568Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2676719Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2677104Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2677372Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2677546Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2677694Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2678063Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2678331Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2678454Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2678603Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2678965Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2679232Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2679360Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2679559Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2679917Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2680191Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2680313Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2680510Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2680865Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2681134Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2681257Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2681409Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2681761Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2682078Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2682203Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2682351Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2682712Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2682977Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2683106Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2683254Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2683613Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2683880Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2684000Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2684152Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2684516Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2684787Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2684961Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2685110Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2685466Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2685739Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2685860Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2686006Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2686373Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2686644Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2686770Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2686973Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2687332Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2687599Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2687774Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2687920Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2688279Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2688552Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2688671Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2688825Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2689180Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2689498Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2689622Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2689771Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2690125Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2690392Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2690514Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2690662Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2691022Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2691290Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2691416Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2691560Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2691921Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2692256Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2692376Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2692526Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2692877Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2693148Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2693269Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2693416Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2693773Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2694041Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2694161Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2694361Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2694720Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2694988Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2695162Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2695307Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2695665Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2695930Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2696055Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2696200Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2696555Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2696877Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2697001Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2697152Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2697513Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2697785Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2697909Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2698059Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2698418Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2698685Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2698812Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2698958Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2699324Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2699655Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2699782Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2699929Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2700295Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2700560Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2700684Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2700835Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2701194Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2701466Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2701639Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2701788Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2702146Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2702412Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2702583Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2702731Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2703089Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2703354Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2703482Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2703627Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2704039Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2704309Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2704439Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2704586Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2704950Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2705223Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2705346Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2705494Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2705855Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2706124Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2706246Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2706395Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2706763Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2707083Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2707207Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2707352Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2707715Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2707977Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2708105Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2708250Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2708654Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2708925Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2709111Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2709263Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2709625Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2709895Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2710069Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2710219Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2710586Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2710855Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2710979Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2711122Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2711536Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2711802Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2711929Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2712076Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2712434Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2712703Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2712830Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2712978Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2713342Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2713612Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2713738Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2713888Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2714252Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2714585Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2714712Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2714864Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2715224Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2715493Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2715622Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2715769Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2716136Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2716404Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2716579Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2716724Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2717090Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2717355Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2717533Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2717678Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2718045Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2718320Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2718443Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2718594Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2719018Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2719289Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2719413Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2719560Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2719929Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2720195Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2720325Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2720474Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2720837Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2721103Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2721230Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2721374Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2721738Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2722064Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2722186Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2722339Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2722640Z FAILED tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2722910Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2723033Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2723184Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2723555Z FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2723823Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2723999Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2724143Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2724520Z FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2724787Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2724961Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2725106Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2725435Z FAILED tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2725702Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2725828Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2725972Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2726293Z FAILED tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2726563Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2726687Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2726836Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2727046Z FAILED tests/test_lazy/test_ops.py::test_lazy_ops - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2727319Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2727443Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2727590Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2727879Z FAILED tests/test_lora/test_lora.py::test_torch_ddp_lora - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2727886Z 
2025-04-11T04:23:19.2728002Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2728102Z Traceback (most recent call last):
2025-04-11T04:23:19.2728401Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2728484Z     fn(i, *args)
2025-04-11T04:23:19.2728692Z   File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
2025-04-11T04:23:19.2728772Z     run_lora_test()
2025-04-11T04:23:19.2728988Z   File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
2025-04-11T04:23:19.2729217Z     check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
2025-04-11T04:23:19.2729435Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2729532Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2729791Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2729886Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2730158Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2730261Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2730355Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2730629Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2730755Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2730907Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2731201Z FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2731471Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2731597Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2731745Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2731986Z FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type1] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2732305Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2732430Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2732577Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2732898Z FAILED tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2732905Z 
2025-04-11T04:23:19.2733021Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2733115Z Traceback (most recent call last):
2025-04-11T04:23:19.2733446Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2733523Z     fn(i, *args)
2025-04-11T04:23:19.2733752Z   File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
2025-04-11T04:23:19.2733842Z     check_moe_checkpoint()
2025-04-11T04:23:19.2734096Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2734181Z     partial_func(**kwargs)
2025-04-11T04:23:19.2734430Z   File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
2025-04-11T04:23:19.2734568Z     dist.broadcast_object_list(broadcast_objects, src=0)
2025-04-11T04:23:19.2734856Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2734949Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2735302Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
2025-04-11T04:23:19.2735516Z     tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T04:23:19.2735840Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
2025-04-11T04:23:19.2736044Z     tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T04:23:19.2736382Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
2025-04-11T04:23:19.2736571Z     byte_tensor = torch.ByteTensor(byte_storage).to(device)
2025-04-11T04:23:19.2736674Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2736952Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2737084Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2737234Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2737580Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2737850Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2737977Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2738126Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2738462Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2738790Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2738915Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2739064Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2739394Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2739719Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2739841Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2739989Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2740318Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2740586Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2740762Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2740908Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2741239Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2741508Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2741632Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2741776Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2742101Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2742370Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2742491Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2742645Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2742968Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2743237Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2743356Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2743502Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2743884Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2744156Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2744276Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2744425Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2744753Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2745023Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2745148Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2745293Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2745622Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2745892Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2746067Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2746215Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2746543Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2746816Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2746997Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2747147Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2747471Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2747746Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2747869Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2748160Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2748511Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2748781Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2748908Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2749053Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2749378Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2749644Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2749768Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2749910Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2750239Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2750509Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2750632Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2750781Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2751105Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2751434Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2751558Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2751705Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2752031Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2752294Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2752422Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2752567Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2752892Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2753159Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2753340Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2753486Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2753815Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2754081Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2754255Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2754403Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2754725Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2754999Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2755121Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2755270Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2755705Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2755974Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2756100Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2756246Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2756629Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2756900Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2757029Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2757174Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2757562Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2757833Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2757959Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2758106Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2758482Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2758805Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2758928Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2759077Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2759450Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2759720Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2759843Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2759992Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2760359Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2760629Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2760809Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2760953Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2761333Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2761600Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2761774Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2761919Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2762296Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2762566Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2762691Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2762835Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2763258Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2763531Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2763657Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2763808Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2764173Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2764445Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2764568Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2764714Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2765093Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2765358Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2765484Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2765629Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2766007Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2766321Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2766447Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2766590Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2766956Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2767224Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2767346Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2767495Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2767859Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2768131Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2768304Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2768453Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2768821Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2769091Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2769265Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2769412Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2769788Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2770056Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2770184Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2770328Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2770769Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2771038Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2771165Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2771311Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2771676Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2771949Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2772072Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2772221Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2772599Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2772870Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2772995Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2773142Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2773516Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2773837Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2773963Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2774107Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2774483Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2774752Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2774879Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2775025Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2775392Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2775663Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2775848Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2775999Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2776375Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2776645Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2776815Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2776963Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2777333Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2777602Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2777727Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2777919Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2778252Z FAILED tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2778257Z 
2025-04-11T04:23:19.2778375Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2778476Z Traceback (most recent call last):
2025-04-11T04:23:19.2778762Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2778842Z     fn(i, *args)
2025-04-11T04:23:19.2779087Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
2025-04-11T04:23:19.2779180Z     exam_dist_adafactor_base()
2025-04-11T04:23:19.2779436Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2779527Z     partial_func(**kwargs)
2025-04-11T04:23:19.2779778Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2779863Z     partial_func(**kwargs)
2025-04-11T04:23:19.2780146Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
2025-04-11T04:23:19.2780301Z     model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
2025-04-11T04:23:19.2780560Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.2780655Z     return self._apply(convert)
2025-04-11T04:23:19.2780920Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2781068Z     param_applied = fn(param)
2025-04-11T04:23:19.2781345Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.2781568Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.2781670Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2781947Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2782072Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2782225Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2782535Z FAILED tests/test_optimizer/test_dist_came.py::test_dist_came - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2782540Z 
2025-04-11T04:23:19.2782656Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2782754Z Traceback (most recent call last):
2025-04-11T04:23:19.2783036Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2783166Z     fn(i, *args)
2025-04-11T04:23:19.2783393Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
2025-04-11T04:23:19.2783537Z     exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
2025-04-11T04:23:19.2783793Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2783877Z     partial_func(**kwargs)
2025-04-11T04:23:19.2784226Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
2025-04-11T04:23:19.2784447Z     ) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
2025-04-11T04:23:19.2784760Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
2025-04-11T04:23:19.2784853Z     org_model = org_model.cuda()
2025-04-11T04:23:19.2785135Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2785230Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2785539Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2785653Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2785914Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2786001Z     module._apply(fn)
2025-04-11T04:23:19.2786260Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2786343Z     module._apply(fn)
2025-04-11T04:23:19.2786599Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2786690Z     param_applied = fn(param)
2025-04-11T04:23:19.2786957Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2787067Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2787170Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2787444Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2787573Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2787729Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2788038Z FAILED tests/test_optimizer/test_dist_galore.py::test_dist_galore - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2788046Z 
2025-04-11T04:23:19.2788160Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2788305Z Traceback (most recent call last):
2025-04-11T04:23:19.2788615Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2788693Z     fn(i, *args)
2025-04-11T04:23:19.2788948Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
2025-04-11T04:23:19.2789032Z     dist.barrier()
2025-04-11T04:23:19.2789319Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2789409Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2789720Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:19.2789823Z     work = default_pg.barrier(opts=opts)
2025-04-11T04:23:19.2789918Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2790193Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2790321Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2790474Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2790835Z FAILED tests/test_optimizer/test_dist_lamb.py::test_dist_lamb - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2790842Z 
2025-04-11T04:23:19.2790956Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2791052Z Traceback (most recent call last):
2025-04-11T04:23:19.2791330Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2791468Z     fn(i, *args)
2025-04-11T04:23:19.2791716Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
2025-04-11T04:23:19.2791803Z     run_dist_lamb_basic()
2025-04-11T04:23:19.2792052Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2792138Z     partial_func(**kwargs)
2025-04-11T04:23:19.2792387Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2792471Z     partial_func(**kwargs)
2025-04-11T04:23:19.2792717Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2792854Z     partial_func(**kwargs)
2025-04-11T04:23:19.2793073Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2793169Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2793424Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2793521Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2793793Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2793894Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2793988Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2794265Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2794395Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2794548Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2794891Z FAILED tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2794895Z 
2025-04-11T04:23:19.2795009Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2795107Z Traceback (most recent call last):
2025-04-11T04:23:19.2795387Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2795464Z     fn(i, *args)
2025-04-11T04:23:19.2795704Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
2025-04-11T04:23:19.2795854Z     check_p2p_communication()
2025-04-11T04:23:19.2796147Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
2025-04-11T04:23:19.2796312Z     tensor = torch.ones(1, device=get_accelerator().get_current_device())
2025-04-11T04:23:19.2796413Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2796684Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2796814Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2796968Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2797316Z FAILED tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2797321Z 
2025-04-11T04:23:19.2797431Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2797524Z Traceback (most recent call last):
2025-04-11T04:23:19.2797806Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2797932Z     fn(i, *args)
2025-04-11T04:23:19.2798164Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
2025-04-11T04:23:19.2798249Z     check_stage_manager()
2025-04-11T04:23:19.2798510Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
2025-04-11T04:23:19.2798598Z     dist.barrier(group=group)
2025-04-11T04:23:19.2798938Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2799029Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2799337Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
2025-04-11T04:23:19.2799438Z     work = group.barrier(opts=opts)
2025-04-11T04:23:19.2799532Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2799809Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2799935Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2800139Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2800485Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2800490Z 
2025-04-11T04:23:19.2800603Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2800696Z Traceback (most recent call last):
2025-04-11T04:23:19.2800969Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2801046Z     fn(i, *args)
2025-04-11T04:23:19.2801301Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:19.2801396Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2801653Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2801762Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2802027Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2802107Z     module._apply(fn)
2025-04-11T04:23:19.2802366Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2802448Z     module._apply(fn)
2025-04-11T04:23:19.2802707Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2802795Z     param_applied = fn(param)
2025-04-11T04:23:19.2803058Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2803239Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2803334Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2803615Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2803742Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2803896Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2804244Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2804250Z 
2025-04-11T04:23:19.2804362Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2804455Z Traceback (most recent call last):
2025-04-11T04:23:19.2804732Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2804811Z     fn(i, *args)
2025-04-11T04:23:19.2805070Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:19.2805212Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2805473Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2805580Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2805849Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2805928Z     module._apply(fn)
2025-04-11T04:23:19.2806243Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2806321Z     module._apply(fn)
2025-04-11T04:23:19.2806583Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2806670Z     param_applied = fn(param)
2025-04-11T04:23:19.2806938Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2807048Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2807143Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2807424Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2807601Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2807759Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2808101Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2808105Z 
2025-04-11T04:23:19.2808217Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2808309Z Traceback (most recent call last):
2025-04-11T04:23:19.2808582Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2808661Z     fn(i, *args)
2025-04-11T04:23:19.2808914Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:19.2809008Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2809265Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2809371Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2809631Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2809714Z     module._apply(fn)
2025-04-11T04:23:19.2809976Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2810055Z     module._apply(fn)
2025-04-11T04:23:19.2810316Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2810456Z     param_applied = fn(param)
2025-04-11T04:23:19.2810723Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2810827Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2810922Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2811204Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2811329Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2811484Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2811828Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2811833Z 
2025-04-11T04:23:19.2811945Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2812038Z Traceback (most recent call last):
2025-04-11T04:23:19.2812314Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2812438Z     fn(i, *args)
2025-04-11T04:23:19.2812693Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:19.2812789Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2813054Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2813159Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2813472Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2813556Z     module._apply(fn)
2025-04-11T04:23:19.2813813Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2813891Z     module._apply(fn)
2025-04-11T04:23:19.2814155Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2814242Z     param_applied = fn(param)
2025-04-11T04:23:19.2814512Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2814665Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2814763Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2815039Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2815170Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2815327Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2815661Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2815666Z 
2025-04-11T04:23:19.2815784Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2815873Z Traceback (most recent call last):
2025-04-11T04:23:19.2816154Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2816229Z     fn(i, *args)
2025-04-11T04:23:19.2816488Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:19.2816595Z     examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:19.2816851Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:19.2816951Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2817207Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2817312Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2817572Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2817704Z     module._apply(fn)
2025-04-11T04:23:19.2817960Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2818040Z     module._apply(fn)
2025-04-11T04:23:19.2818302Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2818391Z     param_applied = fn(param)
2025-04-11T04:23:19.2818658Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2818763Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2818863Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2819138Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2819264Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2819421Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2819746Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2819802Z 
2025-04-11T04:23:19.2819921Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2820015Z Traceback (most recent call last):
2025-04-11T04:23:19.2820292Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2820366Z     fn(i, *args)
2025-04-11T04:23:19.2820625Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:19.2820782Z     examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:19.2821043Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:19.2821137Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2821395Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2821503Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2821763Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2821890Z     module._apply(fn)
2025-04-11T04:23:19.2822155Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2822233Z     module._apply(fn)
2025-04-11T04:23:19.2822490Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2822580Z     param_applied = fn(param)
2025-04-11T04:23:19.2822844Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2822947Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2823044Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2823321Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2823448Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2823603Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2823925Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2823930Z 
2025-04-11T04:23:19.2824044Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2824135Z Traceback (most recent call last):
2025-04-11T04:23:19.2824413Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2824486Z     fn(i, *args)
2025-04-11T04:23:19.2824744Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:19.2824904Z     examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:19.2825162Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:19.2825257Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2825513Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2825619Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2825882Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2825963Z     module._apply(fn)
2025-04-11T04:23:19.2826224Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2826301Z     module._apply(fn)
2025-04-11T04:23:19.2826561Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2826649Z     param_applied = fn(param)
2025-04-11T04:23:19.2826914Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2827071Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2827166Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2827455Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2827581Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2827737Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2828111Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2828116Z 
2025-04-11T04:23:19.2828231Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2828323Z Traceback (most recent call last):
2025-04-11T04:23:19.2828627Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2828703Z     fn(i, *args)
2025-04-11T04:23:19.2828960Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:19.2829061Z     examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:19.2829392Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:19.2829486Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2829742Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2829850Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2830107Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2830186Z     module._apply(fn)
2025-04-11T04:23:19.2830450Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2830527Z     module._apply(fn)
2025-04-11T04:23:19.2830789Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2830875Z     param_applied = fn(param)
2025-04-11T04:23:19.2831142Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2831244Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2831339Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2831621Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2831748Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2831904Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2832238Z FAILED tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2832301Z 
2025-04-11T04:23:19.2832419Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2832510Z Traceback (most recent call last):
2025-04-11T04:23:19.2832787Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2832867Z     fn(i, *args)
2025-04-11T04:23:19.2833142Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
2025-04-11T04:23:19.2833246Z     run_with_booster_moehybridplugin()
2025-04-11T04:23:19.2833501Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2833589Z     partial_func(**kwargs)
2025-04-11T04:23:19.2833924Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
2025-04-11T04:23:19.2834052Z     torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T04:23:19.2834330Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2834481Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2834746Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2834853Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2835117Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2835252Z     module._apply(fn)
2025-04-11T04:23:19.2835514Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2835602Z     param_applied = fn(param)
2025-04-11T04:23:19.2835867Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2835974Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2836068Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2836351Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2836526Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2836683Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2836966Z FAILED tests/test_shardformer/test_flash_attention.py::test_flash_attn_func - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2837238Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2837366Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2837518Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2837780Z FAILED tests/test_shardformer/test_shard_utils.py::test_release_layer - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2838045Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2838173Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2838323Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2838570Z FAILED tests/test_shardformer/test_with_torch_ddp.py::test_gpt2 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2838840Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2838964Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2839114Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2839476Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2839798Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2839923Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2840071Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2840435Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2840706Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2840830Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2840977Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2841335Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2841601Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2841778Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2841924Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2842313Z FAILED tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2842318Z 
2025-04-11T04:23:19.2842430Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2842588Z Traceback (most recent call last):
2025-04-11T04:23:19.2842872Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2842946Z     fn(i, *args)
2025-04-11T04:23:19.2843275Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
2025-04-11T04:23:19.2843409Z     pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
2025-04-11T04:23:19.2843507Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2843776Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2843951Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2844098Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2844426Z FAILED tests/test_shardformer/test_layer/test_dropout.py::test_dropout - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2844436Z 
2025-04-11T04:23:19.2844547Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2844636Z Traceback (most recent call last):
2025-04-11T04:23:19.2844916Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2844994Z     fn(i, *args)
2025-04-11T04:23:19.2845247Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
2025-04-11T04:23:19.2845339Z     check_dropout_parallel_input()
2025-04-11T04:23:19.2845645Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
2025-04-11T04:23:19.2845872Z     dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
2025-04-11T04:23:19.2846138Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
2025-04-11T04:23:19.2846359Z     return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
2025-04-11T04:23:19.2846594Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
2025-04-11T04:23:19.2846793Z     self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
2025-04-11T04:23:19.2847141Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
2025-04-11T04:23:19.2847327Z     is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
2025-04-11T04:23:19.2847635Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
2025-04-11T04:23:19.2847867Z     index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
2025-04-11T04:23:19.2847965Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2848239Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2848370Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2848522Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2848875Z FAILED tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2848881Z 
2025-04-11T04:23:19.2848995Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2849143Z Traceback (most recent call last):
2025-04-11T04:23:19.2849425Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2849503Z     fn(i, *args)
2025-04-11T04:23:19.2849765Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
2025-04-11T04:23:19.2849849Z     check_embedding_1d()
2025-04-11T04:23:19.2850101Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2850235Z     partial_func(**kwargs)
2025-04-11T04:23:19.2850515Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
2025-04-11T04:23:19.2850617Z     embedding = nn.Embedding(32, 128).cuda()
2025-04-11T04:23:19.2850875Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2850985Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2851243Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2851383Z     param_applied = fn(param)
2025-04-11T04:23:19.2851650Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2851759Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2851856Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2852130Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2852258Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2852412Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2852792Z FAILED tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2852799Z 
2025-04-11T04:23:19.2852910Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2853004Z Traceback (most recent call last):
2025-04-11T04:23:19.2853284Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2853360Z     fn(i, *args)
2025-04-11T04:23:19.2853659Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
2025-04-11T04:23:19.2853753Z     check_gpt2_qkv_fused_linear_1d()
2025-04-11T04:23:19.2854006Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2854090Z     partial_func(**kwargs)
2025-04-11T04:23:19.2854339Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2854477Z     partial_func(**kwargs)
2025-04-11T04:23:19.2854829Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
2025-04-11T04:23:19.2854961Z     check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
2025-04-11T04:23:19.2855297Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
2025-04-11T04:23:19.2855390Z     linear = Conv1D(192, 48).cuda()
2025-04-11T04:23:19.2855649Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2855757Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2856019Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2856111Z     param_applied = fn(param)
2025-04-11T04:23:19.2856380Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2856535Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2856633Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2856909Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2857043Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2857197Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2857609Z FAILED tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2857614Z 
2025-04-11T04:23:19.2857725Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2857817Z Traceback (most recent call last):
2025-04-11T04:23:19.2858089Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2858166Z     fn(i, *args)
2025-04-11T04:23:19.2858424Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
2025-04-11T04:23:19.2858507Z     check_layernorm()
2025-04-11T04:23:19.2858755Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2858887Z     partial_func(**kwargs)
2025-04-11T04:23:19.2859163Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
2025-04-11T04:23:19.2859264Z     norm = nn.LayerNorm(128, 0.00001).cuda()
2025-04-11T04:23:19.2859521Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2859628Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2859887Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2859981Z     param_applied = fn(param)
2025-04-11T04:23:19.2860247Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2860354Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2860450Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2860726Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2860856Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2861010Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2861338Z FAILED tests/test_shardformer/test_layer/test_linear_1d.py::test_linear - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2861342Z 
2025-04-11T04:23:19.2861453Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2861597Z Traceback (most recent call last):
2025-04-11T04:23:19.2861876Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2861956Z     fn(i, *args)
2025-04-11T04:23:19.2862237Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
2025-04-11T04:23:19.2862324Z     run_dist_linear_test()
2025-04-11T04:23:19.2862574Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2862656Z     partial_func(**kwargs)
2025-04-11T04:23:19.2862904Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2862989Z     partial_func(**kwargs)
2025-04-11T04:23:19.2863228Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2863314Z     partial_func(**kwargs)
2025-04-11T04:23:19.2863605Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
2025-04-11T04:23:19.2863747Z     check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
2025-04-11T04:23:19.2864081Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T04:23:19.2864177Z     linear = nn.Linear(32, 128).cuda()
2025-04-11T04:23:19.2864434Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2864538Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2864945Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2865034Z     param_applied = fn(param)
2025-04-11T04:23:19.2865303Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2865407Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2865507Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2865780Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2865912Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2866113Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2866480Z FAILED tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2866489Z 
2025-04-11T04:23:19.2866603Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2866694Z Traceback (most recent call last):
2025-04-11T04:23:19.2866972Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2867047Z     fn(i, *args)
2025-04-11T04:23:19.2867334Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
2025-04-11T04:23:19.2867420Z     check_linear_1d_col()
2025-04-11T04:23:19.2867666Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2867752Z     partial_func(**kwargs)
2025-04-11T04:23:19.2868064Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T04:23:19.2868158Z     linear = nn.Linear(8, 80).cuda()
2025-04-11T04:23:19.2868479Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2868589Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2868850Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2868939Z     param_applied = fn(param)
2025-04-11T04:23:19.2869202Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2869364Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2869466Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2869741Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2869874Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2870027Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2870369Z FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2870375Z 
2025-04-11T04:23:19.2870487Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2870577Z Traceback (most recent call last):
2025-04-11T04:23:19.2870858Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2870933Z     fn(i, *args)
2025-04-11T04:23:19.2871221Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
2025-04-11T04:23:19.2871359Z     check_packed_seq()
2025-04-11T04:23:19.2871608Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2871693Z     partial_func(**kwargs)
2025-04-11T04:23:19.2871941Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2872029Z     partial_func(**kwargs)
2025-04-11T04:23:19.2872330Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2872415Z     partial_func(**kwargs)
2025-04-11T04:23:19.2872507Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2872789Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
2025-04-11T04:23:19.2872958Z     padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
2025-04-11T04:23:19.2873054Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2873333Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2873511Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2873669Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2874014Z FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2874021Z 
2025-04-11T04:23:19.2874134Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2874226Z Traceback (most recent call last):
2025-04-11T04:23:19.2874510Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2874587Z     fn(i, *args)
2025-04-11T04:23:19.2874873Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
2025-04-11T04:23:19.2874970Z     check_ring_attn(inner_ring_size=2)
2025-04-11T04:23:19.2875216Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2875304Z     partial_func(**kwargs)
2025-04-11T04:23:19.2875547Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2875631Z     partial_func(**kwargs)
2025-04-11T04:23:19.2875878Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2875958Z     partial_func(**kwargs)
2025-04-11T04:23:19.2876054Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2876328Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
2025-04-11T04:23:19.2876585Z     qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
2025-04-11T04:23:19.2876680Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2876959Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2877086Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2877239Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2877623Z FAILED tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2877629Z 
2025-04-11T04:23:19.2877740Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2877833Z Traceback (most recent call last):
2025-04-11T04:23:19.2878118Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2878197Z     fn(i, *args)
2025-04-11T04:23:19.2878511Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
2025-04-11T04:23:19.2878651Z     run_seq_parallel_attn()
2025-04-11T04:23:19.2878901Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2878984Z     partial_func(**kwargs)
2025-04-11T04:23:19.2879228Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2879310Z     partial_func(**kwargs)
2025-04-11T04:23:19.2879605Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2879686Z     partial_func(**kwargs)
2025-04-11T04:23:19.2879781Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2880104Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
2025-04-11T04:23:19.2880250Z     seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
2025-04-11T04:23:19.2880561Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
2025-04-11T04:23:19.2880693Z     x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
2025-04-11T04:23:19.2880841Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2881116Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2881247Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2881402Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2881802Z FAILED tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2881811Z 
2025-04-11T04:23:19.2881923Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2882016Z Traceback (most recent call last):
2025-04-11T04:23:19.2882308Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2882386Z     fn(i, *args)
2025-04-11T04:23:19.2882693Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
2025-04-11T04:23:19.2882784Z     check_vocab_embedding_1d()
2025-04-11T04:23:19.2883031Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2883117Z     partial_func(**kwargs)
2025-04-11T04:23:19.2883462Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
2025-04-11T04:23:19.2883576Z     embedding = nn.Embedding(128, 32).to("cuda")
2025-04-11T04:23:19.2883836Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.2883981Z     return self._apply(convert)
2025-04-11T04:23:19.2884244Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2884337Z     param_applied = fn(param)
2025-04-11T04:23:19.2884605Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.2884823Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.2884924Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2885202Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2885335Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2885490Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2885758Z FAILED tests/test_shardformer/test_model/test_shard_bert.py::test_bert - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2886027Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2886219Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2886376Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2886644Z FAILED tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2886915Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2887088Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2887238Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2887500Z FAILED tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2887775Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2887899Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2888045Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2888376Z FAILED tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2888640Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2888767Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2888913Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2889192Z FAILED tests/test_shardformer/test_model/test_shard_command.py::test_command - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2889457Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2889582Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2889731Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2890084Z FAILED tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2890090Z 
2025-04-11T04:23:19.2890207Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2890298Z Traceback (most recent call last):
2025-04-11T04:23:19.2890585Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2890659Z     fn(i, *args)
2025-04-11T04:23:19.2890955Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
2025-04-11T04:23:19.2891039Z     run_deepseek_test()
2025-04-11T04:23:19.2891341Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2891427Z     partial_func(**kwargs)
2025-04-11T04:23:19.2891727Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
2025-04-11T04:23:19.2891821Z     run_deepseek_commom(config)
2025-04-11T04:23:19.2892114Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
2025-04-11T04:23:19.2892318Z     torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
2025-04-11T04:23:19.2892599Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2892694Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2892954Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2893064Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2893331Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2893462Z     module._apply(fn)
2025-04-11T04:23:19.2893726Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2893818Z     param_applied = fn(param)
2025-04-11T04:23:19.2894087Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2894191Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2894336Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2894617Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2894741Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2894898Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2895265Z FAILED tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2895272Z 
2025-04-11T04:23:19.2895388Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2895526Z Traceback (most recent call last):
2025-04-11T04:23:19.2895804Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2895882Z     fn(i, *args)
2025-04-11T04:23:19.2896186Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
2025-04-11T04:23:19.2896274Z     run_deepseek_v3_test()
2025-04-11T04:23:19.2896523Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2896608Z     partial_func(**kwargs)
2025-04-11T04:23:19.2896922Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
2025-04-11T04:23:19.2897006Z     check_forward_backward(
2025-04-11T04:23:19.2897328Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
2025-04-11T04:23:19.2897596Z     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
2025-04-11T04:23:19.2897891Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
2025-04-11T04:23:19.2897985Z     org_model = org_model.cuda()
2025-04-11T04:23:19.2898264Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2898358Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2898619Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2898780Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2899050Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2899140Z     module._apply(fn)
2025-04-11T04:23:19.2899402Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2899491Z     module._apply(fn)
2025-04-11T04:23:19.2899755Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2899854Z     param_applied = fn(param)
2025-04-11T04:23:19.2900126Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2900234Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2900337Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2900617Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2900753Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2900956Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2901236Z FAILED tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2901505Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2901628Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2901831Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2902095Z FAILED tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2902370Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2902495Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2902646Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2902912Z FAILED tests/test_shardformer/test_model/test_shard_llama.py::test_llama - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2903227Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2903349Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2903496Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2903777Z FAILED tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2904040Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2904167Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2904315Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2904659Z FAILED tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2904666Z 
2025-04-11T04:23:19.2904780Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2904875Z Traceback (most recent call last):
2025-04-11T04:23:19.2905154Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2905228Z     fn(i, *args)
2025-04-11T04:23:19.2905511Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
2025-04-11T04:23:19.2905590Z     run_mixtral_test()
2025-04-11T04:23:19.2905841Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2905973Z     partial_func(**kwargs)
2025-04-11T04:23:19.2906271Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
2025-04-11T04:23:19.2906359Z     run_mixtral_commom(config)
2025-04-11T04:23:19.2906653Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
2025-04-11T04:23:19.2906781Z     torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T04:23:19.2907060Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2907157Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2907420Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2907530Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2907795Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2907877Z     module._apply(fn)
2025-04-11T04:23:19.2908141Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2908279Z     param_applied = fn(param)
2025-04-11T04:23:19.2908576Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2908682Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2908780Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2909054Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2909235Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2909390Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2909664Z FAILED tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2909938Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2910064Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2910218Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2910534Z FAILED tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2910801Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2910925Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2911070Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2911328Z FAILED tests/test_shardformer/test_model/test_shard_sam.py::test_sam - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2911592Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2911718Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2911867Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2912118Z FAILED tests/test_shardformer/test_model/test_shard_t5.py::test_t5 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2912386Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2912509Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2912657Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2912909Z FAILED tests/test_shardformer/test_model/test_shard_vit.py::test_vit - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2913180Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2913373Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2913523Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2913804Z FAILED tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2914077Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2914202Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2914348Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2914657Z FAILED tests/test_tensor/test_comm_spec_apply.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2914662Z 
2025-04-11T04:23:19.2914776Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2914875Z Traceback (most recent call last):
2025-04-11T04:23:19.2915157Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2915237Z     fn(i, *args)
2025-04-11T04:23:19.2915472Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
2025-04-11T04:23:19.2915640Z     check_all_gather(device_mesh, rank)
2025-04-11T04:23:19.2915891Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
2025-04-11T04:23:19.2916008Z     sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T04:23:19.2916106Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2916379Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2916559Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2916708Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2917028Z FAILED tests/test_tensor/test_padded_tensor.py::test_padded_tensor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2917035Z 
2025-04-11T04:23:19.2917149Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2917242Z Traceback (most recent call last):
2025-04-11T04:23:19.2917526Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2917655Z     fn(i, *args)
2025-04-11T04:23:19.2917911Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
2025-04-11T04:23:19.2918023Z     original_tensor = torch.rand(32, 64).to("cuda")
2025-04-11T04:23:19.2918122Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2918390Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2918516Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2918667Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2918990Z FAILED tests/test_tensor/test_shape_consistency_apply.py::test_apply - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2918997Z 
2025-04-11T04:23:19.2919111Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2919202Z Traceback (most recent call last):
2025-04-11T04:23:19.2919485Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2919559Z     fn(i, *args)
2025-04-11T04:23:19.2919820Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
2025-04-11T04:23:19.2919995Z     tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
2025-04-11T04:23:19.2920089Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2920363Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2920540Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2920695Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2921022Z FAILED tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2921028Z 
2025-04-11T04:23:19.2921144Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2921236Z Traceback (most recent call last):
2025-04-11T04:23:19.2921513Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2921592Z     fn(i, *args)
2025-04-11T04:23:19.2921845Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
2025-04-11T04:23:19.2921956Z     check_all_gather(process_group_dict, rank)
2025-04-11T04:23:19.2922221Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
2025-04-11T04:23:19.2922341Z     sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T04:23:19.2922435Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2922759Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2922885Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2923036Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2923352Z FAILED tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2923405Z 
2025-04-11T04:23:19.2923516Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2923609Z Traceback (most recent call last):
2025-04-11T04:23:19.2923882Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2923958Z     fn(i, *args)
2025-04-11T04:23:19.2924210Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
2025-04-11T04:23:19.2924306Z     test_model = TestModel(8, 8).to("cuda")
2025-04-11T04:23:19.2924571Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.2924710Z     return self._apply(convert)
2025-04-11T04:23:19.2924974Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2925054Z     module._apply(fn)
2025-04-11T04:23:19.2925313Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2925403Z     param_applied = fn(param)
2025-04-11T04:23:19.2925668Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.2925881Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.2925975Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2926251Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2926377Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2926531Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2926899Z FAILED tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2926904Z 
2025-04-11T04:23:19.2927020Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2927110Z Traceback (most recent call last):
2025-04-11T04:23:19.2927384Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2927462Z     fn(i, *args)
2025-04-11T04:23:19.2927787Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
2025-04-11T04:23:19.2927962Z     original_tensor = torch.rand(global_shape).cuda()
2025-04-11T04:23:19.2928058Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2928344Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2928471Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2928623Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2928961Z FAILED tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2928967Z 
2025-04-11T04:23:19.2929078Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2929171Z Traceback (most recent call last):
2025-04-11T04:23:19.2929448Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2929526Z     fn(i, *args)
2025-04-11T04:23:19.2929774Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
2025-04-11T04:23:19.2929909Z     exam_chunk_memory()
2025-04-11T04:23:19.2930164Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2930249Z     partial_func(**kwargs)
2025-04-11T04:23:19.2930497Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2930579Z     partial_func(**kwargs)
2025-04-11T04:23:19.2930894Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
2025-04-11T04:23:19.2930991Z     chunk_manager = ChunkManager(config)
2025-04-11T04:23:19.2931229Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T04:23:19.2931474Z     self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:19.2931570Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2931849Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2931974Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2932180Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2932510Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2932517Z 
2025-04-11T04:23:19.2932632Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2932721Z Traceback (most recent call last):
2025-04-11T04:23:19.2932999Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2933075Z     fn(i, *args)
2025-04-11T04:23:19.2933309Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:19.2933393Z     exam_chunk_basic()
2025-04-11T04:23:19.2933639Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2933724Z     partial_func(**kwargs)
2025-04-11T04:23:19.2933966Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2934047Z     partial_func(**kwargs)
2025-04-11T04:23:19.2934291Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2934374Z     partial_func(**kwargs)
2025-04-11T04:23:19.2934471Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2934721Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:19.2934803Z     my_chunk = Chunk(
2025-04-11T04:23:19.2935088Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:19.2935286Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:19.2935389Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2935664Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2935795Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2935945Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2936277Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2936283Z 
2025-04-11T04:23:19.2936394Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2936485Z Traceback (most recent call last):
2025-04-11T04:23:19.2936766Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2936842Z     fn(i, *args)
2025-04-11T04:23:19.2937079Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:19.2937210Z     exam_chunk_basic()
2025-04-11T04:23:19.2937459Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2937543Z     partial_func(**kwargs)
2025-04-11T04:23:19.2937786Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2937919Z     partial_func(**kwargs)
2025-04-11T04:23:19.2938160Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2938246Z     partial_func(**kwargs)
2025-04-11T04:23:19.2938340Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2938596Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:19.2938676Z     my_chunk = Chunk(
2025-04-11T04:23:19.2938905Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:19.2939103Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:19.2939261Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2939542Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2939668Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2939826Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2940157Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2940162Z 
2025-04-11T04:23:19.2940275Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2940368Z Traceback (most recent call last):
2025-04-11T04:23:19.2940648Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2940726Z     fn(i, *args)
2025-04-11T04:23:19.2940956Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:19.2941039Z     exam_chunk_basic()
2025-04-11T04:23:19.2941285Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2941369Z     partial_func(**kwargs)
2025-04-11T04:23:19.2941610Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2941694Z     partial_func(**kwargs)
2025-04-11T04:23:19.2941939Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2942018Z     partial_func(**kwargs)
2025-04-11T04:23:19.2942166Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2942418Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:19.2942502Z     my_chunk = Chunk(
2025-04-11T04:23:19.2942730Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:19.2942922Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:19.2943022Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2943298Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2943430Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2943584Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2943932Z FAILED tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2943939Z 
2025-04-11T04:23:19.2944050Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2944139Z Traceback (most recent call last):
2025-04-11T04:23:19.2944474Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2944550Z     fn(i, *args)
2025-04-11T04:23:19.2944795Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
2025-04-11T04:23:19.2944877Z     exam_gemini_grad_acc()
2025-04-11T04:23:19.2945125Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2945257Z     partial_func(**kwargs)
2025-04-11T04:23:19.2945498Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2945584Z     partial_func(**kwargs)
2025-04-11T04:23:19.2945822Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2945910Z     partial_func(**kwargs)
2025-04-11T04:23:19.2946002Z   [Previous line repeated 4 more times]
2025-04-11T04:23:19.2946278Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
2025-04-11T04:23:19.2946373Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2946696Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2946793Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2947050Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2947164Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2947422Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2947504Z     module._apply(fn)
2025-04-11T04:23:19.2947764Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2947847Z     module._apply(fn)
2025-04-11T04:23:19.2948104Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2948192Z     param_applied = fn(param)
2025-04-11T04:23:19.2948489Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2948595Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2948695Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2948974Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2949111Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2949266Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2949650Z FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2949654Z 
2025-04-11T04:23:19.2949781Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2949874Z Traceback (most recent call last):
2025-04-11T04:23:19.2950158Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2950239Z     fn(i, *args)
2025-04-11T04:23:19.2950491Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T04:23:19.2950580Z     exam_grad_clipping()
2025-04-11T04:23:19.2950830Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2950919Z     partial_func(**kwargs)
2025-04-11T04:23:19.2951162Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2951253Z     partial_func(**kwargs)
2025-04-11T04:23:19.2951497Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2951637Z     partial_func(**kwargs)
2025-04-11T04:23:19.2951732Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2952002Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T04:23:19.2952105Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2952388Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2952543Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2952802Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2952915Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2953175Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2953258Z     module._apply(fn)
2025-04-11T04:23:19.2953521Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2953605Z     module._apply(fn)
2025-04-11T04:23:19.2953868Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2954009Z     param_applied = fn(param)
2025-04-11T04:23:19.2954280Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2954388Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2954489Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2954773Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2954901Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2955065Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2955386Z FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2955393Z 
2025-04-11T04:23:19.2955512Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2955608Z Traceback (most recent call last):
2025-04-11T04:23:19.2955890Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2955966Z     fn(i, *args)
2025-04-11T04:23:19.2956208Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T04:23:19.2956303Z     exam_grad_clipping()
2025-04-11T04:23:19.2956549Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2956638Z     partial_func(**kwargs)
2025-04-11T04:23:19.2956882Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2957019Z     partial_func(**kwargs)
2025-04-11T04:23:19.2957450Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2957582Z     partial_func(**kwargs)
2025-04-11T04:23:19.2957742Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2958135Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T04:23:19.2958262Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2958611Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2958794Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2959120Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2959255Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2959583Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2959731Z     module._apply(fn)
2025-04-11T04:23:19.2960086Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2960198Z     module._apply(fn)
2025-04-11T04:23:19.2960490Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2960641Z     param_applied = fn(param)
2025-04-11T04:23:19.2960935Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2961150Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2961292Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2961633Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2961793Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2961980Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2962372Z FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2962422Z 
2025-04-11T04:23:19.2962576Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2962740Z Traceback (most recent call last):
2025-04-11T04:23:19.2963142Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2963283Z     fn(i, *args)
2025-04-11T04:23:19.2963566Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T04:23:19.2963704Z     exam_inference()
2025-04-11T04:23:19.2963999Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2964114Z     partial_func(**kwargs)
2025-04-11T04:23:19.2964423Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2964547Z     partial_func(**kwargs)
2025-04-11T04:23:19.2964839Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2964964Z     partial_func(**kwargs)
2025-04-11T04:23:19.2965288Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T04:23:19.2965415Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2965726Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2965891Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2966166Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2966360Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2966702Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2966849Z     module._apply(fn)
2025-04-11T04:23:19.2967138Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2967251Z     module._apply(fn)
2025-04-11T04:23:19.2967571Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2967701Z     param_applied = fn(param)
2025-04-11T04:23:19.2968113Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2968248Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2968409Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2968704Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2968920Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2969107Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2969532Z FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2969573Z 
2025-04-11T04:23:19.2969718Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2969842Z Traceback (most recent call last):
2025-04-11T04:23:19.2970179Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2970359Z     fn(i, *args)
2025-04-11T04:23:19.2970660Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T04:23:19.2970769Z     exam_inference()
2025-04-11T04:23:19.2971043Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2971173Z     partial_func(**kwargs)
2025-04-11T04:23:19.2971452Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2971606Z     partial_func(**kwargs)
2025-04-11T04:23:19.2971876Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2972063Z     partial_func(**kwargs)
2025-04-11T04:23:19.2972342Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T04:23:19.2972496Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2972879Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2972996Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2973306Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2973441Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2973753Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2973878Z     module._apply(fn)
2025-04-11T04:23:19.2974212Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2974320Z     module._apply(fn)
2025-04-11T04:23:19.2974609Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2974754Z     param_applied = fn(param)
2025-04-11T04:23:19.2975049Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2975232Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2975354Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2975695Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2975900Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2976109Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2976453Z FAILED tests/test_zero/test_gemini/test_optim.py::test_optim[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2976460Z 
2025-04-11T04:23:19.2976614Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2976763Z Traceback (most recent call last):
2025-04-11T04:23:19.2977074Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2977220Z     fn(i, *args)
2025-04-11T04:23:19.2977467Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
2025-04-11T04:23:19.2977690Z     exam_model_step()
2025-04-11T04:23:19.2977967Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2978110Z     partial_func(**kwargs)
2025-04-11T04:23:19.2978393Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2978554Z     partial_func(**kwargs)
2025-04-11T04:23:19.2978854Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2978976Z     partial_func(**kwargs)
2025-04-11T04:23:19.2979125Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2987904Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
2025-04-11T04:23:19.2988197Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2988594Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2988700Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2988972Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2989096Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2989370Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2989459Z     module._apply(fn)
2025-04-11T04:23:19.2989789Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2989869Z     module._apply(fn)
2025-04-11T04:23:19.2990132Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2990228Z     param_applied = fn(param)
2025-04-11T04:23:19.2990506Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2990616Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2990722Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2991013Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2991153Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2991316Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2991644Z FAILED tests/test_zero/test_gemini/test_search.py::test_search[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2991653Z 
2025-04-11T04:23:19.2991778Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2991876Z Traceback (most recent call last):
2025-04-11T04:23:19.2992165Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2992242Z     fn(i, *args)
2025-04-11T04:23:19.2992489Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T04:23:19.2992575Z     exam_chunk_manager()
2025-04-11T04:23:19.2992899Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T04:23:19.2992997Z     chunk_manager = init_chunk_manager(
2025-04-11T04:23:19.2993265Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T04:23:19.2993349Z     dist.barrier()
2025-04-11T04:23:19.2993645Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2993740Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2994059Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:19.2994164Z     work = default_pg.barrier(opts=opts)
2025-04-11T04:23:19.2994268Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2994553Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2994695Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2994861Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2995239Z FAILED tests/test_zero/test_gemini/test_search.py::test_search[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2995246Z 
2025-04-11T04:23:19.2995363Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2995457Z Traceback (most recent call last):
2025-04-11T04:23:19.2995745Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2995874Z     fn(i, *args)
2025-04-11T04:23:19.2996114Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T04:23:19.2996198Z     exam_chunk_manager()
2025-04-11T04:23:19.2996459Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T04:23:19.2996551Z     chunk_manager = init_chunk_manager(
2025-04-11T04:23:19.2996815Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T04:23:19.2996901Z     dist.barrier()
2025-04-11T04:23:19.2997190Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2997336Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2997646Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:19.2997748Z     work = default_pg.barrier(opts=opts)
2025-04-11T04:23:19.2997846Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2998122Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2998254Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2998411Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2998768Z FAILED tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2998776Z 
2025-04-11T04:23:19.2998892Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2998987Z Traceback (most recent call last):
2025-04-11T04:23:19.2999270Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2999348Z     fn(i, *args)
2025-04-11T04:23:19.2999614Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
2025-04-11T04:23:19.2999696Z     exam_state_dict()
2025-04-11T04:23:19.2999954Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3000041Z     partial_func(**kwargs)
2025-04-11T04:23:19.3000293Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3000448Z     partial_func(**kwargs)
2025-04-11T04:23:19.3000696Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3000778Z     partial_func(**kwargs)
2025-04-11T04:23:19.3000873Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.3001157Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
2025-04-11T04:23:19.3001402Z     model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
2025-04-11T04:23:19.3001641Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
2025-04-11T04:23:19.3001738Z     self.chunk_manager = ChunkManager(
2025-04-11T04:23:19.3001979Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T04:23:19.3002225Z     self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:19.3002324Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3002658Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3002786Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3002947Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3003263Z FAILED tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3003315Z 
2025-04-11T04:23:19.3003434Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3003525Z Traceback (most recent call last):
2025-04-11T04:23:19.3003807Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3003883Z     fn(i, *args)
2025-04-11T04:23:19.3004121Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
2025-04-11T04:23:19.3004210Z     check_all_gather_2d()
2025-04-11T04:23:19.3004475Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
2025-04-11T04:23:19.3004607Z     tensor = torch.rand(128, device=get_current_device())
2025-04-11T04:23:19.3004750Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3005024Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3005151Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3005307Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3005654Z FAILED tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3005659Z 
2025-04-11T04:23:19.3005776Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3005868Z Traceback (most recent call last):
2025-04-11T04:23:19.3006149Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3006228Z     fn(i, *args)
2025-04-11T04:23:19.3006469Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
2025-04-11T04:23:19.3006565Z     exam_zero_1_grad_acc(sync=True)
2025-04-11T04:23:19.3006836Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
2025-04-11T04:23:19.3006928Z     zero_model = zero_model.to(device)
2025-04-11T04:23:19.3007186Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.3007276Z     return self._apply(convert)
2025-04-11T04:23:19.3007542Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.3007672Z     module._apply(fn)
2025-04-11T04:23:19.3007939Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.3008033Z     param_applied = fn(param)
2025-04-11T04:23:19.3008303Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.3008522Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.3008619Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3008907Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3009037Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3009193Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3009514Z FAILED tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3009521Z 
2025-04-11T04:23:19.3009634Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3009779Z Traceback (most recent call last):
2025-04-11T04:23:19.3010051Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3010131Z     fn(i, *args)
2025-04-11T04:23:19.3010374Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
2025-04-11T04:23:19.3010472Z     exam_mem_leak(world_size=world_size)
2025-04-11T04:23:19.3010768Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
2025-04-11T04:23:19.3010856Z     zero_model = MlpModel().cuda()
2025-04-11T04:23:19.3011119Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.3011226Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3011495Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.3011579Z     module._apply(fn)
2025-04-11T04:23:19.3011846Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.3011982Z     param_applied = fn(param)
2025-04-11T04:23:19.3012250Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.3012359Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3012456Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3012738Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3012863Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3013018Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3013337Z FAILED tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3013344Z 
2025-04-11T04:23:19.3013457Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3013547Z Traceback (most recent call last):
2025-04-11T04:23:19.3013824Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3013901Z     fn(i, *args)
2025-04-11T04:23:19.3014139Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
2025-04-11T04:23:19.3014232Z     exam_zero_1_torch_ddp()
2025-04-11T04:23:19.3014481Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3014570Z     partial_func(**kwargs)
2025-04-11T04:23:19.3014813Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3014966Z     partial_func(**kwargs)
2025-04-11T04:23:19.3015213Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3015297Z     partial_func(**kwargs)
2025-04-11T04:23:19.3015577Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
2025-04-11T04:23:19.3015681Z     torch_model = MlpModel().cuda().to(dtype)
2025-04-11T04:23:19.3015947Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.3016052Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3016314Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.3016395Z     module._apply(fn)
2025-04-11T04:23:19.3016652Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.3016746Z     param_applied = fn(param)
2025-04-11T04:23:19.3017007Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.3017161Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3017257Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3017536Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3017667Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3017821Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3018188Z FAILED tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3018194Z 
2025-04-11T04:23:19.3018306Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3018402Z Traceback (most recent call last):
2025-04-11T04:23:19.3018679Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3018756Z     fn(i, *args)
2025-04-11T04:23:19.3019002Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
2025-04-11T04:23:19.3019094Z     exam_zero_1_torch_ddp_ckpt()
2025-04-11T04:23:19.3019395Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3019478Z     partial_func(**kwargs)
2025-04-11T04:23:19.3019772Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
2025-04-11T04:23:19.3019867Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.3020131Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.3020237Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3020501Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.3020584Z     module._apply(fn)
2025-04-11T04:23:19.3020849Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.3020940Z     param_applied = fn(param)
2025-04-11T04:23:19.3021211Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.3021318Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3021413Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3021688Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3021817Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3021970Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3022158Z = 573 failed, 74 passed, 195 skipped, 23 deselected, 91 warnings in 663.23s (0:11:03) =
2025-04-11T04:23:20.1932389Z ##[error]Process completed with exit code 1.
