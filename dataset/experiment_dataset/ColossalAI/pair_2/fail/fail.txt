2025-04-11T03:53:15.6246952Z Requested labels: gpu-h20-10
2025-04-11T03:53:15.6247211Z Job defined at: hpcaitech/ColossalAI/.github/workflows/build_on_pr.yml@refs/pull/6254/merge
2025-04-11T03:53:15.6247361Z Waiting for a runner to pick up this job...
2025-04-11T03:53:15.7254135Z Job is about to start running on the runner: gpu-h20-10 (repository)
2025-04-11T03:53:22.4256375Z Current runner version: '2.323.0'
2025-04-11T03:53:22.4262257Z Runner name: 'gpu-h20-10'
2025-04-11T03:53:22.4263138Z Runner group name: 'Default'
2025-04-11T03:53:22.4264117Z Machine name: 'gpu-h20-10'
2025-04-11T03:53:22.4268677Z ##[group]GITHUB_TOKEN Permissions
2025-04-11T03:53:22.4271047Z Actions: read
2025-04-11T03:53:22.4271709Z Attestations: read
2025-04-11T03:53:22.4272385Z Checks: read
2025-04-11T03:53:22.4273122Z Contents: read
2025-04-11T03:53:22.4273772Z Deployments: read
2025-04-11T03:53:22.4274489Z Discussions: read
2025-04-11T03:53:22.4275159Z Issues: read
2025-04-11T03:53:22.4275759Z Metadata: read
2025-04-11T03:53:22.4276431Z Models: read
2025-04-11T03:53:22.4277061Z Packages: read
2025-04-11T03:53:22.4277693Z Pages: read
2025-04-11T03:53:22.4278332Z PullRequests: read
2025-04-11T03:53:22.4278962Z RepositoryProjects: read
2025-04-11T03:53:22.4279651Z SecurityEvents: read
2025-04-11T03:53:22.4280394Z Statuses: read
2025-04-11T03:53:22.4281020Z ##[endgroup]
2025-04-11T03:53:22.4284238Z Secret source: None
2025-04-11T03:53:22.4285550Z Runner is running behind proxy server 'http://vpn.luchentech.com:32171' for all HTTP requests.
2025-04-11T03:53:22.4287097Z Runner is running behind proxy server 'http://vpn.luchentech.com:32171' for all HTTPS requests.
2025-04-11T03:53:22.4288253Z Prepare workflow directory
2025-04-11T03:53:22.4709899Z Prepare all required actions
2025-04-11T03:53:22.4745380Z Getting action download info
2025-04-11T03:53:22.9530880Z Download action repository 'actions/checkout@v2' (SHA:ee0669bd1cc54295c223e0bb666b733df41de1c5)
2025-04-11T03:53:24.3987914Z Download action repository 'actions/upload-artifact@v4' (SHA:ea165f8d65b6e75b540449e92b4886f43607fa02)
2025-04-11T03:53:26.2662804Z Complete job name: Build and Test Colossal-AI
2025-04-11T03:53:26.4892033Z ##[group]Checking docker version
2025-04-11T03:53:26.4903928Z ##[command]/usr/bin/docker version --format '{{.Server.APIVersion}}'
2025-04-11T03:53:26.5168741Z '1.41'
2025-04-11T03:53:26.5180593Z Docker daemon API version: '1.41'
2025-04-11T03:53:26.5181123Z ##[command]/usr/bin/docker version --format '{{.Client.APIVersion}}'
2025-04-11T03:53:26.5424852Z '1.41'
2025-04-11T03:53:26.5436990Z Docker client API version: '1.41'
2025-04-11T03:53:26.5441854Z ##[endgroup]
2025-04-11T03:53:26.5444093Z ##[group]Clean up resources from previous jobs
2025-04-11T03:53:26.5448530Z ##[command]/usr/bin/docker ps --all --quiet --no-trunc --filter "label=81704a"
2025-04-11T03:53:26.5639050Z ##[command]/usr/bin/docker network prune --force --filter "label=81704a"
2025-04-11T03:53:26.5817398Z ##[endgroup]
2025-04-11T03:53:26.5817687Z ##[group]Create local container network
2025-04-11T03:53:26.5826048Z ##[command]/usr/bin/docker network create --label 81704a github_network_94998f4534db421f86753cfbbb2319dd
2025-04-11T03:53:26.6226662Z b3822b338ad3014696b5524895dc654196f57c01b7ad8f25a871d12f60d28c99
2025-04-11T03:53:26.6244583Z ##[endgroup]
2025-04-11T03:53:26.6267206Z ##[group]Starting job container
2025-04-11T03:53:26.6284877Z ##[command]/usr/bin/docker pull image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0
2025-04-11T03:53:27.1296353Z 2.2.2-12.1.0: Pulling from hpcaitech/pytorch-cuda
2025-04-11T03:53:27.1306683Z Digest: sha256:c2a777c1361b156ea23925ab311db35b1b450443d25dc292205058aeffea7e64
2025-04-11T03:53:27.1307280Z Status: Image is up to date for image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0
2025-04-11T03:53:27.1314984Z image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0
2025-04-11T03:53:27.1405057Z ##[command]/usr/bin/docker create --name 7d6bacf453dd49bc8aea383939f91c8e_imagecloudluchentechcomhpcaitechpytorchcuda2221210_f16ff6 --label 81704a --workdir /__w/ColossalAI/ColossalAI --network github_network_94998f4534db421f86753cfbbb2319dd --gpus all --shm-size=2g --rm -v /dev/shm -v /data/scratch:/data/scratch -e "HTTP_PROXY=http://vpn.luchentech.com:32171" -e "http_proxy=http://vpn.luchentech.com:32171" -e "HTTPS_PROXY=http://vpn.luchentech.com:32171" -e "https_proxy=http://vpn.luchentech.com:32171" -e "HOME=/github/home" -e GITHUB_ACTIONS=true -e CI=true -v "/var/run/docker.sock":"/var/run/docker.sock" -v "/root/actions-runner/github-gpu":"/__w" -v "/root/actions-runner/externals":"/__e":ro -v "/root/actions-runner/github-gpu/_temp":"/__w/_temp" -v "/root/actions-runner/github-gpu/_actions":"/__w/_actions" -v "/root/actions-runner/github-gpu/_tool":"/__w/_tool" -v "/root/actions-runner/github-gpu/_temp/_github_home":"/github/home" -v "/root/actions-runner/github-gpu/_temp/_github_workflow":"/github/workflow" --entrypoint "tail" image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0 "-f" "/dev/null"
2025-04-11T03:53:27.1675275Z 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
2025-04-11T03:53:27.1694579Z ##[command]/usr/bin/docker start 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
2025-04-11T03:53:27.9487975Z 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
2025-04-11T03:53:27.9508302Z ##[command]/usr/bin/docker ps --all --filter id=920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f --filter status=running --no-trunc --format "{{.ID}} {{.Status}}"
2025-04-11T03:53:27.9691195Z 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f Up Less than a second
2025-04-11T03:53:27.9708171Z ##[command]/usr/bin/docker inspect --format "{{range .Config.Env}}{{println .}}{{end}}" 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
2025-04-11T03:53:27.9876010Z http_proxy=http://vpn.luchentech.com:32171
2025-04-11T03:53:27.9876408Z HTTPS_PROXY=http://vpn.luchentech.com:32171
2025-04-11T03:53:27.9876723Z https_proxy=http://vpn.luchentech.com:32171
2025-04-11T03:53:27.9877015Z HOME=/github/home
2025-04-11T03:53:27.9877242Z GITHUB_ACTIONS=true
2025-04-11T03:53:27.9877460Z CI=true
2025-04-11T03:53:27.9878064Z HTTP_PROXY=http://vpn.luchentech.com:32171
2025-04-11T03:53:27.9878683Z PATH=/opt/conda/envs/pytorch/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
2025-04-11T03:53:27.9879249Z NVARCH=x86_64
2025-04-11T03:53:27.9881216Z NVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526
2025-04-11T03:53:27.9883192Z NV_CUDA_CUDART_VERSION=12.1.55-1
2025-04-11T03:53:27.9883484Z NV_CUDA_COMPAT_PACKAGE=cuda-compat-12-1
2025-04-11T03:53:27.9883770Z CUDA_VERSION=12.1.0
2025-04-11T03:53:27.9884064Z LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2025-04-11T03:53:27.9884393Z NVIDIA_VISIBLE_DEVICES=all
2025-04-11T03:53:27.9884653Z NVIDIA_DRIVER_CAPABILITIES=compute,utility
2025-04-11T03:53:27.9885129Z NV_CUDA_LIB_VERSION=12.1.0-1
2025-04-11T03:53:27.9885406Z NV_NVTX_VERSION=12.1.66-1
2025-04-11T03:53:27.9885653Z NV_LIBNPP_VERSION=12.0.2.50-1
2025-04-11T03:53:27.9885919Z NV_LIBNPP_PACKAGE=libnpp-12-1=12.0.2.50-1
2025-04-11T03:53:27.9886193Z NV_LIBCUSPARSE_VERSION=12.0.2.55-1
2025-04-11T03:53:27.9886472Z NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-1
2025-04-11T03:53:27.9886789Z NV_LIBCUBLAS_VERSION=12.1.0.26-1
2025-04-11T03:53:27.9887069Z NV_LIBCUBLAS_PACKAGE=libcublas-12-1=12.1.0.26-1
2025-04-11T03:53:27.9887362Z NV_LIBNCCL_PACKAGE_NAME=libnccl2
2025-04-11T03:53:27.9887650Z NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1
2025-04-11T03:53:27.9887922Z NCCL_VERSION=2.17.1-1
2025-04-11T03:53:27.9888177Z NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1
2025-04-11T03:53:27.9888459Z NVIDIA_PRODUCT_NAME=CUDA
2025-04-11T03:53:27.9888918Z NVIDIA_CUDA_END_OF_LIFE=1
2025-04-11T03:53:27.9889166Z NV_CUDA_CUDART_DEV_VERSION=12.1.55-1
2025-04-11T03:53:27.9889420Z NV_NVML_DEV_VERSION=12.1.55-1
2025-04-11T03:53:27.9889677Z NV_LIBCUSPARSE_DEV_VERSION=12.0.2.55-1
2025-04-11T03:53:27.9889945Z NV_LIBNPP_DEV_VERSION=12.0.2.50-1
2025-04-11T03:53:27.9890231Z NV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-1=12.0.2.50-1
2025-04-11T03:53:27.9890528Z NV_LIBCUBLAS_DEV_VERSION=12.1.0.26-1
2025-04-11T03:53:27.9890813Z NV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-1
2025-04-11T03:53:27.9891144Z NV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-1=12.1.0.26-1
2025-04-11T03:53:27.9891460Z NV_CUDA_NSIGHT_COMPUTE_VERSION=12.1.0-1
2025-04-11T03:53:27.9891813Z NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-1=12.1.0-1
2025-04-11T03:53:27.9892146Z NV_NVPROF_VERSION=12.1.55-1
2025-04-11T03:53:27.9892417Z NV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-1=12.1.55-1
2025-04-11T03:53:27.9892702Z NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
2025-04-11T03:53:27.9892975Z NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1
2025-04-11T03:53:27.9893278Z NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1
2025-04-11T03:53:27.9893583Z LIBRARY_PATH=/usr/local/cuda/lib64/stubs
2025-04-11T03:53:27.9893852Z NV_CUDNN_VERSION=8.9.0.131
2025-04-11T03:53:27.9894087Z NV_CUDNN_PACKAGE_NAME=libcudnn8
2025-04-11T03:53:27.9894355Z NV_CUDNN_PACKAGE=libcudnn8=8.9.0.131-1+cuda12.1
2025-04-11T03:53:27.9894669Z NV_CUDNN_PACKAGE_DEV=libcudnn8-dev=8.9.0.131-1+cuda12.1
2025-04-11T03:53:27.9894961Z CUDA_HOME=/usr/local/cuda
2025-04-11T03:53:27.9901335Z ##[endgroup]
2025-04-11T03:53:27.9909686Z ##[group]Waiting for all services to be ready
2025-04-11T03:53:27.9911133Z ##[endgroup]
2025-04-11T03:53:28.0108011Z ##[group]Run actions/checkout@v2
2025-04-11T03:53:28.0108510Z with:
2025-04-11T03:53:28.0108735Z   repository: hpcaitech/TensorNVMe
2025-04-11T03:53:28.0109007Z   path: TensorNVMe
2025-04-11T03:53:28.0109391Z   token: ***
2025-04-11T03:53:28.0109607Z   ssh-strict: true
2025-04-11T03:53:28.0109840Z   persist-credentials: true
2025-04-11T03:53:28.0110091Z   clean: true
2025-04-11T03:53:28.0110303Z   fetch-depth: 1
2025-04-11T03:53:28.0110518Z   lfs: false
2025-04-11T03:53:28.0110726Z   submodules: false
2025-04-11T03:53:28.0110949Z   set-safe-directory: true
2025-04-11T03:53:28.0111269Z ##[endgroup]
2025-04-11T03:53:28.0159547Z ##[command]/usr/bin/docker exec  920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f sh -c "cat /etc/*release | grep ^ID"
2025-04-11T03:53:28.2376648Z Syncing repository: hpcaitech/TensorNVMe
2025-04-11T03:53:28.2377924Z ##[group]Getting Git version info
2025-04-11T03:53:28.2378295Z Working directory is '/__w/ColossalAI/ColossalAI/TensorNVMe'
2025-04-11T03:53:28.2378888Z [command]/usr/bin/git version
2025-04-11T03:53:28.2379153Z git version 2.25.1
2025-04-11T03:53:28.2384045Z ##[endgroup]
2025-04-11T03:53:28.2394275Z Temporarily overriding HOME='/__w/_temp/fbfe2622-2566-432d-b025-a8650df70e60' before making global git config changes
2025-04-11T03:53:28.2394876Z Adding repository directory to the temporary git global config as a safe directory
2025-04-11T03:53:28.2397276Z [command]/usr/bin/git config --global --add safe.directory /__w/ColossalAI/ColossalAI/TensorNVMe
2025-04-11T03:53:28.2423542Z ##[group]Initializing the repository
2025-04-11T03:53:28.2426227Z [command]/usr/bin/git init /__w/ColossalAI/ColossalAI/TensorNVMe
2025-04-11T03:53:28.2448821Z Initialized empty Git repository in /__w/ColossalAI/ColossalAI/TensorNVMe/.git/
2025-04-11T03:53:28.2455676Z [command]/usr/bin/git remote add origin https://github.com/hpcaitech/TensorNVMe
2025-04-11T03:53:28.2475196Z ##[endgroup]
2025-04-11T03:53:28.2475579Z ##[group]Disabling automatic garbage collection
2025-04-11T03:53:28.2478228Z [command]/usr/bin/git config --local gc.auto 0
2025-04-11T03:53:28.2498679Z ##[endgroup]
2025-04-11T03:53:28.2499007Z ##[group]Setting up auth
2025-04-11T03:53:28.2504321Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-11T03:53:28.2526505Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-11T03:53:28.2697207Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-11T03:53:28.2716825Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-11T03:53:28.2882161Z [command]/usr/bin/git config --local http.https://github.com/.extraheader AUTHORIZATION: basic ***
2025-04-11T03:53:28.2906706Z ##[endgroup]
2025-04-11T03:53:28.2907056Z ##[group]Determining the default branch
2025-04-11T03:53:28.2909497Z Retrieving the default branch name
2025-04-11T03:53:28.9579634Z Default branch 'main'
2025-04-11T03:53:28.9580505Z ##[endgroup]
2025-04-11T03:53:28.9580937Z ##[group]Fetching the repository
2025-04-11T03:53:28.9583761Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --progress --no-recurse-submodules --depth=1 origin +refs/heads/main:refs/remotes/origin/main
2025-04-11T03:53:30.3110915Z remote: Enumerating objects: 60, done.        
2025-04-11T03:53:30.3111306Z remote: Counting objects:   1% (1/60)        
2025-04-11T03:53:30.3111614Z remote: Counting objects:   3% (2/60)        
2025-04-11T03:53:30.3111913Z remote: Counting objects:   5% (3/60)        
2025-04-11T03:53:30.3112214Z remote: Counting objects:   6% (4/60)        
2025-04-11T03:53:30.3166138Z remote: Counting objects:   8% (5/60)        
2025-04-11T03:53:30.3166764Z remote: Counting objects:  10% (6/60)        
2025-04-11T03:53:30.3167083Z remote: Counting objects:  11% (7/60)        
2025-04-11T03:53:30.3167363Z remote: Counting objects:  13% (8/60)        
2025-04-11T03:53:30.3167720Z remote: Counting objects:  15% (9/60)        
2025-04-11T03:53:30.3168002Z remote: Counting objects:  16% (10/60)        
2025-04-11T03:53:30.3168314Z remote: Counting objects:  18% (11/60)        
2025-04-11T03:53:30.3168611Z remote: Counting objects:  20% (12/60)        
2025-04-11T03:53:30.3168903Z remote: Counting objects:  21% (13/60)        
2025-04-11T03:53:30.3169183Z remote: Counting objects:  23% (14/60)        
2025-04-11T03:53:30.3169462Z remote: Counting objects:  25% (15/60)        
2025-04-11T03:53:30.3169808Z remote: Counting objects:  26% (16/60)        
2025-04-11T03:53:30.3170084Z remote: Counting objects:  28% (17/60)        
2025-04-11T03:53:30.3170354Z remote: Counting objects:  30% (18/60)        
2025-04-11T03:53:30.3170632Z remote: Counting objects:  31% (19/60)        
2025-04-11T03:53:30.3170911Z remote: Counting objects:  33% (20/60)        
2025-04-11T03:53:30.3171186Z remote: Counting objects:  35% (21/60)        
2025-04-11T03:53:30.3171461Z remote: Counting objects:  36% (22/60)        
2025-04-11T03:53:30.3171731Z remote: Counting objects:  38% (23/60)        
2025-04-11T03:53:30.3172009Z remote: Counting objects:  40% (24/60)        
2025-04-11T03:53:30.3172350Z remote: Counting objects:  41% (25/60)        
2025-04-11T03:53:30.3172687Z remote: Counting objects:  43% (26/60)        
2025-04-11T03:53:30.3173031Z remote: Counting objects:  45% (27/60)        
2025-04-11T03:53:30.3173307Z remote: Counting objects:  46% (28/60)        
2025-04-11T03:53:30.3173588Z remote: Counting objects:  48% (29/60)        
2025-04-11T03:53:30.3173862Z remote: Counting objects:  50% (30/60)        
2025-04-11T03:53:30.3174137Z remote: Counting objects:  51% (31/60)        
2025-04-11T03:53:30.3174414Z remote: Counting objects:  53% (32/60)        
2025-04-11T03:53:30.3174693Z remote: Counting objects:  55% (33/60)        
2025-04-11T03:53:30.3174963Z remote: Counting objects:  56% (34/60)        
2025-04-11T03:53:30.3175240Z remote: Counting objects:  58% (35/60)        
2025-04-11T03:53:30.3175516Z remote: Counting objects:  60% (36/60)        
2025-04-11T03:53:30.3175790Z remote: Counting objects:  61% (37/60)        
2025-04-11T03:53:30.3176217Z remote: Counting objects:  63% (38/60)        
2025-04-11T03:53:30.3176492Z remote: Counting objects:  65% (39/60)        
2025-04-11T03:53:30.3176773Z remote: Counting objects:  66% (40/60)        
2025-04-11T03:53:30.3177050Z remote: Counting objects:  68% (41/60)        
2025-04-11T03:53:30.3177335Z remote: Counting objects:  70% (42/60)        
2025-04-11T03:53:30.3177614Z remote: Counting objects:  71% (43/60)        
2025-04-11T03:53:30.3177886Z remote: Counting objects:  73% (44/60)        
2025-04-11T03:53:30.3178165Z remote: Counting objects:  75% (45/60)        
2025-04-11T03:53:30.3178441Z remote: Counting objects:  76% (46/60)        
2025-04-11T03:53:30.3178724Z remote: Counting objects:  78% (47/60)        
2025-04-11T03:53:30.3179002Z remote: Counting objects:  80% (48/60)        
2025-04-11T03:53:30.3179278Z remote: Counting objects:  81% (49/60)        
2025-04-11T03:53:30.3179547Z remote: Counting objects:  83% (50/60)        
2025-04-11T03:53:30.3179826Z remote: Counting objects:  85% (51/60)        
2025-04-11T03:53:30.3180106Z remote: Counting objects:  86% (52/60)        
2025-04-11T03:53:30.3180382Z remote: Counting objects:  88% (53/60)        
2025-04-11T03:53:30.3180662Z remote: Counting objects:  90% (54/60)        
2025-04-11T03:53:30.3180935Z remote: Counting objects:  91% (55/60)        
2025-04-11T03:53:30.3181218Z remote: Counting objects:  93% (56/60)        
2025-04-11T03:53:30.3181497Z remote: Counting objects:  95% (57/60)        
2025-04-11T03:53:30.3181777Z remote: Counting objects:  96% (58/60)        
2025-04-11T03:53:30.3182061Z remote: Counting objects:  98% (59/60)        
2025-04-11T03:53:30.3182455Z remote: Counting objects: 100% (60/60)        
2025-04-11T03:53:30.3182760Z remote: Counting objects: 100% (60/60), done.        
2025-04-11T03:53:30.3183084Z remote: Compressing objects:   1% (1/51)        
2025-04-11T03:53:30.3183389Z remote: Compressing objects:   3% (2/51)        
2025-04-11T03:53:30.3183706Z remote: Compressing objects:   5% (3/51)        
2025-04-11T03:53:30.3184128Z remote: Compressing objects:   7% (4/51)        
2025-04-11T03:53:30.3184518Z remote: Compressing objects:   9% (5/51)        
2025-04-11T03:53:30.3184810Z remote: Compressing objects:  11% (6/51)        
2025-04-11T03:53:30.3185118Z remote: Compressing objects:  13% (7/51)        
2025-04-11T03:53:30.3185422Z remote: Compressing objects:  15% (8/51)        
2025-04-11T03:53:30.3185723Z remote: Compressing objects:  17% (9/51)        
2025-04-11T03:53:30.3186027Z remote: Compressing objects:  19% (10/51)        
2025-04-11T03:53:30.3186333Z remote: Compressing objects:  21% (11/51)        
2025-04-11T03:53:30.3186649Z remote: Compressing objects:  23% (12/51)        
2025-04-11T03:53:30.3186959Z remote: Compressing objects:  25% (13/51)        
2025-04-11T03:53:30.3187263Z remote: Compressing objects:  27% (14/51)        
2025-04-11T03:53:30.3187559Z remote: Compressing objects:  29% (15/51)        
2025-04-11T03:53:30.3187855Z remote: Compressing objects:  31% (16/51)        
2025-04-11T03:53:30.3188147Z remote: Compressing objects:  33% (17/51)        
2025-04-11T03:53:30.3188537Z remote: Compressing objects:  35% (18/51)        
2025-04-11T03:53:30.3188841Z remote: Compressing objects:  37% (19/51)        
2025-04-11T03:53:30.3189134Z remote: Compressing objects:  39% (20/51)        
2025-04-11T03:53:30.3189424Z remote: Compressing objects:  41% (21/51)        
2025-04-11T03:53:30.3189706Z remote: Compressing objects:  43% (22/51)        
2025-04-11T03:53:30.3189998Z remote: Compressing objects:  45% (23/51)        
2025-04-11T03:53:30.3190283Z remote: Compressing objects:  47% (24/51)        
2025-04-11T03:53:30.3190576Z remote: Compressing objects:  49% (25/51)        
2025-04-11T03:53:30.3190866Z remote: Compressing objects:  50% (26/51)        
2025-04-11T03:53:30.3191144Z remote: Compressing objects:  52% (27/51)        
2025-04-11T03:53:30.3191432Z remote: Compressing objects:  54% (28/51)        
2025-04-11T03:53:30.3191719Z remote: Compressing objects:  56% (29/51)        
2025-04-11T03:53:30.3192142Z remote: Compressing objects:  58% (30/51)        
2025-04-11T03:53:30.3192436Z remote: Compressing objects:  60% (31/51)        
2025-04-11T03:53:30.3192733Z remote: Compressing objects:  62% (32/51)        
2025-04-11T03:53:30.3193040Z remote: Compressing objects:  64% (33/51)        
2025-04-11T03:53:30.3193341Z remote: Compressing objects:  66% (34/51)        
2025-04-11T03:53:30.3193639Z remote: Compressing objects:  68% (35/51)        
2025-04-11T03:53:30.3193936Z remote: Compressing objects:  70% (36/51)        
2025-04-11T03:53:30.3194235Z remote: Compressing objects:  72% (37/51)        
2025-04-11T03:53:30.3194529Z remote: Compressing objects:  74% (38/51)        
2025-04-11T03:53:30.3194823Z remote: Compressing objects:  76% (39/51)        
2025-04-11T03:53:30.3195117Z remote: Compressing objects:  78% (40/51)        
2025-04-11T03:53:30.3195412Z remote: Compressing objects:  80% (41/51)        
2025-04-11T03:53:30.3195710Z remote: Compressing objects:  82% (42/51)        
2025-04-11T03:53:30.3196005Z remote: Compressing objects:  84% (43/51)        
2025-04-11T03:53:30.3196302Z remote: Compressing objects:  86% (44/51)        
2025-04-11T03:53:30.3196601Z remote: Compressing objects:  88% (45/51)        
2025-04-11T03:53:30.3196895Z remote: Compressing objects:  90% (46/51)        
2025-04-11T03:53:30.3197191Z remote: Compressing objects:  92% (47/51)        
2025-04-11T03:53:30.3197488Z remote: Compressing objects:  94% (48/51)        
2025-04-11T03:53:30.3197777Z remote: Compressing objects:  96% (49/51)        
2025-04-11T03:53:30.3198078Z remote: Compressing objects:  98% (50/51)        
2025-04-11T03:53:30.3198495Z remote: Compressing objects: 100% (51/51)        
2025-04-11T03:53:30.3198817Z remote: Compressing objects: 100% (51/51), done.        
2025-04-11T03:53:30.5291213Z remote: Total 60 (delta 6), reused 24 (delta 1), pack-reused 0 (from 0)        
2025-04-11T03:53:30.5452903Z From https://github.com/hpcaitech/TensorNVMe
2025-04-11T03:53:30.5453231Z  * [new branch]      main       -> origin/main
2025-04-11T03:53:30.5468014Z ##[endgroup]
2025-04-11T03:53:30.5468522Z ##[group]Determining the checkout info
2025-04-11T03:53:30.5470269Z ##[endgroup]
2025-04-11T03:53:30.5470637Z ##[group]Checking out the ref
2025-04-11T03:53:30.5474132Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2025-04-11T03:53:30.5556875Z Switched to a new branch 'main'
2025-04-11T03:53:30.5557894Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2025-04-11T03:53:30.5561176Z ##[endgroup]
2025-04-11T03:53:30.5586947Z [command]/usr/bin/git log -1 --format='%H'
2025-04-11T03:53:30.5604212Z '6403388f7c8929f43e407c8f7f39c6453d78c5d4'
2025-04-11T03:53:30.5789058Z ##[group]Run if [ -d /github/home/tensornvme_cache ] && [ ! -z "$(ls -A /github/home/tensornvme_cache/)" ]; then
2025-04-11T03:53:30.5789723Z [36;1mif [ -d /github/home/tensornvme_cache ] && [ ! -z "$(ls -A /github/home/tensornvme_cache/)" ]; then[0m
2025-04-11T03:53:30.5790233Z [36;1m  cp -p -r /github/home/tensornvme_cache/* /__w/ColossalAI/ColossalAI/TensorNVMe[0m
2025-04-11T03:53:30.5790616Z [36;1mfi[0m
2025-04-11T03:53:30.5794223Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:53:30.5794522Z ##[endgroup]
2025-04-11T03:53:30.6696462Z ##[group]Run cd TensorNVMe
2025-04-11T03:53:30.6696763Z [36;1mcd TensorNVMe[0m
2025-04-11T03:53:30.6696994Z [36;1mconda install cmake[0m
2025-04-11T03:53:30.6697257Z [36;1mpip install -r requirements.txt[0m
2025-04-11T03:53:30.6697580Z [36;1mDISABLE_URING=1 pip install -v --no-cache-dir .[0m
2025-04-11T03:53:30.6698021Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:53:30.6698312Z ##[endgroup]
2025-04-11T03:53:32.2967599Z Channels:
2025-04-11T03:53:32.2967950Z  - defaults
2025-04-11T03:53:32.2968264Z Platform: linux-64
2025-04-11T03:53:38.4947358Z Collecting package metadata (repodata.json): ...working... done
2025-04-11T03:53:38.6743675Z Solving environment: ...working... done
2025-04-11T03:53:38.7304100Z 
2025-04-11T03:53:38.7304641Z ## Package Plan ##
2025-04-11T03:53:38.7304807Z 
2025-04-11T03:53:38.7304927Z   environment location: /opt/conda
2025-04-11T03:53:38.7305107Z 
2025-04-11T03:53:38.7305209Z   added / updated specs:
2025-04-11T03:53:38.7305513Z     - cmake
2025-04-11T03:53:38.7305625Z 
2025-04-11T03:53:38.7305631Z 
2025-04-11T03:53:38.7305847Z The following packages will be downloaded:
2025-04-11T03:53:38.7306136Z 
2025-04-11T03:53:38.7306304Z     package                    |            build
2025-04-11T03:53:38.7306786Z     ---------------------------|-----------------
2025-04-11T03:53:38.7307294Z     archspec-0.2.3             |     pyhd3eb1b0_0          47 KB
2025-04-11T03:53:38.7307881Z     ca-certificates-2025.2.25  |       h06a4308_0         129 KB
2025-04-11T03:53:38.7308506Z     certifi-2025.1.31          |  py311h06a4308_0         163 KB
2025-04-11T03:53:38.7309044Z     cmake-3.31.2               |       h27e300b_0        20.9 MB
2025-04-11T03:53:38.7309548Z     conda-24.11.3              |  py311h06a4308_0         1.2 MB
2025-04-11T03:53:38.7310062Z     expat-2.6.4                |       h6a678d5_0         180 KB
2025-04-11T03:53:38.7310600Z     frozendict-2.4.2           |  py311h06a4308_0          37 KB
2025-04-11T03:53:38.7311138Z     libuv-1.48.0               |       h5eee18b_0         950 KB
2025-04-11T03:53:38.7311685Z     openssl-3.0.16             |       h5eee18b_0         5.2 MB
2025-04-11T03:53:38.7312217Z     rhash-1.4.3                |       hdbd6064_0         220 KB
2025-04-11T03:53:38.7312786Z     xz-5.6.4                   |       h5eee18b_1         567 KB
2025-04-11T03:53:38.7313305Z     ------------------------------------------------------------
2025-04-11T03:53:38.7313801Z                                            Total:        29.5 MB
2025-04-11T03:53:38.7314107Z 
2025-04-11T03:53:38.7314316Z The following NEW packages will be INSTALLED:
2025-04-11T03:53:38.7314656Z 
2025-04-11T03:53:38.7319277Z   cmake              pkgs/main/linux-64::cmake-3.31.2-h27e300b_0 
2025-04-11T03:53:38.7319921Z   expat              pkgs/main/linux-64::expat-2.6.4-h6a678d5_0 
2025-04-11T03:53:38.7320555Z   frozendict         pkgs/main/linux-64::frozendict-2.4.2-py311h06a4308_0 
2025-04-11T03:53:38.7321202Z   libuv              pkgs/main/linux-64::libuv-1.48.0-h5eee18b_0 
2025-04-11T03:53:38.7321752Z   rhash              pkgs/main/linux-64::rhash-1.4.3-hdbd6064_0 
2025-04-11T03:53:38.7322095Z 
2025-04-11T03:53:38.7322263Z The following packages will be UPDATED:
2025-04-11T03:53:38.7322550Z 
2025-04-11T03:53:38.7322803Z   archspec                               0.2.1-pyhd3eb1b0_0 --> 0.2.3-pyhd3eb1b0_0 
2025-04-11T03:53:38.7323436Z   ca-certificates                     2023.12.12-h06a4308_0 --> 2025.2.25-h06a4308_0 
2025-04-11T03:53:38.7324082Z   certifi                        2023.11.17-py311h06a4308_0 --> 2025.1.31-py311h06a4308_0 
2025-04-11T03:53:38.7324698Z   conda                             23.11.0-py311h06a4308_0 --> 24.11.3-py311h06a4308_0 
2025-04-11T03:53:38.7325283Z   openssl                                 3.0.12-h7f8727e_0 --> 3.0.16-h5eee18b_0 
2025-04-11T03:53:38.7325820Z   xz                                       5.4.5-h5eee18b_0 --> 5.6.4-h5eee18b_1 
2025-04-11T03:53:38.7326128Z 
2025-04-11T03:53:38.7326137Z 
2025-04-11T03:53:38.7326688Z Proceed ([y]/n)? 
2025-04-11T03:53:41.2830046Z 
2025-04-11T03:53:41.2830722Z Downloading and Extracting Packages: ...working... done
2025-04-11T03:53:41.3735101Z Preparing transaction: ...working... done
2025-04-11T03:53:41.7429508Z Verifying transaction: ...working... done
2025-04-11T03:53:42.8069710Z Executing transaction: ...working... done
2025-04-11T03:53:43.2632008Z Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (24.1)
2025-04-11T03:53:43.6736498Z Collecting click (from -r requirements.txt (line 2))
2025-04-11T03:53:43.9699641Z   Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)
2025-04-11T03:53:44.0847381Z Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.2.2)
2025-04-11T03:53:44.0909192Z Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.13.1)
2025-04-11T03:53:44.0913952Z Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (4.11.0)
2025-04-11T03:53:44.0916408Z Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (1.12)
2025-04-11T03:53:44.0918963Z Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.2.1)
2025-04-11T03:53:44.0921440Z Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.1.4)
2025-04-11T03:53:44.0923530Z Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2024.6.1)
2025-04-11T03:53:44.1635522Z Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)
2025-04-11T03:53:44.1775520Z Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)
2025-04-11T03:53:44.2728761Z Downloading click-8.1.8-py3-none-any.whl (98 kB)
2025-04-11T03:53:44.4604484Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.2/98.2 kB 474.7 kB/s eta 0:00:00
2025-04-11T03:53:44.5074911Z Installing collected packages: click
2025-04-11T03:53:44.5578318Z Successfully installed click-8.1.8
2025-04-11T03:53:44.5581776Z WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-11T03:53:45.2495218Z Using pip 24.0 from /opt/conda/envs/pytorch/lib/python3.10/site-packages/pip (python 3.10)
2025-04-11T03:53:45.3096968Z Processing /__w/ColossalAI/ColossalAI/TensorNVMe
2025-04-11T03:53:45.3104118Z   Preparing metadata (setup.py): started
2025-04-11T03:53:45.3104662Z   Running command python setup.py egg_info
2025-04-11T03:53:45.4142556Z   running egg_info
2025-04-11T03:53:45.4144283Z   creating /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info
2025-04-11T03:53:45.4158607Z   writing /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/PKG-INFO
2025-04-11T03:53:45.4161715Z   writing dependency_links to /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/dependency_links.txt
2025-04-11T03:53:45.4163102Z   writing entry points to /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/entry_points.txt
2025-04-11T03:53:45.4164274Z   writing requirements to /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/requires.txt
2025-04-11T03:53:45.4165269Z   writing top-level names to /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/top_level.txt
2025-04-11T03:53:45.4166258Z   writing manifest file '/tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:53:45.4215284Z   reading manifest file '/tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:53:45.4215831Z   reading manifest template 'MANIFEST.in'
2025-04-11T03:53:45.4230290Z   writing manifest file '/tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:53:45.4393385Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T03:53:45.4422792Z Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensornvme==0.1.0) (24.1)
2025-04-11T03:53:45.4425448Z Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensornvme==0.1.0) (8.1.8)
2025-04-11T03:53:45.4427954Z Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensornvme==0.1.0) (2.2.2)
2025-04-11T03:53:45.4478913Z Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (3.13.1)
2025-04-11T03:53:45.4483491Z Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (4.11.0)
2025-04-11T03:53:45.4486040Z Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (1.12)
2025-04-11T03:53:45.4488425Z Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (3.2.1)
2025-04-11T03:53:45.4490618Z Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (3.1.4)
2025-04-11T03:53:45.4492771Z Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (2024.6.1)
2025-04-11T03:53:45.5209845Z Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch->tensornvme==0.1.0) (2.1.3)
2025-04-11T03:53:45.5351514Z Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->tensornvme==0.1.0) (1.3.0)
2025-04-11T03:53:45.5414714Z Building wheels for collected packages: tensornvme
2025-04-11T03:53:45.5418459Z   Building wheel for tensornvme (setup.py): started
2025-04-11T03:53:45.5419854Z   Running command python setup.py bdist_wheel
2025-04-11T03:53:45.6845017Z   -- The CXX compiler identification is GNU 9.4.0
2025-04-11T03:53:45.6915921Z   -- Detecting CXX compiler ABI info
2025-04-11T03:53:45.7534852Z   -- Detecting CXX compiler ABI info - done
2025-04-11T03:53:45.7666155Z   -- Check for working CXX compiler: /usr/bin/c++ - skipped
2025-04-11T03:53:45.7669248Z   -- Detecting CXX compile features
2025-04-11T03:53:45.7674075Z   -- Detecting CXX compile features - done
2025-04-11T03:53:45.7802620Z   liburing is not found, install in /github/home/.tensornvme
2025-04-11T03:53:45.7943083Z   libaio is not found, install in /github/home/.tensornvme
2025-04-11T03:53:45.8057354Z   -- Configuring done (0.2s)
2025-04-11T03:53:45.8130174Z   -- Generating done (0.0s)
2025-04-11T03:53:45.8132043Z   -- Build files have been written to: /__w/ColossalAI/ColossalAI/TensorNVMe/cmake-build
2025-04-11T03:53:45.8397054Z   [ 12%] Creating directories for 'extern_aio'
2025-04-11T03:53:45.8521720Z   [ 25%] Performing download step (git clone) for 'extern_aio'
2025-04-11T03:53:45.8611790Z   Cloning into 'libaio'...
2025-04-11T03:53:49.5338957Z   HEAD is now at 1b18bfa bump libaio version
2025-04-11T03:53:49.5649546Z   [ 37%] No update step for 'extern_aio'
2025-04-11T03:53:49.5771410Z   [ 50%] No patch step for 'extern_aio'
2025-04-11T03:53:49.5890162Z   [ 62%] No configure step for 'extern_aio'
2025-04-11T03:53:49.6007300Z   [ 75%] Performing build step for 'extern_aio'
2025-04-11T03:53:49.8943950Z   ar: creating libaio.a
2025-04-11T03:53:50.2254859Z   [ 87%] No install step for 'extern_aio'
2025-04-11T03:53:50.2375304Z   [100%] Completed 'extern_aio'
2025-04-11T03:53:50.2538642Z   [100%] Built target extern_aio
2025-04-11T03:53:50.2587528Z   /github/home/.bashrc is changed, please source it.
2025-04-11T03:53:52.3781753Z   running bdist_wheel
2025-04-11T03:53:52.4103988Z   running build
2025-04-11T03:53:52.4104761Z   running build_py
2025-04-11T03:53:52.4122579Z   creating build
2025-04-11T03:53:52.4123507Z   creating build/lib.linux-x86_64-cpython-310
2025-04-11T03:53:52.4124665Z   creating build/lib.linux-x86_64-cpython-310/tensornvme
2025-04-11T03:53:52.4125636Z   copying tensornvme/offload.py -> build/lib.linux-x86_64-cpython-310/tensornvme
2025-04-11T03:53:52.4126676Z   copying tensornvme/__init__.py -> build/lib.linux-x86_64-cpython-310/tensornvme
2025-04-11T03:53:52.4127733Z   copying tensornvme/async_file_io.py -> build/lib.linux-x86_64-cpython-310/tensornvme
2025-04-11T03:53:52.4128838Z   creating build/lib.linux-x86_64-cpython-310/tensornvme/cli
2025-04-11T03:53:52.4129842Z   copying tensornvme/cli/check.py -> build/lib.linux-x86_64-cpython-310/tensornvme/cli
2025-04-11T03:53:52.4130791Z   copying tensornvme/cli/__init__.py -> build/lib.linux-x86_64-cpython-310/tensornvme/cli
2025-04-11T03:53:52.4131744Z   copying tensornvme/cli/cli.py -> build/lib.linux-x86_64-cpython-310/tensornvme/cli
2025-04-11T03:53:52.4133091Z   running build_ext
2025-04-11T03:53:52.4163831Z   building 'tensornvme._C' extension
2025-04-11T03:53:52.4164712Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310
2025-04-11T03:53:52.4165709Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w
2025-04-11T03:53:52.4166837Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI
2025-04-11T03:53:52.4167955Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI
2025-04-11T03:53:52.4169087Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe
2025-04-11T03:53:52.4170351Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc
2025-04-11T03:53:52.5215267Z   Emitting ninja build file /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:53:52.5215750Z   Compiling objects...
2025-04-11T03:53:52.5216865Z   Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:53:52.7838517Z   [1/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:54:07.7987646Z   [2/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:54:07.8434446Z   [3/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:54:08.2811588Z   [4/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:54:08.4970113Z   [5/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:54:08.4974344Z   In file included from /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.cpp:15:
2025-04-11T03:54:08.4975739Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h: In constructor ‘PthreadAsyncIO::PthreadAsyncIO(unsigned int, unsigned int)’:
2025-04-11T03:54:08.4976888Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:38:18: warning: ‘PthreadAsyncIO::total_tasks’ will be initialized after [-Wreorder]
2025-04-11T03:54:08.4977474Z      38 |     unsigned int total_tasks;
2025-04-11T03:54:08.4977742Z         |                  ^~~~~~~~~~~
2025-04-11T03:54:08.4978416Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:29:18: warning:   ‘unsigned int PthreadAsyncIO::total_h2d’ [-Wreorder]
2025-04-11T03:54:08.4978930Z      29 |     unsigned int total_h2d;
2025-04-11T03:54:08.4979192Z         |                  ^~~~~~~~~
2025-04-11T03:54:08.4979660Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:41:5: warning:   when initialized here [-Wreorder]
2025-04-11T03:54:08.4980224Z      41 |     PthreadAsyncIO(unsigned int n_entries, unsigned int n_tasks)
2025-04-11T03:54:08.4980562Z         |     ^~~~~~~~~~~~~~
2025-04-11T03:54:09.0345685Z   [6/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:54:09.0350040Z   In file included from /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.cpp:1:
2025-04-11T03:54:09.0351163Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h: In constructor ‘PthreadAsyncIO::PthreadAsyncIO(unsigned int, unsigned int)’:
2025-04-11T03:54:09.0352326Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:38:18: warning: ‘PthreadAsyncIO::total_tasks’ will be initialized after [-Wreorder]
2025-04-11T03:54:09.0352938Z      38 |     unsigned int total_tasks;
2025-04-11T03:54:09.0353205Z         |                  ^~~~~~~~~~~
2025-04-11T03:54:09.0354276Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:29:18: warning:   ‘unsigned int PthreadAsyncIO::total_h2d’ [-Wreorder]
2025-04-11T03:54:09.0354810Z      29 |     unsigned int total_h2d;
2025-04-11T03:54:09.0355066Z         |                  ^~~~~~~~~
2025-04-11T03:54:09.0355539Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:41:5: warning:   when initialized here [-Wreorder]
2025-04-11T03:54:09.0356088Z      41 |     PthreadAsyncIO(unsigned int n_entries, unsigned int n_tasks)
2025-04-11T03:54:09.0356421Z         |     ^~~~~~~~~~~~~~
2025-04-11T03:54:09.5434232Z   [7/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:54:09.5476673Z   g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.o -L/github/home/.tensornvme/lib -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -laio -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/tensornvme/_C.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:54:09.7307584Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
2025-04-11T03:54:09.7310073Z   !!
2025-04-11T03:54:09.7310233Z 
2025-04-11T03:54:09.7310375Z           ********************************************************************************
2025-04-11T03:54:09.7310717Z           Please avoid running ``setup.py`` directly.
2025-04-11T03:54:09.7311535Z           Instead, use pypa/build, pypa/installer or other
2025-04-11T03:54:09.7312070Z           standards-based tools.
2025-04-11T03:54:09.7312818Z 
2025-04-11T03:54:09.7314157Z           See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
2025-04-11T03:54:09.7315320Z           ********************************************************************************
2025-04-11T03:54:09.7315661Z 
2025-04-11T03:54:09.7316764Z   !!
2025-04-11T03:54:09.7317767Z     self.initialize_options()
2025-04-11T03:54:09.7322475Z   installing to build/bdist.linux-x86_64/wheel
2025-04-11T03:54:09.7323448Z   running install
2025-04-11T03:54:09.7364699Z   running install_lib
2025-04-11T03:54:09.7382073Z   creating build/bdist.linux-x86_64
2025-04-11T03:54:09.7383062Z   creating build/bdist.linux-x86_64/wheel
2025-04-11T03:54:09.7384107Z   creating build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:54:09.7385295Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/offload.py -> build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:54:09.7386271Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/__init__.py -> build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:54:09.7387364Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:54:09.7389697Z   creating build/bdist.linux-x86_64/wheel/tensornvme/cli
2025-04-11T03:54:09.7390741Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/cli/check.py -> build/bdist.linux-x86_64/wheel/tensornvme/cli
2025-04-11T03:54:09.7391737Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/cli/__init__.py -> build/bdist.linux-x86_64/wheel/tensornvme/cli
2025-04-11T03:54:09.7392681Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/cli/cli.py -> build/bdist.linux-x86_64/wheel/tensornvme/cli
2025-04-11T03:54:09.7393718Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/async_file_io.py -> build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:54:09.7394456Z   running install_egg_info
2025-04-11T03:54:09.7412272Z   running egg_info
2025-04-11T03:54:09.7413246Z   creating tensornvme.egg-info
2025-04-11T03:54:09.7426007Z   writing tensornvme.egg-info/PKG-INFO
2025-04-11T03:54:09.7429879Z   writing dependency_links to tensornvme.egg-info/dependency_links.txt
2025-04-11T03:54:09.7431006Z   writing entry points to tensornvme.egg-info/entry_points.txt
2025-04-11T03:54:09.7432072Z   writing requirements to tensornvme.egg-info/requires.txt
2025-04-11T03:54:09.7433026Z   writing top-level names to tensornvme.egg-info/top_level.txt
2025-04-11T03:54:09.7434018Z   writing manifest file 'tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:54:09.7452419Z   reading manifest file 'tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:54:09.7453741Z   reading manifest template 'MANIFEST.in'
2025-04-11T03:54:09.7467573Z   writing manifest file 'tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:54:09.7469255Z   Copying tensornvme.egg-info to build/bdist.linux-x86_64/wheel/tensornvme-0.1.0-py3.10.egg-info
2025-04-11T03:54:09.7473620Z   running install_scripts
2025-04-11T03:54:09.7555127Z   creating build/bdist.linux-x86_64/wheel/tensornvme-0.1.0.dist-info/WHEEL
2025-04-11T03:54:09.7558755Z   creating '/tmp/pip-wheel-mebhwq1p/tensornvme-0.1.0-cp310-cp310-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
2025-04-11T03:54:09.7750213Z   adding 'tensornvme/_C.cpython-310-x86_64-linux-gnu.so'
2025-04-11T03:54:09.7754962Z   adding 'tensornvme/__init__.py'
2025-04-11T03:54:09.7756695Z   adding 'tensornvme/async_file_io.py'
2025-04-11T03:54:09.7758278Z   adding 'tensornvme/offload.py'
2025-04-11T03:54:09.7759621Z   adding 'tensornvme/cli/__init__.py'
2025-04-11T03:54:09.7760867Z   adding 'tensornvme/cli/check.py'
2025-04-11T03:54:09.7761957Z   adding 'tensornvme/cli/cli.py'
2025-04-11T03:54:09.7765124Z   adding 'tensornvme-0.1.0.dist-info/METADATA'
2025-04-11T03:54:09.7766069Z   adding 'tensornvme-0.1.0.dist-info/WHEEL'
2025-04-11T03:54:09.7767080Z   adding 'tensornvme-0.1.0.dist-info/entry_points.txt'
2025-04-11T03:54:09.7768018Z   adding 'tensornvme-0.1.0.dist-info/top_level.txt'
2025-04-11T03:54:09.7768971Z   adding 'tensornvme-0.1.0.dist-info/RECORD'
2025-04-11T03:54:09.7769976Z   removing build/bdist.linux-x86_64/wheel
2025-04-11T03:54:10.0891709Z   Building wheel for tensornvme (setup.py): finished with status 'done'
2025-04-11T03:54:10.0896199Z   Created wheel for tensornvme: filename=tensornvme-0.1.0-cp310-cp310-linux_x86_64.whl size=147126 sha256=98ec0dac63584137f1bbdb4b3e3d7ff01d6db14c7eba9fb6e23f56b07e7d651d
2025-04-11T03:54:10.0897092Z   Stored in directory: /tmp/pip-ephem-wheel-cache-8_46q4or/wheels/cb/3c/a4/391c1ae2f1a321082a466078f0646d215673629d6cbb874a65
2025-04-11T03:54:10.0916736Z Successfully built tensornvme
2025-04-11T03:54:10.1358769Z Installing collected packages: tensornvme
2025-04-11T03:54:10.1431008Z   changing mode of /opt/conda/envs/pytorch/bin/tensornvme to 755
2025-04-11T03:54:10.1488596Z Successfully installed tensornvme-0.1.0
2025-04-11T03:54:10.1491168Z WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-11T03:54:10.6363637Z ##[group]Run cd TensorNVMe
2025-04-11T03:54:10.6363971Z [36;1mcd TensorNVMe[0m
2025-04-11T03:54:10.6364261Z [36;1mcp -p -r ./build /github/home/tensornvme_cache/[0m
2025-04-11T03:54:10.6364612Z [36;1mcp -p -r ./cmake-build /github/home/tensornvme_cache/[0m
2025-04-11T03:54:10.6365138Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:54:10.6365428Z ##[endgroup]
2025-04-11T03:54:10.7438610Z ##[group]Run actions/checkout@v2
2025-04-11T03:54:10.7438932Z with:
2025-04-11T03:54:10.7439164Z   repository: hpcaitech/ColossalAI
2025-04-11T03:54:10.7439533Z   token: ***
2025-04-11T03:54:10.7439743Z   ssh-strict: true
2025-04-11T03:54:10.7439963Z   persist-credentials: true
2025-04-11T03:54:10.7440206Z   clean: true
2025-04-11T03:54:10.7440408Z   fetch-depth: 1
2025-04-11T03:54:10.7440617Z   lfs: false
2025-04-11T03:54:10.7440821Z   submodules: false
2025-04-11T03:54:10.7441048Z   set-safe-directory: true
2025-04-11T03:54:10.7441287Z ##[endgroup]
2025-04-11T03:54:10.7445114Z ##[command]/usr/bin/docker exec  920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f sh -c "cat /etc/*release | grep ^ID"
2025-04-11T03:54:10.9221406Z Syncing repository: hpcaitech/ColossalAI
2025-04-11T03:54:10.9224801Z ##[group]Getting Git version info
2025-04-11T03:54:10.9225114Z Working directory is '/__w/ColossalAI/ColossalAI'
2025-04-11T03:54:10.9250020Z [command]/usr/bin/git version
2025-04-11T03:54:10.9279218Z git version 2.25.1
2025-04-11T03:54:10.9299306Z ##[endgroup]
2025-04-11T03:54:10.9310018Z Temporarily overriding HOME='/__w/_temp/10676da8-a365-4caf-9d27-e077551f087c' before making global git config changes
2025-04-11T03:54:10.9310609Z Adding repository directory to the temporary git global config as a safe directory
2025-04-11T03:54:10.9312944Z [command]/usr/bin/git config --global --add safe.directory /__w/ColossalAI/ColossalAI
2025-04-11T03:54:10.9340343Z [command]/usr/bin/git config --local --get remote.origin.url
2025-04-11T03:54:10.9354571Z https://github.com/hpcaitech/ColossalAI
2025-04-11T03:54:10.9363033Z ##[group]Removing previously created refs, to avoid conflicts
2025-04-11T03:54:10.9365734Z [command]/usr/bin/git rev-parse --symbolic-full-name --verify --quiet HEAD
2025-04-11T03:54:10.9379772Z HEAD
2025-04-11T03:54:10.9386490Z [command]/usr/bin/git rev-parse --symbolic-full-name --branches
2025-04-11T03:54:10.9403502Z ##[endgroup]
2025-04-11T03:54:10.9403855Z ##[group]Cleaning the repository
2025-04-11T03:54:10.9406433Z [command]/usr/bin/git clean -ffdx
2025-04-11T03:54:10.9546244Z Removing TensorNVMe/
2025-04-11T03:54:10.9551794Z [command]/usr/bin/git reset --hard HEAD
2025-04-11T03:54:10.9646182Z HEAD is now at dc60efe1 [pre-commit.ci] auto fixes from pre-commit.com hooks
2025-04-11T03:54:10.9650593Z ##[endgroup]
2025-04-11T03:54:10.9651921Z ##[group]Disabling automatic garbage collection
2025-04-11T03:54:10.9654499Z [command]/usr/bin/git config --local gc.auto 0
2025-04-11T03:54:10.9672312Z ##[endgroup]
2025-04-11T03:54:10.9672648Z ##[group]Setting up auth
2025-04-11T03:54:10.9678141Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-11T03:54:10.9698965Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-11T03:54:10.9867512Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-11T03:54:10.9886692Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-11T03:54:11.0054038Z [command]/usr/bin/git config --local http.https://github.com/.extraheader AUTHORIZATION: basic ***
2025-04-11T03:54:11.0081256Z ##[endgroup]
2025-04-11T03:54:11.0081751Z ##[group]Fetching the repository
2025-04-11T03:54:11.0086741Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --progress --no-recurse-submodules --depth=1 origin +178be2e194a3ace2a537d7496efc179ea913eaee:refs/remotes/pull/6254/merge
2025-04-11T03:54:12.0014497Z remote: Enumerating objects: 16, done.        
2025-04-11T03:54:12.0014823Z remote: Counting objects:   6% (1/16)        
2025-04-11T03:54:12.0015102Z remote: Counting objects:  12% (2/16)        
2025-04-11T03:54:12.0015868Z remote: Counting objects:  18% (3/16)        
2025-04-11T03:54:12.0016149Z remote: Counting objects:  25% (4/16)        
2025-04-11T03:54:12.0016423Z remote: Counting objects:  31% (5/16)        
2025-04-11T03:54:12.0016705Z remote: Counting objects:  37% (6/16)        
2025-04-11T03:54:12.0016972Z remote: Counting objects:  43% (7/16)        
2025-04-11T03:54:12.0017237Z remote: Counting objects:  50% (8/16)        
2025-04-11T03:54:12.0718207Z remote: Counting objects:  56% (9/16)        
2025-04-11T03:54:12.0718532Z remote: Counting objects:  62% (10/16)        
2025-04-11T03:54:12.0718829Z remote: Counting objects:  68% (11/16)        
2025-04-11T03:54:12.0719116Z remote: Counting objects:  75% (12/16)        
2025-04-11T03:54:12.0719404Z remote: Counting objects:  81% (13/16)        
2025-04-11T03:54:12.0719692Z remote: Counting objects:  87% (14/16)        
2025-04-11T03:54:12.0719965Z remote: Counting objects:  93% (15/16)        
2025-04-11T03:54:12.0720247Z remote: Counting objects: 100% (16/16)        
2025-04-11T03:54:12.0720557Z remote: Counting objects: 100% (16/16), done.        
2025-04-11T03:54:12.0720879Z remote: Compressing objects: 100% (1/1)        
2025-04-11T03:54:12.0721198Z remote: Compressing objects: 100% (1/1), done.        
2025-04-11T03:54:12.0721582Z remote: Total 6 (delta 4), reused 5 (delta 4), pack-reused 0 (from 0)        
2025-04-11T03:54:12.1114877Z From https://github.com/hpcaitech/ColossalAI
2025-04-11T03:54:12.1115327Z  + 05b5218a...178be2e1 178be2e194a3ace2a537d7496efc179ea913eaee -> pull/6254/merge  (forced update)
2025-04-11T03:54:12.1133577Z ##[endgroup]
2025-04-11T03:54:12.1133980Z ##[group]Determining the checkout info
2025-04-11T03:54:12.1135259Z ##[endgroup]
2025-04-11T03:54:12.1135599Z ##[group]Checking out the ref
2025-04-11T03:54:12.1138392Z [command]/usr/bin/git checkout --progress --force refs/remotes/pull/6254/merge
2025-04-11T03:54:12.1222114Z Warning: you are leaving 44 commits behind, not connected to
2025-04-11T03:54:12.1222431Z any of your branches:
2025-04-11T03:54:12.1222563Z 
2025-04-11T03:54:12.1222742Z   dc60efe1 [pre-commit.ci] auto fixes from pre-commit.com hooks
2025-04-11T03:54:12.1223044Z   fd69a821 fix
2025-04-11T03:54:12.1223240Z   db4c73f6 fix
2025-04-11T03:54:12.1223510Z   0950b07a [pre-commit.ci] auto fixes from pre-commit.com hooks
2025-04-11T03:54:12.1223810Z  ... and 40 more.
2025-04-11T03:54:12.1223926Z 
2025-04-11T03:54:12.1224101Z If you want to keep them by creating a new branch, this may be a good time
2025-04-11T03:54:12.1224406Z to do so with:
2025-04-11T03:54:12.1224529Z 
2025-04-11T03:54:12.1224641Z  git branch <new-branch-name> dc60efe1
2025-04-11T03:54:12.1224815Z 
2025-04-11T03:54:12.1225311Z HEAD is now at 178be2e1 Merge dc60efe1545b4eb9fa84ba2816d45af499f22b40 into 44d4053fec005fe0b06b6bc755fdc962463145df
2025-04-11T03:54:12.1229085Z ##[endgroup]
2025-04-11T03:54:12.1253929Z [command]/usr/bin/git log -1 --format='%H'
2025-04-11T03:54:12.1271311Z '178be2e194a3ace2a537d7496efc179ea913eaee'
2025-04-11T03:54:12.1404330Z ##[group]Run # -p flag is required to preserve the file timestamp to avoid ninja rebuild
2025-04-11T03:54:12.1404843Z [36;1m# -p flag is required to preserve the file timestamp to avoid ninja rebuild[0m
2025-04-11T03:54:12.1405324Z [36;1mif [ -d /github/home/cuda_ext_cache ] && [ ! -z "$(ls -A /github/home/cuda_ext_cache/)" ]; then[0m
2025-04-11T03:54:12.1405771Z [36;1m  cp -p -r /github/home/cuda_ext_cache/* /__w/ColossalAI/ColossalAI/[0m
2025-04-11T03:54:12.1406088Z [36;1mfi[0m
2025-04-11T03:54:12.1406438Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:54:12.1406855Z ##[endgroup]
2025-04-11T03:54:12.2276203Z ##[group]Run BUILD_EXT=1 pip install -v -e .
2025-04-11T03:54:12.2276549Z [36;1mBUILD_EXT=1 pip install -v -e .[0m
2025-04-11T03:54:12.2276911Z [36;1mpip install --no-cache-dir -r requirements/requirements-test.txt[0m
2025-04-11T03:54:12.2277370Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:54:12.2277659Z ##[endgroup]
2025-04-11T03:54:12.4918505Z Using pip 24.0 from /opt/conda/envs/pytorch/lib/python3.10/site-packages/pip (python 3.10)
2025-04-11T03:54:12.5485171Z Obtaining file:///__w/ColossalAI/ColossalAI
2025-04-11T03:54:12.5491181Z   Preparing metadata (setup.py): started
2025-04-11T03:54:12.5492770Z   Running command python setup.py egg_info
2025-04-11T03:54:15.1139862Z   [extension] Building extensionscpu_adam_x86, layernorm_cuda, moe_cuda, fused_optim_cuda, inference_ops_cuda, scaled_masked_softmax_cuda, scaled_upper_triangle_masked_softmax_cuda
2025-04-11T03:54:15.1140539Z   running egg_info
2025-04-11T03:54:15.1140985Z   creating /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info
2025-04-11T03:54:15.1151819Z   writing /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/PKG-INFO
2025-04-11T03:54:15.1157400Z   writing dependency_links to /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/dependency_links.txt
2025-04-11T03:54:15.1158742Z   writing entry points to /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/entry_points.txt
2025-04-11T03:54:15.1160596Z   writing requirements to /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/requires.txt
2025-04-11T03:54:15.1161718Z   writing top-level names to /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/top_level.txt
2025-04-11T03:54:15.1162771Z   writing manifest file '/tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/SOURCES.txt'
2025-04-11T03:54:15.1716917Z   reading manifest file '/tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/SOURCES.txt'
2025-04-11T03:54:15.1718535Z   reading manifest template 'MANIFEST.in'
2025-04-11T03:54:15.2153442Z   warning: no files found matching '*.tr' under directory 'colossalai'
2025-04-11T03:54:15.2364520Z   warning: no files found matching '*.cc' under directory 'colossalai'
2025-04-11T03:54:15.2473550Z   warning: no files found matching '*.pyi' under directory 'colossalai'
2025-04-11T03:54:15.2533306Z   warning: no files found matching '*.tr' under directory 'extensions'
2025-04-11T03:54:15.2561906Z   warning: no files found matching '*.cc' under directory 'extensions'
2025-04-11T03:54:15.2571252Z   warning: no files found matching '*.pyi' under directory 'extensions'
2025-04-11T03:54:15.2572179Z   adding license file 'LICENSE'
2025-04-11T03:54:15.2650522Z   writing manifest file '/tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/SOURCES.txt'
2025-04-11T03:54:15.6214971Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T03:54:15.6386033Z Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (1.26.4)
2025-04-11T03:54:16.0581857Z Collecting tqdm (from colossalai==0.4.9)
2025-04-11T03:54:16.0583363Z   Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata
2025-04-11T03:54:16.3469038Z   Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
2025-04-11T03:54:16.4180033Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.7/57.7 kB 689.2 kB/s eta 0:00:00
2025-04-11T03:54:16.7161583Z Collecting psutil (from colossalai==0.4.9)
2025-04-11T03:54:16.7164004Z   Obtaining dependency information for psutil from https://files.pythonhosted.org/packages/bf/b9/b0eb3f3cbcb734d930fdf839431606844a825b23eaf9a6ab371edac8162c/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:16.8026563Z   Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
2025-04-11T03:54:16.8729355Z Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (24.1)
2025-04-11T03:54:17.0708071Z Collecting pre-commit (from colossalai==0.4.9)
2025-04-11T03:54:17.0710404Z   Obtaining dependency information for pre-commit from https://files.pythonhosted.org/packages/88/74/a88bf1b1efeae488a0c0b7bdf71429c313722d1fc0f377537fbe554e6180/pre_commit-4.2.0-py2.py3-none-any.whl.metadata
2025-04-11T03:54:17.1581525Z   Downloading pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)
2025-04-11T03:54:17.3739340Z Collecting rich (from colossalai==0.4.9)
2025-04-11T03:54:17.3741506Z   Obtaining dependency information for rich from https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl.metadata
2025-04-11T03:54:17.4602566Z   Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)
2025-04-11T03:54:17.5303843Z Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (8.1.8)
2025-04-11T03:54:17.7122341Z Collecting fabric (from colossalai==0.4.9)
2025-04-11T03:54:17.7124555Z   Obtaining dependency information for fabric from https://files.pythonhosted.org/packages/d6/1f/e99e23ee01847147fa194e8d41cfcf2535a2dbfcb51414c541cadb15c5d7/fabric-3.2.2-py3-none-any.whl.metadata
2025-04-11T03:54:17.7998755Z   Downloading fabric-3.2.2-py3-none-any.whl.metadata (3.5 kB)
2025-04-11T03:54:17.8969515Z Collecting contexttimer (from colossalai==0.4.9)
2025-04-11T03:54:17.9840463Z   Downloading contexttimer-0.3.3.tar.gz (4.9 kB)
2025-04-11T03:54:17.9876180Z   Preparing metadata (setup.py): started
2025-04-11T03:54:17.9877778Z   Running command python setup.py egg_info
2025-04-11T03:54:18.0900361Z   running egg_info
2025-04-11T03:54:18.0909496Z   creating /tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info
2025-04-11T03:54:18.0924866Z   writing /tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/PKG-INFO
2025-04-11T03:54:18.0928541Z   writing dependency_links to /tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/dependency_links.txt
2025-04-11T03:54:18.0930433Z   writing top-level names to /tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/top_level.txt
2025-04-11T03:54:18.0931749Z   writing manifest file '/tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:54:18.0984379Z   reading manifest file '/tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:54:18.0989267Z   writing manifest file '/tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:54:18.1150373Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T03:54:18.1157690Z Requirement already satisfied: ninja in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (1.11.1.1)
2025-04-11T03:54:18.1163809Z Requirement already satisfied: torch<=2.5.1,>=2.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (2.2.2)
2025-04-11T03:54:18.3888080Z Collecting safetensors (from colossalai==0.4.9)
2025-04-11T03:54:18.3890413Z   Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:18.4752675Z   Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
2025-04-11T03:54:18.6444668Z Collecting einops (from colossalai==0.4.9)
2025-04-11T03:54:18.6446828Z   Obtaining dependency information for einops from https://files.pythonhosted.org/packages/87/62/9773de14fe6c45c23649e98b83231fffd7b9892b6cf863251dc2afa73643/einops-0.8.1-py3-none-any.whl.metadata
2025-04-11T03:54:18.7327474Z   Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)
2025-04-11T03:54:19.0828628Z Collecting pydantic (from colossalai==0.4.9)
2025-04-11T03:54:19.0830941Z   Obtaining dependency information for pydantic from https://files.pythonhosted.org/packages/b0/1d/407b29780a289868ed696d1616f4aad49d6388e5a77f567dcd2629dcd7b8/pydantic-2.11.3-py3-none-any.whl.metadata
2025-04-11T03:54:19.1770831Z   Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)
2025-04-11T03:54:19.2478614Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.2/65.2 kB 978.3 kB/s eta 0:00:00
2025-04-11T03:54:19.4909799Z Collecting ray (from colossalai==0.4.9)
2025-04-11T03:54:19.4911998Z   Obtaining dependency information for ray from https://files.pythonhosted.org/packages/93/f1/9108c4f878e3cacb767b7dfbbc3a26537c79ab516d2530b9f63b558ba4bb/ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:19.5783095Z   Downloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)
2025-04-11T03:54:19.8434123Z Collecting sentencepiece (from colossalai==0.4.9)
2025-04-11T03:54:19.8436424Z   Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/a6/27/33019685023221ca8ed98e8ceb7ae5e166032686fa3662c68f1f1edf334e/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:19.9305502Z   Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
2025-04-11T03:54:20.0278269Z Collecting google (from colossalai==0.4.9)
2025-04-11T03:54:20.0280427Z   Obtaining dependency information for google from https://files.pythonhosted.org/packages/ac/35/17c9141c4ae21e9a29a43acdfd848e3e468a810517f862cad07977bf8fe9/google-3.0.0-py2.py3-none-any.whl.metadata
2025-04-11T03:54:20.1159279Z   Downloading google-3.0.0-py2.py3-none-any.whl.metadata (627 bytes)
2025-04-11T03:54:20.4225813Z Collecting protobuf (from colossalai==0.4.9)
2025-04-11T03:54:20.4228022Z   Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/28/50/1925de813499546bc8ab3ae857e3ec84efe7d2f19b34529d0c7c3d02d11d/protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:20.5094367Z   Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
2025-04-11T03:54:20.7132683Z Collecting transformers==4.39.3 (from colossalai==0.4.9)
2025-04-11T03:54:20.7134969Z   Obtaining dependency information for transformers==4.39.3 from https://files.pythonhosted.org/packages/15/fc/7b6dd7e1adc0a6407b845ed4be1999e98b6917d0694e57316d140cc85484/transformers-4.39.3-py3-none-any.whl.metadata
2025-04-11T03:54:20.8010687Z   Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)
2025-04-11T03:54:20.8730312Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 2.2 MB/s eta 0:00:00
2025-04-11T03:54:20.9780613Z Collecting peft<=0.13.2,>=0.7.1 (from colossalai==0.4.9)
2025-04-11T03:54:20.9782957Z   Obtaining dependency information for peft<=0.13.2,>=0.7.1 from https://files.pythonhosted.org/packages/78/9d/5f95bfb298c8d3b4e3a107701f9a4e7774a0d4d1f8eb0c9d5420b80f7c9d/peft-0.13.2-py3-none-any.whl.metadata
2025-04-11T03:54:21.0656095Z   Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)
2025-04-11T03:54:21.2441998Z Collecting bitsandbytes>=0.39.0 (from colossalai==0.4.9)
2025-04-11T03:54:21.2444333Z   Obtaining dependency information for bitsandbytes>=0.39.0 from https://files.pythonhosted.org/packages/07/b7/cb5ce4d1a382cf53c19ef06c5fc29e85f5e129b4da6527dd207d90a5b8ad/bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata
2025-04-11T03:54:21.3304101Z   Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)
2025-04-11T03:54:21.8165186Z Collecting rpyc==6.0.0 (from colossalai==0.4.9)
2025-04-11T03:54:21.8167239Z   Obtaining dependency information for rpyc==6.0.0 from https://files.pythonhosted.org/packages/f6/e7/38cd5f2d8ab0d259648f6672fd19de30688a22ae769f87616c6a2fe92581/rpyc-6.0.0-py3-none-any.whl.metadata
2025-04-11T03:54:21.9039187Z   Downloading rpyc-6.0.0-py3-none-any.whl.metadata (3.5 kB)
2025-04-11T03:54:22.2068088Z Collecting fastapi (from colossalai==0.4.9)
2025-04-11T03:54:22.2070515Z   Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/50/b3/b51f09c2ba432a576fe63758bddc81f78f0c6309d9e5c10d194313bf021e/fastapi-0.115.12-py3-none-any.whl.metadata
2025-04-11T03:54:22.2947264Z   Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)
2025-04-11T03:54:22.6021111Z Collecting uvicorn==0.29.0 (from colossalai==0.4.9)
2025-04-11T03:54:22.6023268Z   Obtaining dependency information for uvicorn==0.29.0 from https://files.pythonhosted.org/packages/73/f5/cbb16fcbe277c1e0b8b3ddd188f2df0e0947f545c49119b589643632d156/uvicorn-0.29.0-py3-none-any.whl.metadata
2025-04-11T03:54:22.6900836Z   Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)
2025-04-11T03:54:23.0165035Z Collecting galore_torch (from colossalai==0.4.9)
2025-04-11T03:54:23.0167178Z   Obtaining dependency information for galore_torch from https://files.pythonhosted.org/packages/2b/b9/e9e88f989c62edefaa45df07198ce280ac0d373f5e2842686c6ece2ddb1e/galore_torch-1.0-py3-none-any.whl.metadata
2025-04-11T03:54:23.1053592Z   Downloading galore_torch-1.0-py3-none-any.whl.metadata (355 bytes)
2025-04-11T03:54:23.3576944Z Collecting diffusers==0.29.0 (from colossalai==0.4.9)
2025-04-11T03:54:23.3579237Z   Obtaining dependency information for diffusers==0.29.0 from https://files.pythonhosted.org/packages/d5/68/a84d929518c9fbd65febcedd7810203bcac393f9cddd0603ec58df7f93f7/diffusers-0.29.0-py3-none-any.whl.metadata
2025-04-11T03:54:23.4470004Z   Downloading diffusers-0.29.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:54:23.7531969Z Collecting importlib-metadata (from diffusers==0.29.0->colossalai==0.4.9)
2025-04-11T03:54:23.7534170Z   Obtaining dependency information for importlib-metadata from https://files.pythonhosted.org/packages/79/9d/0fb148dc4d6fa4a7dd1d8378168d9b4cd8d4560a6fbf6f0121c5fc34eb68/importlib_metadata-8.6.1-py3-none-any.whl.metadata
2025-04-11T03:54:23.8395857Z   Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)
2025-04-11T03:54:23.8415050Z Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.9) (3.13.1)
2025-04-11T03:54:24.0416603Z Collecting huggingface-hub>=0.23.2 (from diffusers==0.29.0->colossalai==0.4.9)
2025-04-11T03:54:24.0418863Z   Obtaining dependency information for huggingface-hub>=0.23.2 from https://files.pythonhosted.org/packages/93/27/1fb384a841e9661faad1c31cbfa62864f59632e876df5d795234da51c395/huggingface_hub-0.30.2-py3-none-any.whl.metadata
2025-04-11T03:54:24.1295104Z   Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)
2025-04-11T03:54:24.7004691Z Collecting regex!=2019.12.17 (from diffusers==0.29.0->colossalai==0.4.9)
2025-04-11T03:54:24.7006809Z   Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/f2/98/26d3830875b53071f1f0ae6d547f1d98e964dd29ad35cbf94439120bb67a/regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:24.7876928Z   Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
2025-04-11T03:54:24.8584506Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 541.3 kB/s eta 0:00:00
2025-04-11T03:54:24.8596280Z Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.9) (2.32.2)
2025-04-11T03:54:24.8622233Z Requirement already satisfied: Pillow in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.9) (10.3.0)
2025-04-11T03:54:25.0379409Z Collecting plumbum (from rpyc==6.0.0->colossalai==0.4.9)
2025-04-11T03:54:25.0381568Z   Obtaining dependency information for plumbum from https://files.pythonhosted.org/packages/4f/9d/d03542c93bb3d448406731b80f39c3d5601282f778328c22c77d270f4ed4/plumbum-1.9.0-py3-none-any.whl.metadata
2025-04-11T03:54:25.1258903Z   Downloading plumbum-1.9.0-py3-none-any.whl.metadata (10 kB)
2025-04-11T03:54:25.4823272Z Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.39.3->colossalai==0.4.9) (6.0.1)
2025-04-11T03:54:25.8879064Z Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3->colossalai==0.4.9)
2025-04-11T03:54:25.8880902Z   Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/1c/5d/cf5e122ce4f1a29f165b2a69dc33d1ff30bce303343d58a54775ddba5d51/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:25.9760645Z   Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
2025-04-11T03:54:26.0957324Z Collecting h11>=0.8 (from uvicorn==0.29.0->colossalai==0.4.9)
2025-04-11T03:54:26.0959454Z   Obtaining dependency information for h11>=0.8 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata
2025-04-11T03:54:26.1833958Z   Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)
2025-04-11T03:54:26.1859586Z Requirement already satisfied: typing-extensions>=4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from uvicorn==0.29.0->colossalai==0.4.9) (4.11.0)
2025-04-11T03:54:26.3992416Z Collecting accelerate>=0.21.0 (from peft<=0.13.2,>=0.7.1->colossalai==0.4.9)
2025-04-11T03:54:26.3994412Z   Obtaining dependency information for accelerate>=0.21.0 from https://files.pythonhosted.org/packages/63/b1/8198e3cdd11a426b1df2912e3381018c4a4a55368f6d0857ba3ca418ef93/accelerate-1.6.0-py3-none-any.whl.metadata
2025-04-11T03:54:26.4854947Z   Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:54:26.5919531Z Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (1.12)
2025-04-11T03:54:26.5922102Z Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (3.2.1)
2025-04-11T03:54:26.5924512Z Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (3.1.4)
2025-04-11T03:54:26.5926785Z Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (2024.6.1)
2025-04-11T03:54:26.7875839Z Collecting invoke>=2.0 (from fabric->colossalai==0.4.9)
2025-04-11T03:54:26.7877907Z   Obtaining dependency information for invoke>=2.0 from https://files.pythonhosted.org/packages/0a/66/7f8c48009c72d73bc6bbe6eb87ac838d6a526146f7dab14af671121eb379/invoke-2.2.0-py3-none-any.whl.metadata
2025-04-11T03:54:26.8747987Z   Downloading invoke-2.2.0-py3-none-any.whl.metadata (3.3 kB)
2025-04-11T03:54:27.1302027Z Collecting paramiko>=2.4 (from fabric->colossalai==0.4.9)
2025-04-11T03:54:27.1304183Z   Obtaining dependency information for paramiko>=2.4 from https://files.pythonhosted.org/packages/15/f8/c7bd0ef12954a81a1d3cea60a13946bd9a49a0036a5927770c461eade7ae/paramiko-3.5.1-py3-none-any.whl.metadata
2025-04-11T03:54:27.2193395Z   Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)
2025-04-11T03:54:27.3234834Z Collecting decorator>=5 (from fabric->colossalai==0.4.9)
2025-04-11T03:54:27.3236936Z   Obtaining dependency information for decorator>=5 from https://files.pythonhosted.org/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl.metadata
2025-04-11T03:54:27.4101068Z   Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)
2025-04-11T03:54:27.5170965Z Collecting deprecated>=1.2 (from fabric->colossalai==0.4.9)
2025-04-11T03:54:27.5173205Z   Obtaining dependency information for deprecated>=1.2 from https://files.pythonhosted.org/packages/6e/c6/ac0b6c1e2d138f1002bcf799d330bd6d85084fece321e662a14223794041/Deprecated-1.2.18-py2.py3-none-any.whl.metadata
2025-04-11T03:54:27.6038584Z   Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)
2025-04-11T03:54:27.8821414Z Collecting starlette<0.47.0,>=0.40.0 (from fastapi->colossalai==0.4.9)
2025-04-11T03:54:27.8823748Z   Obtaining dependency information for starlette<0.47.0,>=0.40.0 from https://files.pythonhosted.org/packages/a0/4b/528ccf7a982216885a1ff4908e886b8fb5f19862d1962f56a3fce2435a70/starlette-0.46.1-py3-none-any.whl.metadata
2025-04-11T03:54:27.9682835Z   Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)
2025-04-11T03:54:28.1707146Z Collecting annotated-types>=0.6.0 (from pydantic->colossalai==0.4.9)
2025-04-11T03:54:28.1709514Z   Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata
2025-04-11T03:54:28.2568516Z   Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
2025-04-11T03:54:29.3201824Z Collecting pydantic-core==2.33.1 (from pydantic->colossalai==0.4.9)
2025-04-11T03:54:29.3203463Z   Obtaining dependency information for pydantic-core==2.33.1 from https://files.pythonhosted.org/packages/f2/68/866ce83a51dd37e7c604ce0050ff6ad26de65a7799df89f4db87dd93d1d6/pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:29.4066730Z   Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
2025-04-11T03:54:29.6545176Z Collecting typing-extensions>=4.0 (from uvicorn==0.29.0->colossalai==0.4.9)
2025-04-11T03:54:29.6547387Z   Obtaining dependency information for typing-extensions>=4.0 from https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl.metadata
2025-04-11T03:54:29.7406566Z   Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)
2025-04-11T03:54:29.9084880Z Collecting typing-inspection>=0.4.0 (from pydantic->colossalai==0.4.9)
2025-04-11T03:54:29.9087130Z   Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl.metadata
2025-04-11T03:54:29.9954607Z   Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)
2025-04-11T03:54:30.1858012Z Collecting beautifulsoup4 (from google->colossalai==0.4.9)
2025-04-11T03:54:30.1860181Z   Obtaining dependency information for beautifulsoup4 from https://files.pythonhosted.org/packages/f9/49/6abb616eb3cbab6a7cca303dc02fdf3836de2e0b834bf966a7f5271a34d8/beautifulsoup4-4.13.3-py3-none-any.whl.metadata
2025-04-11T03:54:30.2726383Z   Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)
2025-04-11T03:54:30.4559533Z Collecting cfgv>=2.0.0 (from pre-commit->colossalai==0.4.9)
2025-04-11T03:54:30.4561709Z   Obtaining dependency information for cfgv>=2.0.0 from https://files.pythonhosted.org/packages/c5/55/51844dd50c4fc7a33b653bfaba4c2456f06955289ca770a5dbd5fd267374/cfgv-3.4.0-py2.py3-none-any.whl.metadata
2025-04-11T03:54:30.5431291Z   Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)
2025-04-11T03:54:30.8287106Z Collecting identify>=1.0.0 (from pre-commit->colossalai==0.4.9)
2025-04-11T03:54:30.8289132Z   Obtaining dependency information for identify>=1.0.0 from https://files.pythonhosted.org/packages/07/ce/0845144ed1f0e25db5e7a79c2354c1da4b5ce392b8966449d5db8dca18f1/identify-2.6.9-py2.py3-none-any.whl.metadata
2025-04-11T03:54:30.9337095Z   Downloading identify-2.6.9-py2.py3-none-any.whl.metadata (4.4 kB)
2025-04-11T03:54:31.0399032Z Collecting nodeenv>=0.11.1 (from pre-commit->colossalai==0.4.9)
2025-04-11T03:54:31.0401278Z   Obtaining dependency information for nodeenv>=0.11.1 from https://files.pythonhosted.org/packages/d2/1d/1b658dbd2b9fa9c4c9f32accbfc0205d532c8c6194dc0f2a4c0428e7128a/nodeenv-1.9.1-py2.py3-none-any.whl.metadata
2025-04-11T03:54:31.1313234Z   Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)
2025-04-11T03:54:31.4267038Z Collecting virtualenv>=20.10.0 (from pre-commit->colossalai==0.4.9)
2025-04-11T03:54:31.4269498Z   Obtaining dependency information for virtualenv>=20.10.0 from https://files.pythonhosted.org/packages/4c/ed/3cfeb48175f0671ec430ede81f628f9fb2b1084c9064ca67ebe8c0ed6a05/virtualenv-20.30.0-py3-none-any.whl.metadata
2025-04-11T03:54:31.5131425Z   Downloading virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)
2025-04-11T03:54:31.9390582Z Collecting jsonschema (from ray->colossalai==0.4.9)
2025-04-11T03:54:31.9392720Z   Obtaining dependency information for jsonschema from https://files.pythonhosted.org/packages/69/4a/4f9dbeb84e8850557c02365a0eee0649abe5eb1d84af92a25731c6c0f922/jsonschema-4.23.0-py3-none-any.whl.metadata
2025-04-11T03:54:32.0255979Z   Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)
2025-04-11T03:54:32.2352373Z Collecting msgpack<2.0.0,>=1.0.0 (from ray->colossalai==0.4.9)
2025-04-11T03:54:32.2354638Z   Obtaining dependency information for msgpack<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/ff/75/09081792db60470bef19d9c2be89f024d366b1e1973c197bb59e6aabc647/msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:32.3232716Z   Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)
2025-04-11T03:54:32.5085504Z Collecting aiosignal (from ray->colossalai==0.4.9)
2025-04-11T03:54:32.5087658Z   Obtaining dependency information for aiosignal from https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl.metadata
2025-04-11T03:54:32.5971635Z   Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)
2025-04-11T03:54:32.8013120Z Collecting frozenlist (from ray->colossalai==0.4.9)
2025-04-11T03:54:32.8015567Z   Obtaining dependency information for frozenlist from https://files.pythonhosted.org/packages/ee/59/928322800306f6529d1852323014ee9008551e9bb027cc38d276cbc0b0e7/frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:32.8908327Z   Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)
2025-04-11T03:54:33.1963244Z Collecting markdown-it-py>=2.2.0 (from rich->colossalai==0.4.9)
2025-04-11T03:54:33.1965282Z   Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata
2025-04-11T03:54:33.2828666Z   Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
2025-04-11T03:54:33.5328900Z Collecting pygments<3.0.0,>=2.13.0 (from rich->colossalai==0.4.9)
2025-04-11T03:54:33.5330899Z   Obtaining dependency information for pygments<3.0.0,>=2.13.0 from https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl.metadata
2025-04-11T03:54:33.6199198Z   Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)
2025-04-11T03:54:33.9467092Z Collecting wrapt<2,>=1.10 (from deprecated>=1.2->fabric->colossalai==0.4.9)
2025-04-11T03:54:33.9469523Z   Obtaining dependency information for wrapt<2,>=1.10 from https://files.pythonhosted.org/packages/90/ec/00759565518f268ed707dcc40f7eeec38637d46b098a1f5143bff488fe97/wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:34.0343092Z   Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)
2025-04-11T03:54:34.3733533Z Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->colossalai==0.4.9)
2025-04-11T03:54:34.3735746Z   Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata
2025-04-11T03:54:34.4595462Z   Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
2025-04-11T03:54:34.6768805Z Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:54:34.6770943Z   Obtaining dependency information for bcrypt>=3.2 from https://files.pythonhosted.org/packages/cb/c6/8fedca4c2ada1b6e889c52d2943b2f968d3427e5d65f595620ec4c06fa2f/bcrypt-4.3.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata
2025-04-11T03:54:34.7666533Z   Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (10 kB)
2025-04-11T03:54:35.0822568Z Collecting cryptography>=3.3 (from paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:54:35.0824793Z   Obtaining dependency information for cryptography>=3.3 from https://files.pythonhosted.org/packages/78/2b/999b2a1e1ba2206f2d3bca267d68f350beb2b048a41ea827e08ce7260098/cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata
2025-04-11T03:54:35.1686393Z   Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.7 kB)
2025-04-11T03:54:35.4131552Z Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:54:35.4133905Z   Obtaining dependency information for pynacl>=1.5 from https://files.pythonhosted.org/packages/ee/87/f1bb6a595f14a327e8285b9eb54d41fef76c585a0edef0a45f6fc95de125/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata
2025-04-11T03:54:35.4996349Z   Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)
2025-04-11T03:54:35.7746259Z Collecting anyio<5,>=3.6.2 (from starlette<0.47.0,>=0.40.0->fastapi->colossalai==0.4.9)
2025-04-11T03:54:35.7748473Z   Obtaining dependency information for anyio<5,>=3.6.2 from https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl.metadata
2025-04-11T03:54:35.8612181Z   Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)
2025-04-11T03:54:36.0217905Z Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->colossalai==0.4.9)
2025-04-11T03:54:36.0220075Z   Obtaining dependency information for distlib<1,>=0.3.7 from https://files.pythonhosted.org/packages/91/a1/cf2472db20f7ce4a6be1253a81cfdf85ad9c7885ffbed7047fb72c24cf87/distlib-0.3.9-py2.py3-none-any.whl.metadata
2025-04-11T03:54:36.1083959Z   Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)
2025-04-11T03:54:36.2191414Z Collecting platformdirs<5,>=3.9.1 (from virtualenv>=20.10.0->pre-commit->colossalai==0.4.9)
2025-04-11T03:54:36.2193566Z   Obtaining dependency information for platformdirs<5,>=3.9.1 from https://files.pythonhosted.org/packages/6d/45/59578566b3275b8fd9157885918fcd0c4d74162928a5310926887b856a51/platformdirs-4.3.7-py3-none-any.whl.metadata
2025-04-11T03:54:36.3056414Z   Downloading platformdirs-4.3.7-py3-none-any.whl.metadata (11 kB)
2025-04-11T03:54:36.4461467Z Collecting soupsieve>1.2 (from beautifulsoup4->google->colossalai==0.4.9)
2025-04-11T03:54:36.4463614Z   Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/d1/c2/fe97d779f3ef3b15f05c94a2f1e3d21732574ed441687474db9d342a7315/soupsieve-2.6-py3-none-any.whl.metadata
2025-04-11T03:54:36.5326448Z   Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)
2025-04-11T03:54:36.7311994Z Collecting zipp>=3.20 (from importlib-metadata->diffusers==0.29.0->colossalai==0.4.9)
2025-04-11T03:54:36.7314046Z   Obtaining dependency information for zipp>=3.20 from https://files.pythonhosted.org/packages/b7/1a/7e4798e9339adc931158c9d69ecc34f5e6791489d469f5e50ec15e35f458/zipp-3.21.0-py3-none-any.whl.metadata
2025-04-11T03:54:36.8177962Z   Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)
2025-04-11T03:54:36.8272929Z Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (2.1.3)
2025-04-11T03:54:36.9535816Z Collecting attrs>=22.2.0 (from jsonschema->ray->colossalai==0.4.9)
2025-04-11T03:54:36.9537606Z   Obtaining dependency information for attrs>=22.2.0 from https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl.metadata
2025-04-11T03:54:37.0401691Z   Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)
2025-04-11T03:54:37.2131428Z Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->ray->colossalai==0.4.9)
2025-04-11T03:54:37.2133414Z   Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/d1/0f/8910b19ac0670a0f80ce1008e5e751c4a57e14d2c4c13a482aa6079fa9d6/jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata
2025-04-11T03:54:37.2993245Z   Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)
2025-04-11T03:54:37.4873409Z Collecting referencing>=0.28.4 (from jsonschema->ray->colossalai==0.4.9)
2025-04-11T03:54:37.4875170Z   Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/c1/b1/3baf80dc6d2b7bc27a95a67752d0208e410351e3feb4eb78de5f77454d8d/referencing-0.36.2-py3-none-any.whl.metadata
2025-04-11T03:54:37.5742817Z   Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)
2025-04-11T03:54:38.1524108Z Collecting rpds-py>=0.7.1 (from jsonschema->ray->colossalai==0.4.9)
2025-04-11T03:54:38.1525147Z   Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/a7/a7/6d04d438f53d8bb2356bb000bea9cf5c96a9315e405b577117e344cc7404/rpds_py-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:38.2480283Z   Downloading rpds_py-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
2025-04-11T03:54:38.3096303Z Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (2.0.4)
2025-04-11T03:54:38.3101128Z Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (3.7)
2025-04-11T03:54:38.3107873Z Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (2.2.2)
2025-04-11T03:54:38.3112601Z Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (2024.6.2)
2025-04-11T03:54:38.3175547Z Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (1.3.0)
2025-04-11T03:54:38.4370053Z Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->colossalai==0.4.9)
2025-04-11T03:54:38.4372182Z   Obtaining dependency information for exceptiongroup>=1.0.2 from https://files.pythonhosted.org/packages/02/cc/b7e31358aac6ed1ef2bb790a9746ac2c69bcb3c8588b41616914eb106eaf/exceptiongroup-1.2.2-py3-none-any.whl.metadata
2025-04-11T03:54:38.5231372Z   Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)
2025-04-11T03:54:38.7592765Z Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->colossalai==0.4.9)
2025-04-11T03:54:38.7594813Z   Obtaining dependency information for sniffio>=1.1 from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata
2025-04-11T03:54:38.8458883Z   Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
2025-04-11T03:54:39.2559487Z Collecting cffi>=1.12 (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:54:39.2561404Z   Obtaining dependency information for cffi>=1.12 from https://files.pythonhosted.org/packages/8d/fb/4da72871d177d63649ac449aec2e8a29efe0274035880c7af59101ca2232/cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:54:39.3422248Z   Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)
2025-04-11T03:54:39.5960080Z Collecting pycparser (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:54:39.5961914Z   Obtaining dependency information for pycparser from https://files.pythonhosted.org/packages/13/a3/a812df4e2dd5696d1f351d58b8fe16a405b234ad2886a0dab9183fb78109/pycparser-2.22-py3-none-any.whl.metadata
2025-04-11T03:54:39.6945549Z   Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)
2025-04-11T03:54:39.8193837Z Downloading diffusers-0.29.0-py3-none-any.whl (2.2 MB)
2025-04-11T03:54:40.1719671Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 6.5 MB/s eta 0:00:00
2025-04-11T03:54:40.2596905Z Downloading rpyc-6.0.0-py3-none-any.whl (74 kB)
2025-04-11T03:54:40.3316488Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.5/74.5 kB 928.9 kB/s eta 0:00:00
2025-04-11T03:54:40.4242140Z Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)
2025-04-11T03:54:40.6992048Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 32.2 MB/s eta 0:00:00
2025-04-11T03:54:40.7868075Z Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)
2025-04-11T03:54:40.8575783Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.8/60.8 kB 731.5 kB/s eta 0:00:00
2025-04-11T03:54:40.9455363Z Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)
2025-04-11T03:54:43.8885822Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.1/76.1 MB 17.3 MB/s eta 0:00:00
2025-04-11T03:54:43.9776848Z Downloading peft-0.13.2-py3-none-any.whl (320 kB)
2025-04-11T03:54:44.0489467Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 320.7/320.7 kB 4.5 MB/s eta 0:00:00
2025-04-11T03:54:44.1351964Z Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)
2025-04-11T03:54:44.2085883Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.6/471.6 kB 6.5 MB/s eta 0:00:00
2025-04-11T03:54:44.2956871Z Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)
2025-04-11T03:54:44.3664849Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 987.4 kB/s eta 0:00:00
2025-04-11T03:54:44.4522011Z Downloading einops-0.8.1-py3-none-any.whl (64 kB)
2025-04-11T03:54:44.5230426Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.4/64.4 kB 781.6 kB/s eta 0:00:00
2025-04-11T03:54:44.6106048Z Downloading fabric-3.2.2-py3-none-any.whl (59 kB)
2025-04-11T03:54:44.6811293Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.4/59.4 kB 713.9 kB/s eta 0:00:00
2025-04-11T03:54:44.7675674Z Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)
2025-04-11T03:54:44.8381213Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.2/95.2 kB 1.2 MB/s eta 0:00:00
2025-04-11T03:54:44.9241309Z Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)
2025-04-11T03:54:44.9960914Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 443.6/443.6 kB 6.2 MB/s eta 0:00:00
2025-04-11T03:54:45.0824112Z Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
2025-04-11T03:54:45.1645119Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 24.8 MB/s eta 0:00:00
2025-04-11T03:54:45.2515660Z Downloading galore_torch-1.0-py3-none-any.whl (13 kB)
2025-04-11T03:54:45.4121872Z Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)
2025-04-11T03:54:45.4826285Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.3/45.3 kB 508.6 kB/s eta 0:00:00
2025-04-11T03:54:45.5698827Z Downloading pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)
2025-04-11T03:54:45.6418665Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 220.7/220.7 kB 3.0 MB/s eta 0:00:00
2025-04-11T03:54:45.7280192Z Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)
2025-04-11T03:54:45.7996413Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.2/316.2 kB 4.4 MB/s eta 0:00:00
2025-04-11T03:54:45.8866079Z Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
2025-04-11T03:54:45.9575903Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 278.0/278.0 kB 3.9 MB/s eta 0:00:00
2025-04-11T03:54:46.0449190Z Downloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl (67.9 MB)
2025-04-11T03:54:48.5027245Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.9/67.9 MB 22.0 MB/s eta 0:00:00
2025-04-11T03:54:48.5897514Z Downloading rich-14.0.0-py3-none-any.whl (243 kB)
2025-04-11T03:54:48.6612309Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 243.2/243.2 kB 3.3 MB/s eta 0:00:00
2025-04-11T03:54:48.7491771Z Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
2025-04-11T03:54:48.8233970Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 17.9 MB/s eta 0:00:00
2025-04-11T03:54:48.9111953Z Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)
2025-04-11T03:54:48.9828539Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 354.7/354.7 kB 5.0 MB/s eta 0:00:00
2025-04-11T03:54:49.0701400Z Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
2025-04-11T03:54:49.1604006Z Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)
2025-04-11T03:54:49.2496549Z Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)
2025-04-11T03:54:49.3374387Z Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)
2025-04-11T03:54:49.4258602Z Downloading h11-0.14.0-py3-none-any.whl (58 kB)
2025-04-11T03:54:49.4970614Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 695.5 kB/s eta 0:00:00
2025-04-11T03:54:49.5836698Z Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)
2025-04-11T03:54:49.6558421Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 481.4/481.4 kB 6.7 MB/s eta 0:00:00
2025-04-11T03:54:49.7426123Z Downloading identify-2.6.9-py2.py3-none-any.whl (99 kB)
2025-04-11T03:54:49.8134611Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 1.3 MB/s eta 0:00:00
2025-04-11T03:54:49.9019833Z Downloading invoke-2.2.0-py3-none-any.whl (160 kB)
2025-04-11T03:54:49.9730364Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.3/160.3 kB 2.2 MB/s eta 0:00:00
2025-04-11T03:54:50.0602874Z Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
2025-04-11T03:54:50.1312017Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 1.1 MB/s eta 0:00:00
2025-04-11T03:54:50.2183694Z Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)
2025-04-11T03:54:50.2903407Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 378.0/378.0 kB 5.3 MB/s eta 0:00:00
2025-04-11T03:54:50.3782992Z Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)
2025-04-11T03:54:50.5340474Z Downloading paramiko-3.5.1-py3-none-any.whl (227 kB)
2025-04-11T03:54:50.6054525Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.3/227.3 kB 3.1 MB/s eta 0:00:00
2025-04-11T03:54:50.6928623Z Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)
2025-04-11T03:54:50.7671548Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 16.8 MB/s eta 0:00:00
2025-04-11T03:54:50.8558254Z Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)
2025-04-11T03:54:50.9286288Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.7/781.7 kB 10.9 MB/s eta 0:00:00
2025-04-11T03:54:51.0152621Z Downloading starlette-0.46.1-py3-none-any.whl (71 kB)
2025-04-11T03:54:51.0862410Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 895.9 kB/s eta 0:00:00
2025-04-11T03:54:51.1768412Z Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
2025-04-11T03:54:51.2923333Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 31.4 MB/s eta 0:00:00
2025-04-11T03:54:51.3793755Z Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)
2025-04-11T03:54:51.4505027Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.8/45.8 kB 515.0 kB/s eta 0:00:00
2025-04-11T03:54:51.5372118Z Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)
2025-04-11T03:54:51.6941299Z Downloading virtualenv-20.30.0-py3-none-any.whl (4.3 MB)
2025-04-11T03:54:51.8322231Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 31.8 MB/s eta 0:00:00
2025-04-11T03:54:51.9188786Z Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)
2025-04-11T03:54:52.1196348Z Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)
2025-04-11T03:54:52.1913278Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.9/241.9 kB 3.3 MB/s eta 0:00:00
2025-04-11T03:54:52.2796918Z Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)
2025-04-11T03:54:52.3506836Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186.0/186.0 kB 2.5 MB/s eta 0:00:00
2025-04-11T03:54:52.4373885Z Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)
2025-04-11T03:54:52.5932487Z Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)
2025-04-11T03:54:52.6642930Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.5/88.5 kB 1.1 MB/s eta 0:00:00
2025-04-11T03:54:52.7518571Z Downloading plumbum-1.9.0-py3-none-any.whl (127 kB)
2025-04-11T03:54:52.8226616Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.0/128.0 kB 1.7 MB/s eta 0:00:00
2025-04-11T03:54:52.9088920Z Downloading anyio-4.9.0-py3-none-any.whl (100 kB)
2025-04-11T03:54:52.9801503Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.9/100.9 kB 1.3 MB/s eta 0:00:00
2025-04-11T03:54:53.0667731Z Downloading attrs-25.3.0-py3-none-any.whl (63 kB)
2025-04-11T03:54:53.1376225Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.8/63.8 kB 776.0 kB/s eta 0:00:00
2025-04-11T03:54:53.2260228Z Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_28_x86_64.whl (284 kB)
2025-04-11T03:54:53.2973187Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 284.5/284.5 kB 3.9 MB/s eta 0:00:00
2025-04-11T03:54:53.3838521Z Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl (4.2 MB)
2025-04-11T03:54:53.6478853Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 16.0 MB/s eta 0:00:00
2025-04-11T03:54:53.7355937Z Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)
2025-04-11T03:54:53.8073240Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 6.6 MB/s eta 0:00:00
2025-04-11T03:54:53.8982548Z Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)
2025-04-11T03:54:54.4133712Z Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)
2025-04-11T03:54:54.5934951Z Downloading platformdirs-4.3.7-py3-none-any.whl (18 kB)
2025-04-11T03:54:54.7499396Z Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)
2025-04-11T03:54:54.8678331Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 856.7/856.7 kB 7.3 MB/s eta 0:00:00
2025-04-11T03:54:54.9552263Z Downloading referencing-0.36.2-py3-none-any.whl (26 kB)
2025-04-11T03:54:55.1125557Z Downloading rpds_py-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (389 kB)
2025-04-11T03:54:55.1843967Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 389.5/389.5 kB 5.4 MB/s eta 0:00:00
2025-04-11T03:54:55.2711140Z Downloading soupsieve-2.6-py3-none-any.whl (36 kB)
2025-04-11T03:54:55.4274478Z Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)
2025-04-11T03:54:55.4982001Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.8/82.8 kB 1.1 MB/s eta 0:00:00
2025-04-11T03:54:55.5855660Z Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)
2025-04-11T03:54:55.6734529Z Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)
2025-04-11T03:54:55.7456231Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 446.2/446.2 kB 6.2 MB/s eta 0:00:00
2025-04-11T03:54:55.8321314Z Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)
2025-04-11T03:54:55.9880192Z Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)
2025-04-11T03:54:56.1449448Z Downloading pycparser-2.22-py3-none-any.whl (117 kB)
2025-04-11T03:54:56.2146550Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.6/117.6 kB 1.6 MB/s eta 0:00:00
2025-04-11T03:54:56.4472175Z Building wheels for collected packages: contexttimer
2025-04-11T03:54:56.4477415Z   Building wheel for contexttimer (setup.py): started
2025-04-11T03:54:56.4478862Z   Running command python setup.py bdist_wheel
2025-04-11T03:54:56.5583472Z   running bdist_wheel
2025-04-11T03:54:56.5713780Z   running build
2025-04-11T03:54:56.5714699Z   running build_py
2025-04-11T03:54:56.5734053Z   creating build
2025-04-11T03:54:56.5735054Z   creating build/lib
2025-04-11T03:54:56.5736320Z   creating build/lib/contexttimer
2025-04-11T03:54:56.5737208Z   copying contexttimer/__init__.py -> build/lib/contexttimer
2025-04-11T03:54:56.5738260Z   copying contexttimer/timeout.py -> build/lib/contexttimer
2025-04-11T03:54:56.5763662Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
2025-04-11T03:54:56.5765799Z   !!
2025-04-11T03:54:56.5767569Z 
2025-04-11T03:54:56.5767755Z           ********************************************************************************
2025-04-11T03:54:56.5768188Z           Please avoid running ``setup.py`` directly.
2025-04-11T03:54:56.5768572Z           Instead, use pypa/build, pypa/installer or other
2025-04-11T03:54:56.5769217Z           standards-based tools.
2025-04-11T03:54:56.5769948Z 
2025-04-11T03:54:56.5771374Z           See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
2025-04-11T03:54:56.5772098Z           ********************************************************************************
2025-04-11T03:54:56.5772909Z 
2025-04-11T03:54:56.5774046Z   !!
2025-04-11T03:54:56.5774962Z     self.initialize_options()
2025-04-11T03:54:56.5780173Z   installing to build/bdist.linux-x86_64/wheel
2025-04-11T03:54:56.5781085Z   running install
2025-04-11T03:54:56.5824106Z   running install_lib
2025-04-11T03:54:56.5842347Z   creating build/bdist.linux-x86_64
2025-04-11T03:54:56.5843270Z   creating build/bdist.linux-x86_64/wheel
2025-04-11T03:54:56.5844299Z   creating build/bdist.linux-x86_64/wheel/contexttimer
2025-04-11T03:54:56.5845457Z   copying build/lib/contexttimer/__init__.py -> build/bdist.linux-x86_64/wheel/contexttimer
2025-04-11T03:54:56.5846388Z   copying build/lib/contexttimer/timeout.py -> build/bdist.linux-x86_64/wheel/contexttimer
2025-04-11T03:54:56.5847272Z   running install_egg_info
2025-04-11T03:54:56.5873010Z   running egg_info
2025-04-11T03:54:56.5887507Z   writing contexttimer.egg-info/PKG-INFO
2025-04-11T03:54:56.5891687Z   writing dependency_links to contexttimer.egg-info/dependency_links.txt
2025-04-11T03:54:56.5893777Z   writing top-level names to contexttimer.egg-info/top_level.txt
2025-04-11T03:54:56.5911583Z   reading manifest file 'contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:54:56.5917009Z   writing manifest file 'contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:54:56.5918485Z   Copying contexttimer.egg-info to build/bdist.linux-x86_64/wheel/contexttimer-0.3.3-py3.10.egg-info
2025-04-11T03:54:56.5922300Z   running install_scripts
2025-04-11T03:54:56.5930528Z   creating build/bdist.linux-x86_64/wheel/contexttimer-0.3.3.dist-info/WHEEL
2025-04-11T03:54:56.5933872Z   creating '/tmp/pip-wheel-4e9r1l_e/contexttimer-0.3.3-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
2025-04-11T03:54:56.5937631Z   adding 'contexttimer/__init__.py'
2025-04-11T03:54:56.5939777Z   adding 'contexttimer/timeout.py'
2025-04-11T03:54:56.5942622Z   adding 'contexttimer-0.3.3.dist-info/METADATA'
2025-04-11T03:54:56.5943599Z   adding 'contexttimer-0.3.3.dist-info/WHEEL'
2025-04-11T03:54:56.5944593Z   adding 'contexttimer-0.3.3.dist-info/top_level.txt'
2025-04-11T03:54:56.5945740Z   adding 'contexttimer-0.3.3.dist-info/RECORD'
2025-04-11T03:54:56.5946767Z   removing build/bdist.linux-x86_64/wheel
2025-04-11T03:54:56.6160240Z   Building wheel for contexttimer (setup.py): finished with status 'done'
2025-04-11T03:54:56.6163885Z   Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5804 sha256=ccdc87db8df71092a69c43fa7fbfca4e2715eee68f9137797a7d5496c0a20364
2025-04-11T03:54:56.6165034Z   Stored in directory: /github/home/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4
2025-04-11T03:54:56.6178553Z Successfully built contexttimer
2025-04-11T03:54:57.5034368Z Installing collected packages: sentencepiece, distlib, contexttimer, zipp, wrapt, typing-extensions, tqdm, soupsieve, sniffio, safetensors, rpds-py, regex, pygments, pycparser, psutil, protobuf, plumbum, platformdirs, nodeenv, msgpack, mdurl, invoke, identify, h11, frozenlist, exceptiongroup, einops, decorator, cfgv, bcrypt, attrs, annotated-types, virtualenv, uvicorn, typing-inspection, rpyc, referencing, pydantic-core, markdown-it-py, importlib-metadata, huggingface-hub, deprecated, cffi, beautifulsoup4, anyio, aiosignal, tokenizers, starlette, rich, pynacl, pydantic, pre-commit, jsonschema-specifications, google, diffusers, cryptography, bitsandbytes, accelerate, transformers, paramiko, jsonschema, fastapi, ray, peft, galore_torch, fabric, colossalai
2025-04-11T03:54:57.6063189Z   Attempting uninstall: typing-extensions
2025-04-11T03:54:57.6068787Z     Found existing installation: typing_extensions 4.11.0
2025-04-11T03:54:57.6082001Z     Uninstalling typing_extensions-4.11.0:
2025-04-11T03:54:57.6787907Z       Removing file or directory /opt/conda/envs/pytorch/lib/python3.10/site-packages/__pycache__/typing_extensions.cpython-310.pyc
2025-04-11T03:54:57.6798202Z       Removing file or directory /opt/conda/envs/pytorch/lib/python3.10/site-packages/typing_extensions-4.11.0.dist-info/
2025-04-11T03:54:57.6805151Z       Removing file or directory /opt/conda/envs/pytorch/lib/python3.10/site-packages/typing_extensions.py
2025-04-11T03:54:57.6806245Z       Successfully uninstalled typing_extensions-4.11.0
2025-04-11T03:54:57.7285441Z   changing mode of /opt/conda/envs/pytorch/bin/tqdm to 755
2025-04-11T03:54:58.3000625Z   changing mode of /opt/conda/envs/pytorch/bin/pygmentize to 755
2025-04-11T03:54:58.6521012Z   changing mode of /opt/conda/envs/pytorch/bin/nodeenv to 755
2025-04-11T03:54:58.7370410Z   changing mode of /opt/conda/envs/pytorch/bin/inv to 755
2025-04-11T03:54:58.7372398Z   changing mode of /opt/conda/envs/pytorch/bin/invoke to 755
2025-04-11T03:54:58.7499631Z   changing mode of /opt/conda/envs/pytorch/bin/identify-cli to 755
2025-04-11T03:54:58.9609215Z   changing mode of /opt/conda/envs/pytorch/bin/virtualenv to 755
2025-04-11T03:54:58.9968221Z   changing mode of /opt/conda/envs/pytorch/bin/uvicorn to 755
2025-04-11T03:54:59.0406535Z   changing mode of /opt/conda/envs/pytorch/bin/rpyc_classic to 755
2025-04-11T03:54:59.0408778Z   changing mode of /opt/conda/envs/pytorch/bin/rpyc_classic.py to 755
2025-04-11T03:54:59.0410903Z   changing mode of /opt/conda/envs/pytorch/bin/rpyc_registry to 755
2025-04-11T03:54:59.0412971Z   changing mode of /opt/conda/envs/pytorch/bin/rpyc_registry.py to 755
2025-04-11T03:54:59.1462201Z   changing mode of /opt/conda/envs/pytorch/bin/markdown-it to 755
2025-04-11T03:54:59.3153319Z   changing mode of /opt/conda/envs/pytorch/bin/huggingface-cli to 755
2025-04-11T03:55:00.0192816Z   changing mode of /opt/conda/envs/pytorch/bin/pre-commit to 755
2025-04-11T03:55:00.8962983Z   changing mode of /opt/conda/envs/pytorch/bin/diffusers-cli to 755
2025-04-11T03:55:02.3393614Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate to 755
2025-04-11T03:55:02.3395537Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate-config to 755
2025-04-11T03:55:02.3397211Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate-estimate-memory to 755
2025-04-11T03:55:02.3399032Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate-launch to 755
2025-04-11T03:55:02.3400871Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate-merge-weights to 755
2025-04-11T03:55:05.9098644Z   changing mode of /opt/conda/envs/pytorch/bin/transformers-cli to 755
2025-04-11T03:55:06.0472779Z   changing mode of /opt/conda/envs/pytorch/bin/jsonschema to 755
2025-04-11T03:55:06.0980169Z   changing mode of /opt/conda/envs/pytorch/bin/fastapi to 755
2025-04-11T03:55:09.1062232Z   changing mode of /opt/conda/envs/pytorch/bin/ray to 755
2025-04-11T03:55:09.1063543Z   changing mode of /opt/conda/envs/pytorch/bin/serve to 755
2025-04-11T03:55:09.1065529Z   changing mode of /opt/conda/envs/pytorch/bin/tune to 755
2025-04-11T03:55:09.2749839Z   changing mode of /opt/conda/envs/pytorch/bin/fab to 755
2025-04-11T03:55:09.2760750Z   Running setup.py develop for colossalai
2025-04-11T03:55:09.2762662Z     Running command python setup.py develop
2025-04-11T03:55:11.8570761Z     [extension] Building extensionscpu_adam_x86, layernorm_cuda, moe_cuda, fused_optim_cuda, inference_ops_cuda, scaled_masked_softmax_cuda, scaled_upper_triangle_masked_softmax_cuda
2025-04-11T03:55:11.8571582Z     running develop
2025-04-11T03:55:11.8572330Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.
2025-04-11T03:55:11.8573028Z     !!
2025-04-11T03:55:11.8573168Z 
2025-04-11T03:55:11.8574076Z             ********************************************************************************
2025-04-11T03:55:11.8575078Z             Please avoid running ``setup.py`` and ``easy_install``.
2025-04-11T03:55:11.8576081Z             Instead, use pypa/build, pypa/installer or other
2025-04-11T03:55:11.8577109Z             standards-based tools.
2025-04-11T03:55:11.8577714Z 
2025-04-11T03:55:11.8578999Z             See https://github.com/pypa/setuptools/issues/917 for details.
2025-04-11T03:55:11.8579807Z             ********************************************************************************
2025-04-11T03:55:11.8580637Z 
2025-04-11T03:55:11.8582455Z     !!
2025-04-11T03:55:11.8582904Z       easy_install.initialize_options(self)
2025-04-11T03:55:11.8633996Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
2025-04-11T03:55:11.8634700Z     !!
2025-04-11T03:55:11.8635124Z 
2025-04-11T03:55:11.8636348Z             ********************************************************************************
2025-04-11T03:55:11.8637276Z             Please avoid running ``setup.py`` directly.
2025-04-11T03:55:11.8638235Z             Instead, use pypa/build, pypa/installer or other
2025-04-11T03:55:11.8639184Z             standards-based tools.
2025-04-11T03:55:11.8639938Z 
2025-04-11T03:55:11.8641551Z             See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
2025-04-11T03:55:11.8642136Z             ********************************************************************************
2025-04-11T03:55:11.8642772Z 
2025-04-11T03:55:11.8643850Z     !!
2025-04-11T03:55:11.8644895Z       self.initialize_options()
2025-04-11T03:55:11.9073645Z     running egg_info
2025-04-11T03:55:11.9074552Z     creating colossalai.egg-info
2025-04-11T03:55:11.9114389Z     writing colossalai.egg-info/PKG-INFO
2025-04-11T03:55:11.9119477Z     writing dependency_links to colossalai.egg-info/dependency_links.txt
2025-04-11T03:55:11.9120740Z     writing entry points to colossalai.egg-info/entry_points.txt
2025-04-11T03:55:11.9122628Z     writing requirements to colossalai.egg-info/requires.txt
2025-04-11T03:55:11.9123765Z     writing top-level names to colossalai.egg-info/top_level.txt
2025-04-11T03:55:11.9124689Z     writing manifest file 'colossalai.egg-info/SOURCES.txt'
2025-04-11T03:55:11.9681994Z     reading manifest file 'colossalai.egg-info/SOURCES.txt'
2025-04-11T03:55:11.9683498Z     reading manifest template 'MANIFEST.in'
2025-04-11T03:55:12.0110381Z     warning: no files found matching '*.tr' under directory 'colossalai'
2025-04-11T03:55:12.0320897Z     warning: no files found matching '*.cc' under directory 'colossalai'
2025-04-11T03:55:12.0424569Z     warning: no files found matching '*.pyi' under directory 'colossalai'
2025-04-11T03:55:12.0488355Z     warning: no files found matching '*.tr' under directory 'extensions'
2025-04-11T03:55:12.0509206Z     warning: no files found matching '*.cc' under directory 'extensions'
2025-04-11T03:55:12.0519900Z     warning: no files found matching '*.pyi' under directory 'extensions'
2025-04-11T03:55:12.0520784Z     adding license file 'LICENSE'
2025-04-11T03:55:12.0591076Z     writing manifest file 'colossalai.egg-info/SOURCES.txt'
2025-04-11T03:55:12.0593439Z     running build_ext
2025-04-11T03:55:12.0641197Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/cpp_extension.py:425: UserWarning: There are no g++ version bounds defined for CUDA version 12.1
2025-04-11T03:55:12.0642259Z       warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')
2025-04-11T03:55:12.0650589Z     building 'colossalai._C.cpu_adam_x86' extension
2025-04-11T03:55:12.0651755Z     creating /__w/ColossalAI/ColossalAI/build
2025-04-11T03:55:12.0652805Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310
2025-04-11T03:55:12.0653936Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w
2025-04-11T03:55:12.0654741Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI
2025-04-11T03:55:12.0655869Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI
2025-04-11T03:55:12.0656823Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions
2025-04-11T03:55:12.0657834Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc
2025-04-11T03:55:12.0658827Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel
2025-04-11T03:55:12.0659831Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86
2025-04-11T03:55:12.0906169Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:55:12.0906850Z     Compiling objects...
2025-04-11T03:55:12.0908078Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:55:36.9671563Z     [1/1] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -std=c++14 -std=c++17 -lcudart -lcublas -g -Wno-reorder -fopenmp -march=native -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=cpu_adam_x86 -D_GLIBCXX_USE_CXX11_ABI=0
2025-04-11T03:55:36.9676369Z     /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:237: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
2025-04-11T03:55:36.9676985Z       237 | #pragma unroll 4
2025-04-11T03:55:36.9677318Z           |
2025-04-11T03:55:36.9677839Z     /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:352: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
2025-04-11T03:55:36.9678459Z       352 | #pragma unroll 8
2025-04-11T03:55:36.9678875Z           |
2025-04-11T03:55:36.9679414Z     In file included from /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/Exceptions.h:14,
2025-04-11T03:55:36.9680233Z                      from /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/python.h:11,
2025-04-11T03:55:36.9681254Z                      from /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/extension.h:9,
2025-04-11T03:55:36.9682003Z                      from /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.h:29,
2025-04-11T03:55:36.9682651Z                      from /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:22:
2025-04-11T03:55:36.9685218Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<Adam_Optimizer>’:
2025-04-11T03:55:36.9686079Z     /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:443:51:   required from here
2025-04-11T03:55:36.9688742Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/pybind11/pybind11.h:1496:7: warning: ‘pybind11::class_<Adam_Optimizer>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]
2025-04-11T03:55:36.9689781Z      1496 | class class_ : public detail::generic_type {
2025-04-11T03:55:36.9690712Z           |       ^~~~~~
2025-04-11T03:55:36.9765980Z     creating build/lib.linux-x86_64-cpython-310
2025-04-11T03:55:36.9766958Z     creating build/lib.linux-x86_64-cpython-310/colossalai
2025-04-11T03:55:36.9767978Z     creating build/lib.linux-x86_64-cpython-310/colossalai/_C
2025-04-11T03:55:36.9770938Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/cpu_adam_x86.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:55:37.3348095Z     building 'colossalai._C.layernorm_cuda' extension
2025-04-11T03:55:37.3349051Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda
2025-04-11T03:55:37.3349883Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind
2025-04-11T03:55:37.3350755Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm
2025-04-11T03:55:37.3610963Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:55:37.3612521Z     Compiling objects...
2025-04-11T03:55:37.3613761Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:55:53.0685838Z     [1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/usr/local/cuda/include -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=layernorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:56:31.3045024Z     [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.o.d -I/usr/local/cuda/include -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -maxrregcount=50 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=layernorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:56:31.3087869Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/layernorm_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:56:31.6252122Z     building 'colossalai._C.moe_cuda' extension
2025-04-11T03:56:31.6253984Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe
2025-04-11T03:56:31.6517364Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:56:31.6518811Z     Compiling objects...
2025-04-11T03:56:31.6519871Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:56:47.1568758Z     [1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=moe_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:59:20.0665435Z     [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=moe_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:59:20.0707127Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/moe_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:59:20.2954691Z     building 'colossalai._C.fused_optim_cuda' extension
2025-04-11T03:59:20.2956019Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer
2025-04-11T03:59:20.3215918Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:59:20.3217479Z     Compiling objects...
2025-04-11T03:59:20.3218767Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:59:36.2535013Z     [1/6] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:00:10.4836939Z     [2/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:00:11.2566841Z     [3/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:00:11.3424825Z     [4/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:00:11.5807319Z     [5/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:00:12.2878778Z     [6/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:00:12.2922645Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T04:00:12.5420082Z     building 'colossalai._C.inference_ops_cuda' extension
2025-04-11T04:00:12.5421735Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference
2025-04-11T04:00:12.5679659Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T04:00:12.5681276Z     Compiling objects...
2025-04-11T04:00:12.5682506Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T04:00:28.7208056Z     [1/9] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:02:43.3804163Z     [2/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:02:45.5637022Z     [3/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:02:45.7376745Z     [4/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:02:47.4247096Z     [5/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:02:49.1440020Z     [6/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:02:49.4249552Z     [7/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:02:56.6445216Z     [8/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:06:55.9979662Z     [9/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:06:56.0020611Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/inference_ops_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T04:06:56.4372809Z     building 'colossalai._C.scaled_masked_softmax_cuda' extension
2025-04-11T04:06:56.4374030Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax
2025-04-11T04:06:56.4635546Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T04:06:56.4636668Z     Compiling objects...
2025-04-11T04:06:56.4637976Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T04:07:12.0010886Z     [1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:07:41.6651724Z     [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -std=c++14 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90
2025-04-11T04:07:41.6656781Z     nvcc warning : incompatible redefinition for option 'std', the last value of this option was used
2025-04-11T04:07:41.6690445Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T04:07:41.8897432Z     building 'colossalai._C.scaled_upper_triangle_masked_softmax_cuda' extension
2025-04-11T04:07:41.9157146Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T04:07:41.9158601Z     Compiling objects...
2025-04-11T04:07:41.9159865Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T04:07:57.0156955Z     [1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_upper_triangle_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:10:18.6077207Z     [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_upper_triangle_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T04:10:18.6113778Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_upper_triangle_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T04:10:18.8358019Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/cpu_adam_x86.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T04:10:18.8447807Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/layernorm_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T04:10:18.8461492Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/moe_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T04:10:18.8483500Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T04:10:18.8516549Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/inference_ops_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T04:10:18.9026661Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T04:10:18.9032699Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_upper_triangle_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T04:10:18.9050944Z     Creating /opt/conda/envs/pytorch/lib/python3.10/site-packages/colossalai.egg-link (link to .)
2025-04-11T04:10:18.9057004Z     Adding colossalai 0.4.9 to easy-install.pth file
2025-04-11T04:10:18.9070156Z     Installing colossalai script to /opt/conda/envs/pytorch/bin
2025-04-11T04:10:18.9073150Z 
2025-04-11T04:10:18.9074520Z     Installed /__w/ColossalAI/ColossalAI
2025-04-11T04:10:20.5250717Z Successfully installed accelerate-1.6.0 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 bcrypt-4.3.0 beautifulsoup4-4.13.3 bitsandbytes-0.45.5 cffi-1.17.1 cfgv-3.4.0 colossalai-0.4.9 contexttimer-0.3.3 cryptography-44.0.2 decorator-5.2.1 deprecated-1.2.18 diffusers-0.29.0 distlib-0.3.9 einops-0.8.1 exceptiongroup-1.2.2 fabric-3.2.2 fastapi-0.115.12 frozenlist-1.5.0 galore_torch-1.0 google-3.0.0 h11-0.14.0 huggingface-hub-0.30.2 identify-2.6.9 importlib-metadata-8.6.1 invoke-2.2.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 markdown-it-py-3.0.0 mdurl-0.1.2 msgpack-1.1.0 nodeenv-1.9.1 paramiko-3.5.1 peft-0.13.2 platformdirs-4.3.7 plumbum-1.9.0 pre-commit-4.2.0 protobuf-6.30.2 psutil-7.0.0 pycparser-2.22 pydantic-2.11.3 pydantic-core-2.33.1 pygments-2.19.1 pynacl-1.5.0 ray-2.44.1 referencing-0.36.2 regex-2024.11.6 rich-14.0.0 rpds-py-0.24.0 rpyc-6.0.0 safetensors-0.5.3 sentencepiece-0.2.0 sniffio-1.3.1 soupsieve-2.6 starlette-0.46.1 tokenizers-0.15.2 tqdm-4.67.1 transformers-4.39.3 typing-extensions-4.13.2 typing-inspection-0.4.0 uvicorn-0.29.0 virtualenv-20.30.0 wrapt-1.17.2 zipp-3.21.0
2025-04-11T04:10:20.5254136Z WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-11T04:10:21.3154200Z Collecting git+https://github.com/hpcaitech/pytest-testmon (from -r requirements/requirements-test.txt (line 3))
2025-04-11T04:10:21.3158697Z   Cloning https://github.com/hpcaitech/pytest-testmon to /tmp/pip-req-build-pyk829u3
2025-04-11T04:10:21.3171268Z   Running command git clone --filter=blob:none --quiet https://github.com/hpcaitech/pytest-testmon /tmp/pip-req-build-pyk829u3
2025-04-11T04:10:23.5389870Z   Resolved https://github.com/hpcaitech/pytest-testmon to commit be30f4ac384656daae1a9157e6de97825caaf0ba
2025-04-11T04:10:23.5418345Z   Installing build dependencies: started
2025-04-11T04:10:27.5185908Z   Installing build dependencies: finished with status 'done'
2025-04-11T04:10:27.5191480Z   Getting requirements to build wheel: started
2025-04-11T04:10:27.6804635Z   Getting requirements to build wheel: finished with status 'done'
2025-04-11T04:10:27.6811273Z   Preparing metadata (pyproject.toml): started
2025-04-11T04:10:27.8413753Z   Preparing metadata (pyproject.toml): finished with status 'done'
2025-04-11T04:10:28.3473354Z Collecting pytest (from -r requirements/requirements-test.txt (line 1))
2025-04-11T04:10:28.6518501Z   Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)
2025-04-11T04:10:29.2132160Z Collecting coverage==7.2.3 (from -r requirements/requirements-test.txt (line 2))
2025-04-11T04:10:29.5386550Z   Downloading coverage-7.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)
2025-04-11T04:10:29.6113884Z Requirement already satisfied: torchvision in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 4)) (0.17.2)
2025-04-11T04:10:29.7970959Z Collecting timm (from -r requirements/requirements-test.txt (line 5))
2025-04-11T04:10:29.8882401Z   Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)
2025-04-11T04:10:29.9616205Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.0/52.0 kB 733.8 kB/s eta 0:00:00
2025-04-11T04:10:30.3056771Z Collecting titans (from -r requirements/requirements-test.txt (line 6))
2025-04-11T04:10:30.3978514Z   Downloading titans-0.0.7.tar.gz (38 kB)
2025-04-11T04:10:30.4812000Z   Preparing metadata (setup.py): started
2025-04-11T04:10:30.6264842Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T04:10:30.6274846Z Requirement already satisfied: torchaudio>=0.13.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 7)) (2.2.2)
2025-04-11T04:10:30.9606245Z Collecting torchx-nightly==2022.6.29 (from -r requirements/requirements-test.txt (line 8))
2025-04-11T04:10:31.0521473Z   Downloading torchx_nightly-2022.6.29-py3-none-any.whl.metadata (4.9 kB)
2025-04-11T04:10:31.4614637Z Collecting torchrec==0.2.0 (from -r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:31.5539497Z   Downloading torchrec-0.2.0-py39-none-any.whl.metadata (6.7 kB)
2025-04-11T04:10:31.5547953Z Requirement already satisfied: contexttimer in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 10)) (0.3.3)
2025-04-11T04:10:31.5551041Z Requirement already satisfied: einops in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 11)) (0.8.1)
2025-04-11T04:10:31.5553424Z Requirement already satisfied: triton in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 12)) (2.2.0)
2025-04-11T04:10:31.7447673Z Collecting requests==2.27.1 (from -r requirements/requirements-test.txt (line 13))
2025-04-11T04:10:31.8359438Z   Downloading requests-2.27.1-py2.py3-none-any.whl.metadata (5.0 kB)
2025-04-11T04:10:31.9082959Z Requirement already satisfied: SentencePiece in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 14)) (0.2.0)
2025-04-11T04:10:31.9085494Z Requirement already satisfied: ninja in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 15)) (1.11.1.1)
2025-04-11T04:10:32.0859641Z Collecting flash_attn (from -r requirements/requirements-test.txt (line 16))
2025-04-11T04:10:32.1785454Z   Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)
2025-04-11T04:10:32.7604569Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 10.6 MB/s eta 0:00:00
2025-04-11T04:10:33.8010027Z   Preparing metadata (setup.py): started
2025-04-11T04:10:38.2260778Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T04:10:38.4166654Z Collecting datasets (from -r requirements/requirements-test.txt (line 17))
2025-04-11T04:10:38.5081101Z   Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:38.5806097Z Requirement already satisfied: pydantic in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 18)) (2.11.3)
2025-04-11T04:10:38.5808878Z Requirement already satisfied: ray in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 19)) (2.44.1)
2025-04-11T04:10:38.5814325Z Requirement already satisfied: peft>=0.7.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 20)) (0.13.2)
2025-04-11T04:10:38.7770378Z Collecting pyre-extensions (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T04:10:38.8691809Z   Downloading pyre_extensions-0.0.32-py3-none-any.whl.metadata (4.0 kB)
2025-04-11T04:10:38.9719432Z Collecting docstring-parser==0.8.1 (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T04:10:39.0665238Z   Downloading docstring_parser-0.8.1.tar.gz (14 kB)
2025-04-11T04:10:39.1426609Z   Installing build dependencies: started
2025-04-11T04:10:41.5126536Z   Installing build dependencies: finished with status 'done'
2025-04-11T04:10:41.5131010Z   Getting requirements to build wheel: started
2025-04-11T04:10:41.6446271Z   Getting requirements to build wheel: finished with status 'done'
2025-04-11T04:10:41.6452457Z   Preparing metadata (pyproject.toml): started
2025-04-11T04:10:41.7794990Z   Preparing metadata (pyproject.toml): finished with status 'done'
2025-04-11T04:10:41.7805257Z Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8)) (6.0.1)
2025-04-11T04:10:41.9668476Z Collecting docker (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T04:10:42.0581540Z   Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)
2025-04-11T04:10:42.1303563Z Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8)) (3.13.1)
2025-04-11T04:10:42.1306133Z Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8)) (2024.6.1)
2025-04-11T04:10:42.2540579Z Collecting arrow (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:42.3451337Z   Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)
2025-04-11T04:10:42.3459388Z Requirement already satisfied: attrs in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (25.3.0)
2025-04-11T04:10:42.3462105Z Requirement already satisfied: certifi in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2024.6.2)
2025-04-11T04:10:42.3464629Z Requirement already satisfied: charset-normalizer in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2.0.4)
2025-04-11T04:10:42.6240701Z Collecting cmake (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:42.9386288Z   Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)
2025-04-11T04:10:43.4579187Z Collecting Cython (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:43.5486862Z   Downloading Cython-3.0.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)
2025-04-11T04:10:43.7922666Z Collecting distro (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:43.8834214Z   Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)
2025-04-11T04:10:44.4425105Z Collecting hypothesis (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:44.5342346Z   Downloading hypothesis-6.131.0-py3-none-any.whl.metadata (5.6 kB)
2025-04-11T04:10:44.6065506Z Requirement already satisfied: idna in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (3.7)
2025-04-11T04:10:44.7778315Z Collecting iopath (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:44.8696202Z   Downloading iopath-0.1.10.tar.gz (42 kB)
2025-04-11T04:10:44.9425526Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 564.4 kB/s eta 0:00:00
2025-04-11T04:10:44.9503601Z   Preparing metadata (setup.py): started
2025-04-11T04:10:45.0992193Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T04:10:45.1001263Z Requirement already satisfied: Jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (3.1.4)
2025-04-11T04:10:45.1005696Z Requirement already satisfied: MarkupSafe in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2.1.3)
2025-04-11T04:10:45.1983723Z Collecting mypy-extensions (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:45.2898551Z   Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)
2025-04-11T04:10:45.2905896Z Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (1.26.4)
2025-04-11T04:10:45.2908586Z Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (24.1)
2025-04-11T04:10:45.5674213Z Collecting pandas (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:45.6589146Z   Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)
2025-04-11T04:10:45.7316644Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.9/89.9 kB 1.4 MB/s eta 0:00:00
2025-04-11T04:10:45.8387244Z Collecting portalocker (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:45.9306886Z   Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)
2025-04-11T04:10:46.2334955Z Collecting pyarrow (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:46.5422533Z   Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)
2025-04-11T04:10:46.9536836Z Collecting pyDeprecate (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:47.0453775Z   Downloading pyDeprecate-0.3.2-py3-none-any.whl.metadata (10 kB)
2025-04-11T04:10:47.3063955Z Collecting pyparsing (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:47.3969974Z   Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)
2025-04-11T04:10:47.4028126Z Collecting pyre-extensions (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T04:10:47.4949494Z   Downloading pyre_extensions-0.0.27-py3-none-any.whl.metadata (4.0 kB)
2025-04-11T04:10:47.7103965Z Collecting python-dateutil (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:47.8021744Z   Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
2025-04-11T04:10:48.0029654Z Collecting pytz (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:48.0942101Z   Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
2025-04-11T04:10:48.3438669Z Collecting scikit-build (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:48.4359160Z   Downloading scikit_build-0.18.1-py3-none-any.whl.metadata (18 kB)
2025-04-11T04:10:48.6810277Z Collecting six (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:48.7723214Z   Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
2025-04-11T04:10:48.9465477Z Collecting sortedcontainers (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:49.0379818Z   Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)
2025-04-11T04:10:49.1378244Z Collecting tabulate (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:49.2288617Z   Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)
2025-04-11T04:10:49.4856797Z Collecting torchmetrics (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:49.7935621Z   Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)
2025-04-11T04:10:49.8662628Z Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (4.67.1)
2025-04-11T04:10:50.0366198Z Collecting typing-inspect (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:50.1278551Z   Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)
2025-04-11T04:10:50.1997857Z Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (4.13.2)
2025-04-11T04:10:50.2000575Z Requirement already satisfied: urllib3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2.2.2)
2025-04-11T04:10:50.6018520Z Collecting usort (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:50.6941421Z   Downloading usort-1.0.8.post1-py3-none-any.whl.metadata (3.5 kB)
2025-04-11T04:10:50.8759574Z Collecting websocket-client (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:50.9670514Z   Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)
2025-04-11T04:10:51.3590859Z Collecting fbgemm-gpu (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:51.6746264Z   Downloading fbgemm_gpu-1.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (2.8 kB)
2025-04-11T04:10:51.8817706Z Collecting urllib3 (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:10:51.9730533Z   Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)
2025-04-11T04:10:52.0458893Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.1/50.1 kB 714.3 kB/s eta 0:00:00
2025-04-11T04:10:52.0615217Z Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pytest->-r requirements/requirements-test.txt (line 1)) (1.2.2)
2025-04-11T04:10:52.2319548Z Collecting iniconfig (from pytest->-r requirements/requirements-test.txt (line 1))
2025-04-11T04:10:52.3232349Z   Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)
2025-04-11T04:10:52.5007103Z Collecting pluggy<2,>=1.5 (from pytest->-r requirements/requirements-test.txt (line 1))
2025-04-11T04:10:52.5922454Z   Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)
2025-04-11T04:10:52.7718102Z Collecting tomli>=1 (from pytest->-r requirements/requirements-test.txt (line 1))
2025-04-11T04:10:52.8628313Z   Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)
2025-04-11T04:10:52.8822123Z Collecting pytest (from -r requirements/requirements-test.txt (line 1))
2025-04-11T04:10:52.9732529Z   Downloading pytest-7.4.4-py3-none-any.whl.metadata (7.9 kB)
2025-04-11T04:10:53.1980182Z Collecting requirements-parser (from pytest-testmon==2.0.7b1->-r requirements/requirements-test.txt (line 3))
2025-04-11T04:10:53.2906087Z   Downloading requirements_parser-0.11.0-py3-none-any.whl.metadata (4.7 kB)
2025-04-11T04:10:53.3386619Z Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision->-r requirements/requirements-test.txt (line 4)) (2.2.2)
2025-04-11T04:10:53.3391545Z Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision->-r requirements/requirements-test.txt (line 4)) (10.3.0)
2025-04-11T04:10:53.3432313Z Requirement already satisfied: huggingface_hub in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm->-r requirements/requirements-test.txt (line 5)) (0.30.2)
2025-04-11T04:10:53.3434990Z Requirement already satisfied: safetensors in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm->-r requirements/requirements-test.txt (line 5)) (0.5.3)
2025-04-11T04:10:53.3467115Z Requirement already satisfied: colossalai in /__w/ColossalAI/ColossalAI (from titans->-r requirements/requirements-test.txt (line 6)) (0.4.9)
2025-04-11T04:10:53.5588547Z Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:10:53.6495300Z   Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)
2025-04-11T04:10:53.6554485Z INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.
2025-04-11T04:10:53.6594451Z Collecting datasets (from -r requirements/requirements-test.txt (line 17))
2025-04-11T04:10:53.7517686Z   Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:53.9990853Z   Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:54.2453480Z   Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:54.4909401Z   Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:54.7356243Z   Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:54.9807215Z   Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)
2025-04-11T04:10:55.2269664Z   Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)
2025-04-11T04:10:55.3762843Z INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.
2025-04-11T04:10:55.4719691Z   Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)
2025-04-11T04:10:55.7147049Z   Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)
2025-04-11T04:10:55.9582224Z   Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:56.1992485Z   Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)
2025-04-11T04:10:56.4567883Z   Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:56.7606467Z Collecting pyarrow-hotfix (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:10:56.8530110Z   Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)
2025-04-11T04:10:56.8539065Z INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
2025-04-11T04:10:56.8576477Z Collecting datasets (from -r requirements/requirements-test.txt (line 17))
2025-04-11T04:10:56.9494465Z   Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:57.1890892Z   Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)
2025-04-11T04:10:57.7832975Z Collecting xxhash (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:10:57.8754009Z   Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
2025-04-11T04:10:58.0570498Z Collecting multiprocess (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:10:58.1493085Z   Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)
2025-04-11T04:10:58.3385918Z Collecting fsspec (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T04:10:58.4304523Z   Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)
2025-04-11T04:10:59.1870516Z Collecting aiohttp (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:10:59.2792079Z   Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
2025-04-11T04:10:59.2870740Z Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic->-r requirements/requirements-test.txt (line 18)) (0.7.0)
2025-04-11T04:10:59.2875439Z Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic->-r requirements/requirements-test.txt (line 18)) (2.33.1)
2025-04-11T04:10:59.2882849Z Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic->-r requirements/requirements-test.txt (line 18)) (0.4.0)
2025-04-11T04:10:59.4469784Z Requirement already satisfied: click>=7.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (8.1.8)
2025-04-11T04:10:59.4474789Z Requirement already satisfied: jsonschema in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (4.23.0)
2025-04-11T04:10:59.4479486Z Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (1.1.0)
2025-04-11T04:10:59.4484509Z Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (6.30.2)
2025-04-11T04:10:59.4487543Z Requirement already satisfied: aiosignal in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (1.3.2)
2025-04-11T04:10:59.4489776Z Requirement already satisfied: frozenlist in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (1.5.0)
2025-04-11T04:10:59.4636786Z Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (7.0.0)
2025-04-11T04:10:59.4640923Z Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (4.39.3)
2025-04-11T04:10:59.4646098Z Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (1.6.0)
2025-04-11T04:10:59.7585282Z Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:10:59.8498721Z   Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)
2025-04-11T04:11:00.0254904Z Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:11:00.1164554Z   Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)
2025-04-11T04:11:00.5647061Z Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:11:00.6566907Z   Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)
2025-04-11T04:11:00.8648959Z Collecting propcache>=0.2.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:11:00.9571437Z   Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
2025-04-11T04:11:01.4767651Z Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:11:01.5685044Z   Downloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)
2025-04-11T04:11:01.6411722Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.8/71.8 kB 1.1 MB/s eta 0:00:00
2025-04-11T04:11:01.8182247Z Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->torchvision->-r requirements/requirements-test.txt (line 4)) (1.12)
2025-04-11T04:11:01.8184786Z Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->torchvision->-r requirements/requirements-test.txt (line 4)) (3.2.1)
2025-04-11T04:11:01.9732616Z Collecting types-python-dateutil>=2.8.10 (from arrow->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:11:02.0638000Z   Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)
2025-04-11T04:11:02.1683817Z Requirement already satisfied: pre-commit in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.2.0)
2025-04-11T04:11:02.1686243Z Requirement already satisfied: rich in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (14.0.0)
2025-04-11T04:11:02.1689252Z Requirement already satisfied: fabric in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.2.2)
2025-04-11T04:11:02.1695634Z Requirement already satisfied: google in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.0.0)
2025-04-11T04:11:02.1702572Z Requirement already satisfied: bitsandbytes>=0.39.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.45.5)
2025-04-11T04:11:02.1707770Z Requirement already satisfied: rpyc==6.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (6.0.0)
2025-04-11T04:11:02.1710366Z Requirement already satisfied: fastapi in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.115.12)
2025-04-11T04:11:02.1714648Z Requirement already satisfied: uvicorn==0.29.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.29.0)
2025-04-11T04:11:02.1718662Z Requirement already satisfied: galore_torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.0)
2025-04-11T04:11:02.1722625Z Requirement already satisfied: diffusers==0.29.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.29.0)
2025-04-11T04:11:02.4540769Z Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (2024.11.6)
2025-04-11T04:11:02.4548447Z Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (0.15.2)
2025-04-11T04:11:02.4958831Z Requirement already satisfied: importlib-metadata in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (8.6.1)
2025-04-11T04:11:02.5028156Z Requirement already satisfied: plumbum in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rpyc==6.0.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.9.0)
2025-04-11T04:11:02.5156678Z Requirement already satisfied: h11>=0.8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from uvicorn==0.29.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.14.0)
2025-04-11T04:11:02.7201404Z Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonschema->ray->-r requirements/requirements-test.txt (line 19)) (2024.10.1)
2025-04-11T04:11:02.7205769Z Requirement already satisfied: referencing>=0.28.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonschema->ray->-r requirements/requirements-test.txt (line 19)) (0.36.2)
2025-04-11T04:11:02.7210414Z Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonschema->ray->-r requirements/requirements-test.txt (line 19)) (0.24.0)
2025-04-11T04:11:02.7292086Z INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.
2025-04-11T04:11:02.7331189Z Collecting multiprocess (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T04:11:03.0425959Z   Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)
2025-04-11T04:11:03.2340690Z Collecting tzdata>=2022.7 (from pandas->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:11:03.3253945Z   Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)
2025-04-11T04:11:03.5984734Z Collecting types-setuptools>=69.1.0 (from requirements-parser->pytest-testmon==2.0.7b1->-r requirements/requirements-test.txt (line 3))
2025-04-11T04:11:03.6910322Z   Downloading types_setuptools-78.1.0.20250329-py3-none-any.whl.metadata (2.2 kB)
2025-04-11T04:11:03.7841628Z Requirement already satisfied: setuptools>=42.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-build->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (69.5.1)
2025-04-11T04:11:03.7847221Z Requirement already satisfied: wheel>=0.32.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-build->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (0.43.0)
2025-04-11T04:11:03.9893760Z Collecting lightning-utilities>=0.8.0 (from torchmetrics->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:11:04.0809316Z   Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)
2025-04-11T04:11:04.3144044Z Collecting LibCST>=0.3.7 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:11:04.4062952Z   Downloading libcst-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)
2025-04-11T04:11:04.8938443Z Collecting moreorless>=0.3.0 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:11:04.9859938Z   Downloading moreorless-0.4.0-py2.py3-none-any.whl.metadata (1.5 kB)
2025-04-11T04:11:05.3075143Z Collecting stdlibs>=2021.4.1 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:11:05.3992032Z   Downloading stdlibs-2025.4.4-py3-none-any.whl.metadata (5.0 kB)
2025-04-11T04:11:05.5714899Z Collecting toml>=0.10.0 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:11:05.8816912Z   Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)
2025-04-11T04:11:06.7648732Z Collecting trailrunner>=1.0 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:11:06.8571177Z   Downloading trailrunner-1.4.0-py3-none-any.whl.metadata (4.2 kB)
2025-04-11T04:11:07.2604265Z Collecting pathspec>=0.8.1 (from trailrunner>=1.0->usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T04:11:07.3517742Z   Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)
2025-04-11T04:11:07.4982322Z Requirement already satisfied: invoke>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.2.0)
2025-04-11T04:11:07.4986862Z Requirement already satisfied: paramiko>=2.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.5.1)
2025-04-11T04:11:07.4991155Z Requirement already satisfied: decorator>=5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (5.2.1)
2025-04-11T04:11:07.4995329Z Requirement already satisfied: deprecated>=1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.2.18)
2025-04-11T04:11:07.5214934Z Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fastapi->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.46.1)
2025-04-11T04:11:07.5406867Z Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.13.3)
2025-04-11T04:11:07.5726991Z Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.4.0)
2025-04-11T04:11:07.5731431Z Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.6.9)
2025-04-11T04:11:07.5735928Z Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.9.1)
2025-04-11T04:11:07.5741237Z Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (20.30.0)
2025-04-11T04:11:07.5867096Z Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.0.0)
2025-04-11T04:11:07.5871812Z Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.19.1)
2025-04-11T04:11:07.5974954Z Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->torchvision->-r requirements/requirements-test.txt (line 4)) (1.3.0)
2025-04-11T04:11:07.6278396Z Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from deprecated>=1.2->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.17.2)
2025-04-11T04:11:07.6785332Z Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.1.2)
2025-04-11T04:11:07.7182679Z Requirement already satisfied: bcrypt>=3.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.3.0)
2025-04-11T04:11:07.7187269Z Requirement already satisfied: cryptography>=3.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (44.0.2)
2025-04-11T04:11:07.7191910Z Requirement already satisfied: pynacl>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.5.0)
2025-04-11T04:11:07.7548705Z Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from starlette<0.47.0,>=0.40.0->fastapi->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.9.0)
2025-04-11T04:11:07.7793081Z Requirement already satisfied: distlib<1,>=0.3.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.3.9)
2025-04-11T04:11:07.7799007Z Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.3.7)
2025-04-11T04:11:07.7936597Z Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from beautifulsoup4->google->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.6)
2025-04-11T04:11:07.8164326Z Requirement already satisfied: zipp>=3.20 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.29.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.21.0)
2025-04-11T04:11:07.8585322Z Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.3.1)
2025-04-11T04:11:07.8948743Z Requirement already satisfied: cffi>=1.12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.17.1)
2025-04-11T04:11:07.9940526Z Requirement already satisfied: pycparser in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.22)
2025-04-11T04:11:08.1183335Z Downloading coverage-7.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)
2025-04-11T04:11:08.2080716Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.0/228.0 kB 3.0 MB/s eta 0:00:00
2025-04-11T04:11:08.3003800Z Downloading torchx_nightly-2022.6.29-py3-none-any.whl (177 kB)
2025-04-11T04:11:08.3735472Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.8/177.8 kB 2.3 MB/s eta 0:00:00
2025-04-11T04:11:08.4656049Z Downloading torchrec-0.2.0-py39-none-any.whl (293 kB)
2025-04-11T04:11:08.5392478Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 293.4/293.4 kB 4.0 MB/s eta 0:00:00
2025-04-11T04:11:08.6308321Z Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)
2025-04-11T04:11:08.7034368Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.1/63.1 kB 744.5 kB/s eta 0:00:00
2025-04-11T04:11:08.7962083Z Downloading pyre_extensions-0.0.27-py3-none-any.whl (12 kB)
2025-04-11T04:11:08.9603333Z Downloading pytest-7.4.4-py3-none-any.whl (325 kB)
2025-04-11T04:11:09.0330535Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 325.3/325.3 kB 4.4 MB/s eta 0:00:00
2025-04-11T04:11:09.1254336Z Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)
2025-04-11T04:11:09.3459310Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 10.7 MB/s eta 0:00:00
2025-04-11T04:11:09.6539991Z Downloading datasets-2.19.1-py3-none-any.whl (542 kB)
2025-04-11T04:11:09.7280319Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 7.5 MB/s eta 0:00:00
2025-04-11T04:11:09.8192315Z Downloading dill-0.3.8-py3-none-any.whl (116 kB)
2025-04-11T04:11:09.8921257Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 1.5 MB/s eta 0:00:00
2025-04-11T04:11:09.9844096Z Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)
2025-04-11T04:11:10.0572088Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 2.3 MB/s eta 0:00:00
2025-04-11T04:11:10.1481746Z Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)
2025-04-11T04:11:10.2829564Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 11.9 MB/s eta 0:00:00
2025-04-11T04:11:10.3744359Z Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)
2025-04-11T04:11:10.5389724Z Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)
2025-04-11T04:11:13.2290151Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 MB 18.7 MB/s eta 0:00:00
2025-04-11T04:11:13.3203730Z Downloading tomli-2.2.1-py3-none-any.whl (14 kB)
2025-04-11T04:11:13.6292993Z Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)
2025-04-11T04:11:13.7018358Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 144.2/144.2 kB 1.9 MB/s eta 0:00:00
2025-04-11T04:11:13.7927183Z Downloading arrow-1.3.0-py3-none-any.whl (66 kB)
2025-04-11T04:11:13.8657279Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 kB 786.9 kB/s eta 0:00:00
2025-04-11T04:11:13.9570009Z Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
2025-04-11T04:11:14.0299012Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 3.1 MB/s eta 0:00:00
2025-04-11T04:11:14.1211723Z Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)
2025-04-11T04:11:14.2147637Z Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)
2025-04-11T04:11:15.5604003Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 27.9/27.9 MB 22.3 MB/s eta 0:00:00
2025-04-11T04:11:15.6535098Z Downloading Cython-3.0.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
2025-04-11T04:11:15.8718120Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 16.7 MB/s eta 0:00:00
2025-04-11T04:11:15.9636594Z Downloading distro-1.9.0-py3-none-any.whl (20 kB)
2025-04-11T04:11:16.1261303Z Downloading docker-7.1.0-py3-none-any.whl (147 kB)
2025-04-11T04:11:16.1995556Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.8/147.8 kB 1.9 MB/s eta 0:00:00
2025-04-11T04:11:16.2913525Z Downloading fbgemm_gpu-1.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (417.2 MB)
2025-04-11T04:11:33.2186116Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 417.2/417.2 MB 26.7 MB/s eta 0:00:00
2025-04-11T04:11:33.3120285Z Downloading hypothesis-6.131.0-py3-none-any.whl (495 kB)
2025-04-11T04:11:33.3844044Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 495.7/495.7 kB 6.9 MB/s eta 0:00:00
2025-04-11T04:11:33.4756585Z Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)
2025-04-11T04:11:33.6387427Z Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)
2025-04-11T04:11:33.7309311Z Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)
2025-04-11T04:11:33.8037030Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 1.7 MB/s eta 0:00:00
2025-04-11T04:11:33.8949928Z Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)
2025-04-11T04:11:33.9880668Z Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)
2025-04-11T04:11:34.4506523Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 28.4 MB/s eta 0:00:00
2025-04-11T04:11:34.5414028Z Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)
2025-04-11T04:11:34.6143092Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 509.2/509.2 kB 7.0 MB/s eta 0:00:00
2025-04-11T04:11:34.7064152Z Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)
2025-04-11T04:11:34.8735860Z Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)
2025-04-11T04:11:34.9656278Z Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)
2025-04-11T04:11:35.1294425Z Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)
2025-04-11T04:11:35.2021526Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111.1/111.1 kB 1.4 MB/s eta 0:00:00
2025-04-11T04:11:35.2947727Z Downloading requirements_parser-0.11.0-py3-none-any.whl (14 kB)
2025-04-11T04:11:35.3871215Z Downloading scikit_build-0.18.1-py3-none-any.whl (85 kB)
2025-04-11T04:11:35.4596939Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 1.1 MB/s eta 0:00:00
2025-04-11T04:11:35.5511272Z Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)
2025-04-11T04:11:35.7136016Z Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)
2025-04-11T04:11:35.7871797Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 961.5/961.5 kB 13.3 MB/s eta 0:00:00
2025-04-11T04:11:35.8784866Z Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)
2025-04-11T04:11:35.9715952Z Downloading usort-1.0.8.post1-py3-none-any.whl (37 kB)
2025-04-11T04:11:36.1336285Z Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)
2025-04-11T04:11:36.2063399Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 682.5 kB/s eta 0:00:00
2025-04-11T04:11:36.2982398Z Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)
2025-04-11T04:11:36.3709457Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 2.6 MB/s eta 0:00:00
2025-04-11T04:11:36.4624171Z Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)
2025-04-11T04:11:36.6245610Z Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)
2025-04-11T04:11:36.8349988Z Downloading libcst-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)
2025-04-11T04:11:36.9798461Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 16.2 MB/s eta 0:00:00
2025-04-11T04:11:37.0710846Z Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)
2025-04-11T04:11:37.2338591Z Downloading moreorless-0.4.0-py2.py3-none-any.whl (9.3 kB)
2025-04-11T04:11:37.3249872Z Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)
2025-04-11T04:11:37.3977263Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 219.8/219.8 kB 3.0 MB/s eta 0:00:00
2025-04-11T04:11:37.4898630Z Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)
2025-04-11T04:11:37.5625893Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.6/206.6 kB 2.8 MB/s eta 0:00:00
2025-04-11T04:11:37.6543936Z Downloading stdlibs-2025.4.4-py3-none-any.whl (57 kB)
2025-04-11T04:11:37.7273057Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.2/57.2 kB 657.8 kB/s eta 0:00:00
2025-04-11T04:11:37.8185256Z Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)
2025-04-11T04:11:37.9811375Z Downloading trailrunner-1.4.0-py3-none-any.whl (11 kB)
2025-04-11T04:11:38.1437555Z Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)
2025-04-11T04:11:38.3072375Z Downloading types_setuptools-78.1.0.20250329-py3-none-any.whl (66 kB)
2025-04-11T04:11:38.3799040Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 798.2 kB/s eta 0:00:00
2025-04-11T04:11:38.4705015Z Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)
2025-04-11T04:11:38.5434028Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 347.8/347.8 kB 4.7 MB/s eta 0:00:00
2025-04-11T04:11:38.6345761Z Downloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)
2025-04-11T04:11:38.7082241Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 334.0/334.0 kB 4.5 MB/s eta 0:00:00
2025-04-11T04:11:38.7991712Z Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)
2025-04-11T04:11:39.4430663Z Building wheels for collected packages: docstring-parser, pytest-testmon, titans, flash_attn, iopath
2025-04-11T04:11:39.4441574Z   Building wheel for docstring-parser (pyproject.toml): started
2025-04-11T04:11:39.6000726Z   Building wheel for docstring-parser (pyproject.toml): finished with status 'done'
2025-04-11T04:11:39.6005274Z   Created wheel for docstring-parser: filename=docstring_parser-0.8.1-py3-none-any.whl size=19697 sha256=e200f555702049d4b6e66e10ffef729c660ad58ac03df5cc4d3e50a4739c3989
2025-04-11T04:11:39.6006379Z   Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/26/e4/54/64439f1d0c5d3721041ddc0f001e4b57756a394880a2af8981
2025-04-11T04:11:39.6025715Z   Building wheel for pytest-testmon (pyproject.toml): started
2025-04-11T04:11:39.7819751Z   Building wheel for pytest-testmon (pyproject.toml): finished with status 'done'
2025-04-11T04:11:39.7823897Z   Created wheel for pytest-testmon: filename=pytest_testmon-2.0.7b1-py3-none-any.whl size=35044 sha256=3ec1c2943987eee4e37f71553f8a3ec3f4b057719fd1287d03993a8cfa2d02d1
2025-04-11T04:11:39.7825034Z   Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/cb/f6/db/609602674f7b7c7ecbfd91b3d67b1ea34fe598f7e344495c44
2025-04-11T04:11:39.7841986Z   Building wheel for titans (setup.py): started
2025-04-11T04:11:40.0118156Z   Building wheel for titans (setup.py): finished with status 'done'
2025-04-11T04:11:40.0121535Z   Created wheel for titans: filename=titans-0.0.7-py3-none-any.whl size=63319 sha256=51438bdb24a0ed111e84a5cf71094315c38422b0a7d97c45adedf6277b3086a7
2025-04-11T04:11:40.0122759Z   Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/65/21/1b/3dfb7cdd10cdc650f08fbb72b6f28dd75e1e9b7b42f6695a16
2025-04-11T04:11:40.0138842Z   Building wheel for flash_attn (setup.py): started
2025-04-11T04:11:54.7386699Z   Building wheel for flash_attn (setup.py): finished with status 'done'
2025-04-11T04:11:54.8960583Z   Created wheel for flash_attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187696268 sha256=1776769f7ae3a8be3b31ec3a4c875ad1764da74be2d9b1751e5c01162ad0096f
2025-04-11T04:11:54.8963005Z   Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca
2025-04-11T04:11:54.8987247Z   Building wheel for iopath (setup.py): started
2025-04-11T04:11:55.1013058Z   Building wheel for iopath (setup.py): finished with status 'done'
2025-04-11T04:11:55.1016265Z   Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=5176f3f57e1853d850a4cae931e4621391ba12acb872d984ae182fd093813c96
2025-04-11T04:11:55.1017416Z   Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d
2025-04-11T04:11:55.1029755Z Successfully built docstring-parser pytest-testmon titans flash_attn iopath
2025-04-11T04:11:55.6518770Z Installing collected packages: sortedcontainers, pytz, xxhash, websocket-client, urllib3, tzdata, types-setuptools, types-python-dateutil, tomli, toml, tabulate, stdlibs, six, pyparsing, pyDeprecate, pyarrow-hotfix, pyarrow, propcache, portalocker, pluggy, pathspec, mypy-extensions, multidict, moreorless, lightning-utilities, LibCST, iniconfig, hypothesis, fsspec, fbgemm-gpu, docstring-parser, distro, dill, Cython, coverage, cmake, async-timeout, aiohappyeyeballs, yarl, typing-inspect, trailrunner, scikit-build, requirements-parser, requests, python-dateutil, pytest, multiprocess, iopath, usort, torchmetrics, pytest-testmon, pyre-extensions, pandas, flash_attn, docker, arrow, aiohttp, torchx-nightly, timm, torchrec, datasets, titans
2025-04-11T04:11:55.7706742Z   Attempting uninstall: urllib3
2025-04-11T04:11:55.7714878Z     Found existing installation: urllib3 2.2.2
2025-04-11T04:11:55.7749183Z     Uninstalling urllib3-2.2.2:
2025-04-11T04:11:55.7824540Z       Successfully uninstalled urllib3-2.2.2
2025-04-11T04:11:57.7132626Z   Attempting uninstall: fsspec
2025-04-11T04:11:57.7140416Z     Found existing installation: fsspec 2024.6.1
2025-04-11T04:11:57.7182290Z     Uninstalling fsspec-2024.6.1:
2025-04-11T04:11:57.7276556Z       Successfully uninstalled fsspec-2024.6.1
2025-04-11T04:12:05.4112865Z   Attempting uninstall: requests
2025-04-11T04:12:05.4118647Z     Found existing installation: requests 2.32.2
2025-04-11T04:12:05.4140929Z     Uninstalling requests-2.32.2:
2025-04-11T04:12:05.4182324Z       Successfully uninstalled requests-2.32.2
2025-04-11T04:12:13.3894804Z Successfully installed Cython-3.0.12 LibCST-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 arrow-1.3.0 async-timeout-5.0.1 cmake-4.0.0 coverage-7.2.3 datasets-2.19.1 dill-0.3.8 distro-1.9.0 docker-7.1.0 docstring-parser-0.8.1 fbgemm-gpu-1.1.0 flash_attn-2.7.4.post1 fsspec-2024.3.1 hypothesis-6.131.0 iniconfig-2.1.0 iopath-0.1.10 lightning-utilities-0.14.3 moreorless-0.4.0 multidict-6.4.3 multiprocess-0.70.16 mypy-extensions-1.0.0 pandas-2.2.3 pathspec-0.12.1 pluggy-1.5.0 portalocker-3.1.1 propcache-0.3.1 pyDeprecate-0.3.2 pyarrow-19.0.1 pyarrow-hotfix-0.6 pyparsing-3.2.3 pyre-extensions-0.0.27 pytest-7.4.4 pytest-testmon-2.0.7b1 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.27.1 requirements-parser-0.11.0 scikit-build-0.18.1 six-1.17.0 sortedcontainers-2.4.0 stdlibs-2025.4.4 tabulate-0.9.0 timm-1.0.15 titans-0.0.7 toml-0.10.2 tomli-2.2.1 torchmetrics-1.7.1 torchrec-0.2.0 torchx-nightly-2022.6.29 trailrunner-1.4.0 types-python-dateutil-2.9.0.20241206 types-setuptools-78.1.0.20250329 typing-inspect-0.9.0 tzdata-2025.2 urllib3-1.26.20 usort-1.0.8.post1 websocket-client-1.8.0 xxhash-3.5.0 yarl-1.19.0
2025-04-11T04:12:13.3898305Z WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-11T04:12:14.4672534Z ##[group]Run # -p flag is required to preserve the file timestamp to avoid ninja rebuild
2025-04-11T04:12:14.4673069Z [36;1m# -p flag is required to preserve the file timestamp to avoid ninja rebuild[0m
2025-04-11T04:12:14.4673527Z [36;1mcp -p -r /__w/ColossalAI/ColossalAI/build /github/home/cuda_ext_cache/[0m
2025-04-11T04:12:14.4674022Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T04:12:14.4674304Z ##[endgroup]
2025-04-11T04:12:14.6717037Z ##[group]Run CURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \
2025-04-11T04:12:14.6717494Z [36;1mCURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \[0m
2025-04-11T04:12:14.6717825Z [36;1m-m "not largedist" \[0m
2025-04-11T04:12:14.6718066Z [36;1m--durations=0 \[0m
2025-04-11T04:12:14.6718310Z [36;1m--ignore tests/test_analyzer \[0m
2025-04-11T04:12:14.6718593Z [36;1m--ignore tests/test_auto_parallel \[0m
2025-04-11T04:12:14.6718865Z [36;1m--ignore tests/test_fx \[0m
2025-04-11T04:12:14.6719125Z [36;1m--ignore tests/test_autochunk \[0m
2025-04-11T04:12:14.6719384Z [36;1m--ignore tests/test_gptq \[0m
2025-04-11T04:12:14.6719643Z [36;1m--ignore tests/test_infer_ops \[0m
2025-04-11T04:12:14.6719904Z [36;1m--ignore tests/test_legacy \[0m
2025-04-11T04:12:14.6720170Z [36;1m--ignore tests/test_smoothquant \[0m
2025-04-11T04:12:14.6720418Z [36;1mtests/[0m
2025-04-11T04:12:14.6720755Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T04:12:14.6721070Z env:
2025-04-11T04:12:14.6721410Z   LD_LIBRARY_PATH: /github/home/.tensornvme/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2025-04-11T04:12:14.6721801Z   LLAMA_PATH: /data/scratch/llama-tiny
2025-04-11T04:12:14.6722245Z   MOE_TENSOR_PATH: /data/scratch/moe_tensors
2025-04-11T04:12:14.6722536Z   HF_ENDPOINT: https://hf-mirror.com
2025-04-11T04:12:14.6722793Z ##[endgroup]
2025-04-11T04:12:24.7053686Z ============================= test session starts ==============================
2025-04-11T04:12:24.7054119Z platform linux -- Python 3.10.14, pytest-7.4.4, pluggy-1.5.0
2025-04-11T04:12:24.7054459Z rootdir: /__w/ColossalAI/ColossalAI
2025-04-11T04:12:24.7054749Z configfile: pytest.ini
2025-04-11T04:12:24.7055039Z plugins: hypothesis-6.131.0, anyio-4.9.0, testmon-2.0.7b1
2025-04-11T04:12:24.7055376Z collected 865 items / 23 deselected / 842 selected
2025-04-11T04:12:24.7055579Z 
2025-04-11T04:12:24.9018244Z tests/test_booster/test_accelerator.py F                                 [  0%]
2025-04-11T04:12:24.9024574Z tests/test_booster/test_mixed_precision/test_fp16_torch.py s             [  0%]
2025-04-11T04:12:33.4974055Z tests/test_booster/test_plugin/test_3d_plugin.py F                       [  0%]
2025-04-11T04:12:37.7207506Z tests/test_booster/test_plugin/test_dp_plugin_base.py F                  [  0%]
2025-04-11T04:12:46.4076281Z tests/test_booster/test_plugin/test_gemini_plugin.py F                   [  0%]
2025-04-11T04:12:52.9323131Z tests/test_booster/test_plugin/test_low_level_zero_plugin.py F           [  0%]
2025-04-11T04:12:59.9410352Z tests/test_booster/test_plugin/test_torch_ddp_plugin.py F                [  0%]
2025-04-11T04:13:06.5194356Z tests/test_booster/test_plugin/test_torch_fsdp_plugin.py F               [  0%]
2025-04-11T04:13:15.0019612Z tests/test_checkpoint_io/test_gemini_checkpoint_io.py F                  [  1%]
2025-04-11T04:13:22.5340184Z tests/test_checkpoint_io/test_gemini_torch_compability.py F              [  1%]
2025-04-11T04:13:27.4920291Z tests/test_checkpoint_io/test_general_checkpoint_io.py F..FFFFFF         [  2%]
2025-04-11T04:13:36.2931626Z tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py F  [  2%]
2025-04-11T04:13:36.5921931Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py F          [  2%]
2025-04-11T04:13:44.2187905Z tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py F     [  2%]
2025-04-11T04:13:45.6676900Z tests/test_checkpoint_io/test_safetensors_async_io.py FFFFF              [  3%]
2025-04-11T04:13:51.2937089Z tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py F               [  3%]
2025-04-11T04:13:55.8909001Z tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py F              [  3%]
2025-04-11T04:14:00.6559552Z tests/test_cluster/test_device_mesh_manager.py .                         [  3%]
2025-04-11T04:14:04.8568045Z tests/test_cluster/test_process_group_mesh.py .                          [  3%]
2025-04-11T04:14:04.9701280Z tests/test_config/test_load_config.py .                                  [  3%]
2025-04-11T04:14:04.9706709Z tests/test_device/test_alpha_beta.py s                                   [  3%]
2025-04-11T04:14:10.0923062Z tests/test_device/test_device_mesh.py ..                                 [  4%]
2025-04-11T04:14:10.0928786Z tests/test_device/test_extract_alpha_beta.py s                           [  4%]
2025-04-11T04:14:15.2944012Z tests/test_device/test_init_logical_pg.py F                              [  4%]
2025-04-11T04:14:15.2949520Z tests/test_device/test_search_logical_device_mesh.py s                   [  4%]
2025-04-11T04:14:20.8161352Z tests/test_fp8/test_all_to_all_single.py F                               [  4%]
2025-04-11T04:14:25.3461343Z tests/test_fp8/test_fp8_all_to_all.py F                                  [  4%]
2025-04-11T04:14:29.7709532Z tests/test_fp8/test_fp8_all_to_all_single.py F                           [  4%]
2025-04-11T04:14:35.2929557Z tests/test_fp8/test_fp8_allgather.py F                                   [  4%]
2025-04-11T04:14:40.5670573Z tests/test_fp8/test_fp8_allreduce.py F                                   [  5%]
2025-04-11T04:14:40.9027770Z tests/test_fp8/test_fp8_cast.py F                                        [  5%]
2025-04-11T04:14:48.0862246Z tests/test_fp8/test_fp8_fsdp_comm_hook.py F                              [  5%]
2025-04-11T04:14:48.2924544Z tests/test_fp8/test_fp8_hook.py F                                        [  5%]
2025-04-11T04:14:49.1002393Z tests/test_fp8/test_fp8_linear.py FFFF                                   [  5%]
2025-04-11T04:14:54.9228337Z tests/test_fp8/test_fp8_reduce_scatter.py F                              [  6%]
2025-04-11T04:14:55.1395027Z tests/test_infer/test_batch_bucket.py F                                  [  6%]
2025-04-11T04:14:58.9163249Z tests/test_infer/test_config_and_struct.py .                             [  6%]
2025-04-11T04:15:14.3817640Z tests/test_infer/test_continuous_batching.py F                           [  6%]
2025-04-11T04:15:25.2014596Z tests/test_infer/test_drafter.py FF                                      [  6%]
2025-04-11T04:15:29.3620698Z tests/test_infer/test_kvcache_manager.py .F                              [  6%]
2025-04-11T04:15:33.2856732Z tests/test_infer/test_request_handler.py F                               [  7%]
2025-04-11T04:15:39.2988681Z tests/test_infer/test_streamingllm.py F                                  [  7%]
2025-04-11T04:15:39.4378924Z tests/test_infer/test_async_engine/test_async_engine.py s                [  7%]
2025-04-11T04:15:39.5409123Z tests/test_infer/test_async_engine/test_request_tracer.py .              [  7%]
2025-04-11T04:15:39.5485701Z tests/test_infer/test_kernels/cuda/test_convert_fp8.py sssssssssssssssss [  9%]
2025-04-11T04:15:39.5792601Z ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 17%]
2025-04-11T04:15:39.5873614Z sssssssssssssssssss                                                      [ 20%]
2025-04-11T04:15:39.9633795Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py FF   [ 20%]
2025-04-11T04:15:40.3238908Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py FF            [ 20%]
2025-04-11T04:15:42.7169953Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py FFFFFFFFFFFFF [ 22%]
2025-04-11T04:15:46.9371074Z FFFFFFFFFFFFFFFFFFFFFFF                                                  [ 24%]
2025-04-11T04:15:49.6671891Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py FFFFFFFFFFFFFFF [ 26%]
2025-04-11T04:15:49.8480160Z F                                                                        [ 26%]
2025-04-11T04:15:50.6893297Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py FFFF    [ 27%]
2025-04-11T04:15:51.0549910Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py FF               [ 27%]
2025-04-11T04:15:51.7352079Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py ........ [ 28%]
2025-04-11T04:16:02.6155685Z ........................FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 37%]
2025-04-11T04:16:11.4780180Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                         [ 42%]
2025-04-11T04:16:12.6140278Z tests/test_infer/test_kernels/triton/test_decoding_attn.py sssssssssssss [ 44%]
2025-04-11T04:16:20.9794876Z sssssssssssssssssssssssssssssssssssssssssssssssssssFFFFFFFFFFFFFFFFFFFFF [ 52%]
2025-04-11T04:16:34.2605254Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 61%]
2025-04-11T04:16:47.5659396Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 69%]
2025-04-11T04:16:52.5962536Z FFFFFFFFFFFFFFFFFFFFFFFFFFF                                              [ 73%]
2025-04-11T04:16:52.5968563Z tests/test_infer/test_kernels/triton/test_fused_rotary_embedding.py s    [ 73%]
2025-04-11T04:16:55.2424398Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py FFFFFFFFFFFFFF [ 74%]
2025-04-11T04:17:01.5967765Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                                       [ 78%]
2025-04-11T04:17:01.7845828Z tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py F            [ 79%]
2025-04-11T04:17:02.1914862Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py FF    [ 79%]
2025-04-11T04:17:02.3792424Z tests/test_infer/test_kernels/triton/test_xine_copy.py F                 [ 79%]
2025-04-11T04:17:02.3810577Z tests/test_infer/test_models/test_attention.py ssss                      [ 79%]
2025-04-11T04:17:02.4708322Z tests/test_infer/test_models/test_custom_model.py s                      [ 80%]
2025-04-11T04:17:07.2731329Z tests/test_lazy/test_from_pretrained.py .                                [ 80%]
2025-04-11T04:17:24.8380903Z tests/test_lazy/test_models.py .F                                        [ 80%]
2025-04-11T04:17:25.0779923Z tests/test_lazy/test_ops.py F                                            [ 80%]
2025-04-11T04:17:32.5810933Z tests/test_lora/test_lora.py F                                           [ 80%]
2025-04-11T04:17:32.5817009Z tests/test_moe/test_deepseek_layer.py s                                  [ 80%]
2025-04-11T04:17:33.0624038Z tests/test_moe/test_kernel.py FF                                         [ 80%]
2025-04-11T04:17:33.0630098Z tests/test_moe/test_mixtral_layer.py s                                   [ 81%]
2025-04-11T04:17:39.4381108Z tests/test_moe/test_moe_checkpoint.py F                                  [ 81%]
2025-04-11T04:17:39.4386286Z tests/test_moe/test_moe_ep_tp.py s                                       [ 81%]
2025-04-11T04:17:39.4392910Z tests/test_moe/test_moe_ep_zero.py s                                     [ 81%]
2025-04-11T04:17:44.6190323Z tests/test_optimizer/test_adam_kernel.py FFFFFFFFFFFFFFFFFFFF........... [ 85%]
2025-04-11T04:17:44.7110175Z .                                                                        [ 85%]
2025-04-11T04:17:54.2324735Z tests/test_optimizer/test_adam_optim.py F.FFFF.FFFF.FFFF.FFFF.FFFF.FFF   [ 88%]
2025-04-11T04:18:03.2867421Z tests/test_optimizer/test_dist_adafactor.py F                            [ 88%]
2025-04-11T04:18:12.3188482Z tests/test_optimizer/test_dist_came.py F                                 [ 89%]
2025-04-11T04:18:21.2138564Z tests/test_optimizer/test_dist_galore.py F                               [ 89%]
2025-04-11T04:18:29.5554279Z tests/test_optimizer/test_dist_lamb.py F                                 [ 89%]
2025-04-11T04:18:29.7183865Z tests/test_optimizer/test_lr_scheduler.py .                              [ 89%]
2025-04-11T04:18:29.7190240Z tests/test_optimizer/test_nvme.py s                                      [ 89%]
2025-04-11T04:18:33.8709345Z tests/test_pipeline/test_p2p_communication.py F                          [ 89%]
2025-04-11T04:18:40.2516045Z tests/test_pipeline/test_stage_manager.py F                              [ 89%]
2025-04-11T04:18:40.5212628Z tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py ..     [ 90%]
2025-04-11T04:18:40.7044283Z tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py . [ 90%]
2025-04-11T04:18:40.8065217Z .                                                                        [ 90%]
2025-04-11T04:19:01.0590795Z tests/test_pipeline/test_schedule/test_interleaved.py FFFF               [ 90%]
2025-04-11T04:19:23.0158872Z tests/test_pipeline/test_schedule/test_oneF_oneB.py FFFF                 [ 91%]
2025-04-11T04:19:23.3599708Z tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py ...    [ 91%]
2025-04-11T04:19:28.7028101Z tests/test_pipeline/test_schedule/test_zerobubble_pp.py F                [ 91%]
2025-04-11T04:19:28.9431357Z tests/test_shardformer/test_flash_attention.py F                         [ 91%]
2025-04-11T04:19:29.1888125Z tests/test_shardformer/test_shard_utils.py F                             [ 91%]
2025-04-11T04:19:29.5615723Z tests/test_shardformer/test_with_torch_ddp.py F                          [ 92%]
2025-04-11T04:19:29.7685560Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py F [ 92%]
2025-04-11T04:19:29.7687198Z                                                                          [ 92%]
2025-04-11T04:19:29.9728795Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py F [ 92%]
2025-04-11T04:19:29.9729230Z                                                                          [ 92%]
2025-04-11T04:19:30.1773801Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py F [ 92%]
2025-04-11T04:19:30.1775107Z                                                                          [ 92%]
2025-04-11T04:19:35.4275237Z tests/test_shardformer/test_layer/test_dist_crossentropy.py F            [ 92%]
2025-04-11T04:19:39.6004044Z tests/test_shardformer/test_layer/test_dropout.py F                      [ 92%]
2025-04-11T04:19:43.8541095Z tests/test_shardformer/test_layer/test_embedding.py F                    [ 92%]
2025-04-11T04:19:49.1545311Z tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py F     [ 92%]
2025-04-11T04:19:53.3552443Z tests/test_shardformer/test_layer/test_layernorm.py F                    [ 92%]
2025-04-11T04:19:57.6679075Z tests/test_shardformer/test_layer/test_linear_1d.py F                    [ 93%]
2025-04-11T04:20:01.8960470Z tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py F          [ 93%]
2025-04-11T04:20:11.3312082Z tests/test_shardformer/test_layer/test_ring_attn.py FF                   [ 93%]
2025-04-11T04:20:16.6309850Z tests/test_shardformer/test_layer/test_sequence_parallel.py F            [ 93%]
2025-04-11T04:20:20.8815334Z tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py F  [ 93%]
2025-04-11T04:20:21.1274993Z tests/test_shardformer/test_model/test_shard_bert.py F                   [ 93%]
2025-04-11T04:20:21.3301860Z tests/test_shardformer/test_model/test_shard_blip2.py F                  [ 93%]
2025-04-11T04:20:21.5316016Z tests/test_shardformer/test_model/test_shard_bloom.py F                  [ 94%]
2025-04-11T04:20:21.7328540Z tests/test_shardformer/test_model/test_shard_chatglm2.py F               [ 94%]
2025-04-11T04:20:21.9338276Z tests/test_shardformer/test_model/test_shard_command.py F                [ 94%]
2025-04-11T04:20:30.7694401Z tests/test_shardformer/test_model/test_shard_deepseek.py F               [ 94%]
2025-04-11T04:20:41.8757322Z tests/test_shardformer/test_model/test_shard_deepseek_v3.py F            [ 94%]
2025-04-11T04:20:42.1233336Z tests/test_shardformer/test_model/test_shard_falcon.py F                 [ 94%]
2025-04-11T04:20:42.3249599Z tests/test_shardformer/test_model/test_shard_gpt2.py F                   [ 94%]
2025-04-11T04:20:42.3254770Z tests/test_shardformer/test_model/test_shard_gptj.py s                   [ 94%]
2025-04-11T04:20:42.5259811Z tests/test_shardformer/test_model/test_shard_llama.py F                  [ 95%]
2025-04-11T04:20:42.7264798Z tests/test_shardformer/test_model/test_shard_mistral.py F                [ 95%]
2025-04-11T04:20:48.1881370Z tests/test_shardformer/test_model/test_shard_mixtral.py F                [ 95%]
2025-04-11T04:20:48.4267822Z tests/test_shardformer/test_model/test_shard_opt.py F                    [ 95%]
2025-04-11T04:20:48.6282838Z tests/test_shardformer/test_model/test_shard_qwen2.py F                  [ 95%]
2025-04-11T04:20:48.8310467Z tests/test_shardformer/test_model/test_shard_sam.py F                    [ 95%]
2025-04-11T04:20:49.0316415Z tests/test_shardformer/test_model/test_shard_t5.py F                     [ 95%]
2025-04-11T04:20:49.2323804Z tests/test_shardformer/test_model/test_shard_vit.py F                    [ 95%]
2025-04-11T04:20:49.4337922Z tests/test_shardformer/test_model/test_shard_whisper.py F                [ 95%]
2025-04-11T04:20:54.9206501Z tests/test_tensor/test_comm_spec_apply.py F                              [ 96%]
2025-04-11T04:20:54.9211792Z tests/test_tensor/test_mix_gather.py s                                   [ 96%]
2025-04-11T04:21:00.0951763Z tests/test_tensor/test_padded_tensor.py F                                [ 96%]
2025-04-11T04:21:00.3365567Z tests/test_tensor/test_shape_consistency.py ..                           [ 96%]
2025-04-11T04:21:04.7300366Z tests/test_tensor/test_shape_consistency_apply.py F                      [ 96%]
2025-04-11T04:21:04.8695049Z tests/test_tensor/test_sharding_spec.py .                                [ 96%]
2025-04-11T04:21:10.0368708Z tests/test_tensor/test_dtensor/test_comm_spec.py F                       [ 96%]
2025-04-11T04:21:14.5622855Z tests/test_tensor/test_dtensor/test_dtensor.py F                         [ 97%]
2025-04-11T04:21:14.7010800Z tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py .           [ 97%]
2025-04-11T04:21:29.9140111Z tests/test_tensor/test_dtensor/test_layout_converter.py F                [ 97%]
2025-04-11T04:21:34.1305410Z tests/test_zero/test_gemini/test_chunk_mgrv2.py F                        [ 97%]
2025-04-11T04:21:49.2087855Z tests/test_zero/test_gemini/test_chunkv2.py FFF                          [ 97%]
2025-04-11T04:21:49.2097061Z tests/test_zero/test_gemini/test_gemini_use_rmt.py ss                    [ 97%]
2025-04-11T04:21:55.7792069Z tests/test_zero/test_gemini/test_grad_accum.py F                         [ 98%]
2025-04-11T04:22:08.7688145Z tests/test_zero/test_gemini/test_grad_clip.py FF                         [ 98%]
2025-04-11T04:22:25.0198664Z tests/test_zero/test_gemini/test_inference.py FF                         [ 98%]
2025-04-11T04:22:33.0884598Z tests/test_zero/test_gemini/test_optim.py F                              [ 98%]
2025-04-11T04:22:33.0889632Z tests/test_zero/test_gemini/test_runtime_mem_tracer.py s                 [ 98%]
2025-04-11T04:22:43.4814711Z tests/test_zero/test_gemini/test_search.py FF                            [ 99%]
2025-04-11T04:22:50.8777088Z tests/test_zero/test_gemini/test_zeroddp_state_dict.py F                 [ 99%]
2025-04-11T04:22:50.8786994Z tests/test_zero/test_gemini/test_zerooptim_state_dict.py ss              [ 99%]
2025-04-11T04:22:56.8249763Z tests/test_zero/test_low_level/test_coll_nd.py F                         [ 99%]
2025-04-11T04:23:01.9996927Z tests/test_zero/test_low_level/test_grad_acc.py F                        [ 99%]
2025-04-11T04:23:06.3249251Z tests/test_zero/test_low_level/test_mem_leak.py F                        [ 99%]
2025-04-11T04:23:12.4347018Z tests/test_zero/test_low_level/test_zero1_2.py F                         [ 99%]
2025-04-11T04:23:18.3942175Z tests/test_zero/test_low_level/test_zero_ckpt.py F                       [100%]
2025-04-11T04:23:18.3942503Z 
2025-04-11T04:23:18.3942624Z =================================== FAILURES ===================================
2025-04-11T04:23:18.3943975Z _______________________________ test_accelerator _______________________________
2025-04-11T04:23:18.3944292Z 
2025-04-11T04:23:18.3944404Z args = (), kwargs = {}
2025-04-11T04:23:18.3944549Z 
2025-04-11T04:23:18.3944657Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.3944948Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.3945250Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.3945570Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.3945874Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.3946521Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.3946731Z 
2025-04-11T04:23:18.3946861Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.3947170Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.3947527Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.3947850Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.3948124Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.3948314Z 
2025-04-11T04:23:18.3948395Z device = None
2025-04-11T04:23:18.3948562Z 
2025-04-11T04:23:18.3948710Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.3949060Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.3949375Z     
2025-04-11T04:23:18.3949544Z         Args:
2025-04-11T04:23:18.3949832Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.3950254Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.3950612Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.3951041Z         """
2025-04-11T04:23:18.3951232Z         _lazy_init()
2025-04-11T04:23:18.3951466Z         with torch.cuda.device(device):
2025-04-11T04:23:18.3951748Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.3952043Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.3952554Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.3953078Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.3953462Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.3953705Z 
2025-04-11T04:23:18.3953972Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.3954438Z ________________________________ test_3d_plugin ________________________________
2025-04-11T04:23:18.3954650Z 
2025-04-11T04:23:18.3954757Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.3955550Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.3956236Z 
2025-04-11T04:23:18.3956352Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.3956613Z         try_count = 0
2025-04-11T04:23:18.3956860Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.3957141Z             max_try, int
2025-04-11T04:23:18.3957434Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.3957736Z     
2025-04-11T04:23:18.3957957Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.3958219Z             try:
2025-04-11T04:23:18.3958438Z                 try_count += 1
2025-04-11T04:23:18.3958689Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.3958944Z                 return ret
2025-04-11T04:23:18.3959183Z             except exception_type as e:
2025-04-11T04:23:18.3959448Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.3959816Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.3960200Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.3960547Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.3960963Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.3961288Z                     continue
2025-04-11T04:23:18.3961505Z                 else:
2025-04-11T04:23:18.3961951Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.3962330Z >                   raise e
2025-04-11T04:23:18.3962463Z 
2025-04-11T04:23:18.3962565Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.3962843Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.3963155Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.3963443Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.3963759Z tests/test_booster/test_plugin/test_3d_plugin.py:277: in test_3d_plugin
2025-04-11T04:23:18.3964116Z     spawn(run_dist, 4, early_stop=early_stop)
2025-04-11T04:23:18.3964399Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.3964680Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.3965107Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.3965624Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.3966167Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.3966622Z     while not context.join():
2025-04-11T04:23:18.3966888Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.3967161Z 
2025-04-11T04:23:18.3967373Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8493580>
2025-04-11T04:23:18.3967710Z timeout = None
2025-04-11T04:23:18.3967833Z 
2025-04-11T04:23:18.3967927Z     def join(self, timeout=None):
2025-04-11T04:23:18.3968209Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.3968479Z     
2025-04-11T04:23:18.3968737Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.3969277Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.3969663Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.3970032Z         of the first process exiting.
2025-04-11T04:23:18.3970308Z     
2025-04-11T04:23:18.3970565Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.3970919Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.3971212Z     
2025-04-11T04:23:18.3971407Z         Args:
2025-04-11T04:23:18.3971660Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.3971943Z         """
2025-04-11T04:23:18.3972176Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.3972472Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.3972713Z             return True
2025-04-11T04:23:18.3972926Z     
2025-04-11T04:23:18.3973153Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.3973465Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.3973782Z             self.sentinels.keys(),
2025-04-11T04:23:18.3974044Z             timeout=timeout,
2025-04-11T04:23:18.3974268Z         )
2025-04-11T04:23:18.3974450Z     
2025-04-11T04:23:18.3974624Z         error_index = None
2025-04-11T04:23:18.3974850Z         for sentinel in ready:
2025-04-11T04:23:18.3975108Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.3975386Z             process = self.processes[index]
2025-04-11T04:23:18.3975646Z             process.join()
2025-04-11T04:23:18.3975871Z             if process.exitcode != 0:
2025-04-11T04:23:18.3976120Z                 error_index = index
2025-04-11T04:23:18.3976352Z                 break
2025-04-11T04:23:18.3976544Z     
2025-04-11T04:23:18.3976732Z         # Return if there was no error.
2025-04-11T04:23:18.3976974Z         if error_index is None:
2025-04-11T04:23:18.3977262Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.3977569Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.3977805Z     
2025-04-11T04:23:18.3978137Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.3978447Z         for process in self.processes:
2025-04-11T04:23:18.3978702Z             if process.is_alive():
2025-04-11T04:23:18.3978958Z                 process.terminate()
2025-04-11T04:23:18.3979214Z             process.join()
2025-04-11T04:23:18.3979434Z     
2025-04-11T04:23:18.3979672Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.3980013Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.3980320Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.3980639Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.3980931Z             if exitcode < 0:
2025-04-11T04:23:18.3981182Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.3981476Z                 raise ProcessExitedException(
2025-04-11T04:23:18.3981807Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.3982135Z                     error_index=error_index,
2025-04-11T04:23:18.3982410Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.3982682Z                     exit_code=exitcode,
2025-04-11T04:23:18.3982937Z                     signal_name=name,
2025-04-11T04:23:18.3983271Z                 )
2025-04-11T04:23:18.3983460Z             else:
2025-04-11T04:23:18.3983707Z                 raise ProcessExitedException(
2025-04-11T04:23:18.3984047Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.3984380Z                     error_index=error_index,
2025-04-11T04:23:18.3984644Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.3984897Z                     exit_code=exitcode,
2025-04-11T04:23:18.3985127Z                 )
2025-04-11T04:23:18.3985307Z     
2025-04-11T04:23:18.3985531Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.3985907Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.3986235Z         msg += original_trace
2025-04-11T04:23:18.3986554Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.3986963Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.3987266Z E       
2025-04-11T04:23:18.3987496Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.3987787Z E       Traceback (most recent call last):
2025-04-11T04:23:18.3988249Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.3988761Z E           fn(i, *args)
2025-04-11T04:23:18.3989162Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
2025-04-11T04:23:18.3989626Z E           check_3d_plugin(early_stop=early_stop)
2025-04-11T04:23:18.3990071Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.3990509Z E           partial_func(**kwargs)
2025-04-11T04:23:18.3990948Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
2025-04-11T04:23:18.3991489Z E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:18.3991950Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.3992346Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.3992770Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.3993225Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.3993666Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.3994118Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.3994538Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.3995012Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.3995531Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.3995900Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.3996144Z 
2025-04-11T04:23:18.3996454Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.3997011Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.3997408Z [04/11/25 04:12:32] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.3997774Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.3998090Z                              :75 launch                                         
2025-04-11T04:23:18.3998412Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.3998740Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.3999253Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.3999669Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4001039Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4002414Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4003784Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4005128Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4006480Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4007831Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4009166Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4010486Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4011392Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4012211Z   warnings.warn(
2025-04-11T04:23:18.4013093Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4013924Z   warnings.warn(
2025-04-11T04:23:18.4014714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4015544Z   warnings.warn(
2025-04-11T04:23:18.4016331Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4017155Z   warnings.warn(
2025-04-11T04:23:18.4018102Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4019156Z   warnings.warn(
2025-04-11T04:23:18.4020059Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4021007Z   warnings.warn(
2025-04-11T04:23:18.4021940Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4022888Z   warnings.warn(
2025-04-11T04:23:18.4023799Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4024744Z   warnings.warn(
2025-04-11T04:23:18.4025648Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4026601Z   warnings.warn(
2025-04-11T04:23:18.4027512Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4028487Z   warnings.warn(
2025-04-11T04:23:18.4029405Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4030355Z   warnings.warn(
2025-04-11T04:23:18.4031265Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4032207Z   warnings.warn(
2025-04-11T04:23:18.4033219Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4034185Z   warnings.warn(
2025-04-11T04:23:18.4035086Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4036021Z   warnings.warn(
2025-04-11T04:23:18.4036675Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4037372Z   warnings.warn(
2025-04-11T04:23:18.4038026Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4038815Z   warnings.warn(
2025-04-11T04:23:18.4039735Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4040680Z   warnings.warn(
2025-04-11T04:23:18.4041336Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4042023Z   warnings.warn(
2025-04-11T04:23:18.4042666Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4043357Z   warnings.warn(
2025-04-11T04:23:18.4044267Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4045222Z   warnings.warn(
2025-04-11T04:23:18.4045882Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4046573Z   warnings.warn(
2025-04-11T04:23:18.4047222Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4047898Z   warnings.warn(
2025-04-11T04:23:18.4048560Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4049260Z   warnings.warn(
2025-04-11T04:23:18.4049910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4050594Z   warnings.warn(
2025-04-11T04:23:18.4051250Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4052022Z   warnings.warn(
2025-04-11T04:23:18.4052659Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4053366Z   warnings.warn(
2025-04-11T04:23:18.4053629Z __________________________ test_dp_plugin_dataloader ___________________________
2025-04-11T04:23:18.4053850Z 
2025-04-11T04:23:18.4053945Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4054731Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4055384Z 
2025-04-11T04:23:18.4055498Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4055754Z         try_count = 0
2025-04-11T04:23:18.4055983Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4056239Z             max_try, int
2025-04-11T04:23:18.4056518Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4056806Z     
2025-04-11T04:23:18.4057124Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4057399Z             try:
2025-04-11T04:23:18.4057605Z                 try_count += 1
2025-04-11T04:23:18.4057849Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4058106Z                 return ret
2025-04-11T04:23:18.4058347Z             except exception_type as e:
2025-04-11T04:23:18.4058616Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4058987Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4059374Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4059725Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4060118Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4060439Z                     continue
2025-04-11T04:23:18.4060666Z                 else:
2025-04-11T04:23:18.4061041Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4061423Z >                   raise e
2025-04-11T04:23:18.4061558Z 
2025-04-11T04:23:18.4061664Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4061943Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4062259Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4062555Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4062914Z tests/test_booster/test_plugin/test_dp_plugin_base.py:94: in test_dp_plugin_dataloader
2025-04-11T04:23:18.4063283Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4063521Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4063794Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4064227Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4064742Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4065314Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4065788Z     while not context.join():
2025-04-11T04:23:18.4066056Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4066240Z 
2025-04-11T04:23:18.4066444Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb83b3179a0>
2025-04-11T04:23:18.4066804Z timeout = None
2025-04-11T04:23:18.4066930Z 
2025-04-11T04:23:18.4067024Z     def join(self, timeout=None):
2025-04-11T04:23:18.4067313Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4067591Z     
2025-04-11T04:23:18.4067927Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4068298Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4068747Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4069088Z         of the first process exiting.
2025-04-11T04:23:18.4069329Z     
2025-04-11T04:23:18.4069580Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4069957Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4070257Z     
2025-04-11T04:23:18.4070445Z         Args:
2025-04-11T04:23:18.4070709Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4071017Z         """
2025-04-11T04:23:18.4071258Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4071564Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4071810Z             return True
2025-04-11T04:23:18.4072014Z     
2025-04-11T04:23:18.4072249Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4072566Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4072853Z             self.sentinels.keys(),
2025-04-11T04:23:18.4073209Z             timeout=timeout,
2025-04-11T04:23:18.4073435Z         )
2025-04-11T04:23:18.4073618Z     
2025-04-11T04:23:18.4073803Z         error_index = None
2025-04-11T04:23:18.4074041Z         for sentinel in ready:
2025-04-11T04:23:18.4074312Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4074610Z             process = self.processes[index]
2025-04-11T04:23:18.4074872Z             process.join()
2025-04-11T04:23:18.4075119Z             if process.exitcode != 0:
2025-04-11T04:23:18.4075386Z                 error_index = index
2025-04-11T04:23:18.4075631Z                 break
2025-04-11T04:23:18.4075835Z     
2025-04-11T04:23:18.4076026Z         # Return if there was no error.
2025-04-11T04:23:18.4076290Z         if error_index is None:
2025-04-11T04:23:18.4076588Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4076911Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4077163Z     
2025-04-11T04:23:18.4077411Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4077741Z         for process in self.processes:
2025-04-11T04:23:18.4078009Z             if process.is_alive():
2025-04-11T04:23:18.4078264Z                 process.terminate()
2025-04-11T04:23:18.4078536Z             process.join()
2025-04-11T04:23:18.4078748Z     
2025-04-11T04:23:18.4078998Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4079336Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4079640Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4079963Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4080255Z             if exitcode < 0:
2025-04-11T04:23:18.4080513Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4080818Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4081157Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4081495Z                     error_index=error_index,
2025-04-11T04:23:18.4081774Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4082043Z                     exit_code=exitcode,
2025-04-11T04:23:18.4082299Z                     signal_name=name,
2025-04-11T04:23:18.4082541Z                 )
2025-04-11T04:23:18.4082740Z             else:
2025-04-11T04:23:18.4082968Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4083303Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4083649Z                     error_index=error_index,
2025-04-11T04:23:18.4083926Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4084302Z                     exit_code=exitcode,
2025-04-11T04:23:18.4084541Z                 )
2025-04-11T04:23:18.4084720Z     
2025-04-11T04:23:18.4084954Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4085332Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4085662Z         msg += original_trace
2025-04-11T04:23:18.4085986Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4086410Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4086721Z E       
2025-04-11T04:23:18.4086960Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4087257Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4087731Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4088182Z E           fn(i, *args)
2025-04-11T04:23:18.4088592Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
2025-04-11T04:23:18.4089035Z E           check_dataloader_sharding()
2025-04-11T04:23:18.4089513Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
2025-04-11T04:23:18.4090135Z E           batch = next(iter(train_dataloader))[0].cuda()
2025-04-11T04:23:18.4090439Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4090900Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4091410Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4091789Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4092032Z 
2025-04-11T04:23:18.4092347Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4092913Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4093308Z [04/11/25 04:12:37] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4093684Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4094001Z                              :75 launch                                         
2025-04-11T04:23:18.4094335Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4094678Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4095086Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4095501Z ______________________________ test_gemini_plugin ______________________________
2025-04-11T04:23:18.4095710Z 
2025-04-11T04:23:18.4095818Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4096593Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4097275Z 
2025-04-11T04:23:18.4097383Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4097641Z         try_count = 0
2025-04-11T04:23:18.4097877Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4098136Z             max_try, int
2025-04-11T04:23:18.4098418Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4098703Z     
2025-04-11T04:23:18.4098919Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4099183Z             try:
2025-04-11T04:23:18.4099391Z                 try_count += 1
2025-04-11T04:23:18.4099628Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4099877Z                 return ret
2025-04-11T04:23:18.4100212Z             except exception_type as e:
2025-04-11T04:23:18.4100483Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4100841Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4101219Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4101546Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4101922Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4102248Z                     continue
2025-04-11T04:23:18.4102481Z                 else:
2025-04-11T04:23:18.4102839Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4103221Z >                   raise e
2025-04-11T04:23:18.4103373Z 
2025-04-11T04:23:18.4103471Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4103755Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4104077Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4104371Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4104713Z tests/test_booster/test_plugin/test_gemini_plugin.py:172: in test_gemini_plugin
2025-04-11T04:23:18.4105170Z     spawn(run_dist, 4, early_stop=early_stop)
2025-04-11T04:23:18.4105453Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4105728Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4106170Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4106691Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4107241Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4107702Z     while not context.join():
2025-04-11T04:23:18.4107980Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4108191Z 
2025-04-11T04:23:18.4108459Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84917e0>
2025-04-11T04:23:18.4108830Z timeout = None
2025-04-11T04:23:18.4108951Z 
2025-04-11T04:23:18.4109055Z     def join(self, timeout=None):
2025-04-11T04:23:18.4109354Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4109617Z     
2025-04-11T04:23:18.4109877Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4110250Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4110650Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4110998Z         of the first process exiting.
2025-04-11T04:23:18.4111233Z     
2025-04-11T04:23:18.4111486Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4111844Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4112128Z     
2025-04-11T04:23:18.4112303Z         Args:
2025-04-11T04:23:18.4112547Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4112839Z         """
2025-04-11T04:23:18.4113085Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4113388Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4113662Z             return True
2025-04-11T04:23:18.4113856Z     
2025-04-11T04:23:18.4114086Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4114416Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4114707Z             self.sentinels.keys(),
2025-04-11T04:23:18.4114952Z             timeout=timeout,
2025-04-11T04:23:18.4115160Z         )
2025-04-11T04:23:18.4115334Z     
2025-04-11T04:23:18.4115522Z         error_index = None
2025-04-11T04:23:18.4115754Z         for sentinel in ready:
2025-04-11T04:23:18.4116102Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4116385Z             process = self.processes[index]
2025-04-11T04:23:18.4116651Z             process.join()
2025-04-11T04:23:18.4116887Z             if process.exitcode != 0:
2025-04-11T04:23:18.4117153Z                 error_index = index
2025-04-11T04:23:18.4117387Z                 break
2025-04-11T04:23:18.4117590Z     
2025-04-11T04:23:18.4117790Z         # Return if there was no error.
2025-04-11T04:23:18.4118051Z         if error_index is None:
2025-04-11T04:23:18.4118350Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4118657Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4118901Z     
2025-04-11T04:23:18.4119142Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4119457Z         for process in self.processes:
2025-04-11T04:23:18.4119717Z             if process.is_alive():
2025-04-11T04:23:18.4119957Z                 process.terminate()
2025-04-11T04:23:18.4120204Z             process.join()
2025-04-11T04:23:18.4120421Z     
2025-04-11T04:23:18.4120665Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4121001Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4121413Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4121724Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4122007Z             if exitcode < 0:
2025-04-11T04:23:18.4122270Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4122564Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4122898Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4123218Z                     error_index=error_index,
2025-04-11T04:23:18.4123500Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4123771Z                     exit_code=exitcode,
2025-04-11T04:23:18.4124030Z                     signal_name=name,
2025-04-11T04:23:18.4124269Z                 )
2025-04-11T04:23:18.4124453Z             else:
2025-04-11T04:23:18.4124674Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4125012Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4125348Z                     error_index=error_index,
2025-04-11T04:23:18.4125612Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4125868Z                     exit_code=exitcode,
2025-04-11T04:23:18.4126100Z                 )
2025-04-11T04:23:18.4126286Z     
2025-04-11T04:23:18.4126518Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4126892Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4127214Z         msg += original_trace
2025-04-11T04:23:18.4127534Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4127941Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4128245Z E       
2025-04-11T04:23:18.4128478Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4128768Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4129243Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4129699Z E           fn(i, *args)
2025-04-11T04:23:18.4130112Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
2025-04-11T04:23:18.4130594Z E           check_gemini_plugin(early_stop=early_stop)
2025-04-11T04:23:18.4131041Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4131454Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4131864Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4132359Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4132772Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4133182Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4133452Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.4133935Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
2025-04-11T04:23:18.4134545Z E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
2025-04-11T04:23:18.4135052Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4135468Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4135914Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4136387Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4136844Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4137331Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4137715Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4138198Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4138719Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4139102Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4139338Z 
2025-04-11T04:23:18.4139651Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4140218Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4140621Z [04/11/25 04:12:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4140993Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4141319Z                              :75 launch                                         
2025-04-11T04:23:18.4141654Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4141993Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4142399Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4142828Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4144215Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4145590Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4146999Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4148365Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4149844Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4151202Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4152551Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4153890Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4154823Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4155662Z   warnings.warn(
2025-04-11T04:23:18.4156558Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4157390Z   warnings.warn(
2025-04-11T04:23:18.4158182Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4159013Z   warnings.warn(
2025-04-11T04:23:18.4159816Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4160657Z   warnings.warn(
2025-04-11T04:23:18.4161608Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4162575Z   warnings.warn(
2025-04-11T04:23:18.4163491Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4164445Z   warnings.warn(
2025-04-11T04:23:18.4165363Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4166309Z   warnings.warn(
2025-04-11T04:23:18.4167219Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4168168Z   warnings.warn(
2025-04-11T04:23:18.4169168Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4170113Z   warnings.warn(
2025-04-11T04:23:18.4171023Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4171959Z   warnings.warn(
2025-04-11T04:23:18.4172877Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4173816Z   warnings.warn(
2025-04-11T04:23:18.4174725Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4175762Z   warnings.warn(
2025-04-11T04:23:18.4176676Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4177624Z   warnings.warn(
2025-04-11T04:23:18.4178544Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4179490Z   warnings.warn(
2025-04-11T04:23:18.4180398Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4181346Z   warnings.warn(
2025-04-11T04:23:18.4182257Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4183193Z   warnings.warn(
2025-04-11T04:23:18.4183615Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25212 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4184270Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25212 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4184922Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25212 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4185844Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4186542Z   warnings.warn(
2025-04-11T04:23:18.4187193Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4187880Z   warnings.warn(
2025-04-11T04:23:18.4188733Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4189429Z   warnings.warn(
2025-04-11T04:23:18.4190083Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4190774Z   warnings.warn(
2025-04-11T04:23:18.4191425Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4192115Z   warnings.warn(
2025-04-11T04:23:18.4192772Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4193463Z   warnings.warn(
2025-04-11T04:23:18.4194143Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4194842Z   warnings.warn(
2025-04-11T04:23:18.4195588Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4196269Z   warnings.warn(
2025-04-11T04:23:18.4196931Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4197638Z   warnings.warn(
2025-04-11T04:23:18.4198311Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4199002Z   warnings.warn(
2025-04-11T04:23:18.4199668Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4200366Z   warnings.warn(
2025-04-11T04:23:18.4201010Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4201691Z   warnings.warn(
2025-04-11T04:23:18.4202343Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4203030Z   warnings.warn(
2025-04-11T04:23:18.4203680Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4204360Z   warnings.warn(
2025-04-11T04:23:18.4204634Z __________________________ test_low_level_zero_plugin __________________________
2025-04-11T04:23:18.4204857Z 
2025-04-11T04:23:18.4204957Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4205727Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4206392Z 
2025-04-11T04:23:18.4206502Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4206766Z         try_count = 0
2025-04-11T04:23:18.4207001Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4207270Z             max_try, int
2025-04-11T04:23:18.4207665Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4207956Z     
2025-04-11T04:23:18.4208178Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4208444Z             try:
2025-04-11T04:23:18.4208646Z                 try_count += 1
2025-04-11T04:23:18.4208879Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4209127Z                 return ret
2025-04-11T04:23:18.4209360Z             except exception_type as e:
2025-04-11T04:23:18.4209629Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4209990Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4210361Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4210697Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4211072Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4211384Z                     continue
2025-04-11T04:23:18.4211600Z                 else:
2025-04-11T04:23:18.4211941Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4212406Z >                   raise e
2025-04-11T04:23:18.4212546Z 
2025-04-11T04:23:18.4212645Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4212917Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4213238Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4213525Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4213900Z tests/test_booster/test_plugin/test_low_level_zero_plugin.py:141: in test_low_level_zero_plugin
2025-04-11T04:23:18.4214309Z     spawn(run_dist, 2, early_stop=early_stop)
2025-04-11T04:23:18.4214587Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4214863Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4215297Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4215807Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4216337Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4216790Z     while not context.join():
2025-04-11T04:23:18.4217047Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4217228Z 
2025-04-11T04:23:18.4217433Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb83b36d6c0>
2025-04-11T04:23:18.4217777Z timeout = None
2025-04-11T04:23:18.4217889Z 
2025-04-11T04:23:18.4217989Z     def join(self, timeout=None):
2025-04-11T04:23:18.4218263Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4218530Z     
2025-04-11T04:23:18.4218778Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4219142Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4219528Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4219851Z         of the first process exiting.
2025-04-11T04:23:18.4220095Z     
2025-04-11T04:23:18.4220337Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4220699Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4220985Z     
2025-04-11T04:23:18.4221146Z         Args:
2025-04-11T04:23:18.4221396Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4221681Z         """
2025-04-11T04:23:18.4221921Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4222219Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4222450Z             return True
2025-04-11T04:23:18.4222647Z     
2025-04-11T04:23:18.4222874Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4223294Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4223580Z             self.sentinels.keys(),
2025-04-11T04:23:18.4223818Z             timeout=timeout,
2025-04-11T04:23:18.4224035Z         )
2025-04-11T04:23:18.4224206Z     
2025-04-11T04:23:18.4224388Z         error_index = None
2025-04-11T04:23:18.4224617Z         for sentinel in ready:
2025-04-11T04:23:18.4224868Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4225149Z             process = self.processes[index]
2025-04-11T04:23:18.4225408Z             process.join()
2025-04-11T04:23:18.4225639Z             if process.exitcode != 0:
2025-04-11T04:23:18.4225886Z                 error_index = index
2025-04-11T04:23:18.4226110Z                 break
2025-04-11T04:23:18.4226302Z     
2025-04-11T04:23:18.4226489Z         # Return if there was no error.
2025-04-11T04:23:18.4226739Z         if error_index is None:
2025-04-11T04:23:18.4227017Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4227312Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4227554Z     
2025-04-11T04:23:18.4227791Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4228196Z         for process in self.processes:
2025-04-11T04:23:18.4228502Z             if process.is_alive():
2025-04-11T04:23:18.4228743Z                 process.terminate()
2025-04-11T04:23:18.4228984Z             process.join()
2025-04-11T04:23:18.4229191Z     
2025-04-11T04:23:18.4229425Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4229749Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4230035Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4230341Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4230619Z             if exitcode < 0:
2025-04-11T04:23:18.4230863Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4231150Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4231468Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4231782Z                     error_index=error_index,
2025-04-11T04:23:18.4232062Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4232323Z                     exit_code=exitcode,
2025-04-11T04:23:18.4232579Z                     signal_name=name,
2025-04-11T04:23:18.4232802Z                 )
2025-04-11T04:23:18.4233007Z             else:
2025-04-11T04:23:18.4233225Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4233569Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4233898Z                     error_index=error_index,
2025-04-11T04:23:18.4234147Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4234406Z                     exit_code=exitcode,
2025-04-11T04:23:18.4234635Z                 )
2025-04-11T04:23:18.4234823Z     
2025-04-11T04:23:18.4235050Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4235415Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4235750Z         msg += original_trace
2025-04-11T04:23:18.4236065Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4236462Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4236774Z E       
2025-04-11T04:23:18.4236997Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4237289Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4237758Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4238218Z E           fn(i, *args)
2025-04-11T04:23:18.4238648Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
2025-04-11T04:23:18.4239257Z E           check_low_level_zero_plugin(early_stop=early_stop)
2025-04-11T04:23:18.4239711Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4240157Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4240668Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
2025-04-11T04:23:18.4241237Z E           err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:18.4241681Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4242085Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4242507Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4242949Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4243417Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4243888Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4244270Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4244741Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4245239Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4245614Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4245850Z 
2025-04-11T04:23:18.4246160Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4246698Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4247086Z [04/11/25 04:12:52] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4247445Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4247754Z                              :75 launch                                         
2025-04-11T04:23:18.4248078Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4248414Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4248810Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4249228Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4250606Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4251982Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4253337Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4254705Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4255724Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4256553Z   warnings.warn(
2025-04-11T04:23:18.4257351Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4258183Z   warnings.warn(
2025-04-11T04:23:18.4259124Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4260087Z   warnings.warn(
2025-04-11T04:23:18.4261013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4261956Z   warnings.warn(
2025-04-11T04:23:18.4262874Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4263907Z   warnings.warn(
2025-04-11T04:23:18.4264812Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4265754Z   warnings.warn(
2025-04-11T04:23:18.4266667Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4267610Z   warnings.warn(
2025-04-11T04:23:18.4268545Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4269481Z   warnings.warn(
2025-04-11T04:23:18.4270149Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4270836Z   warnings.warn(
2025-04-11T04:23:18.4271481Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4272180Z   warnings.warn(
2025-04-11T04:23:18.4272852Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4273539Z   warnings.warn(
2025-04-11T04:23:18.4274190Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4274872Z   warnings.warn(
2025-04-11T04:23:18.4275132Z ____________________________ test_torch_ddp_plugin _____________________________
2025-04-11T04:23:18.4275351Z 
2025-04-11T04:23:18.4275536Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4276298Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4276960Z 
2025-04-11T04:23:18.4277067Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4277323Z         try_count = 0
2025-04-11T04:23:18.4277553Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4277811Z             max_try, int
2025-04-11T04:23:18.4278085Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4278375Z     
2025-04-11T04:23:18.4278585Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4278849Z             try:
2025-04-11T04:23:18.4279046Z                 try_count += 1
2025-04-11T04:23:18.4279274Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4279523Z                 return ret
2025-04-11T04:23:18.4279753Z             except exception_type as e:
2025-04-11T04:23:18.4280019Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4280373Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4323232Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4323649Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4324055Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4324391Z                     continue
2025-04-11T04:23:18.4324612Z                 else:
2025-04-11T04:23:18.4324985Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4325374Z >                   raise e
2025-04-11T04:23:18.4325521Z 
2025-04-11T04:23:18.4325628Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4325933Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4326276Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4326571Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4326948Z tests/test_booster/test_plugin/test_torch_ddp_plugin.py:119: in test_torch_ddp_plugin
2025-04-11T04:23:18.4327326Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4327578Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4327859Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4328319Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4328871Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4329422Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4329900Z     while not context.join():
2025-04-11T04:23:18.4330182Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4330369Z 
2025-04-11T04:23:18.4330587Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84938e0>
2025-04-11T04:23:18.4330967Z timeout = None
2025-04-11T04:23:18.4331097Z 
2025-04-11T04:23:18.4331195Z     def join(self, timeout=None):
2025-04-11T04:23:18.4331486Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4331762Z     
2025-04-11T04:23:18.4332019Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4332377Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4332756Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4333090Z         of the first process exiting.
2025-04-11T04:23:18.4333334Z     
2025-04-11T04:23:18.4333586Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4334151Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4334438Z     
2025-04-11T04:23:18.4334618Z         Args:
2025-04-11T04:23:18.4334878Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4335176Z         """
2025-04-11T04:23:18.4335429Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4335724Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4335975Z             return True
2025-04-11T04:23:18.4336173Z     
2025-04-11T04:23:18.4336406Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4336722Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4337011Z             self.sentinels.keys(),
2025-04-11T04:23:18.4337251Z             timeout=timeout,
2025-04-11T04:23:18.4337477Z         )
2025-04-11T04:23:18.4337652Z     
2025-04-11T04:23:18.4337828Z         error_index = None
2025-04-11T04:23:18.4338105Z         for sentinel in ready:
2025-04-11T04:23:18.4338362Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4338647Z             process = self.processes[index]
2025-04-11T04:23:18.4338911Z             process.join()
2025-04-11T04:23:18.4339135Z             if process.exitcode != 0:
2025-04-11T04:23:18.4339507Z                 error_index = index
2025-04-11T04:23:18.4339749Z                 break
2025-04-11T04:23:18.4339947Z     
2025-04-11T04:23:18.4340139Z         # Return if there was no error.
2025-04-11T04:23:18.4340384Z         if error_index is None:
2025-04-11T04:23:18.4340666Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4340971Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4341212Z     
2025-04-11T04:23:18.4341453Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4341757Z         for process in self.processes:
2025-04-11T04:23:18.4342010Z             if process.is_alive():
2025-04-11T04:23:18.4342255Z                 process.terminate()
2025-04-11T04:23:18.4342498Z             process.join()
2025-04-11T04:23:18.4342705Z     
2025-04-11T04:23:18.4342931Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4343262Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4343558Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4343864Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4344137Z             if exitcode < 0:
2025-04-11T04:23:18.4344380Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4344666Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4344989Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4345304Z                     error_index=error_index,
2025-04-11T04:23:18.4345572Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4345826Z                     exit_code=exitcode,
2025-04-11T04:23:18.4346074Z                     signal_name=name,
2025-04-11T04:23:18.4346303Z                 )
2025-04-11T04:23:18.4346493Z             else:
2025-04-11T04:23:18.4346715Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4347059Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4347398Z                     error_index=error_index,
2025-04-11T04:23:18.4347659Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4347927Z                     exit_code=exitcode,
2025-04-11T04:23:18.4348160Z                 )
2025-04-11T04:23:18.4348339Z     
2025-04-11T04:23:18.4348628Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4349006Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4349336Z         msg += original_trace
2025-04-11T04:23:18.4349657Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4350162Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4350469Z E       
2025-04-11T04:23:18.4350698Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4350994Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4351477Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4351934Z E           fn(i, *args)
2025-04-11T04:23:18.4352351Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
2025-04-11T04:23:18.4352801Z E           check_torch_ddp_plugin()
2025-04-11T04:23:18.4353284Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
2025-04-11T04:23:18.4353802Z E           run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:18.4354222Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4354619Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4355044Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4355609Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4356065Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4356540Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4356822Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4357307Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4357805Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4358178Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4358417Z 
2025-04-11T04:23:18.4358730Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4359266Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4359660Z [04/11/25 04:12:59] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4360011Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4360311Z                              :75 launch                                         
2025-04-11T04:23:18.4360627Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4360956Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4361346Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4361760Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4363112Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4364482Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4365822Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4367241Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4368156Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4368995Z   warnings.warn(
2025-04-11T04:23:18.4369786Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4370612Z   warnings.warn(
2025-04-11T04:23:18.4371546Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4372490Z   warnings.warn(
2025-04-11T04:23:18.4373385Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4374404Z   warnings.warn(
2025-04-11T04:23:18.4375318Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4376256Z   warnings.warn(
2025-04-11T04:23:18.4377161Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4378098Z   warnings.warn(
2025-04-11T04:23:18.4379003Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4379970Z   warnings.warn(
2025-04-11T04:23:18.4380878Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4381818Z   warnings.warn(
2025-04-11T04:23:18.4382500Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4383209Z   warnings.warn(
2025-04-11T04:23:18.4383849Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4384528Z   warnings.warn(
2025-04-11T04:23:18.4385182Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4385869Z   warnings.warn(
2025-04-11T04:23:18.4386595Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4387277Z   warnings.warn(
2025-04-11T04:23:18.4387537Z ____________________________ test_torch_fsdp_plugin ____________________________
2025-04-11T04:23:18.4387771Z 
2025-04-11T04:23:18.4387871Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4388678Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4389336Z 
2025-04-11T04:23:18.4389445Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4389703Z         try_count = 0
2025-04-11T04:23:18.4389937Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4390203Z             max_try, int
2025-04-11T04:23:18.4390484Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4390783Z     
2025-04-11T04:23:18.4391000Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4391268Z             try:
2025-04-11T04:23:18.4391467Z                 try_count += 1
2025-04-11T04:23:18.4391809Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4392070Z                 return ret
2025-04-11T04:23:18.4392317Z             except exception_type as e:
2025-04-11T04:23:18.4392581Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4392948Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4393325Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4393652Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4394023Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4394322Z                     continue
2025-04-11T04:23:18.4394534Z                 else:
2025-04-11T04:23:18.4394869Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4395230Z >                   raise e
2025-04-11T04:23:18.4395366Z 
2025-04-11T04:23:18.4395461Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4395723Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4396036Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4396316Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4396660Z tests/test_booster/test_plugin/test_torch_fsdp_plugin.py:83: in test_torch_fsdp_plugin
2025-04-11T04:23:18.4397013Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4397234Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4397500Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4397923Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4398424Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4398944Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4399393Z     while not context.join():
2025-04-11T04:23:18.4399641Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4399824Z 
2025-04-11T04:23:18.4400023Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b838ccd0>
2025-04-11T04:23:18.4400362Z timeout = None
2025-04-11T04:23:18.4400471Z 
2025-04-11T04:23:18.4400568Z     def join(self, timeout=None):
2025-04-11T04:23:18.4400835Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4401097Z     
2025-04-11T04:23:18.4401335Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4401702Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4402198Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4402521Z         of the first process exiting.
2025-04-11T04:23:18.4402746Z     
2025-04-11T04:23:18.4402989Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4403342Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4403613Z     
2025-04-11T04:23:18.4403772Z         Args:
2025-04-11T04:23:18.4404017Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4404292Z         """
2025-04-11T04:23:18.4404532Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4404824Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4405054Z             return True
2025-04-11T04:23:18.4405247Z     
2025-04-11T04:23:18.4405465Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4405778Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4406048Z             self.sentinels.keys(),
2025-04-11T04:23:18.4406282Z             timeout=timeout,
2025-04-11T04:23:18.4406491Z         )
2025-04-11T04:23:18.4406655Z     
2025-04-11T04:23:18.4406929Z         error_index = None
2025-04-11T04:23:18.4407147Z         for sentinel in ready:
2025-04-11T04:23:18.4407395Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4407666Z             process = self.processes[index]
2025-04-11T04:23:18.4407915Z             process.join()
2025-04-11T04:23:18.4408140Z             if process.exitcode != 0:
2025-04-11T04:23:18.4408381Z                 error_index = index
2025-04-11T04:23:18.4408602Z                 break
2025-04-11T04:23:18.4408792Z     
2025-04-11T04:23:18.4408974Z         # Return if there was no error.
2025-04-11T04:23:18.4409218Z         if error_index is None:
2025-04-11T04:23:18.4410918Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4411211Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4411451Z     
2025-04-11T04:23:18.4411681Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4411996Z         for process in self.processes:
2025-04-11T04:23:18.4412247Z             if process.is_alive():
2025-04-11T04:23:18.4412493Z                 process.terminate()
2025-04-11T04:23:18.4412726Z             process.join()
2025-04-11T04:23:18.4412934Z     
2025-04-11T04:23:18.4413163Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4413483Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4413765Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4414065Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4414333Z             if exitcode < 0:
2025-04-11T04:23:18.4414576Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4414852Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4415169Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4415476Z                     error_index=error_index,
2025-04-11T04:23:18.4415737Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4415996Z                     exit_code=exitcode,
2025-04-11T04:23:18.4416233Z                     signal_name=name,
2025-04-11T04:23:18.4416456Z                 )
2025-04-11T04:23:18.4416634Z             else:
2025-04-11T04:23:18.4416839Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4417162Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4417484Z                     error_index=error_index,
2025-04-11T04:23:18.4417737Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4417983Z                     exit_code=exitcode,
2025-04-11T04:23:18.4418206Z                 )
2025-04-11T04:23:18.4418382Z     
2025-04-11T04:23:18.4418718Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4419088Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4419406Z         msg += original_trace
2025-04-11T04:23:18.4419721Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4420119Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4420427Z E       
2025-04-11T04:23:18.4420660Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4420951Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4421424Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4421864Z E           fn(i, *args)
2025-04-11T04:23:18.4422278Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
2025-04-11T04:23:18.4422721Z E           check_torch_fsdp_plugin()
2025-04-11T04:23:18.4423199Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
2025-04-11T04:23:18.4423327Z E           run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:18.4423644Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4423747Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4424004Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4424107Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4424385Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4424491Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4424595Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4424886Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4425026Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4425190Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4425195Z 
2025-04-11T04:23:18.4425510Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4425666Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4425823Z [04/11/25 04:13:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4425952Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4426064Z                              :75 launch                                         
2025-04-11T04:23:18.4426205Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4426330Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4426422Z custom_hanging_param_model
2025-04-11T04:23:18.4426509Z custom_hanging_param_model
2025-04-11T04:23:18.4426714Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4426860Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4427991Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4428156Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4429392Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4429563Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4430251Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4430337Z   warnings.warn(
2025-04-11T04:23:18.4431012Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4431183Z   warnings.warn(
2025-04-11T04:23:18.4432000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4432080Z   warnings.warn(
2025-04-11T04:23:18.4432876Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4432955Z   warnings.warn(
2025-04-11T04:23:18.4433738Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4433823Z   warnings.warn(
2025-04-11T04:23:18.4434608Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4434688Z   warnings.warn(
2025-04-11T04:23:18.4435473Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4435556Z   warnings.warn(
2025-04-11T04:23:18.4436339Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4436427Z   warnings.warn(
2025-04-11T04:23:18.4436976Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4437059Z   warnings.warn(
2025-04-11T04:23:18.4437584Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4437753Z   warnings.warn(
2025-04-11T04:23:18.4438291Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4438375Z   warnings.warn(
2025-04-11T04:23:18.4438899Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4438974Z   warnings.warn(
2025-04-11T04:23:18.4439114Z ______________________________ test_gemini_ckpIO _______________________________
2025-04-11T04:23:18.4439118Z 
2025-04-11T04:23:18.4439209Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4439814Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4439820Z 
2025-04-11T04:23:18.4439924Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4440092Z         try_count = 0
2025-04-11T04:23:18.4440194Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4440278Z             max_try, int
2025-04-11T04:23:18.4440425Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4440498Z     
2025-04-11T04:23:18.4440632Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4440709Z             try:
2025-04-11T04:23:18.4440797Z                 try_count += 1
2025-04-11T04:23:18.4440894Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4440973Z                 return ret
2025-04-11T04:23:18.4441080Z             except exception_type as e:
2025-04-11T04:23:18.4441187Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4441426Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4441567Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4441719Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4441885Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4441964Z                     continue
2025-04-11T04:23:18.4442046Z                 else:
2025-04-11T04:23:18.4442275Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4442361Z >                   raise e
2025-04-11T04:23:18.4442365Z 
2025-04-11T04:23:18.4442458Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4442572Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4442703Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4442792Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4442987Z tests/test_checkpoint_io/test_gemini_checkpoint_io.py:220: in test_gemini_ckpIO
2025-04-11T04:23:18.4443069Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4443176Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4443273Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4443541Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4443719Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4444005Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4444099Z     while not context.join():
2025-04-11T04:23:18.4444205Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4444210Z 
2025-04-11T04:23:18.4444410Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8493940>
2025-04-11T04:23:18.4444576Z timeout = None
2025-04-11T04:23:18.4444581Z 
2025-04-11T04:23:18.4444678Z     def join(self, timeout=None):
2025-04-11T04:23:18.4444804Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4444877Z     
2025-04-11T04:23:18.4445029Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4445172Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4445341Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4445435Z         of the first process exiting.
2025-04-11T04:23:18.4445512Z     
2025-04-11T04:23:18.4445661Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4445801Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4445873Z     
2025-04-11T04:23:18.4445946Z         Args:
2025-04-11T04:23:18.4446090Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4446161Z         """
2025-04-11T04:23:18.4446299Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4446397Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4446565Z             return True
2025-04-11T04:23:18.4446642Z     
2025-04-11T04:23:18.4446774Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4446893Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4446985Z             self.sentinels.keys(),
2025-04-11T04:23:18.4447071Z             timeout=timeout,
2025-04-11T04:23:18.4447147Z         )
2025-04-11T04:23:18.4447216Z     
2025-04-11T04:23:18.4447302Z         error_index = None
2025-04-11T04:23:18.4447388Z         for sentinel in ready:
2025-04-11T04:23:18.4447493Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4447596Z             process = self.processes[index]
2025-04-11T04:23:18.4447680Z             process.join()
2025-04-11T04:23:18.4447780Z             if process.exitcode != 0:
2025-04-11T04:23:18.4447868Z                 error_index = index
2025-04-11T04:23:18.4447942Z                 break
2025-04-11T04:23:18.4448016Z     
2025-04-11T04:23:18.4448109Z         # Return if there was no error.
2025-04-11T04:23:18.4448198Z         if error_index is None:
2025-04-11T04:23:18.4448347Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4448463Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4448533Z     
2025-04-11T04:23:18.4448679Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4448780Z         for process in self.processes:
2025-04-11T04:23:18.4448869Z             if process.is_alive():
2025-04-11T04:23:18.4448965Z                 process.terminate()
2025-04-11T04:23:18.4449048Z             process.join()
2025-04-11T04:23:18.4449118Z     
2025-04-11T04:23:18.4449265Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4449382Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4449493Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4449613Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4449704Z             if exitcode < 0:
2025-04-11T04:23:18.4449807Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4449911Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4450068Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4450162Z                     error_index=error_index,
2025-04-11T04:23:18.4450267Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4450353Z                     exit_code=exitcode,
2025-04-11T04:23:18.4450438Z                     signal_name=name,
2025-04-11T04:23:18.4450519Z                 )
2025-04-11T04:23:18.4450591Z             else:
2025-04-11T04:23:18.4450697Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4450972Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4451073Z                     error_index=error_index,
2025-04-11T04:23:18.4451177Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4451261Z                     exit_code=exitcode,
2025-04-11T04:23:18.4451341Z                 )
2025-04-11T04:23:18.4451410Z     
2025-04-11T04:23:18.4451545Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4451713Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4451797Z         msg += original_trace
2025-04-11T04:23:18.4451968Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4452125Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4452208Z E       
2025-04-11T04:23:18.4452333Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4452441Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4452741Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4452906Z E           fn(i, *args)
2025-04-11T04:23:18.4453191Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
2025-04-11T04:23:18.4453275Z E           exam_state_dict()
2025-04-11T04:23:18.4453501Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4453601Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4453866Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4453965Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4454242Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4454356Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4454463Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4454755Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4454893Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4455056Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4455062Z 
2025-04-11T04:23:18.4455369Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4455524Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4455679Z [04/11/25 04:13:14] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4455809Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4455922Z                              :75 launch                                         
2025-04-11T04:23:18.4456060Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4456195Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4456394Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4456546Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4457690Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4457957Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4459092Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4459267Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4460391Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4460557Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4461665Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4461913Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4462606Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4462696Z   warnings.warn(
2025-04-11T04:23:18.4463383Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4463472Z   warnings.warn(
2025-04-11T04:23:18.4464161Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4464242Z   warnings.warn(
2025-04-11T04:23:18.4464934Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4465017Z   warnings.warn(
2025-04-11T04:23:18.4465831Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4465913Z   warnings.warn(
2025-04-11T04:23:18.4466714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4466791Z   warnings.warn(
2025-04-11T04:23:18.4467663Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4467751Z   warnings.warn(
2025-04-11T04:23:18.4468608Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4468694Z   warnings.warn(
2025-04-11T04:23:18.4469507Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4469586Z   warnings.warn(
2025-04-11T04:23:18.4470387Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4470565Z   warnings.warn(
2025-04-11T04:23:18.4471371Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4471448Z   warnings.warn(
2025-04-11T04:23:18.4472260Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4472336Z   warnings.warn(
2025-04-11T04:23:18.4473148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4473230Z   warnings.warn(
2025-04-11T04:23:18.4474043Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4474121Z   warnings.warn(
2025-04-11T04:23:18.4474936Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4475011Z   warnings.warn(
2025-04-11T04:23:18.4475828Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4475909Z   warnings.warn(
2025-04-11T04:23:18.4476209Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:55733 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4476783Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4476966Z   warnings.warn(
2025-04-11T04:23:18.4477514Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4477596Z   warnings.warn(
2025-04-11T04:23:18.4478140Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4478218Z   warnings.warn(
2025-04-11T04:23:18.4478744Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4478819Z   warnings.warn(
2025-04-11T04:23:18.4479365Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4479440Z   warnings.warn(
2025-04-11T04:23:18.4479961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4480147Z   warnings.warn(
2025-04-11T04:23:18.4480677Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4480759Z   warnings.warn(
2025-04-11T04:23:18.4481284Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4481370Z   warnings.warn(
2025-04-11T04:23:18.4481539Z Exception ignored in: <function GeminiDDP.__del__ at 0x7fc3b46f5f30>
2025-04-11T04:23:18.4481639Z Traceback (most recent call last):
2025-04-11T04:23:18.4481878Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.4481966Z     self.remove_hooks()
2025-04-11T04:23:18.4482218Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.4482320Z     for p in self.module.parameters():
2025-04-11T04:23:18.4482619Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.4482812Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.4482966Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.4483128Z Exception ignored in: <function GeminiDDP.__del__ at 0x7fec640edf30>
2025-04-11T04:23:18.4483225Z Traceback (most recent call last):
2025-04-11T04:23:18.4483460Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.4483543Z     self.remove_hooks()
2025-04-11T04:23:18.4483793Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.4483892Z     for p in self.module.parameters():
2025-04-11T04:23:18.4484187Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.4484374Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.4484523Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.4484689Z Exception ignored in: <function GeminiDDP.__del__ at 0x7fd220b15ea0>
2025-04-11T04:23:18.4484779Z Traceback (most recent call last):
2025-04-11T04:23:18.4485099Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.4485187Z     self.remove_hooks()
2025-04-11T04:23:18.4485427Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.4485530Z     for p in self.module.parameters():
2025-04-11T04:23:18.4485817Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.4486014Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.4486163Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.4486316Z _____________________________ test_gemini_ckpIO[2] _____________________________
2025-04-11T04:23:18.4486321Z 
2025-04-11T04:23:18.4486447Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.4487057Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4487063Z 
2025-04-11T04:23:18.4487164Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4487341Z         try_count = 0
2025-04-11T04:23:18.4487443Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4487524Z             max_try, int
2025-04-11T04:23:18.4487677Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4487749Z     
2025-04-11T04:23:18.4487864Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4487937Z             try:
2025-04-11T04:23:18.4488022Z                 try_count += 1
2025-04-11T04:23:18.4488117Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4488197Z                 return ret
2025-04-11T04:23:18.4488293Z             except exception_type as e:
2025-04-11T04:23:18.4488391Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4488585Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4488702Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4488856Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4489018Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4489100Z                     continue
2025-04-11T04:23:18.4489187Z                 else:
2025-04-11T04:23:18.4489415Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4489504Z >                   raise e
2025-04-11T04:23:18.4489509Z 
2025-04-11T04:23:18.4489603Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4489717Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4489852Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4489940Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4490148Z tests/test_checkpoint_io/test_gemini_torch_compability.py:175: in test_gemini_ckpIO
2025-04-11T04:23:18.4490237Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.4490345Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4490444Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4490699Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4490880Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4491163Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4491257Z     while not context.join():
2025-04-11T04:23:18.4491368Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4491372Z 
2025-04-11T04:23:18.4491664Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb83b36d570>
2025-04-11T04:23:18.4491749Z timeout = None
2025-04-11T04:23:18.4491753Z 
2025-04-11T04:23:18.4491843Z     def join(self, timeout=None):
2025-04-11T04:23:18.4491970Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4492045Z     
2025-04-11T04:23:18.4492195Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4492342Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4492509Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4492602Z         of the first process exiting.
2025-04-11T04:23:18.4492670Z     
2025-04-11T04:23:18.4492822Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4492959Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4493032Z     
2025-04-11T04:23:18.4493105Z         Args:
2025-04-11T04:23:18.4493247Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4493319Z         """
2025-04-11T04:23:18.4493458Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4493642Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4493723Z             return True
2025-04-11T04:23:18.4493795Z     
2025-04-11T04:23:18.4493927Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4494042Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4494136Z             self.sentinels.keys(),
2025-04-11T04:23:18.4494221Z             timeout=timeout,
2025-04-11T04:23:18.4494298Z         )
2025-04-11T04:23:18.4494368Z     
2025-04-11T04:23:18.4494449Z         error_index = None
2025-04-11T04:23:18.4494537Z         for sentinel in ready:
2025-04-11T04:23:18.4494643Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4494745Z             process = self.processes[index]
2025-04-11T04:23:18.4494831Z             process.join()
2025-04-11T04:23:18.4494936Z             if process.exitcode != 0:
2025-04-11T04:23:18.4495025Z                 error_index = index
2025-04-11T04:23:18.4495102Z                 break
2025-04-11T04:23:18.4495182Z     
2025-04-11T04:23:18.4495275Z         # Return if there was no error.
2025-04-11T04:23:18.4495365Z         if error_index is None:
2025-04-11T04:23:18.4495507Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4495610Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4495684Z     
2025-04-11T04:23:18.4495820Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4495919Z         for process in self.processes:
2025-04-11T04:23:18.4496007Z             if process.is_alive():
2025-04-11T04:23:18.4496107Z                 process.terminate()
2025-04-11T04:23:18.4496195Z             process.join()
2025-04-11T04:23:18.4496265Z     
2025-04-11T04:23:18.4496423Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4496537Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4496646Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4496764Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4496852Z             if exitcode < 0:
2025-04-11T04:23:18.4496966Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4497069Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4497221Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4497318Z                     error_index=error_index,
2025-04-11T04:23:18.4497421Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4497508Z                     exit_code=exitcode,
2025-04-11T04:23:18.4497595Z                     signal_name=name,
2025-04-11T04:23:18.4497680Z                 )
2025-04-11T04:23:18.4497755Z             else:
2025-04-11T04:23:18.4497951Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4498118Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4498212Z                     error_index=error_index,
2025-04-11T04:23:18.4498321Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4498406Z                     exit_code=exitcode,
2025-04-11T04:23:18.4498487Z                 )
2025-04-11T04:23:18.4498557Z     
2025-04-11T04:23:18.4498689Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4498860Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4498946Z         msg += original_trace
2025-04-11T04:23:18.4499123Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4499281Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4499360Z E       
2025-04-11T04:23:18.4499490Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4499590Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4499891Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4500059Z E           fn(i, *args)
2025-04-11T04:23:18.4500353Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
2025-04-11T04:23:18.4500448Z E           exam_torch_load_from_gemini()
2025-04-11T04:23:18.4500672Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4500772Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4501035Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4501134Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4501415Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4501523Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4501630Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4501924Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4502057Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4502226Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4502231Z 
2025-04-11T04:23:18.4502538Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4502688Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4502848Z [04/11/25 04:13:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4502981Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4503093Z                              :75 launch                                         
2025-04-11T04:23:18.4503230Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4503362Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4503559Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4503707Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4504933Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4505115Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4506237Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4506409Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4507088Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4507176Z   warnings.warn(
2025-04-11T04:23:18.4507853Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4508043Z   warnings.warn(
2025-04-11T04:23:18.4508908Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4508989Z   warnings.warn(
2025-04-11T04:23:18.4509797Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4509875Z   warnings.warn(
2025-04-11T04:23:18.4510673Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4510756Z   warnings.warn(
2025-04-11T04:23:18.4511553Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4511632Z   warnings.warn(
2025-04-11T04:23:18.4512432Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4512512Z   warnings.warn(
2025-04-11T04:23:18.4513316Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4513393Z   warnings.warn(
2025-04-11T04:23:18.4513950Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4514027Z   warnings.warn(
2025-04-11T04:23:18.4514664Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4514746Z   warnings.warn(
2025-04-11T04:23:18.4515280Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4515365Z   warnings.warn(
2025-04-11T04:23:18.4515889Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4515970Z   warnings.warn(
2025-04-11T04:23:18.4516135Z Exception ignored in: <function GeminiDDP.__del__ at 0x7f4b31341750>
2025-04-11T04:23:18.4516234Z Traceback (most recent call last):
2025-04-11T04:23:18.4516475Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.4516562Z     self.remove_hooks()
2025-04-11T04:23:18.4516804Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.4516994Z     for p in self.module.parameters():
2025-04-11T04:23:18.4517290Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.4517481Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.4517636Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.4517782Z __________________________ test_unsharded_checkpoint ___________________________
2025-04-11T04:23:18.4517786Z 
2025-04-11T04:23:18.4517871Z args = (), kwargs = {}
2025-04-11T04:23:18.4517875Z 
2025-04-11T04:23:18.4517966Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.4518061Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.4518180Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.4518295Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.4518404Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.4518501Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.4518506Z 
2025-04-11T04:23:18.4518605Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.4518719Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4518873Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4518971Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4519082Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4519086Z 
2025-04-11T04:23:18.4519169Z device = None
2025-04-11T04:23:18.4519173Z 
2025-04-11T04:23:18.4519295Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4519461Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4519540Z     
2025-04-11T04:23:18.4519614Z         Args:
2025-04-11T04:23:18.4519787Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4519965Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4520077Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4520150Z         """
2025-04-11T04:23:18.4520233Z         _lazy_init()
2025-04-11T04:23:18.4520326Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4520429Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4520539Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4520832Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4520973Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4521222Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4521227Z 
2025-04-11T04:23:18.4521489Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4521650Z ___________________ test_sharded_model_checkpoint[True-True] ___________________
2025-04-11T04:23:18.4521654Z 
2025-04-11T04:23:18.4521754Z use_safetensors = True, use_async = True
2025-04-11T04:23:18.4521758Z 
2025-04-11T04:23:18.4521910Z     @pytest.mark.parametrize("use_safetensors", [True, False])
2025-04-11T04:23:18.4522044Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4522225Z     def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
2025-04-11T04:23:18.4522316Z         # create a model and optimizer
2025-04-11T04:23:18.4522404Z         model = resnet18()
2025-04-11T04:23:18.4522520Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4522614Z         # create test data sample
2025-04-11T04:23:18.4522704Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4522774Z     
2025-04-11T04:23:18.4522864Z         # run fwd and bwd
2025-04-11T04:23:18.4522940Z         y = model(x)
2025-04-11T04:23:18.4523103Z         loss = y.sum()
2025-04-11T04:23:18.4523186Z         loss.backward()
2025-04-11T04:23:18.4523275Z         optimizer.step()
2025-04-11T04:23:18.4523350Z     
2025-04-11T04:23:18.4523474Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4523625Z         optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T04:23:18.4523696Z     
2025-04-11T04:23:18.4523785Z         # save the model and optimizer
2025-04-11T04:23:18.4523884Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4523953Z     
2025-04-11T04:23:18.4524049Z >       ckpt_io.save_model(
2025-04-11T04:23:18.4524271Z             model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
2025-04-11T04:23:18.4524349Z         )
2025-04-11T04:23:18.4524357Z 
2025-04-11T04:23:18.4524506Z tests/test_checkpoint_io/test_general_checkpoint_io.py:96: 
2025-04-11T04:23:18.4524616Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4524790Z colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
2025-04-11T04:23:18.4524877Z     self.save_sharded_model(
2025-04-11T04:23:18.4525073Z colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
2025-04-11T04:23:18.4525254Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T04:23:18.4525445Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T04:23:18.4525585Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T04:23:18.4525749Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4525989Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4526229Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4526368Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4526608Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4526744Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4526863Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4527092Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4527208Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4527212Z 
2025-04-11T04:23:18.4527365Z tensor = tensor([[[[-7.9533e-03,  2.4479e-03,  2.2101e-02,  ...,  2.2613e-02,
2025-04-11T04:23:18.4527462Z            -7.4241e-03,  4.8188e-02],
2025-04-11T04:23:18.4527541Z           [...594e-02],
2025-04-11T04:23:18.4527760Z           [-3.3366e-02, -1.8918e-02, -3.1868e-02,  ..., -2.9370e-02,
2025-04-11T04:23:18.4527852Z            -1.6090e-02, -4.3804e-02]]]])
2025-04-11T04:23:18.4527934Z empty = True
2025-04-11T04:23:18.4527948Z 
2025-04-11T04:23:18.4528122Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4528200Z         if empty:
2025-04-11T04:23:18.4528359Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4528466Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4528765Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4528906Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4529074Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4529078Z 
2025-04-11T04:23:18.4529205Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4529361Z __________________ test_sharded_model_checkpoint[True-False] ___________________
2025-04-11T04:23:18.4529365Z 
2025-04-11T04:23:18.4529469Z use_safetensors = False, use_async = True
2025-04-11T04:23:18.4529561Z 
2025-04-11T04:23:18.4529709Z     @pytest.mark.parametrize("use_safetensors", [True, False])
2025-04-11T04:23:18.4529842Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4530017Z     def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
2025-04-11T04:23:18.4530111Z         # create a model and optimizer
2025-04-11T04:23:18.4530194Z         model = resnet18()
2025-04-11T04:23:18.4530309Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4530402Z         # create test data sample
2025-04-11T04:23:18.4530489Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4530562Z     
2025-04-11T04:23:18.4530642Z         # run fwd and bwd
2025-04-11T04:23:18.4530718Z         y = model(x)
2025-04-11T04:23:18.4530807Z         loss = y.sum()
2025-04-11T04:23:18.4530892Z         loss.backward()
2025-04-11T04:23:18.4530987Z         optimizer.step()
2025-04-11T04:23:18.4531057Z     
2025-04-11T04:23:18.4531186Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4531331Z         optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T04:23:18.4531400Z     
2025-04-11T04:23:18.4531497Z         # save the model and optimizer
2025-04-11T04:23:18.4531592Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4531667Z     
2025-04-11T04:23:18.4531753Z >       ckpt_io.save_model(
2025-04-11T04:23:18.4531969Z             model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
2025-04-11T04:23:18.4532047Z         )
2025-04-11T04:23:18.4532052Z 
2025-04-11T04:23:18.4532197Z tests/test_checkpoint_io/test_general_checkpoint_io.py:96: 
2025-04-11T04:23:18.4532314Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4532475Z colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
2025-04-11T04:23:18.4532568Z     self.save_sharded_model(
2025-04-11T04:23:18.4532753Z colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
2025-04-11T04:23:18.4532939Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T04:23:18.4533125Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T04:23:18.4533265Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T04:23:18.4533432Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4533659Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4533898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4534137Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4534387Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4534519Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4534639Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4534870Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4534979Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4534983Z 
2025-04-11T04:23:18.4535138Z tensor = tensor([[[[ 1.0525e-02,  3.3759e-02, -3.8922e-03,  ...,  1.3324e-02,
2025-04-11T04:23:18.4535229Z            -1.5804e-03, -4.7921e-03],
2025-04-11T04:23:18.4535308Z           [...171e-03],
2025-04-11T04:23:18.4535431Z           [-3.2414e-03, -5.0440e-02,  3.3628e-02,  ..., -8.8520e-03,
2025-04-11T04:23:18.4535517Z             4.6587e-03,  4.6623e-02]]]])
2025-04-11T04:23:18.4535602Z empty = True
2025-04-11T04:23:18.4535610Z 
2025-04-11T04:23:18.4535781Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4535862Z         if empty:
2025-04-11T04:23:18.4536015Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4536213Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4536498Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4536636Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4536800Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4536805Z 
2025-04-11T04:23:18.4536925Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4537079Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4537249Z [04/11/25 04:13:24] WARNING  colossalai - colossalai - WARNING:                 
2025-04-11T04:23:18.4537377Z                              /__w/ColossalAI/ColossalAI/colossalai/checkpoint_io
2025-04-11T04:23:18.4537501Z                              /checkpoint_io_base.py:183 save_model              
2025-04-11T04:23:18.4537652Z                     WARNING  colossalai - colossalai - WARNING: Async save is   
2025-04-11T04:23:18.4537779Z                              only supported when use_safetensors is set to True.
2025-04-11T04:23:18.4537901Z                              Setting use_safetensors to True for async save.    
2025-04-11T04:23:18.4538059Z ___________________ test_sharded_optimizer_checkpoint[False] ___________________
2025-04-11T04:23:18.4538063Z 
2025-04-11T04:23:18.4538141Z use_async = False
2025-04-11T04:23:18.4538145Z 
2025-04-11T04:23:18.4538281Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4538413Z     def test_sharded_optimizer_checkpoint(use_async: bool):
2025-04-11T04:23:18.4538513Z         # create a model and optimizer
2025-04-11T04:23:18.4538600Z         model = resnet18()
2025-04-11T04:23:18.4538715Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4538792Z     
2025-04-11T04:23:18.4538882Z         # create test data sample
2025-04-11T04:23:18.4538974Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4539046Z     
2025-04-11T04:23:18.4539125Z         # run fwd and bwd
2025-04-11T04:23:18.4539206Z         y = model(x)
2025-04-11T04:23:18.4539283Z         loss = y.sum()
2025-04-11T04:23:18.4539367Z         loss.backward()
2025-04-11T04:23:18.4539451Z         optimizer.step()
2025-04-11T04:23:18.4539526Z     
2025-04-11T04:23:18.4539638Z         # create temp directories for checkpoint
2025-04-11T04:23:18.4539760Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4539895Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4539964Z     
2025-04-11T04:23:18.4540058Z         # save the model and optimizer
2025-04-11T04:23:18.4540248Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4540328Z     
2025-04-11T04:23:18.4540539Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T04:23:18.4540813Z         ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4540886Z     
2025-04-11T04:23:18.4540971Z         ckpt_io._sync_d2h()
2025-04-11T04:23:18.4541057Z         ckpt_io._sync_io()
2025-04-11T04:23:18.4541131Z     
2025-04-11T04:23:18.4541210Z         # create new model
2025-04-11T04:23:18.4541321Z         new_model = resnet18()
2025-04-11T04:23:18.4541453Z         new_optimizer = Adam(new_model.parameters(), lr=0.001)
2025-04-11T04:23:18.4541526Z     
2025-04-11T04:23:18.4541680Z         ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
2025-04-11T04:23:18.4541837Z >       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T04:23:18.4541845Z 
2025-04-11T04:23:18.4541999Z tests/test_checkpoint_io/test_general_checkpoint_io.py:149: 
2025-04-11T04:23:18.4542112Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4542382Z colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
2025-04-11T04:23:18.4542477Z     self.load_sharded_optimizer(
2025-04-11T04:23:18.4542677Z colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
2025-04-11T04:23:18.4542814Z     load_states_into_optimizer(optimizer, state_dict, id_map)
2025-04-11T04:23:18.4542983Z colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
2025-04-11T04:23:18.4543083Z     get_accelerator().synchronize()
2025-04-11T04:23:18.4543236Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4543334Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4543443Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4543447Z 
2025-04-11T04:23:18.4543530Z device = None
2025-04-11T04:23:18.4543534Z 
2025-04-11T04:23:18.4543653Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4543806Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4543884Z     
2025-04-11T04:23:18.4543958Z         Args:
2025-04-11T04:23:18.4544131Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4544300Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4544415Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4544488Z         """
2025-04-11T04:23:18.4544567Z         _lazy_init()
2025-04-11T04:23:18.4544664Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4544766Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4544876Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4545167Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4545308Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4545467Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4545472Z 
2025-04-11T04:23:18.4545714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4545875Z ___________________ test_sharded_optimizer_checkpoint[True] ____________________
2025-04-11T04:23:18.4545880Z 
2025-04-11T04:23:18.4545959Z use_async = True
2025-04-11T04:23:18.4545963Z 
2025-04-11T04:23:18.4546097Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4546227Z     def test_sharded_optimizer_checkpoint(use_async: bool):
2025-04-11T04:23:18.4546325Z         # create a model and optimizer
2025-04-11T04:23:18.4546411Z         model = resnet18()
2025-04-11T04:23:18.4546610Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4546689Z     
2025-04-11T04:23:18.4546779Z         # create test data sample
2025-04-11T04:23:18.4546872Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4546945Z     
2025-04-11T04:23:18.4547030Z         # run fwd and bwd
2025-04-11T04:23:18.4547105Z         y = model(x)
2025-04-11T04:23:18.4547181Z         loss = y.sum()
2025-04-11T04:23:18.4547264Z         loss.backward()
2025-04-11T04:23:18.4547348Z         optimizer.step()
2025-04-11T04:23:18.4547420Z     
2025-04-11T04:23:18.4547522Z         # create temp directories for checkpoint
2025-04-11T04:23:18.4547643Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4547777Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4547845Z     
2025-04-11T04:23:18.4547942Z         # save the model and optimizer
2025-04-11T04:23:18.4548038Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4548107Z     
2025-04-11T04:23:18.4548318Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T04:23:18.4548619Z >       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4548717Z 
2025-04-11T04:23:18.4548874Z tests/test_checkpoint_io/test_general_checkpoint_io.py:139: 
2025-04-11T04:23:18.4548986Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4549162Z colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
2025-04-11T04:23:18.4549254Z     self.save_sharded_optimizer(
2025-04-11T04:23:18.4549453Z colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
2025-04-11T04:23:18.4549639Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T04:23:18.4549828Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T04:23:18.4549977Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T04:23:18.4550139Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4550384Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4550629Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4550775Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4551016Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4551148Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4551272Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4551499Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4551617Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4551621Z 
2025-04-11T04:23:18.4551713Z tensor = tensor(1.), empty = True
2025-04-11T04:23:18.4551718Z 
2025-04-11T04:23:18.4551891Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4551973Z         if empty:
2025-04-11T04:23:18.4552134Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4552240Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4552544Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4552687Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4552845Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4552849Z 
2025-04-11T04:23:18.4552970Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4553235Z _____________ test_sharded_optimizer_multiple_param_groups[False] ______________
2025-04-11T04:23:18.4553240Z 
2025-04-11T04:23:18.4553325Z use_async = False
2025-04-11T04:23:18.4553329Z 
2025-04-11T04:23:18.4553463Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4553626Z     def test_sharded_optimizer_multiple_param_groups(use_async: bool):
2025-04-11T04:23:18.4553721Z         # create a model and optimizer
2025-04-11T04:23:18.4553805Z         model = resnet18()
2025-04-11T04:23:18.4553896Z         optimizer = Adam(
2025-04-11T04:23:18.4554133Z             [{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T04:23:18.4554212Z         )
2025-04-11T04:23:18.4554282Z     
2025-04-11T04:23:18.4554372Z         # create test data sample
2025-04-11T04:23:18.4554464Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4554532Z     
2025-04-11T04:23:18.4554620Z         # run fwd and bwd
2025-04-11T04:23:18.4554696Z         y = model(x)
2025-04-11T04:23:18.4554779Z         loss = y.sum()
2025-04-11T04:23:18.4554867Z         loss.backward()
2025-04-11T04:23:18.4554952Z         optimizer.step()
2025-04-11T04:23:18.4555030Z     
2025-04-11T04:23:18.4555139Z         # create temp directories for checkpoint
2025-04-11T04:23:18.4555349Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4555483Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4555552Z     
2025-04-11T04:23:18.4555647Z         # save the model and optimizer
2025-04-11T04:23:18.4555742Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4555816Z     
2025-04-11T04:23:18.4556019Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T04:23:18.4556276Z         ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4556352Z     
2025-04-11T04:23:18.4556436Z         ckpt_io._sync_d2h()
2025-04-11T04:23:18.4556526Z         ckpt_io._sync_io()
2025-04-11T04:23:18.4556596Z     
2025-04-11T04:23:18.4556676Z         # create new model
2025-04-11T04:23:18.4556764Z         new_model = resnet18()
2025-04-11T04:23:18.4556851Z         new_optimizer = Adam(
2025-04-11T04:23:18.4557108Z             [{"params": new_model.layer1.parameters()}, {"params": new_model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T04:23:18.4557182Z         )
2025-04-11T04:23:18.4557255Z     
2025-04-11T04:23:18.4557415Z         ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
2025-04-11T04:23:18.4557574Z >       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T04:23:18.4557578Z 
2025-04-11T04:23:18.4557729Z tests/test_checkpoint_io/test_general_checkpoint_io.py:222: 
2025-04-11T04:23:18.4557839Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4558013Z colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
2025-04-11T04:23:18.4558109Z     self.load_sharded_optimizer(
2025-04-11T04:23:18.4558305Z colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
2025-04-11T04:23:18.4558443Z     load_states_into_optimizer(optimizer, state_dict, id_map)
2025-04-11T04:23:18.4558612Z colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
2025-04-11T04:23:18.4558711Z     get_accelerator().synchronize()
2025-04-11T04:23:18.4558861Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4558959Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4559067Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4559071Z 
2025-04-11T04:23:18.4559157Z device = None
2025-04-11T04:23:18.4559162Z 
2025-04-11T04:23:18.4559289Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4559449Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4559522Z     
2025-04-11T04:23:18.4559598Z         Args:
2025-04-11T04:23:18.4559888Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4560063Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4560179Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4560252Z         """
2025-04-11T04:23:18.4560331Z         _lazy_init()
2025-04-11T04:23:18.4560431Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4560531Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4560639Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4560927Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4561067Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4561225Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4561233Z 
2025-04-11T04:23:18.4561475Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4561644Z ______________ test_sharded_optimizer_multiple_param_groups[True] ______________
2025-04-11T04:23:18.4561736Z 
2025-04-11T04:23:18.4561817Z use_async = True
2025-04-11T04:23:18.4561822Z 
2025-04-11T04:23:18.4561953Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T04:23:18.4562110Z     def test_sharded_optimizer_multiple_param_groups(use_async: bool):
2025-04-11T04:23:18.4562205Z         # create a model and optimizer
2025-04-11T04:23:18.4562289Z         model = resnet18()
2025-04-11T04:23:18.4562372Z         optimizer = Adam(
2025-04-11T04:23:18.4562602Z             [{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T04:23:18.4562675Z         )
2025-04-11T04:23:18.4562748Z     
2025-04-11T04:23:18.4562837Z         # create test data sample
2025-04-11T04:23:18.4562932Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4563001Z     
2025-04-11T04:23:18.4563081Z         # run fwd and bwd
2025-04-11T04:23:18.4563164Z         y = model(x)
2025-04-11T04:23:18.4563240Z         loss = y.sum()
2025-04-11T04:23:18.4563329Z         loss.backward()
2025-04-11T04:23:18.4563414Z         optimizer.step()
2025-04-11T04:23:18.4563485Z     
2025-04-11T04:23:18.4563594Z         # create temp directories for checkpoint
2025-04-11T04:23:18.4563718Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4563854Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4563927Z     
2025-04-11T04:23:18.4564017Z         # save the model and optimizer
2025-04-11T04:23:18.4564116Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4564185Z     
2025-04-11T04:23:18.4564387Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T04:23:18.4564645Z >       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4564650Z 
2025-04-11T04:23:18.4564811Z tests/test_checkpoint_io/test_general_checkpoint_io.py:210: 
2025-04-11T04:23:18.4564926Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4565103Z colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
2025-04-11T04:23:18.4565198Z     self.save_sharded_optimizer(
2025-04-11T04:23:18.4565389Z colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
2025-04-11T04:23:18.4565577Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T04:23:18.4565758Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T04:23:18.4565902Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T04:23:18.4566067Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4566387Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4566627Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4566766Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4567005Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4567136Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4567258Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4567483Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4567595Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4567599Z 
2025-04-11T04:23:18.4567691Z tensor = tensor(1.), empty = True
2025-04-11T04:23:18.4567695Z 
2025-04-11T04:23:18.4567872Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4567949Z         if empty:
2025-04-11T04:23:18.4568097Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4568303Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4568595Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4568743Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4568900Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4568905Z 
2025-04-11T04:23:18.4569028Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4569162Z _____________________________ test_hybrid_ckpIO[4] _____________________________
2025-04-11T04:23:18.4569166Z 
2025-04-11T04:23:18.4569284Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.4569902Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4569910Z 
2025-04-11T04:23:18.4570017Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4570102Z         try_count = 0
2025-04-11T04:23:18.4570202Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4570289Z             max_try, int
2025-04-11T04:23:18.4570435Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4570513Z     
2025-04-11T04:23:18.4570624Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4570701Z             try:
2025-04-11T04:23:18.4570795Z                 try_count += 1
2025-04-11T04:23:18.4570888Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4570970Z                 return ret
2025-04-11T04:23:18.4571066Z             except exception_type as e:
2025-04-11T04:23:18.4571165Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4571354Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4571472Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4571621Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4571774Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4571860Z                     continue
2025-04-11T04:23:18.4571935Z                 else:
2025-04-11T04:23:18.4572158Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4572239Z >                   raise e
2025-04-11T04:23:18.4572243Z 
2025-04-11T04:23:18.4572335Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4572543Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4572676Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4572766Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4573004Z tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py:155: in test_hybrid_ckpIO
2025-04-11T04:23:18.4573091Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.4573197Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4573296Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4573554Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4573733Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4574020Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4574108Z     while not context.join():
2025-04-11T04:23:18.4574218Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4574226Z 
2025-04-11T04:23:18.4574426Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840d6f0>
2025-04-11T04:23:18.4574504Z timeout = None
2025-04-11T04:23:18.4574593Z 
2025-04-11T04:23:18.4574690Z     def join(self, timeout=None):
2025-04-11T04:23:18.4574814Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4574894Z     
2025-04-11T04:23:18.4575037Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4575181Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4575348Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4575442Z         of the first process exiting.
2025-04-11T04:23:18.4575516Z     
2025-04-11T04:23:18.4575662Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4575806Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4575876Z     
2025-04-11T04:23:18.4575948Z         Args:
2025-04-11T04:23:18.4576089Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4576167Z         """
2025-04-11T04:23:18.4576305Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4576397Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4576474Z             return True
2025-04-11T04:23:18.4576546Z     
2025-04-11T04:23:18.4576676Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4576795Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4576886Z             self.sentinels.keys(),
2025-04-11T04:23:18.4576971Z             timeout=timeout,
2025-04-11T04:23:18.4577044Z         )
2025-04-11T04:23:18.4577113Z     
2025-04-11T04:23:18.4577198Z         error_index = None
2025-04-11T04:23:18.4577283Z         for sentinel in ready:
2025-04-11T04:23:18.4577395Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4577492Z             process = self.processes[index]
2025-04-11T04:23:18.4577576Z             process.join()
2025-04-11T04:23:18.4577683Z             if process.exitcode != 0:
2025-04-11T04:23:18.4577774Z                 error_index = index
2025-04-11T04:23:18.4577859Z                 break
2025-04-11T04:23:18.4577932Z     
2025-04-11T04:23:18.4578022Z         # Return if there was no error.
2025-04-11T04:23:18.4578110Z         if error_index is None:
2025-04-11T04:23:18.4578242Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4578349Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4578418Z     
2025-04-11T04:23:18.4578558Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4578654Z         for process in self.processes:
2025-04-11T04:23:18.4578744Z             if process.is_alive():
2025-04-11T04:23:18.4578839Z                 process.terminate()
2025-04-11T04:23:18.4579015Z             process.join()
2025-04-11T04:23:18.4579092Z     
2025-04-11T04:23:18.4579232Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4579345Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4579460Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4579579Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4579667Z             if exitcode < 0:
2025-04-11T04:23:18.4579773Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4579881Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4580028Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4580123Z                     error_index=error_index,
2025-04-11T04:23:18.4580226Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4580314Z                     exit_code=exitcode,
2025-04-11T04:23:18.4580409Z                     signal_name=name,
2025-04-11T04:23:18.4580484Z                 )
2025-04-11T04:23:18.4580555Z             else:
2025-04-11T04:23:18.4580662Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4580928Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4581027Z                     error_index=error_index,
2025-04-11T04:23:18.4581127Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4581215Z                     exit_code=exitcode,
2025-04-11T04:23:18.4581288Z                 )
2025-04-11T04:23:18.4581357Z     
2025-04-11T04:23:18.4581492Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4581661Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4581752Z         msg += original_trace
2025-04-11T04:23:18.4581922Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4582091Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4582166Z E       
2025-04-11T04:23:18.4582292Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4582393Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4582692Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4582777Z E           fn(i, *args)
2025-04-11T04:23:18.4583101Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
2025-04-11T04:23:18.4583191Z E           exam_state_dict()
2025-04-11T04:23:18.4583449Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4583536Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4583794Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4583885Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4584133Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4584222Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4584329Z E         [Previous line repeated 3 more times]
2025-04-11T04:23:18.4584548Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4584647Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4584905Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4585004Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4585283Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4585385Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4585593Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4585881Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4586019Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4586190Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4586195Z 
2025-04-11T04:23:18.4586498Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4586655Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4586812Z [04/11/25 04:13:35] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4586975Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4587093Z                              :75 launch                                         
2025-04-11T04:23:18.4587243Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4587367Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4587653Z [04/11/25 04:13:35] WARNING  colossalai - colossalai.shardformer.modeling.llama 
2025-04-11T04:23:18.4587782Z                              - WARNING: `use_cache=True` is incompatible with   
2025-04-11T04:23:18.4587906Z                              pipeline parallelism. Setting `use_cache=False`... 
2025-04-11T04:23:18.4588105Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4588250Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4589429Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4589597Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4590719Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4590883Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4591994Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4592155Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4593258Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4593414Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4594204Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4594295Z   warnings.warn(
2025-04-11T04:23:18.4594985Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4595064Z   warnings.warn(
2025-04-11T04:23:18.4595748Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4595826Z   warnings.warn(
2025-04-11T04:23:18.4596526Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4596707Z   warnings.warn(
2025-04-11T04:23:18.4597543Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4597622Z   warnings.warn(
2025-04-11T04:23:18.4598438Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4598521Z   warnings.warn(
2025-04-11T04:23:18.4599326Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4599414Z   warnings.warn(
2025-04-11T04:23:18.4600211Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4600294Z   warnings.warn(
2025-04-11T04:23:18.4601101Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4601184Z   warnings.warn(
2025-04-11T04:23:18.4601991Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4602074Z   warnings.warn(
2025-04-11T04:23:18.4602882Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4602963Z   warnings.warn(
2025-04-11T04:23:18.4603853Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4603940Z   warnings.warn(
2025-04-11T04:23:18.4604741Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4604822Z   warnings.warn(
2025-04-11T04:23:18.4605623Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4605706Z   warnings.warn(
2025-04-11T04:23:18.4606502Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4606669Z   warnings.warn(
2025-04-11T04:23:18.4607497Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4607573Z   warnings.warn(
2025-04-11T04:23:18.4607879Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57270 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4608167Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57270 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4608748Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4608830Z   warnings.warn(
2025-04-11T04:23:18.4609380Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4609456Z   warnings.warn(
2025-04-11T04:23:18.4610012Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4610089Z   warnings.warn(
2025-04-11T04:23:18.4610633Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4610714Z   warnings.warn(
2025-04-11T04:23:18.4611264Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4611346Z   warnings.warn(
2025-04-11T04:23:18.4611884Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4611966Z   warnings.warn(
2025-04-11T04:23:18.4612591Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4612674Z   warnings.warn(
2025-04-11T04:23:18.4613200Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4613282Z   warnings.warn(
2025-04-11T04:23:18.4613435Z _______________________ test_low_level_zero_checkpointIO _______________________
2025-04-11T04:23:18.4613439Z 
2025-04-11T04:23:18.4613536Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4613540Z 
2025-04-11T04:23:18.4613644Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4613723Z         try_count = 0
2025-04-11T04:23:18.4613827Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4613908Z             max_try, int
2025-04-11T04:23:18.4614058Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4614128Z     
2025-04-11T04:23:18.4614243Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4614320Z             try:
2025-04-11T04:23:18.4614403Z                 try_count += 1
2025-04-11T04:23:18.4614500Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.4614695Z 
2025-04-11T04:23:18.4614792Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.4614911Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4615026Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.4615119Z     get_accelerator().synchronize()
2025-04-11T04:23:18.4615279Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4615383Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4615517Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4615521Z 
2025-04-11T04:23:18.4615599Z device = None
2025-04-11T04:23:18.4615603Z 
2025-04-11T04:23:18.4615741Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4615899Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4615969Z     
2025-04-11T04:23:18.4616055Z         Args:
2025-04-11T04:23:18.4616228Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4616403Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4616513Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4616590Z         """
2025-04-11T04:23:18.4616669Z         _lazy_init()
2025-04-11T04:23:18.4616767Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4616875Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4616981Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4617276Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4617420Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4617587Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4617595Z 
2025-04-11T04:23:18.4617835Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4617987Z ______________________ test_huggingface_compatibility[2] _______________________
2025-04-11T04:23:18.4617995Z 
2025-04-11T04:23:18.4618111Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.4618717Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4618725Z 
2025-04-11T04:23:18.4618827Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4618907Z         try_count = 0
2025-04-11T04:23:18.4619095Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4619180Z             max_try, int
2025-04-11T04:23:18.4619328Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4619401Z     
2025-04-11T04:23:18.4619511Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4619590Z             try:
2025-04-11T04:23:18.4619674Z                 try_count += 1
2025-04-11T04:23:18.4619768Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4619847Z                 return ret
2025-04-11T04:23:18.4619940Z             except exception_type as e:
2025-04-11T04:23:18.4620043Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4620232Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4620357Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4620505Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4620664Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4620744Z                     continue
2025-04-11T04:23:18.4620903Z                 else:
2025-04-11T04:23:18.4621132Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4621210Z >                   raise e
2025-04-11T04:23:18.4621214Z 
2025-04-11T04:23:18.4621311Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4621422Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4621556Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4621642Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4621911Z tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py:79: in test_huggingface_compatibility
2025-04-11T04:23:18.4622005Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.4622107Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4622210Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4622468Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4622654Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4622941Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4623034Z     while not context.join():
2025-04-11T04:23:18.4623141Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4623145Z 
2025-04-11T04:23:18.4623343Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840ea10>
2025-04-11T04:23:18.4623428Z timeout = None
2025-04-11T04:23:18.4623433Z 
2025-04-11T04:23:18.4623522Z     def join(self, timeout=None):
2025-04-11T04:23:18.4623651Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4623725Z     
2025-04-11T04:23:18.4623875Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4624019Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4624188Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4624285Z         of the first process exiting.
2025-04-11T04:23:18.4624354Z     
2025-04-11T04:23:18.4624504Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4624646Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4624719Z     
2025-04-11T04:23:18.4624810Z         Args:
2025-04-11T04:23:18.4624959Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4625040Z         """
2025-04-11T04:23:18.4625179Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4625274Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4625438Z             return True
2025-04-11T04:23:18.4625511Z     
2025-04-11T04:23:18.4625648Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4625765Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4625861Z             self.sentinels.keys(),
2025-04-11T04:23:18.4625945Z             timeout=timeout,
2025-04-11T04:23:18.4626017Z         )
2025-04-11T04:23:18.4626095Z     
2025-04-11T04:23:18.4626180Z         error_index = None
2025-04-11T04:23:18.4626271Z         for sentinel in ready:
2025-04-11T04:23:18.4626374Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4626474Z             process = self.processes[index]
2025-04-11T04:23:18.4626563Z             process.join()
2025-04-11T04:23:18.4626655Z             if process.exitcode != 0:
2025-04-11T04:23:18.4626745Z                 error_index = index
2025-04-11T04:23:18.4626821Z                 break
2025-04-11T04:23:18.4626903Z     
2025-04-11T04:23:18.4627002Z         # Return if there was no error.
2025-04-11T04:23:18.4627089Z         if error_index is None:
2025-04-11T04:23:18.4627236Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4627333Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4627500Z     
2025-04-11T04:23:18.4627644Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4627743Z         for process in self.processes:
2025-04-11T04:23:18.4627843Z             if process.is_alive():
2025-04-11T04:23:18.4627938Z                 process.terminate()
2025-04-11T04:23:18.4628032Z             process.join()
2025-04-11T04:23:18.4628104Z     
2025-04-11T04:23:18.4628254Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4628371Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4628548Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4628680Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4628771Z             if exitcode < 0:
2025-04-11T04:23:18.4628887Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4628994Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4629150Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4629256Z                     error_index=error_index,
2025-04-11T04:23:18.4629358Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4629456Z                     exit_code=exitcode,
2025-04-11T04:23:18.4629544Z                     signal_name=name,
2025-04-11T04:23:18.4629628Z                 )
2025-04-11T04:23:18.4629706Z             else:
2025-04-11T04:23:18.4629808Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4629980Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4630076Z                     error_index=error_index,
2025-04-11T04:23:18.4630187Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4630275Z                     exit_code=exitcode,
2025-04-11T04:23:18.4630350Z                 )
2025-04-11T04:23:18.4630429Z     
2025-04-11T04:23:18.4630563Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4630744Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4630830Z         msg += original_trace
2025-04-11T04:23:18.4631010Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4631173Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4631248Z E       
2025-04-11T04:23:18.4631383Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4631483Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4631792Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4631969Z E           fn(i, *args)
2025-04-11T04:23:18.4632298Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
2025-04-11T04:23:18.4632416Z E           exam_from_pretrained()
2025-04-11T04:23:18.4632656Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4632764Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4633024Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4633131Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4633411Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4633525Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4633632Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4633928Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4634076Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4634242Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4634340Z 
2025-04-11T04:23:18.4634657Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4634811Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4634976Z [04/11/25 04:13:43] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4635107Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4635222Z                              :75 launch                                         
2025-04-11T04:23:18.4635361Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4635492Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4635698Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4635849Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4636994Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4637166Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4638285Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4638458Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4639143Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4639230Z   warnings.warn(
2025-04-11T04:23:18.4640033Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4640121Z   warnings.warn(
2025-04-11T04:23:18.4640956Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4641040Z   warnings.warn(
2025-04-11T04:23:18.4641864Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4641952Z   warnings.warn(
2025-04-11T04:23:18.4642779Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4642861Z   warnings.warn(
2025-04-11T04:23:18.4643751Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4643828Z   warnings.warn(
2025-04-11T04:23:18.4644632Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4644710Z   warnings.warn(
2025-04-11T04:23:18.4645518Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.4645602Z   warnings.warn(
2025-04-11T04:23:18.4646153Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4646240Z   warnings.warn(
2025-04-11T04:23:18.4646767Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4646856Z   warnings.warn(
2025-04-11T04:23:18.4647390Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4647474Z   warnings.warn(
2025-04-11T04:23:18.4648003Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4648088Z   warnings.warn(
2025-04-11T04:23:18.4648231Z ___________________________ test_create_pin[1-True] ____________________________
2025-04-11T04:23:18.4648235Z 
2025-04-11T04:23:18.4648327Z empty = True, num_threads = 1
2025-04-11T04:23:18.4648338Z 
2025-04-11T04:23:18.4648465Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T04:23:18.4648584Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T04:23:18.4648721Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T04:23:18.4648920Z         model_state_dict = gen_model_state_dict()
2025-04-11T04:23:18.4649184Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4649189Z 
2025-04-11T04:23:18.4649342Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T04:23:18.4649466Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4649637Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4649876Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4650122Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4650263Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4650510Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4650650Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4650779Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4651012Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4651213Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4651225Z 
2025-04-11T04:23:18.4651406Z tensor = tensor([[8.6847e-01, 8.1748e-04, 4.0391e-02,  ..., 7.9576e-01, 7.4807e-01,
2025-04-11T04:23:18.4651487Z          1.9106e-01],
2025-04-11T04:23:18.4651589Z         [8.6654e-02, ...         9.2385e-01],
2025-04-11T04:23:18.4651730Z         [3.8017e-01, 2.4513e-01, 9.6436e-01,  ..., 3.2863e-01, 2.9029e-01,
2025-04-11T04:23:18.4651819Z          5.3874e-01]])
2025-04-11T04:23:18.4651900Z empty = True
2025-04-11T04:23:18.4651904Z 
2025-04-11T04:23:18.4652095Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4652175Z         if empty:
2025-04-11T04:23:18.4652340Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4652455Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4652745Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4652896Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4653057Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4653061Z 
2025-04-11T04:23:18.4653192Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4653337Z ___________________________ test_create_pin[1-False] ___________________________
2025-04-11T04:23:18.4653341Z 
2025-04-11T04:23:18.4653435Z empty = False, num_threads = 1
2025-04-11T04:23:18.4653446Z 
2025-04-11T04:23:18.4653569Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T04:23:18.4653689Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T04:23:18.4653819Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T04:23:18.4653925Z         model_state_dict = gen_model_state_dict()
2025-04-11T04:23:18.4654178Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4654186Z 
2025-04-11T04:23:18.4654334Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T04:23:18.4654451Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4654616Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T04:23:18.4654845Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4655088Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T04:23:18.4655226Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4655555Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T04:23:18.4655694Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T04:23:18.4655823Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T04:23:18.4656053Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T04:23:18.4656164Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4656175Z 
2025-04-11T04:23:18.4656320Z tensor = tensor([[0.3194, 0.1471, 0.8299,  ..., 0.0654, 0.6019, 0.1725],
2025-04-11T04:23:18.4656492Z         [0.6552, 0.0854, 0.7144,  ..., 0.4938, 0.4092,...0.2009, 0.3128, 0.3861,  ..., 0.5467, 0.1594, 0.9723],
2025-04-11T04:23:18.4656612Z         [0.6342, 0.6712, 0.4807,  ..., 0.1087, 0.7685, 0.4644]])
2025-04-11T04:23:18.4656693Z empty = False
2025-04-11T04:23:18.4656697Z 
2025-04-11T04:23:18.4656887Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4656967Z         if empty:
2025-04-11T04:23:18.4657127Z             return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4657312Z >       return tensor.pin_memory()
2025-04-11T04:23:18.4657422Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4657719Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4657858Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4658028Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4658032Z 
2025-04-11T04:23:18.4658152Z colossalai/checkpoint_io/utils.py:968: RuntimeError
2025-04-11T04:23:18.4658301Z ___________________________ test_create_pin[4-True] ____________________________
2025-04-11T04:23:18.4658305Z 
2025-04-11T04:23:18.4658396Z empty = True, num_threads = 4
2025-04-11T04:23:18.4658400Z 
2025-04-11T04:23:18.4658529Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T04:23:18.4658645Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T04:23:18.4658765Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T04:23:18.4658877Z         model_state_dict = gen_model_state_dict()
2025-04-11T04:23:18.4659122Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4659127Z 
2025-04-11T04:23:18.4659275Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T04:23:18.4659382Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4659550Z colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
2025-04-11T04:23:18.4659641Z     elems[idx] = future.result()
2025-04-11T04:23:18.4659843Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
2025-04-11T04:23:18.4659941Z     return self.__get_result()
2025-04-11T04:23:18.4660153Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
2025-04-11T04:23:18.4660246Z     raise self._exception
2025-04-11T04:23:18.4660436Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
2025-04-11T04:23:18.4660552Z     result = self.fn(*self.args, **self.kwargs)
2025-04-11T04:23:18.4660659Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4660663Z 
2025-04-11T04:23:18.4660803Z tensor = tensor([[0.5821, 0.7269, 0.9384,  ..., 0.4998, 0.7969, 0.4132],
2025-04-11T04:23:18.4660989Z         [0.1225, 0.3994, 0.1804,  ..., 0.1278, 0.2117,...0.5724, 0.2939, 0.7177,  ..., 0.9228, 0.0104, 0.5268],
2025-04-11T04:23:18.4661106Z         [0.9206, 0.8477, 0.4199,  ..., 0.2250, 0.8432, 0.0800]])
2025-04-11T04:23:18.4661188Z empty = True
2025-04-11T04:23:18.4661193Z 
2025-04-11T04:23:18.4661365Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4661532Z         if empty:
2025-04-11T04:23:18.4661683Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4661797Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4662092Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4662229Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4662391Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4662395Z 
2025-04-11T04:23:18.4662512Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T04:23:18.4662652Z ___________________________ test_create_pin[4-False] ___________________________
2025-04-11T04:23:18.4662656Z 
2025-04-11T04:23:18.4662744Z empty = False, num_threads = 4
2025-04-11T04:23:18.4662748Z 
2025-04-11T04:23:18.4662873Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T04:23:18.4662989Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T04:23:18.4663110Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T04:23:18.4663217Z         model_state_dict = gen_model_state_dict()
2025-04-11T04:23:18.4663562Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4663566Z 
2025-04-11T04:23:18.4663714Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T04:23:18.4663820Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4663986Z colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
2025-04-11T04:23:18.4664077Z     elems[idx] = future.result()
2025-04-11T04:23:18.4664281Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
2025-04-11T04:23:18.4664368Z     return self.__get_result()
2025-04-11T04:23:18.4664586Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
2025-04-11T04:23:18.4664674Z     raise self._exception
2025-04-11T04:23:18.4664862Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
2025-04-11T04:23:18.4664977Z     result = self.fn(*self.args, **self.kwargs)
2025-04-11T04:23:18.4665084Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4665088Z 
2025-04-11T04:23:18.4665229Z tensor = tensor([[0.6302, 0.2935, 0.7903,  ..., 0.0028, 0.9982, 0.4133],
2025-04-11T04:23:18.4665393Z         [0.3542, 0.4418, 0.9797,  ..., 0.7308, 0.7955,...0.9666, 0.5375, 0.3321,  ..., 0.7164, 0.7701, 0.3787],
2025-04-11T04:23:18.4665503Z         [0.0016, 0.1229, 0.2857,  ..., 0.3524, 0.9990, 0.6592]])
2025-04-11T04:23:18.4665584Z empty = False
2025-04-11T04:23:18.4665588Z 
2025-04-11T04:23:18.4665758Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T04:23:18.4665836Z         if empty:
2025-04-11T04:23:18.4665988Z             return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T04:23:18.4666082Z >       return tensor.pin_memory()
2025-04-11T04:23:18.4666188Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4666475Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4666614Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4666772Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4666777Z 
2025-04-11T04:23:18.4666898Z colossalai/checkpoint_io/utils.py:968: RuntimeError
2025-04-11T04:23:18.4667033Z ________________________________ test_save_load ________________________________
2025-04-11T04:23:18.4667038Z 
2025-04-11T04:23:18.4667127Z args = (), kwargs = {}
2025-04-11T04:23:18.4667131Z 
2025-04-11T04:23:18.4667226Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.4667323Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.4667527Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.4667647Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.4667755Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.4667850Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.4667854Z 
2025-04-11T04:23:18.4667951Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.4668060Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4668218Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4668311Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4668475Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4668480Z 
2025-04-11T04:23:18.4668564Z device = None
2025-04-11T04:23:18.4668568Z 
2025-04-11T04:23:18.4668689Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4668853Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4668926Z     
2025-04-11T04:23:18.4669005Z         Args:
2025-04-11T04:23:18.4669175Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4669441Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4669556Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4669630Z         """
2025-04-11T04:23:18.4669711Z         _lazy_init()
2025-04-11T04:23:18.4669803Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4669908Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4670016Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4670303Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4670443Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4670605Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4670609Z 
2025-04-11T04:23:18.4670906Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4671063Z _________________________ test_torch_ddp_checkpointIO __________________________
2025-04-11T04:23:18.4671067Z 
2025-04-11T04:23:18.4671162Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4671780Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4671785Z 
2025-04-11T04:23:18.4671891Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4671976Z         try_count = 0
2025-04-11T04:23:18.4672076Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4672166Z             max_try, int
2025-04-11T04:23:18.4672311Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4672390Z     
2025-04-11T04:23:18.4672503Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4672579Z             try:
2025-04-11T04:23:18.4672666Z                 try_count += 1
2025-04-11T04:23:18.4672755Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4672838Z                 return ret
2025-04-11T04:23:18.4672928Z             except exception_type as e:
2025-04-11T04:23:18.4673032Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4673217Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4673334Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4673484Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4673736Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4673824Z                     continue
2025-04-11T04:23:18.4673901Z                 else:
2025-04-11T04:23:18.4674131Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4674213Z >                   raise e
2025-04-11T04:23:18.4674217Z 
2025-04-11T04:23:18.4674309Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4674423Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4674552Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4674640Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4674861Z tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py:81: in test_torch_ddp_checkpointIO
2025-04-11T04:23:18.4674944Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4675044Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4675145Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4675411Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4675588Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4675968Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4676057Z     while not context.join():
2025-04-11T04:23:18.4676171Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4676176Z 
2025-04-11T04:23:18.4676378Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b7870280>
2025-04-11T04:23:18.4676457Z timeout = None
2025-04-11T04:23:18.4676461Z 
2025-04-11T04:23:18.4676556Z     def join(self, timeout=None):
2025-04-11T04:23:18.4676680Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4676753Z     
2025-04-11T04:23:18.4676902Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4677053Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4677215Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4677312Z         of the first process exiting.
2025-04-11T04:23:18.4677385Z     
2025-04-11T04:23:18.4677535Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4677673Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4677743Z     
2025-04-11T04:23:18.4677815Z         Args:
2025-04-11T04:23:18.4677956Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4678029Z         """
2025-04-11T04:23:18.4678172Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4678264Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4678347Z             return True
2025-04-11T04:23:18.4678416Z     
2025-04-11T04:23:18.4678548Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4678669Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4678760Z             self.sentinels.keys(),
2025-04-11T04:23:18.4678850Z             timeout=timeout,
2025-04-11T04:23:18.4678925Z         )
2025-04-11T04:23:18.4678993Z     
2025-04-11T04:23:18.4679081Z         error_index = None
2025-04-11T04:23:18.4679166Z         for sentinel in ready:
2025-04-11T04:23:18.4679277Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4679376Z             process = self.processes[index]
2025-04-11T04:23:18.4679465Z             process.join()
2025-04-11T04:23:18.4679556Z             if process.exitcode != 0:
2025-04-11T04:23:18.4679643Z                 error_index = index
2025-04-11T04:23:18.4679722Z                 break
2025-04-11T04:23:18.4679793Z     
2025-04-11T04:23:18.4679886Z         # Return if there was no error.
2025-04-11T04:23:18.4679975Z         if error_index is None:
2025-04-11T04:23:18.4680195Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4680307Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4680382Z     
2025-04-11T04:23:18.4680527Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4680627Z         for process in self.processes:
2025-04-11T04:23:18.4680721Z             if process.is_alive():
2025-04-11T04:23:18.4680817Z                 process.terminate()
2025-04-11T04:23:18.4680901Z             process.join()
2025-04-11T04:23:18.4680976Z     
2025-04-11T04:23:18.4681134Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4681252Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4681359Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4681477Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4681564Z             if exitcode < 0:
2025-04-11T04:23:18.4681675Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4681785Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4681933Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4682125Z                     error_index=error_index,
2025-04-11T04:23:18.4682227Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4682316Z                     exit_code=exitcode,
2025-04-11T04:23:18.4682423Z                     signal_name=name,
2025-04-11T04:23:18.4682500Z                 )
2025-04-11T04:23:18.4682584Z             else:
2025-04-11T04:23:18.4682690Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4682857Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4682961Z                     error_index=error_index,
2025-04-11T04:23:18.4683062Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4683159Z                     exit_code=exitcode,
2025-04-11T04:23:18.4683239Z                 )
2025-04-11T04:23:18.4683317Z     
2025-04-11T04:23:18.4683452Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4683630Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4683733Z         msg += original_trace
2025-04-11T04:23:18.4683909Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4684083Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4684160Z E       
2025-04-11T04:23:18.4684287Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4684396Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4684716Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4684805Z E           fn(i, *args)
2025-04-11T04:23:18.4685099Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
2025-04-11T04:23:18.4685206Z E           check_torch_ddp_checkpointIO()
2025-04-11T04:23:18.4685474Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4685567Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4685835Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4685926Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4686188Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4686275Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4686386Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.4686728Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
2025-04-11T04:23:18.4687070Z E           model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
2025-04-11T04:23:18.4687283Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T04:23:18.4687483Z E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T04:23:18.4687759Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
2025-04-11T04:23:18.4687867Z E           model = model.to(get_current_device())
2025-04-11T04:23:18.4688140Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.4688235Z E           return self._apply(convert)
2025-04-11T04:23:18.4688516Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.4688606Z E           module._apply(fn)
2025-04-11T04:23:18.4688880Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.4688982Z E           param_applied = fn(param)
2025-04-11T04:23:18.4689376Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.4689599Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.4689709Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4690012Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4690151Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4690321Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4690326Z 
2025-04-11T04:23:18.4690642Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4690799Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4690966Z [04/11/25 04:13:50] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4691101Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4691221Z                              :75 launch                                         
2025-04-11T04:23:18.4691361Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4691496Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4691696Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4691853Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4692153Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:28471 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4692294Z _____________________________ test_torch_fsdp_ckpt _____________________________
2025-04-11T04:23:18.4692298Z 
2025-04-11T04:23:18.4692405Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4693016Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4693021Z 
2025-04-11T04:23:18.4693132Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4693214Z         try_count = 0
2025-04-11T04:23:18.4693325Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4693411Z             max_try, int
2025-04-11T04:23:18.4693567Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4693642Z     
2025-04-11T04:23:18.4693838Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4693927Z             try:
2025-04-11T04:23:18.4694016Z                 try_count += 1
2025-04-11T04:23:18.4694119Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4694205Z                 return ret
2025-04-11T04:23:18.4694301Z             except exception_type as e:
2025-04-11T04:23:18.4694410Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4694598Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4694723Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4694869Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4695032Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4695114Z                     continue
2025-04-11T04:23:18.4695190Z                 else:
2025-04-11T04:23:18.4695424Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4695505Z >                   raise e
2025-04-11T04:23:18.4695509Z 
2025-04-11T04:23:18.4695610Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4695813Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4695952Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4696041Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4696251Z tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py:162: in test_torch_fsdp_ckpt
2025-04-11T04:23:18.4696345Z     spawn(run_dist, 2)
2025-04-11T04:23:18.4696449Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4696557Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4696811Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4697001Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4697294Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4697394Z     while not context.join():
2025-04-11T04:23:18.4697520Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4697524Z 
2025-04-11T04:23:18.4697749Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5e6115c90>
2025-04-11T04:23:18.4697839Z timeout = None
2025-04-11T04:23:18.4697843Z 
2025-04-11T04:23:18.4697937Z     def join(self, timeout=None):
2025-04-11T04:23:18.4698072Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4698146Z     
2025-04-11T04:23:18.4698296Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4698454Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4698622Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4698725Z         of the first process exiting.
2025-04-11T04:23:18.4698797Z     
2025-04-11T04:23:18.4698953Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4699097Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4699168Z     
2025-04-11T04:23:18.4699249Z         Args:
2025-04-11T04:23:18.4699389Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4699469Z         """
2025-04-11T04:23:18.4699608Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4699702Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4699790Z             return True
2025-04-11T04:23:18.4699862Z     
2025-04-11T04:23:18.4700003Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4700122Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4700221Z             self.sentinels.keys(),
2025-04-11T04:23:18.4700408Z             timeout=timeout,
2025-04-11T04:23:18.4700487Z         )
2025-04-11T04:23:18.4700568Z     
2025-04-11T04:23:18.4700653Z         error_index = None
2025-04-11T04:23:18.4700744Z         for sentinel in ready:
2025-04-11T04:23:18.4700854Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4700954Z             process = self.processes[index]
2025-04-11T04:23:18.4701047Z             process.join()
2025-04-11T04:23:18.4701140Z             if process.exitcode != 0:
2025-04-11T04:23:18.4701235Z                 error_index = index
2025-04-11T04:23:18.4701312Z                 break
2025-04-11T04:23:18.4701385Z     
2025-04-11T04:23:18.4701484Z         # Return if there was no error.
2025-04-11T04:23:18.4701570Z         if error_index is None:
2025-04-11T04:23:18.4701710Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4701808Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4701888Z     
2025-04-11T04:23:18.4702032Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4702130Z         for process in self.processes:
2025-04-11T04:23:18.4702226Z             if process.is_alive():
2025-04-11T04:23:18.4702412Z                 process.terminate()
2025-04-11T04:23:18.4702505Z             process.join()
2025-04-11T04:23:18.4702576Z     
2025-04-11T04:23:18.4702719Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4702843Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4702952Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4703079Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4703164Z             if exitcode < 0:
2025-04-11T04:23:18.4703279Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4703387Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4703544Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4703647Z                     error_index=error_index,
2025-04-11T04:23:18.4703751Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4703849Z                     exit_code=exitcode,
2025-04-11T04:23:18.4703942Z                     signal_name=name,
2025-04-11T04:23:18.4704020Z                 )
2025-04-11T04:23:18.4704105Z             else:
2025-04-11T04:23:18.4704209Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4704387Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4704486Z                     error_index=error_index,
2025-04-11T04:23:18.4704597Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4704686Z                     exit_code=exitcode,
2025-04-11T04:23:18.4704763Z                 )
2025-04-11T04:23:18.4704841Z     
2025-04-11T04:23:18.4704976Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4705160Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4705247Z         msg += original_trace
2025-04-11T04:23:18.4705424Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4705598Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4705673Z E       
2025-04-11T04:23:18.4705809Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4705910Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4706227Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4706310Z E           fn(i, *args)
2025-04-11T04:23:18.4706639Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
2025-04-11T04:23:18.4706753Z E           check_torch_fsdp_ckpt()
2025-04-11T04:23:18.4707108Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4707211Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4707530Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
2025-04-11T04:23:18.4707747Z E           fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
2025-04-11T04:23:18.4707954Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T04:23:18.4708158Z E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T04:23:18.4708465Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
2025-04-11T04:23:18.4708692Z E           fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
2025-04-11T04:23:18.4708971Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
2025-04-11T04:23:18.4709106Z E           self.module = FSDP(module, *args, **kwargs)
2025-04-11T04:23:18.4709487Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
2025-04-11T04:23:18.4709695Z E           _init_param_handle_from_module(
2025-04-11T04:23:18.4710080Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
2025-04-11T04:23:18.4710172Z E           _move_module_to_device(
2025-04-11T04:23:18.4710531Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
2025-04-11T04:23:18.4710712Z E           _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
2025-04-11T04:23:18.4711066Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
2025-04-11T04:23:18.4711193Z E           param.data = param.to(device_from_device_id)
2025-04-11T04:23:18.4711300Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4711598Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4711735Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4711903Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4711907Z 
2025-04-11T04:23:18.4712213Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4712369Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4712524Z [04/11/25 04:13:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4712657Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4712775Z                              :75 launch                                         
2025-04-11T04:23:18.4712919Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4713056Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.4713253Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4713396Z _______________________________ test_logical_pg ________________________________
2025-04-11T04:23:18.4713400Z 
2025-04-11T04:23:18.4713495Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4714119Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4714212Z 
2025-04-11T04:23:18.4714320Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4714402Z         try_count = 0
2025-04-11T04:23:18.4714510Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4714597Z             max_try, int
2025-04-11T04:23:18.4714753Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4714827Z     
2025-04-11T04:23:18.4714944Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4715020Z             try:
2025-04-11T04:23:18.4715106Z                 try_count += 1
2025-04-11T04:23:18.4715205Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4715288Z                 return ret
2025-04-11T04:23:18.4715389Z             except exception_type as e:
2025-04-11T04:23:18.4715490Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4715678Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4715805Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4715951Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4716123Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4716313Z                     continue
2025-04-11T04:23:18.4716397Z                 else:
2025-04-11T04:23:18.4716622Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4716704Z >                   raise e
2025-04-11T04:23:18.4716715Z 
2025-04-11T04:23:18.4716811Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4716923Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4717061Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4717150Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4717317Z tests/test_device/test_init_logical_pg.py:33: in test_logical_pg
2025-04-11T04:23:18.4717406Z     spawn(check_layer, 4)
2025-04-11T04:23:18.4717508Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4717622Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4717884Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4718068Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4718353Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4718452Z     while not context.join():
2025-04-11T04:23:18.4718564Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4718568Z 
2025-04-11T04:23:18.4718777Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b841ba00>
2025-04-11T04:23:18.4718859Z timeout = None
2025-04-11T04:23:18.4718864Z 
2025-04-11T04:23:18.4718957Z     def join(self, timeout=None):
2025-04-11T04:23:18.4719094Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4719168Z     
2025-04-11T04:23:18.4719321Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4719469Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4719632Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4719732Z         of the first process exiting.
2025-04-11T04:23:18.4719804Z     
2025-04-11T04:23:18.4719958Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4720095Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4720173Z     
2025-04-11T04:23:18.4720248Z         Args:
2025-04-11T04:23:18.4720387Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4720469Z         """
2025-04-11T04:23:18.4720698Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4720804Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4720887Z             return True
2025-04-11T04:23:18.4720959Z     
2025-04-11T04:23:18.4721105Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4721223Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4721325Z             self.sentinels.keys(),
2025-04-11T04:23:18.4721411Z             timeout=timeout,
2025-04-11T04:23:18.4721494Z         )
2025-04-11T04:23:18.4721566Z     
2025-04-11T04:23:18.4721650Z         error_index = None
2025-04-11T04:23:18.4721743Z         for sentinel in ready:
2025-04-11T04:23:18.4721848Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4721955Z             process = self.processes[index]
2025-04-11T04:23:18.4722042Z             process.join()
2025-04-11T04:23:18.4722135Z             if process.exitcode != 0:
2025-04-11T04:23:18.4722232Z                 error_index = index
2025-04-11T04:23:18.4722312Z                 break
2025-04-11T04:23:18.4722390Z     
2025-04-11T04:23:18.4722485Z         # Return if there was no error.
2025-04-11T04:23:18.4722573Z         if error_index is None:
2025-04-11T04:23:18.4722903Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4723000Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4723082Z     
2025-04-11T04:23:18.4723222Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4723327Z         for process in self.processes:
2025-04-11T04:23:18.4723419Z             if process.is_alive():
2025-04-11T04:23:18.4723512Z                 process.terminate()
2025-04-11T04:23:18.4723606Z             process.join()
2025-04-11T04:23:18.4723679Z     
2025-04-11T04:23:18.4723828Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4723944Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4724058Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4724189Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4724275Z             if exitcode < 0:
2025-04-11T04:23:18.4724388Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4724496Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4724653Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4724752Z                     error_index=error_index,
2025-04-11T04:23:18.4724851Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4724949Z                     exit_code=exitcode,
2025-04-11T04:23:18.4725036Z                     signal_name=name,
2025-04-11T04:23:18.4725118Z                 )
2025-04-11T04:23:18.4725195Z             else:
2025-04-11T04:23:18.4725297Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4725467Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4725571Z                     error_index=error_index,
2025-04-11T04:23:18.4725693Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4725780Z                     exit_code=exitcode,
2025-04-11T04:23:18.4725872Z                 )
2025-04-11T04:23:18.4725944Z     
2025-04-11T04:23:18.4726077Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4726255Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4726343Z         msg += original_trace
2025-04-11T04:23:18.4726523Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4726682Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4726763Z E       
2025-04-11T04:23:18.4726889Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4726990Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4727380Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4727467Z E           fn(i, *args)
2025-04-11T04:23:18.4727730Z E         File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
2025-04-11T04:23:18.4727859Z E           tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
2025-04-11T04:23:18.4727972Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4728256Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4728394Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4728562Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4728567Z 
2025-04-11T04:23:18.4728873Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4729036Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4729192Z [04/11/25 04:14:14] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4729328Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4729523Z                              :75 launch                                         
2025-04-11T04:23:18.4729669Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4729796Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4729995Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4730150Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4730452Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:64432 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4730748Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:64432 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4730887Z ____________________________ test_all_to_all_single ____________________________
2025-04-11T04:23:18.4730895Z 
2025-04-11T04:23:18.4730994Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4731598Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4731603Z 
2025-04-11T04:23:18.4731712Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4731793Z         try_count = 0
2025-04-11T04:23:18.4731895Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4731985Z             max_try, int
2025-04-11T04:23:18.4732138Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4732224Z     
2025-04-11T04:23:18.4732339Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4732427Z             try:
2025-04-11T04:23:18.4732513Z                 try_count += 1
2025-04-11T04:23:18.4732614Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4732703Z                 return ret
2025-04-11T04:23:18.4732800Z             except exception_type as e:
2025-04-11T04:23:18.4732910Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4733097Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4733215Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4733368Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4733523Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4733614Z                     continue
2025-04-11T04:23:18.4733774Z                 else:
2025-04-11T04:23:18.4734012Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4734092Z >                   raise e
2025-04-11T04:23:18.4734100Z 
2025-04-11T04:23:18.4734198Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4734317Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4734450Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4734543Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4734712Z tests/test_fp8/test_all_to_all_single.py:73: in test_all_to_all_single
2025-04-11T04:23:18.4734804Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4734908Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4735013Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4735302Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4735485Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4735778Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4735953Z     while not context.join():
2025-04-11T04:23:18.4736073Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4736077Z 
2025-04-11T04:23:18.4736279Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8236230>
2025-04-11T04:23:18.4736361Z timeout = None
2025-04-11T04:23:18.4736373Z 
2025-04-11T04:23:18.4736465Z     def join(self, timeout=None):
2025-04-11T04:23:18.4736589Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4736668Z     
2025-04-11T04:23:18.4736816Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4736968Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4737136Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4737229Z         of the first process exiting.
2025-04-11T04:23:18.4737308Z     
2025-04-11T04:23:18.4737457Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4737607Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4737679Z     
2025-04-11T04:23:18.4737762Z         Args:
2025-04-11T04:23:18.4737903Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4737979Z         """
2025-04-11T04:23:18.4738129Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4738225Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4738316Z             return True
2025-04-11T04:23:18.4738389Z     
2025-04-11T04:23:18.4738522Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4738647Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4738743Z             self.sentinels.keys(),
2025-04-11T04:23:18.4738835Z             timeout=timeout,
2025-04-11T04:23:18.4738909Z         )
2025-04-11T04:23:18.4738987Z     
2025-04-11T04:23:18.4739072Z         error_index = None
2025-04-11T04:23:18.4739165Z         for sentinel in ready:
2025-04-11T04:23:18.4739278Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4739382Z             process = self.processes[index]
2025-04-11T04:23:18.4739473Z             process.join()
2025-04-11T04:23:18.4739569Z             if process.exitcode != 0:
2025-04-11T04:23:18.4739659Z                 error_index = index
2025-04-11T04:23:18.4739741Z                 break
2025-04-11T04:23:18.4739815Z     
2025-04-11T04:23:18.4739915Z         # Return if there was no error.
2025-04-11T04:23:18.4740003Z         if error_index is None:
2025-04-11T04:23:18.4740137Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4740241Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4740416Z     
2025-04-11T04:23:18.4740572Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4740671Z         for process in self.processes:
2025-04-11T04:23:18.4740770Z             if process.is_alive():
2025-04-11T04:23:18.4740869Z                 process.terminate()
2025-04-11T04:23:18.4740955Z             process.join()
2025-04-11T04:23:18.4741036Z     
2025-04-11T04:23:18.4741179Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4741303Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4741413Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4741537Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4741632Z             if exitcode < 0:
2025-04-11T04:23:18.4741743Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4741860Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4742013Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4742124Z                     error_index=error_index,
2025-04-11T04:23:18.4742227Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4742401Z                     exit_code=exitcode,
2025-04-11T04:23:18.4742500Z                     signal_name=name,
2025-04-11T04:23:18.4742578Z                 )
2025-04-11T04:23:18.4742665Z             else:
2025-04-11T04:23:18.4742770Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4742935Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4743038Z                     error_index=error_index,
2025-04-11T04:23:18.4743140Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4743237Z                     exit_code=exitcode,
2025-04-11T04:23:18.4743312Z                 )
2025-04-11T04:23:18.4743391Z     
2025-04-11T04:23:18.4743523Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4743697Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4743793Z         msg += original_trace
2025-04-11T04:23:18.4743969Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4744144Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4744219Z E       
2025-04-11T04:23:18.4744346Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4744455Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4744772Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4744869Z E           fn(i, *args)
2025-04-11T04:23:18.4745106Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
2025-04-11T04:23:18.4745197Z E           check_all2all()
2025-04-11T04:23:18.4745423Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4745529Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4745793Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4745898Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4746185Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4746290Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4746404Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4746688Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4746830Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4746993Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4747082Z 
2025-04-11T04:23:18.4747390Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4747548Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4747705Z [04/11/25 04:14:20] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4747842Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4747949Z                              :75 launch                                         
2025-04-11T04:23:18.4748092Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4748217Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4748466Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4748617Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4748916Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:62642 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4749217Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:62642 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4749880Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4749974Z   warnings.warn(
2025-04-11T04:23:18.4750527Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4750621Z   warnings.warn(
2025-04-11T04:23:18.4751186Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4751277Z   warnings.warn(
2025-04-11T04:23:18.4751831Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4751923Z   warnings.warn(
2025-04-11T04:23:18.4752484Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4752564Z   warnings.warn(
2025-04-11T04:23:18.4753108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4753189Z   warnings.warn(
2025-04-11T04:23:18.4753744Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4753829Z   warnings.warn(
2025-04-11T04:23:18.4754378Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4754459Z   warnings.warn(
2025-04-11T04:23:18.4754606Z _______________________________ test_all_to_all ________________________________
2025-04-11T04:23:18.4754610Z 
2025-04-11T04:23:18.4754703Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4755410Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4755422Z 
2025-04-11T04:23:18.4755529Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4755615Z         try_count = 0
2025-04-11T04:23:18.4755733Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4755822Z             max_try, int
2025-04-11T04:23:18.4755985Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4756061Z     
2025-04-11T04:23:18.4756180Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4756266Z             try:
2025-04-11T04:23:18.4756357Z                 try_count += 1
2025-04-11T04:23:18.4756463Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4756550Z                 return ret
2025-04-11T04:23:18.4756657Z             except exception_type as e:
2025-04-11T04:23:18.4756763Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4756962Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4757093Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4757244Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4757492Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4757574Z                     continue
2025-04-11T04:23:18.4757659Z                 else:
2025-04-11T04:23:18.4757886Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4757968Z >                   raise e
2025-04-11T04:23:18.4757972Z 
2025-04-11T04:23:18.4758075Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4758187Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4758325Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4758416Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4758573Z tests/test_fp8/test_fp8_all_to_all.py:36: in test_all_to_all
2025-04-11T04:23:18.4758657Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4758761Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4758871Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4759127Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4759312Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4759602Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4759702Z     while not context.join():
2025-04-11T04:23:18.4759817Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4759822Z 
2025-04-11T04:23:18.4760021Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b841a410>
2025-04-11T04:23:18.4760109Z timeout = None
2025-04-11T04:23:18.4760117Z 
2025-04-11T04:23:18.4760210Z     def join(self, timeout=None):
2025-04-11T04:23:18.4760342Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4760414Z     
2025-04-11T04:23:18.4760567Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4760711Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4760874Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4760974Z         of the first process exiting.
2025-04-11T04:23:18.4761047Z     
2025-04-11T04:23:18.4761201Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4761338Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4761416Z     
2025-04-11T04:23:18.4761491Z         Args:
2025-04-11T04:23:18.4761638Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4761815Z         """
2025-04-11T04:23:18.4761967Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4762068Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4762148Z             return True
2025-04-11T04:23:18.4762223Z     
2025-04-11T04:23:18.4762363Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4762484Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4762581Z             self.sentinels.keys(),
2025-04-11T04:23:18.4762667Z             timeout=timeout,
2025-04-11T04:23:18.4762740Z         )
2025-04-11T04:23:18.4762818Z     
2025-04-11T04:23:18.4762902Z         error_index = None
2025-04-11T04:23:18.4762996Z         for sentinel in ready:
2025-04-11T04:23:18.4763104Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4763210Z             process = self.processes[index]
2025-04-11T04:23:18.4763297Z             process.join()
2025-04-11T04:23:18.4763388Z             if process.exitcode != 0:
2025-04-11T04:23:18.4763489Z                 error_index = index
2025-04-11T04:23:18.4763569Z                 break
2025-04-11T04:23:18.4763649Z     
2025-04-11T04:23:18.4763742Z         # Return if there was no error.
2025-04-11T04:23:18.4763917Z         if error_index is None:
2025-04-11T04:23:18.4764058Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4764156Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4764234Z     
2025-04-11T04:23:18.4764374Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4764472Z         for process in self.processes:
2025-04-11T04:23:18.4764569Z             if process.is_alive():
2025-04-11T04:23:18.4764662Z                 process.terminate()
2025-04-11T04:23:18.4764754Z             process.join()
2025-04-11T04:23:18.4764826Z     
2025-04-11T04:23:18.4764975Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4765093Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4765201Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4765328Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4765416Z             if exitcode < 0:
2025-04-11T04:23:18.4765529Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4765635Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4765794Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4765889Z                     error_index=error_index,
2025-04-11T04:23:18.4765991Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4766087Z                     exit_code=exitcode,
2025-04-11T04:23:18.4766176Z                     signal_name=name,
2025-04-11T04:23:18.4766261Z                 )
2025-04-11T04:23:18.4766338Z             else:
2025-04-11T04:23:18.4766444Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4766623Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4766716Z                     error_index=error_index,
2025-04-11T04:23:18.4766823Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4766914Z                     exit_code=exitcode,
2025-04-11T04:23:18.4766995Z                 )
2025-04-11T04:23:18.4767067Z     
2025-04-11T04:23:18.4767200Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4767378Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4767465Z         msg += original_trace
2025-04-11T04:23:18.4767643Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4767804Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4767880Z E       
2025-04-11T04:23:18.4768013Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4768212Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4768527Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4768609Z E           fn(i, *args)
2025-04-11T04:23:18.4768852Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
2025-04-11T04:23:18.4768939Z E           check_4gpu()
2025-04-11T04:23:18.4769164Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4769273Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4769536Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4769646Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4769927Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4770052Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4770161Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4770449Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4770687Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4770850Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4770855Z 
2025-04-11T04:23:18.4771171Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4771323Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4771492Z [04/11/25 04:14:24] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4771622Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4771741Z                              :75 launch                                         
2025-04-11T04:23:18.4771879Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4772004Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4772217Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4772363Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4772930Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4773014Z   warnings.warn(
2025-04-11T04:23:18.4773559Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4773644Z   warnings.warn(
2025-04-11T04:23:18.4774193Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4774278Z   warnings.warn(
2025-04-11T04:23:18.4774815Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4774895Z   warnings.warn(
2025-04-11T04:23:18.4775432Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4775519Z   warnings.warn(
2025-04-11T04:23:18.4776128Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4776217Z   warnings.warn(
2025-04-11T04:23:18.4776758Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4776847Z   warnings.warn(
2025-04-11T04:23:18.4777370Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4777457Z   warnings.warn(
2025-04-11T04:23:18.4777593Z ____________________________ test_all_to_all_single ____________________________
2025-04-11T04:23:18.4777597Z 
2025-04-11T04:23:18.4777689Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4778296Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4778384Z 
2025-04-11T04:23:18.4778490Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4778579Z         try_count = 0
2025-04-11T04:23:18.4778682Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4778770Z             max_try, int
2025-04-11T04:23:18.4778918Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4778998Z     
2025-04-11T04:23:18.4779111Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4779186Z             try:
2025-04-11T04:23:18.4779278Z                 try_count += 1
2025-04-11T04:23:18.4779371Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4779460Z                 return ret
2025-04-11T04:23:18.4779555Z             except exception_type as e:
2025-04-11T04:23:18.4779660Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4779854Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4779976Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4780130Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4780286Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4780376Z                     continue
2025-04-11T04:23:18.4780453Z                 else:
2025-04-11T04:23:18.4780683Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4780779Z >                   raise e
2025-04-11T04:23:18.4780783Z 
2025-04-11T04:23:18.4780886Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4781007Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4781140Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4781234Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4781407Z tests/test_fp8/test_fp8_all_to_all_single.py:34: in test_all_to_all_single
2025-04-11T04:23:18.4781496Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4781606Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4781708Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4781971Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4782151Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4782442Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4782533Z     while not context.join():
2025-04-11T04:23:18.4782644Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4782648Z 
2025-04-11T04:23:18.4782943Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b835abf0>
2025-04-11T04:23:18.4783029Z timeout = None
2025-04-11T04:23:18.4783033Z 
2025-04-11T04:23:18.4783133Z     def join(self, timeout=None):
2025-04-11T04:23:18.4783262Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4783339Z     
2025-04-11T04:23:18.4783487Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4783632Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4783804Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4783898Z         of the first process exiting.
2025-04-11T04:23:18.4783982Z     
2025-04-11T04:23:18.4784132Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4784279Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4784354Z     
2025-04-11T04:23:18.4784434Z         Args:
2025-04-11T04:23:18.4784585Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4784664Z         """
2025-04-11T04:23:18.4784817Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4785000Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4785083Z             return True
2025-04-11T04:23:18.4785163Z     
2025-04-11T04:23:18.4785294Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4785418Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4785512Z             self.sentinels.keys(),
2025-04-11T04:23:18.4785605Z             timeout=timeout,
2025-04-11T04:23:18.4785679Z         )
2025-04-11T04:23:18.4785750Z     
2025-04-11T04:23:18.4785842Z         error_index = None
2025-04-11T04:23:18.4785929Z         for sentinel in ready:
2025-04-11T04:23:18.4786040Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4786141Z             process = self.processes[index]
2025-04-11T04:23:18.4786228Z             process.join()
2025-04-11T04:23:18.4786326Z             if process.exitcode != 0:
2025-04-11T04:23:18.4786415Z                 error_index = index
2025-04-11T04:23:18.4786502Z                 break
2025-04-11T04:23:18.4786574Z     
2025-04-11T04:23:18.4786666Z         # Return if there was no error.
2025-04-11T04:23:18.4786761Z         if error_index is None:
2025-04-11T04:23:18.4786896Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4786999Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4787071Z     
2025-04-11T04:23:18.4787218Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4787316Z         for process in self.processes:
2025-04-11T04:23:18.4787404Z             if process.is_alive():
2025-04-11T04:23:18.4787503Z                 process.terminate()
2025-04-11T04:23:18.4787587Z             process.join()
2025-04-11T04:23:18.4787662Z     
2025-04-11T04:23:18.4787807Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4787922Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4788033Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4788157Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4788246Z             if exitcode < 0:
2025-04-11T04:23:18.4788353Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4788517Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4788671Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4788770Z                     error_index=error_index,
2025-04-11T04:23:18.4788879Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4788969Z                     exit_code=exitcode,
2025-04-11T04:23:18.4789065Z                     signal_name=name,
2025-04-11T04:23:18.4789141Z                 )
2025-04-11T04:23:18.4789314Z             else:
2025-04-11T04:23:18.4789430Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4789596Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4789701Z                     error_index=error_index,
2025-04-11T04:23:18.4789802Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4789896Z                     exit_code=exitcode,
2025-04-11T04:23:18.4789972Z                 )
2025-04-11T04:23:18.4790045Z     
2025-04-11T04:23:18.4790191Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4790368Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4790471Z         msg += original_trace
2025-04-11T04:23:18.4790657Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4790825Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4790910Z E       
2025-04-11T04:23:18.4791042Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4791153Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4791455Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4791663Z E           fn(i, *args)
2025-04-11T04:23:18.4791915Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
2025-04-11T04:23:18.4792001Z E           check_4gpu()
2025-04-11T04:23:18.4792234Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4792337Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4792604Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4792708Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4792995Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4793101Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4793209Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4793511Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4793648Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4793825Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4793829Z 
2025-04-11T04:23:18.4794132Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4794294Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4794450Z [04/11/25 04:14:29] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4794591Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4794701Z                              :75 launch                                         
2025-04-11T04:23:18.4794844Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4794983Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4795184Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4795338Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4795910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4795999Z   warnings.warn(
2025-04-11T04:23:18.4796636Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4796729Z   warnings.warn(
2025-04-11T04:23:18.4797288Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4797376Z   warnings.warn(
2025-04-11T04:23:18.4797919Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4798001Z   warnings.warn(
2025-04-11T04:23:18.4798574Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4798659Z   warnings.warn(
2025-04-11T04:23:18.4799224Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4799391Z   warnings.warn(
2025-04-11T04:23:18.4799970Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4800052Z   warnings.warn(
2025-04-11T04:23:18.4800604Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4800682Z   warnings.warn(
2025-04-11T04:23:18.4800818Z _______________________________ test_all_gather ________________________________
2025-04-11T04:23:18.4800833Z 
2025-04-11T04:23:18.4800929Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4801531Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4801538Z 
2025-04-11T04:23:18.4801647Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4801729Z         try_count = 0
2025-04-11T04:23:18.4801836Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4801916Z             max_try, int
2025-04-11T04:23:18.4802073Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4802147Z     
2025-04-11T04:23:18.4802262Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4802346Z             try:
2025-04-11T04:23:18.4802433Z                 try_count += 1
2025-04-11T04:23:18.4802536Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4802617Z                 return ret
2025-04-11T04:23:18.4802712Z             except exception_type as e:
2025-04-11T04:23:18.4802819Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4803011Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4803135Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4803281Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4803442Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4803525Z                     continue
2025-04-11T04:23:18.4803604Z                 else:
2025-04-11T04:23:18.4803834Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4803914Z >                   raise e
2025-04-11T04:23:18.4803918Z 
2025-04-11T04:23:18.4804114Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4804228Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4804366Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4804460Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4804608Z tests/test_fp8/test_fp8_allgather.py:42: in test_all_gather
2025-04-11T04:23:18.4804700Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4804802Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4804909Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4805166Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4805351Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4805637Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4805730Z     while not context.join():
2025-04-11T04:23:18.4805852Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4805857Z 
2025-04-11T04:23:18.4806058Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8571db0>
2025-04-11T04:23:18.4806229Z timeout = None
2025-04-11T04:23:18.4806234Z 
2025-04-11T04:23:18.4806327Z     def join(self, timeout=None):
2025-04-11T04:23:18.4806458Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4806533Z     
2025-04-11T04:23:18.4806683Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4806836Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4807001Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4807102Z         of the first process exiting.
2025-04-11T04:23:18.4807176Z     
2025-04-11T04:23:18.4807331Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4807474Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4807545Z     
2025-04-11T04:23:18.4807627Z         Args:
2025-04-11T04:23:18.4807767Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4807851Z         """
2025-04-11T04:23:18.4807993Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4808088Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4808178Z             return True
2025-04-11T04:23:18.4808252Z     
2025-04-11T04:23:18.4808392Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4808510Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4808610Z             self.sentinels.keys(),
2025-04-11T04:23:18.4808695Z             timeout=timeout,
2025-04-11T04:23:18.4808769Z         )
2025-04-11T04:23:18.4808848Z     
2025-04-11T04:23:18.4808933Z         error_index = None
2025-04-11T04:23:18.4809031Z         for sentinel in ready:
2025-04-11T04:23:18.4809146Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4809247Z             process = self.processes[index]
2025-04-11T04:23:18.4809340Z             process.join()
2025-04-11T04:23:18.4809445Z             if process.exitcode != 0:
2025-04-11T04:23:18.4809542Z                 error_index = index
2025-04-11T04:23:18.4809620Z                 break
2025-04-11T04:23:18.4809693Z     
2025-04-11T04:23:18.4809794Z         # Return if there was no error.
2025-04-11T04:23:18.4809883Z         if error_index is None:
2025-04-11T04:23:18.4810030Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4810128Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4810208Z     
2025-04-11T04:23:18.4810354Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4810453Z         for process in self.processes:
2025-04-11T04:23:18.4810550Z             if process.is_alive():
2025-04-11T04:23:18.4810740Z                 process.terminate()
2025-04-11T04:23:18.4810836Z             process.join()
2025-04-11T04:23:18.4810910Z     
2025-04-11T04:23:18.4811054Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4811182Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4811290Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4811418Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4811501Z             if exitcode < 0:
2025-04-11T04:23:18.4811616Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4811720Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4811870Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4811973Z                     error_index=error_index,
2025-04-11T04:23:18.4812074Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4812172Z                     exit_code=exitcode,
2025-04-11T04:23:18.4812261Z                     signal_name=name,
2025-04-11T04:23:18.4812336Z                 )
2025-04-11T04:23:18.4812420Z             else:
2025-04-11T04:23:18.4812524Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4812782Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4812878Z                     error_index=error_index,
2025-04-11T04:23:18.4812985Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4813073Z                     exit_code=exitcode,
2025-04-11T04:23:18.4813149Z                 )
2025-04-11T04:23:18.4813228Z     
2025-04-11T04:23:18.4813359Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4813535Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4813625Z         msg += original_trace
2025-04-11T04:23:18.4813806Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4813967Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4814042Z E       
2025-04-11T04:23:18.4814176Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4814279Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4814580Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4814661Z E           fn(i, *args)
2025-04-11T04:23:18.4814891Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
2025-04-11T04:23:18.4814983Z E           check_4gpu()
2025-04-11T04:23:18.4815204Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4815311Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4815577Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4815685Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4815966Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4816077Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4816183Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4816468Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4816658Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4816819Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4816823Z 
2025-04-11T04:23:18.4817138Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4817292Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4817536Z [04/11/25 04:14:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4817669Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4817784Z                              :75 launch                                         
2025-04-11T04:23:18.4817931Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4818058Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4818263Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4818409Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4818716Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60836 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4819282Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4819375Z   warnings.warn(
2025-04-11T04:23:18.4820013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4820105Z   warnings.warn(
2025-04-11T04:23:18.4820643Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4820725Z   warnings.warn(
2025-04-11T04:23:18.4821261Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4821346Z   warnings.warn(
2025-04-11T04:23:18.4821891Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4821975Z   warnings.warn(
2025-04-11T04:23:18.4822505Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4822583Z   warnings.warn(
2025-04-11T04:23:18.4823127Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4823207Z   warnings.warn(
2025-04-11T04:23:18.4823734Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4823823Z   warnings.warn(
2025-04-11T04:23:18.4823963Z _______________________________ test_all_reduce ________________________________
2025-04-11T04:23:18.4823967Z 
2025-04-11T04:23:18.4824069Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4824661Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4824665Z 
2025-04-11T04:23:18.4824776Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4824860Z         try_count = 0
2025-04-11T04:23:18.4824974Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4825057Z             max_try, int
2025-04-11T04:23:18.4825292Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4825376Z     
2025-04-11T04:23:18.4825491Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4825573Z             try:
2025-04-11T04:23:18.4825663Z                 try_count += 1
2025-04-11T04:23:18.4825762Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4825844Z                 return ret
2025-04-11T04:23:18.4825940Z             except exception_type as e:
2025-04-11T04:23:18.4826047Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4826242Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4826364Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4826510Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4826681Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4826767Z                     continue
2025-04-11T04:23:18.4826845Z                 else:
2025-04-11T04:23:18.4827083Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4827251Z >                   raise e
2025-04-11T04:23:18.4827256Z 
2025-04-11T04:23:18.4827358Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4827471Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4827610Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4827698Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4827848Z tests/test_fp8/test_fp8_allreduce.py:52: in test_all_reduce
2025-04-11T04:23:18.4827941Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4828044Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4828152Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4828405Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4828615Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4828905Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4829000Z     while not context.join():
2025-04-11T04:23:18.4829117Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4829121Z 
2025-04-11T04:23:18.4829323Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e1fc0>
2025-04-11T04:23:18.4829410Z timeout = None
2025-04-11T04:23:18.4829414Z 
2025-04-11T04:23:18.4829506Z     def join(self, timeout=None):
2025-04-11T04:23:18.4829637Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4829709Z     
2025-04-11T04:23:18.4829857Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4830008Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4830178Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4830278Z         of the first process exiting.
2025-04-11T04:23:18.4830349Z     
2025-04-11T04:23:18.4830500Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4830646Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4830720Z     
2025-04-11T04:23:18.4830801Z         Args:
2025-04-11T04:23:18.4830941Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4831023Z         """
2025-04-11T04:23:18.4831164Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4831260Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4831350Z             return True
2025-04-11T04:23:18.4831422Z     
2025-04-11T04:23:18.4831559Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4831863Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4831962Z             self.sentinels.keys(),
2025-04-11T04:23:18.4832057Z             timeout=timeout,
2025-04-11T04:23:18.4832133Z         )
2025-04-11T04:23:18.4832214Z     
2025-04-11T04:23:18.4832306Z         error_index = None
2025-04-11T04:23:18.4832396Z         for sentinel in ready:
2025-04-11T04:23:18.4832512Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4832612Z             process = self.processes[index]
2025-04-11T04:23:18.4832709Z             process.join()
2025-04-11T04:23:18.4832804Z             if process.exitcode != 0:
2025-04-11T04:23:18.4832901Z                 error_index = index
2025-04-11T04:23:18.4832980Z                 break
2025-04-11T04:23:18.4833054Z     
2025-04-11T04:23:18.4833157Z         # Return if there was no error.
2025-04-11T04:23:18.4833246Z         if error_index is None:
2025-04-11T04:23:18.4833389Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4833491Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4833566Z     
2025-04-11T04:23:18.4833714Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4833814Z         for process in self.processes:
2025-04-11T04:23:18.4834006Z             if process.is_alive():
2025-04-11T04:23:18.4834100Z                 process.terminate()
2025-04-11T04:23:18.4834193Z             process.join()
2025-04-11T04:23:18.4834265Z     
2025-04-11T04:23:18.4834408Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4834529Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4834638Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4834765Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4834862Z             if exitcode < 0:
2025-04-11T04:23:18.4834971Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4835100Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4835257Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4835369Z                     error_index=error_index,
2025-04-11T04:23:18.4835482Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4835598Z                     exit_code=exitcode,
2025-04-11T04:23:18.4835691Z                     signal_name=name,
2025-04-11T04:23:18.4835773Z                 )
2025-04-11T04:23:18.4835871Z             else:
2025-04-11T04:23:18.4835987Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4836162Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4836259Z                     error_index=error_index,
2025-04-11T04:23:18.4836361Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4836458Z                     exit_code=exitcode,
2025-04-11T04:23:18.4836534Z                 )
2025-04-11T04:23:18.4836617Z     
2025-04-11T04:23:18.4836754Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4836932Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4837022Z         msg += original_trace
2025-04-11T04:23:18.4837200Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4837372Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4837449Z E       
2025-04-11T04:23:18.4837583Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4837683Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4837991Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4838075Z E           fn(i, *args)
2025-04-11T04:23:18.4838303Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
2025-04-11T04:23:18.4838397Z E           check_4gpu()
2025-04-11T04:23:18.4838739Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4838844Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4839065Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4839179Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4839436Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4839539Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4839823Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4839931Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4840048Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4840337Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4840485Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4840650Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4840737Z 
2025-04-11T04:23:18.4841047Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4841208Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4841364Z [04/11/25 04:14:40] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4841505Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4841616Z                              :75 launch                                         
2025-04-11T04:23:18.4841767Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4841899Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4842107Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4842253Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4842554Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26964 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4843120Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4843205Z   warnings.warn(
2025-04-11T04:23:18.4843752Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4843836Z   warnings.warn(
2025-04-11T04:23:18.4844380Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4844466Z   warnings.warn(
2025-04-11T04:23:18.4845005Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4845114Z   warnings.warn(
2025-04-11T04:23:18.4845677Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4845762Z   warnings.warn(
2025-04-11T04:23:18.4846394Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4846494Z   warnings.warn(
2025-04-11T04:23:18.4847033Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4847127Z   warnings.warn(
2025-04-11T04:23:18.4847650Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4847740Z   warnings.warn(
2025-04-11T04:23:18.4847878Z ________________________________ test_fp8_cast _________________________________
2025-04-11T04:23:18.4847882Z 
2025-04-11T04:23:18.4847977Z args = (), kwargs = {}
2025-04-11T04:23:18.4847981Z 
2025-04-11T04:23:18.4848080Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.4848196Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.4848316Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.4848440Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.4848642Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.4848738Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.4848743Z 
2025-04-11T04:23:18.4848849Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.4848965Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4849125Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.4849235Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.4849347Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4849352Z 
2025-04-11T04:23:18.4849446Z device = None
2025-04-11T04:23:18.4849451Z 
2025-04-11T04:23:18.4849574Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.4849745Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4849820Z     
2025-04-11T04:23:18.4849909Z         Args:
2025-04-11T04:23:18.4850087Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.4850265Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.4850389Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.4850466Z         """
2025-04-11T04:23:18.4850558Z         _lazy_init()
2025-04-11T04:23:18.4850660Z         with torch.cuda.device(device):
2025-04-11T04:23:18.4850767Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4850896Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4851200Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4851367Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4851535Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4851539Z 
2025-04-11T04:23:18.4851790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.4851927Z __________________________________ test_fsdp ___________________________________
2025-04-11T04:23:18.4851931Z 
2025-04-11T04:23:18.4852034Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4852635Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4852640Z 
2025-04-11T04:23:18.4852750Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4852832Z         try_count = 0
2025-04-11T04:23:18.4852933Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4853107Z             max_try, int
2025-04-11T04:23:18.4853260Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4853345Z     
2025-04-11T04:23:18.4853461Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4853540Z             try:
2025-04-11T04:23:18.4853638Z                 try_count += 1
2025-04-11T04:23:18.4853733Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4853824Z                 return ret
2025-04-11T04:23:18.4853919Z             except exception_type as e:
2025-04-11T04:23:18.4854030Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4854221Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4854341Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4854499Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4854660Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4854762Z                     continue
2025-04-11T04:23:18.4854841Z                 else:
2025-04-11T04:23:18.4855089Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4855266Z >                   raise e
2025-04-11T04:23:18.4855271Z 
2025-04-11T04:23:18.4855368Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4855489Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4855621Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4855726Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4855876Z tests/test_fp8/test_fp8_fsdp_comm_hook.py:104: in test_fsdp
2025-04-11T04:23:18.4855976Z     spawn(demo_basic, n_gpus)
2025-04-11T04:23:18.4856079Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4856181Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4856449Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4856630Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4856927Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4857018Z     while not context.join():
2025-04-11T04:23:18.4857140Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4857144Z 
2025-04-11T04:23:18.4857342Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e3ee0>
2025-04-11T04:23:18.4857426Z timeout = None
2025-04-11T04:23:18.4857430Z 
2025-04-11T04:23:18.4857534Z     def join(self, timeout=None):
2025-04-11T04:23:18.4857663Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4857749Z     
2025-04-11T04:23:18.4857900Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4858058Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4858226Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4858324Z         of the first process exiting.
2025-04-11T04:23:18.4858406Z     
2025-04-11T04:23:18.4858554Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4858703Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4858777Z     
2025-04-11T04:23:18.4858853Z         Args:
2025-04-11T04:23:18.4859001Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4859078Z         """
2025-04-11T04:23:18.4859228Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4859324Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4859416Z             return True
2025-04-11T04:23:18.4859489Z     
2025-04-11T04:23:18.4859706Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4859841Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4859938Z             self.sentinels.keys(),
2025-04-11T04:23:18.4860045Z             timeout=timeout,
2025-04-11T04:23:18.4860124Z         )
2025-04-11T04:23:18.4860198Z     
2025-04-11T04:23:18.4860294Z         error_index = None
2025-04-11T04:23:18.4860384Z         for sentinel in ready:
2025-04-11T04:23:18.4860502Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4860606Z             process = self.processes[index]
2025-04-11T04:23:18.4860695Z             process.join()
2025-04-11T04:23:18.4860800Z             if process.exitcode != 0:
2025-04-11T04:23:18.4860891Z                 error_index = index
2025-04-11T04:23:18.4860979Z                 break
2025-04-11T04:23:18.4861054Z     
2025-04-11T04:23:18.4861156Z         # Return if there was no error.
2025-04-11T04:23:18.4861249Z         if error_index is None:
2025-04-11T04:23:18.4861392Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4861508Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4861587Z     
2025-04-11T04:23:18.4861745Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4861941Z         for process in self.processes:
2025-04-11T04:23:18.4862033Z             if process.is_alive():
2025-04-11T04:23:18.4862141Z                 process.terminate()
2025-04-11T04:23:18.4862229Z             process.join()
2025-04-11T04:23:18.4862312Z     
2025-04-11T04:23:18.4862458Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4862584Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4862694Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4862817Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4862911Z             if exitcode < 0:
2025-04-11T04:23:18.4863024Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4863142Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4863294Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4863395Z                     error_index=error_index,
2025-04-11T04:23:18.4863506Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4863597Z                     exit_code=exitcode,
2025-04-11T04:23:18.4863696Z                     signal_name=name,
2025-04-11T04:23:18.4863773Z                 )
2025-04-11T04:23:18.4863862Z             else:
2025-04-11T04:23:18.4863970Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4864136Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4864241Z                     error_index=error_index,
2025-04-11T04:23:18.4864344Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4864477Z                     exit_code=exitcode,
2025-04-11T04:23:18.4864566Z                 )
2025-04-11T04:23:18.4864639Z     
2025-04-11T04:23:18.4864791Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4864964Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4865072Z         msg += original_trace
2025-04-11T04:23:18.4865249Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4865422Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4865498Z E       
2025-04-11T04:23:18.4865625Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4865735Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4866034Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4866126Z E           fn(i, *args)
2025-04-11T04:23:18.4866464Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
2025-04-11T04:23:18.4866560Z E           run_model()
2025-04-11T04:23:18.4866782Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4866888Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4867157Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4867262Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4867548Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4867655Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4867769Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4868056Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4868196Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4868369Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4868374Z 
2025-04-11T04:23:18.4868704Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4868971Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4869075Z Running basic FSDP example on rank 1.
2025-04-11T04:23:18.4869186Z Running basic FSDP example on rank 5.
2025-04-11T04:23:18.4869282Z Running basic FSDP example on rank 0.
2025-04-11T04:23:18.4869453Z [04/11/25 04:14:47] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4869587Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4869698Z                              :75 launch                                         
2025-04-11T04:23:18.4869855Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4869984Z                              environment is initialized, world size: 8          
2025-04-11T04:23:18.4870088Z Running basic FSDP example on rank 6.
2025-04-11T04:23:18.4870187Z Running basic FSDP example on rank 4.
2025-04-11T04:23:18.4870289Z Running basic FSDP example on rank 7.
2025-04-11T04:23:18.4870384Z Running basic FSDP example on rank 2.
2025-04-11T04:23:18.4870477Z Running basic FSDP example on rank 3.
2025-04-11T04:23:18.4870693Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4870845Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4871161Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4871452Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4871750Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4872029Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4872310Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4872881Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4872967Z   warnings.warn(
2025-04-11T04:23:18.4873511Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4873707Z   warnings.warn(
2025-04-11T04:23:18.4874310Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4874396Z   warnings.warn(
2025-04-11T04:23:18.4874944Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4875025Z   warnings.warn(
2025-04-11T04:23:18.4875593Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4875673Z   warnings.warn(
2025-04-11T04:23:18.4876235Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4876318Z   warnings.warn(
2025-04-11T04:23:18.4876961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4877054Z   warnings.warn(
2025-04-11T04:23:18.4877594Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4877688Z   warnings.warn(
2025-04-11T04:23:18.4878246Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4878344Z   warnings.warn(
2025-04-11T04:23:18.4878883Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4878976Z   warnings.warn(
2025-04-11T04:23:18.4879524Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4879604Z   warnings.warn(
2025-04-11T04:23:18.4880155Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4880235Z   warnings.warn(
2025-04-11T04:23:18.4880802Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4880882Z   warnings.warn(
2025-04-11T04:23:18.4881424Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4881521Z   warnings.warn(
2025-04-11T04:23:18.4882086Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4882166Z   warnings.warn(
2025-04-11T04:23:18.4882715Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4882794Z   warnings.warn(
2025-04-11T04:23:18.4883650Z [rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T04:23:18.4884402Z [rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T04:23:18.4885148Z [rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T04:23:18.4885884Z [rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T04:23:18.4886125Z ________________________________ test_fp8_hook _________________________________
2025-04-11T04:23:18.4886129Z 
2025-04-11T04:23:18.4886413Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4886507Z     def test_fp8_hook():
2025-04-11T04:23:18.4886591Z         # create tensors
2025-04-11T04:23:18.4886786Z >       w = nn.Parameter(torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE))
2025-04-11T04:23:18.4886904Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4887196Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4887340Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4887504Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4887509Z 
2025-04-11T04:23:18.4887633Z tests/test_fp8/test_fp8_hook.py:41: RuntimeError
2025-04-11T04:23:18.4887776Z __________________________ test_fp8_linear[True-True] __________________________
2025-04-11T04:23:18.4887780Z 
2025-04-11T04:23:18.4887874Z use_bias = True, use_batch = True
2025-04-11T04:23:18.4887885Z 
2025-04-11T04:23:18.4888156Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4888287Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T04:23:18.4888424Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T04:23:18.4888554Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T04:23:18.4888641Z         # create tensors
2025-04-11T04:23:18.4888838Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T04:23:18.4888953Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4889235Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4889368Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4889533Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4889537Z 
2025-04-11T04:23:18.4889658Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T04:23:18.4889808Z _________________________ test_fp8_linear[True-False] __________________________
2025-04-11T04:23:18.4889813Z 
2025-04-11T04:23:18.4889909Z use_bias = False, use_batch = True
2025-04-11T04:23:18.4889913Z 
2025-04-11T04:23:18.4890268Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4890398Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T04:23:18.4890534Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T04:23:18.4890663Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T04:23:18.4890746Z         # create tensors
2025-04-11T04:23:18.4890947Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T04:23:18.4891055Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4891346Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4891486Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4891656Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4891661Z 
2025-04-11T04:23:18.4891784Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T04:23:18.4891929Z _________________________ test_fp8_linear[False-True] __________________________
2025-04-11T04:23:18.4891940Z 
2025-04-11T04:23:18.4892120Z use_bias = True, use_batch = False
2025-04-11T04:23:18.4892125Z 
2025-04-11T04:23:18.4892396Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4892529Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T04:23:18.4892654Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T04:23:18.4892785Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T04:23:18.4892867Z         # create tensors
2025-04-11T04:23:18.4893065Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T04:23:18.4893170Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4893454Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4893593Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4893754Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4893758Z 
2025-04-11T04:23:18.4893883Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T04:23:18.4894027Z _________________________ test_fp8_linear[False-False] _________________________
2025-04-11T04:23:18.4894031Z 
2025-04-11T04:23:18.4894136Z use_bias = False, use_batch = False
2025-04-11T04:23:18.4894141Z 
2025-04-11T04:23:18.4894408Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T04:23:18.4894536Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T04:23:18.4894662Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T04:23:18.4894787Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T04:23:18.4894881Z         # create tensors
2025-04-11T04:23:18.4895081Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T04:23:18.4895198Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4895478Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4895618Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4895776Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4895781Z 
2025-04-11T04:23:18.4895898Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T04:23:18.4896044Z _____________________________ test_reduce_scatter ______________________________
2025-04-11T04:23:18.4896048Z 
2025-04-11T04:23:18.4896141Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4896846Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4896855Z 
2025-04-11T04:23:18.4896962Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4897052Z         try_count = 0
2025-04-11T04:23:18.4897153Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4897239Z             max_try, int
2025-04-11T04:23:18.4897389Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4897462Z     
2025-04-11T04:23:18.4897583Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4897659Z             try:
2025-04-11T04:23:18.4897751Z                 try_count += 1
2025-04-11T04:23:18.4897846Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4897927Z                 return ret
2025-04-11T04:23:18.4898031Z             except exception_type as e:
2025-04-11T04:23:18.4898136Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4898330Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4898540Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4898695Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4898856Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4898938Z                     continue
2025-04-11T04:23:18.4899022Z                 else:
2025-04-11T04:23:18.4899251Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4899338Z >                   raise e
2025-04-11T04:23:18.4899343Z 
2025-04-11T04:23:18.4899438Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4899564Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4899699Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4899787Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4899962Z tests/test_fp8/test_fp8_reduce_scatter.py:41: in test_reduce_scatter
2025-04-11T04:23:18.4900050Z     spawn(run_dist, 4)
2025-04-11T04:23:18.4900160Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4900263Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4900543Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4900737Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4901034Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4901130Z     while not context.join():
2025-04-11T04:23:18.4901243Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4901247Z 
2025-04-11T04:23:18.4901458Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e06d0>
2025-04-11T04:23:18.4901541Z timeout = None
2025-04-11T04:23:18.4901546Z 
2025-04-11T04:23:18.4901643Z     def join(self, timeout=None):
2025-04-11T04:23:18.4901773Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4901847Z     
2025-04-11T04:23:18.4902001Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4902147Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4902321Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4902416Z         of the first process exiting.
2025-04-11T04:23:18.4902496Z     
2025-04-11T04:23:18.4902644Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4902784Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4902863Z     
2025-04-11T04:23:18.4903023Z         Args:
2025-04-11T04:23:18.4903173Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4903249Z         """
2025-04-11T04:23:18.4903397Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4903496Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4903579Z             return True
2025-04-11T04:23:18.4903660Z     
2025-04-11T04:23:18.4903793Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4903920Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4904015Z             self.sentinels.keys(),
2025-04-11T04:23:18.4904101Z             timeout=timeout,
2025-04-11T04:23:18.4904183Z         )
2025-04-11T04:23:18.4904254Z     
2025-04-11T04:23:18.4904345Z         error_index = None
2025-04-11T04:23:18.4904433Z         for sentinel in ready:
2025-04-11T04:23:18.4904540Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4904654Z             process = self.processes[index]
2025-04-11T04:23:18.4904744Z             process.join()
2025-04-11T04:23:18.4904847Z             if process.exitcode != 0:
2025-04-11T04:23:18.4904938Z                 error_index = index
2025-04-11T04:23:18.4905110Z                 break
2025-04-11T04:23:18.4905184Z     
2025-04-11T04:23:18.4905278Z         # Return if there was no error.
2025-04-11T04:23:18.4905373Z         if error_index is None:
2025-04-11T04:23:18.4905506Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4905609Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4905682Z     
2025-04-11T04:23:18.4905822Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4905927Z         for process in self.processes:
2025-04-11T04:23:18.4906017Z             if process.is_alive():
2025-04-11T04:23:18.4906116Z                 process.terminate()
2025-04-11T04:23:18.4906202Z             process.join()
2025-04-11T04:23:18.4906273Z     
2025-04-11T04:23:18.4906425Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4906541Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4906661Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4906787Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4906883Z             if exitcode < 0:
2025-04-11T04:23:18.4906994Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4907103Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4907262Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4907359Z                     error_index=error_index,
2025-04-11T04:23:18.4907466Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4907556Z                     exit_code=exitcode,
2025-04-11T04:23:18.4907650Z                     signal_name=name,
2025-04-11T04:23:18.4907727Z                 )
2025-04-11T04:23:18.4907805Z             else:
2025-04-11T04:23:18.4907914Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4908080Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4908184Z                     error_index=error_index,
2025-04-11T04:23:18.4908284Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4908375Z                     exit_code=exitcode,
2025-04-11T04:23:18.4908525Z                 )
2025-04-11T04:23:18.4908601Z     
2025-04-11T04:23:18.4908746Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4908919Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4909015Z         msg += original_trace
2025-04-11T04:23:18.4909191Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4909355Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4909440Z E       
2025-04-11T04:23:18.4909659Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4909768Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4910072Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4910166Z E           fn(i, *args)
2025-04-11T04:23:18.4910413Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
2025-04-11T04:23:18.4910499Z E           check_4gpu()
2025-04-11T04:23:18.4910725Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.4910822Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.4911093Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.4911193Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.4911482Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.4911596Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.4911702Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4912091Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4912228Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4912397Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4912402Z 
2025-04-11T04:23:18.4912711Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4912870Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4913031Z [04/11/25 04:14:54] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4913415Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4913736Z                              :75 launch                                         
2025-04-11T04:23:18.4914070Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4914412Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.4914806Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4915235Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4915766Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24751 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4916441Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24751 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4917111Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24751 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.4918122Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4918857Z   warnings.warn(
2025-04-11T04:23:18.4919530Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4920242Z   warnings.warn(
2025-04-11T04:23:18.4920921Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4921622Z   warnings.warn(
2025-04-11T04:23:18.4922382Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4923081Z   warnings.warn(
2025-04-11T04:23:18.4923756Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4924445Z   warnings.warn(
2025-04-11T04:23:18.4925096Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4925764Z   warnings.warn(
2025-04-11T04:23:18.4926415Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4927099Z   warnings.warn(
2025-04-11T04:23:18.4927745Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.4928517Z   warnings.warn(
2025-04-11T04:23:18.4928762Z _________________________________ test_bucket __________________________________
2025-04-11T04:23:18.4928976Z 
2025-04-11T04:23:18.4929054Z kwargs = {}
2025-04-11T04:23:18.4929371Z val = {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}
2025-04-11T04:23:18.4929877Z arg_map = {'test_config': {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}}
2025-04-11T04:23:18.4930638Z partial_func = functools.partial(<function test_bucket at 0x7fb5e55d57e0>, test_config={'block_size': 4, 'max_batch_size': 4, 'max_input_len': 32, 'max_output_len': 8, 'dtype': torch.float16, 'tp_size': 1})
2025-04-11T04:23:18.4931142Z 
2025-04-11T04:23:18.4931259Z     def _execute_function_by_param(**kwargs):
2025-04-11T04:23:18.4931520Z         for val in values:
2025-04-11T04:23:18.4931764Z             arg_map = {argument: val}
2025-04-11T04:23:18.4932035Z             partial_func = partial(func, **arg_map)
2025-04-11T04:23:18.4932308Z >           partial_func(**kwargs)
2025-04-11T04:23:18.4932458Z 
2025-04-11T04:23:18.4932565Z colossalai/testing/utils.py:64: 
2025-04-11T04:23:18.4932834Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4933154Z tests/test_infer/test_batch_bucket.py:42: in test_bucket
2025-04-11T04:23:18.4933517Z     cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T04:23:18.4933908Z colossalai/inference/kv_cache/kvcache_manager.py:105: in __init__
2025-04-11T04:23:18.4934300Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:18.4934649Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4934833Z 
2025-04-11T04:23:18.4935078Z self = <colossalai.inference.kv_cache.kvcache_manager.KVCacheManager object at 0x7fb5b8491750>
2025-04-11T04:23:18.4935510Z kalloc_shape = (40, 4, 4, 32), valloc_shape = (40, 4, 4, 32)
2025-04-11T04:23:18.4935714Z 
2025-04-11T04:23:18.4935804Z     def _init_device_caches(
2025-04-11T04:23:18.4936107Z         self, kalloc_shape: Tuple[int, ...], valloc_shape: Tuple[int, ...]
2025-04-11T04:23:18.4936439Z     ) -> Tuple[torch.Tensor, torch.Tensor]:
2025-04-11T04:23:18.4936729Z         """Initialize the physical cache on the device.
2025-04-11T04:23:18.4936998Z     
2025-04-11T04:23:18.4937290Z         For each layer of the model, we allocate two tensors for key and value respectively,
2025-04-11T04:23:18.4937707Z         with shape of [num_blocks, num_kv_heads, block_size, head_size]
2025-04-11T04:23:18.4937997Z         """
2025-04-11T04:23:18.4938197Z         k_cache: List[torch.Tensor] = []
2025-04-11T04:23:18.4938552Z         v_cache: List[torch.Tensor] = []
2025-04-11T04:23:18.4938811Z         for _ in range(self.num_layers):
2025-04-11T04:23:18.4939186Z >           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:18.4939585Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4940055Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4940549Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4940907Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4941145Z 
2025-04-11T04:23:18.4941310Z colossalai/inference/kv_cache/kvcache_manager.py:519: RuntimeError
2025-04-11T04:23:18.4941690Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4942061Z [04/11/25 04:14:55] INFO     colossalai -                                       
2025-04-11T04:23:18.4942409Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.4942715Z                              INFO:                                              
2025-04-11T04:23:18.4943118Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T04:23:18.4943437Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T04:23:18.4943753Z                     INFO     colossalai -                                       
2025-04-11T04:23:18.4944074Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.4944397Z                              INFO: Allocating KV cache with shape: (40, 4, 4,   
2025-04-11T04:23:18.4944708Z                              32) consisting of 40 blocks.                       
2025-04-11T04:23:18.4945030Z ___________________________ test_continuous_batching ___________________________
2025-04-11T04:23:18.4945252Z 
2025-04-11T04:23:18.4945349Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4946127Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4946800Z 
2025-04-11T04:23:18.4946905Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.4947192Z         try_count = 0
2025-04-11T04:23:18.4947423Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.4947670Z             max_try, int
2025-04-11T04:23:18.4947949Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4948248Z     
2025-04-11T04:23:18.4948584Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.4948859Z             try:
2025-04-11T04:23:18.4949067Z                 try_count += 1
2025-04-11T04:23:18.4949330Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.4949593Z                 return ret
2025-04-11T04:23:18.4949843Z             except exception_type as e:
2025-04-11T04:23:18.4950123Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.4950487Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.4950882Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.4951238Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.4951622Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.4951943Z                     continue
2025-04-11T04:23:18.4952166Z                 else:
2025-04-11T04:23:18.4952506Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.4952888Z >                   raise e
2025-04-11T04:23:18.4953124Z 
2025-04-11T04:23:18.4953227Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.4953499Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4953828Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.4954119Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.4954446Z tests/test_infer/test_continuous_batching.py:67: in test_continuous_batching
2025-04-11T04:23:18.4954790Z     spawn(run_dist, 1)
2025-04-11T04:23:18.4955030Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.4955309Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.4955744Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.4956337Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.4956884Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.4957358Z     while not context.join():
2025-04-11T04:23:18.4957618Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.4957797Z 
2025-04-11T04:23:18.4958003Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5e5782c50>
2025-04-11T04:23:18.4958469Z timeout = None
2025-04-11T04:23:18.4958585Z 
2025-04-11T04:23:18.4958677Z     def join(self, timeout=None):
2025-04-11T04:23:18.4958960Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4959225Z     
2025-04-11T04:23:18.4959469Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.4959827Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.4960202Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.4960536Z         of the first process exiting.
2025-04-11T04:23:18.4960775Z     
2025-04-11T04:23:18.4961033Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.4961396Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4961676Z     
2025-04-11T04:23:18.4961839Z         Args:
2025-04-11T04:23:18.4962122Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.4962408Z         """
2025-04-11T04:23:18.4962652Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.4962950Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.4963185Z             return True
2025-04-11T04:23:18.4963386Z     
2025-04-11T04:23:18.4963612Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.4963931Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.4964210Z             self.sentinels.keys(),
2025-04-11T04:23:18.4964444Z             timeout=timeout,
2025-04-11T04:23:18.4964660Z         )
2025-04-11T04:23:18.4964835Z     
2025-04-11T04:23:18.4965018Z         error_index = None
2025-04-11T04:23:18.4965240Z         for sentinel in ready:
2025-04-11T04:23:18.4965493Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.4965782Z             process = self.processes[index]
2025-04-11T04:23:18.4966052Z             process.join()
2025-04-11T04:23:18.4966286Z             if process.exitcode != 0:
2025-04-11T04:23:18.4966529Z                 error_index = index
2025-04-11T04:23:18.4966765Z                 break
2025-04-11T04:23:18.4966968Z     
2025-04-11T04:23:18.4967161Z         # Return if there was no error.
2025-04-11T04:23:18.4967413Z         if error_index is None:
2025-04-11T04:23:18.4967687Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.4967990Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.4968229Z     
2025-04-11T04:23:18.4968469Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.4968772Z         for process in self.processes:
2025-04-11T04:23:18.4969110Z             if process.is_alive():
2025-04-11T04:23:18.4969361Z                 process.terminate()
2025-04-11T04:23:18.4969603Z             process.join()
2025-04-11T04:23:18.4969816Z     
2025-04-11T04:23:18.4970053Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.4970378Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.4970682Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.4970992Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.4971271Z             if exitcode < 0:
2025-04-11T04:23:18.4971519Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.4971802Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4972132Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.4972452Z                     error_index=error_index,
2025-04-11T04:23:18.4972723Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4972993Z                     exit_code=exitcode,
2025-04-11T04:23:18.4973240Z                     signal_name=name,
2025-04-11T04:23:18.4973466Z                 )
2025-04-11T04:23:18.4973657Z             else:
2025-04-11T04:23:18.4973967Z                 raise ProcessExitedException(
2025-04-11T04:23:18.4974304Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.4974634Z                     error_index=error_index,
2025-04-11T04:23:18.4974889Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.4975166Z                     exit_code=exitcode,
2025-04-11T04:23:18.4975415Z                 )
2025-04-11T04:23:18.4975602Z     
2025-04-11T04:23:18.4975838Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.4976208Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.4976540Z         msg += original_trace
2025-04-11T04:23:18.4976860Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.4977262Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.4977565Z E       
2025-04-11T04:23:18.4977786Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.4978082Z E       Traceback (most recent call last):
2025-04-11T04:23:18.4978553Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.4979005Z E           fn(i, *args)
2025-04-11T04:23:18.4979386Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
2025-04-11T04:23:18.4979791Z E           check_inference_engine()
2025-04-11T04:23:18.4980198Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4980616Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4981025Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4981442Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4981851Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.4982260Z E           partial_func(**kwargs)
2025-04-11T04:23:18.4982519Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.4982985Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
2025-04-11T04:23:18.4983504Z E           model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
2025-04-11T04:23:18.4984028Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.4984493Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.4985115Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.4985589Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.4986072Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.4986508Z E           module._apply(fn)
2025-04-11T04:23:18.4986923Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.4987349Z E           module._apply(fn)
2025-04-11T04:23:18.4987756Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.4988194Z E           param_applied = fn(param)
2025-04-11T04:23:18.4988671Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.4989139Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.4989435Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.4989907Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.4990500Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.4990865Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4991102Z 
2025-04-11T04:23:18.4991413Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.4991943Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.4992343Z [04/11/25 04:15:02] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.4992716Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.4993024Z                              :75 launch                                         
2025-04-11T04:23:18.4993350Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.4993688Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.4994086Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.4994500Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.4995873Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.4997238Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.4998160Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.4998986Z   warnings.warn(
2025-04-11T04:23:18.4999938Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5000887Z   warnings.warn(
2025-04-11T04:23:18.5001802Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5002853Z   warnings.warn(
2025-04-11T04:23:18.5003768Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5004708Z   warnings.warn(
2025-04-11T04:23:18.5005619Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5006554Z   warnings.warn(
2025-04-11T04:23:18.5007448Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5008388Z   warnings.warn(
2025-04-11T04:23:18.5009291Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5010322Z   warnings.warn(
2025-04-11T04:23:18.5011245Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5012196Z   warnings.warn(
2025-04-11T04:23:18.5013118Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5014072Z   warnings.warn(
2025-04-11T04:23:18.5014979Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5015926Z   warnings.warn(
2025-04-11T04:23:18.5016169Z _______________________________ test_drafter[5] ________________________________
2025-04-11T04:23:18.5016383Z 
2025-04-11T04:23:18.5016757Z tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2025-04-11T04:23:18.5017435Z 	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
2025-04-11T04:23:18.5017811Z }
2025-04-11T04:23:18.5017990Z spec_num = 5
2025-04-11T04:23:18.5018104Z 
2025-04-11T04:23:18.5018242Z     @pytest.mark.parametrize("spec_num", [SPEC_NUM])
2025-04-11T04:23:18.5018546Z     def test_drafter(tokenizer, spec_num: int):
2025-04-11T04:23:18.5018821Z         torch.manual_seed(123)
2025-04-11T04:23:18.5019042Z     
2025-04-11T04:23:18.5019232Z         device = get_current_device()
2025-04-11T04:23:18.5019527Z         toy_config = LlamaConfig(num_hidden_layers=NUM_LAYERS)
2025-04-11T04:23:18.5019846Z         toy_config.pad_token_id = tokenizer.eos_token_id
2025-04-11T04:23:18.5020157Z         drafter_model = LlamaForCausalLM(toy_config)
2025-04-11T04:23:18.5020454Z >       drafter_model = drafter_model.eval().cuda()
2025-04-11T04:23:18.5020637Z 
2025-04-11T04:23:18.5020780Z tests/test_infer/test_drafter.py:27: 
2025-04-11T04:23:18.5021169Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5021611Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2548: in cuda
2025-04-11T04:23:18.5022038Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.5022439Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
2025-04-11T04:23:18.5022858Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.5023275Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.5023671Z     module._apply(fn)
2025-04-11T04:23:18.5024028Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.5024419Z     module._apply(fn)
2025-04-11T04:23:18.5024777Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.5025182Z     param_applied = fn(param)
2025-04-11T04:23:18.5025447Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5025630Z 
2025-04-11T04:23:18.5025726Z t = Parameter containing:
2025-04-11T04:23:18.5025993Z tensor([[-0.0259,  0.0026,  0.0006,  ...,  0.0104,  0.0194,  0.0062],
2025-04-11T04:23:18.5026377Z         [-0.0076,  0.0020,...5,  0.0329,  0.0046],
2025-04-11T04:23:18.5026671Z         [-0.0124,  0.0230, -0.0264,  ..., -0.0224, -0.0274, -0.0157]],
2025-04-11T04:23:18.5026950Z        requires_grad=True)
2025-04-11T04:23:18.5027085Z 
2025-04-11T04:23:18.5027200Z >   return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.5027476Z E   RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5027939Z E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5028462Z E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5028841Z E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5029067Z 
2025-04-11T04:23:18.5029329Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
2025-04-11T04:23:18.5029787Z ________________________________ test_spec_dec _________________________________
2025-04-11T04:23:18.5029995Z 
2025-04-11T04:23:18.5030359Z tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2025-04-11T04:23:18.5031024Z 	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
2025-04-11T04:23:18.5031391Z }
2025-04-11T04:23:18.5031494Z 
2025-04-11T04:23:18.5031591Z     def test_spec_dec(tokenizer):
2025-04-11T04:23:18.5031840Z         spec_num = SPEC_NUM
2025-04-11T04:23:18.5032078Z         device = get_current_device()
2025-04-11T04:23:18.5032346Z         tokenizer.pad_token = tokenizer.eos_token
2025-04-11T04:23:18.5032603Z     
2025-04-11T04:23:18.5032807Z         # Dummy config for Glide Model
2025-04-11T04:23:18.5033070Z         glide_config = GlideLlamaConfig(
2025-04-11T04:23:18.5033328Z             intermediate_size=8192,
2025-04-11T04:23:18.5033570Z             large_hidden_size=4096,
2025-04-11T04:23:18.5033823Z             large_num_attention_heads=32,
2025-04-11T04:23:18.5034083Z             num_hidden_layers=NUM_LAYERS,
2025-04-11T04:23:18.5034321Z         )
2025-04-11T04:23:18.5034551Z         drafter_model = GlideLlamaForCausalLM(glide_config)
2025-04-11T04:23:18.5034815Z     
2025-04-11T04:23:18.5035015Z         assert hasattr(drafter_model, "model")
2025-04-11T04:23:18.5035301Z         assert hasattr(drafter_model.model, "layers")
2025-04-11T04:23:18.5035618Z         for _, layer in enumerate(drafter_model.model.layers):
2025-04-11T04:23:18.5035919Z             assert hasattr(layer, "cross_attn")
2025-04-11T04:23:18.5036158Z     
2025-04-11T04:23:18.5036391Z         # Init the Drafter by providing the sharded drafter model
2025-04-11T04:23:18.5036878Z >       drafter = Drafter(drafter_model, tokenizer, device=device, dtype=torch.float16)
2025-04-11T04:23:18.5037150Z 
2025-04-11T04:23:18.5037257Z tests/test_infer/test_drafter.py:65: 
2025-04-11T04:23:18.5037530Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5037852Z colossalai/inference/spec/drafter.py:31: in __init__
2025-04-11T04:23:18.5038150Z     self._drafter_model = model.to(self._device)
2025-04-11T04:23:18.5038572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.5038978Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.5039366Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.5039759Z     return self._apply(convert)
2025-04-11T04:23:18.5040158Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.5040549Z     module._apply(fn)
2025-04-11T04:23:18.5040911Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.5041296Z     module._apply(fn)
2025-04-11T04:23:18.5041650Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.5042135Z     param_applied = fn(param)
2025-04-11T04:23:18.5042392Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5042587Z 
2025-04-11T04:23:18.5042682Z t = Parameter containing:
2025-04-11T04:23:18.5042967Z tensor([[-0.0389,  0.0039, -0.0004,  ...,  0.0133,  0.0029, -0.0177],
2025-04-11T04:23:18.5043271Z         [-0.0144,  0.0054,...4,  0.0227,  0.0264],
2025-04-11T04:23:18.5043564Z         [ 0.0320, -0.0080,  0.0294,  ...,  0.0173,  0.0005, -0.0045]],
2025-04-11T04:23:18.5043839Z        requires_grad=True)
2025-04-11T04:23:18.5043984Z 
2025-04-11T04:23:18.5044068Z     def convert(t):
2025-04-11T04:23:18.5044329Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.5044721Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.5045098Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.5045504Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.5045889Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5046357Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5046864Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5047236Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5047469Z 
2025-04-11T04:23:18.5047733Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.5048208Z ______________________________ test_cache_manager ______________________________
2025-04-11T04:23:18.5048419Z 
2025-04-11T04:23:18.5048519Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.5049290Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.5050001Z 
2025-04-11T04:23:18.5050110Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.5050380Z         try_count = 0
2025-04-11T04:23:18.5050616Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.5050891Z             max_try, int
2025-04-11T04:23:18.5051168Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.5051466Z     
2025-04-11T04:23:18.5051687Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.5051957Z             try:
2025-04-11T04:23:18.5052167Z                 try_count += 1
2025-04-11T04:23:18.5052489Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.5052736Z                 return ret
2025-04-11T04:23:18.5052966Z             except exception_type as e:
2025-04-11T04:23:18.5053233Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.5053594Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.5053964Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.5054290Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.5054661Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.5054971Z                     continue
2025-04-11T04:23:18.5055187Z                 else:
2025-04-11T04:23:18.5055530Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.5055890Z >                   raise e
2025-04-11T04:23:18.5056031Z 
2025-04-11T04:23:18.5056127Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.5056391Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5056818Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.5057115Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.5057430Z tests/test_infer/test_kvcache_manager.py:174: in test_cache_manager
2025-04-11T04:23:18.5057737Z     spawn(run_dist, 1)
2025-04-11T04:23:18.5057964Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.5058235Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.5058660Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.5059166Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.5059691Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.5060145Z     while not context.join():
2025-04-11T04:23:18.5060399Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5060578Z 
2025-04-11T04:23:18.5060784Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840c1f0>
2025-04-11T04:23:18.5061131Z timeout = None
2025-04-11T04:23:18.5061244Z 
2025-04-11T04:23:18.5061344Z     def join(self, timeout=None):
2025-04-11T04:23:18.5061617Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.5061882Z     
2025-04-11T04:23:18.5062128Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.5062487Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.5062866Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.5063185Z         of the first process exiting.
2025-04-11T04:23:18.5063416Z     
2025-04-11T04:23:18.5063663Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.5064021Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.5064299Z     
2025-04-11T04:23:18.5064463Z         Args:
2025-04-11T04:23:18.5064720Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.5065004Z         """
2025-04-11T04:23:18.5065250Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.5065543Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.5065774Z             return True
2025-04-11T04:23:18.5065974Z     
2025-04-11T04:23:18.5066196Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.5066522Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.5066808Z             self.sentinels.keys(),
2025-04-11T04:23:18.5067043Z             timeout=timeout,
2025-04-11T04:23:18.5067264Z         )
2025-04-11T04:23:18.5067435Z     
2025-04-11T04:23:18.5067611Z         error_index = None
2025-04-11T04:23:18.5067960Z         for sentinel in ready:
2025-04-11T04:23:18.5068215Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.5068530Z             process = self.processes[index]
2025-04-11T04:23:18.5068791Z             process.join()
2025-04-11T04:23:18.5069022Z             if process.exitcode != 0:
2025-04-11T04:23:18.5069268Z                 error_index = index
2025-04-11T04:23:18.5069489Z                 break
2025-04-11T04:23:18.5069681Z     
2025-04-11T04:23:18.5069868Z         # Return if there was no error.
2025-04-11T04:23:18.5070119Z         if error_index is None:
2025-04-11T04:23:18.5070399Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.5070693Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.5070933Z     
2025-04-11T04:23:18.5071170Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.5071477Z         for process in self.processes:
2025-04-11T04:23:18.5071730Z             if process.is_alive():
2025-04-11T04:23:18.5071968Z                 process.terminate()
2025-04-11T04:23:18.5072204Z             process.join()
2025-04-11T04:23:18.5072413Z     
2025-04-11T04:23:18.5072644Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.5073073Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.5073360Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.5073663Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.5073941Z             if exitcode < 0:
2025-04-11T04:23:18.5074190Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.5074479Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5074801Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.5075121Z                     error_index=error_index,
2025-04-11T04:23:18.5075389Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5075661Z                     exit_code=exitcode,
2025-04-11T04:23:18.5075916Z                     signal_name=name,
2025-04-11T04:23:18.5076172Z                 )
2025-04-11T04:23:18.5076356Z             else:
2025-04-11T04:23:18.5076594Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5076930Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.5077261Z                     error_index=error_index,
2025-04-11T04:23:18.5077510Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5077762Z                     exit_code=exitcode,
2025-04-11T04:23:18.5077987Z                 )
2025-04-11T04:23:18.5078163Z     
2025-04-11T04:23:18.5078383Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.5078746Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.5079072Z         msg += original_trace
2025-04-11T04:23:18.5079382Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.5079776Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.5080082Z E       
2025-04-11T04:23:18.5080305Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.5080592Z E       Traceback (most recent call last):
2025-04-11T04:23:18.5081059Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.5081518Z E           fn(i, *args)
2025-04-11T04:23:18.5081884Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
2025-04-11T04:23:18.5082281Z E           check_cache_manager()
2025-04-11T04:23:18.5082684Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.5083100Z E           partial_func(**kwargs)
2025-04-11T04:23:18.5117801Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
2025-04-11T04:23:18.5118370Z E           cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T04:23:18.5118895Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T04:23:18.5119464Z E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:18.5120053Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T04:23:18.5120673Z E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:18.5121096Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5121611Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5122126Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5122519Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5122750Z 
2025-04-11T04:23:18.5123106Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.5123805Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.5124185Z [04/11/25 04:15:28] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.5124534Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.5124843Z                              :75 launch                                         
2025-04-11T04:23:18.5125162Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.5125492Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.5125825Z [04/11/25 04:15:28] INFO     colossalai -                                       
2025-04-11T04:23:18.5126157Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.5126462Z                              INFO:                                              
2025-04-11T04:23:18.5126759Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T04:23:18.5127072Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T04:23:18.5127381Z                     INFO     colossalai -                                       
2025-04-11T04:23:18.5127699Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.5128013Z                              INFO: Allocating KV cache with shape: (80, 16, 8,  
2025-04-11T04:23:18.5128316Z                              32) consisting of 80 blocks.                       
2025-04-11T04:23:18.5128701Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.5129112Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.5130469Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.5131828Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.5132211Z ____________________ test_running_list_and_request_handler _____________________
2025-04-11T04:23:18.5132432Z 
2025-04-11T04:23:18.5132529Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.5133398Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.5134068Z 
2025-04-11T04:23:18.5134181Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.5134437Z         try_count = 0
2025-04-11T04:23:18.5134668Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.5134921Z             max_try, int
2025-04-11T04:23:18.5135188Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.5135479Z     
2025-04-11T04:23:18.5135693Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.5135965Z             try:
2025-04-11T04:23:18.5136166Z                 try_count += 1
2025-04-11T04:23:18.5136400Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.5136643Z                 return ret
2025-04-11T04:23:18.5136870Z             except exception_type as e:
2025-04-11T04:23:18.5137134Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.5137492Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.5137612Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.5137850Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.5138011Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.5138098Z                     continue
2025-04-11T04:23:18.5138179Z                 else:
2025-04-11T04:23:18.5138402Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.5138487Z >                   raise e
2025-04-11T04:23:18.5138491Z 
2025-04-11T04:23:18.5138589Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.5138700Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5138840Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.5138927Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.5139136Z tests/test_infer/test_request_handler.py:101: in test_running_list_and_request_handler
2025-04-11T04:23:18.5139221Z     spawn(run_dist, 1)
2025-04-11T04:23:18.5139324Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.5139433Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.5139690Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.5139876Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.5140166Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.5140285Z     while not context.join():
2025-04-11T04:23:18.5140398Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5140402Z 
2025-04-11T04:23:18.5140614Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e17b0>
2025-04-11T04:23:18.5140696Z timeout = None
2025-04-11T04:23:18.5140700Z 
2025-04-11T04:23:18.5140789Z     def join(self, timeout=None):
2025-04-11T04:23:18.5140924Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.5140995Z     
2025-04-11T04:23:18.5141150Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.5141293Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.5141457Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.5141549Z         of the first process exiting.
2025-04-11T04:23:18.5141621Z     
2025-04-11T04:23:18.5141770Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.5141907Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.5141981Z     
2025-04-11T04:23:18.5142055Z         Args:
2025-04-11T04:23:18.5142290Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.5142371Z         """
2025-04-11T04:23:18.5142511Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.5142614Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.5142692Z             return True
2025-04-11T04:23:18.5142768Z     
2025-04-11T04:23:18.5142900Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.5143018Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.5143115Z             self.sentinels.keys(),
2025-04-11T04:23:18.5143199Z             timeout=timeout,
2025-04-11T04:23:18.5143276Z         )
2025-04-11T04:23:18.5143346Z     
2025-04-11T04:23:18.5143430Z         error_index = None
2025-04-11T04:23:18.5143520Z         for sentinel in ready:
2025-04-11T04:23:18.5143624Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.5143730Z             process = self.processes[index]
2025-04-11T04:23:18.5143819Z             process.join()
2025-04-11T04:23:18.5143913Z             if process.exitcode != 0:
2025-04-11T04:23:18.5144008Z                 error_index = index
2025-04-11T04:23:18.5144085Z                 break
2025-04-11T04:23:18.5144266Z     
2025-04-11T04:23:18.5144362Z         # Return if there was no error.
2025-04-11T04:23:18.5144454Z         if error_index is None:
2025-04-11T04:23:18.5144587Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.5144685Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.5144762Z     
2025-04-11T04:23:18.5144898Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.5144999Z         for process in self.processes:
2025-04-11T04:23:18.5145090Z             if process.is_alive():
2025-04-11T04:23:18.5145181Z                 process.terminate()
2025-04-11T04:23:18.5145271Z             process.join()
2025-04-11T04:23:18.5145344Z     
2025-04-11T04:23:18.5145495Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.5145612Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.5145726Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.5145851Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.5145935Z             if exitcode < 0:
2025-04-11T04:23:18.5146048Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.5146156Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5146314Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.5146411Z                     error_index=error_index,
2025-04-11T04:23:18.5146512Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5146606Z                     exit_code=exitcode,
2025-04-11T04:23:18.5146692Z                     signal_name=name,
2025-04-11T04:23:18.5146773Z                 )
2025-04-11T04:23:18.5146848Z             else:
2025-04-11T04:23:18.5146961Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5147124Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.5147218Z                     error_index=error_index,
2025-04-11T04:23:18.5147324Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5147413Z                     exit_code=exitcode,
2025-04-11T04:23:18.5147490Z                 )
2025-04-11T04:23:18.5147560Z     
2025-04-11T04:23:18.5147691Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.5147870Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.5147958Z         msg += original_trace
2025-04-11T04:23:18.5148137Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.5148302Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.5148379Z E       
2025-04-11T04:23:18.5148643Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.5148745Z E       Traceback (most recent call last):
2025-04-11T04:23:18.5149056Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.5149143Z E           fn(i, *args)
2025-04-11T04:23:18.5149387Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
2025-04-11T04:23:18.5149479Z E           check_request_handler()
2025-04-11T04:23:18.5149755Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
2025-04-11T04:23:18.5149917Z E           request_handler = RequestHandler(inference_config, model_config)
2025-04-11T04:23:18.5150183Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
2025-04-11T04:23:18.5150286Z E           self._init_cache(model_config)
2025-04-11T04:23:18.5150556Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
2025-04-11T04:23:18.5150738Z E           self.cache_manager = KVCacheManager(self.inference_config, model_config)
2025-04-11T04:23:18.5151099Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T04:23:18.5151259Z E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:18.5151555Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T04:23:18.5151777Z E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:18.5151886Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5152170Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5152315Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5152475Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5152480Z 
2025-04-11T04:23:18.5152797Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.5152948Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.5153110Z [04/11/25 04:15:32] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.5153236Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.5153343Z                              :75 launch                                         
2025-04-11T04:23:18.5153477Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.5153597Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.5153743Z [04/11/25 04:15:32] INFO     colossalai -                                       
2025-04-11T04:23:18.5153873Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.5153980Z                              INFO:                                              
2025-04-11T04:23:18.5154102Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T04:23:18.5154226Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T04:23:18.5154343Z                     INFO     colossalai -                                       
2025-04-11T04:23:18.5154470Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T04:23:18.5154596Z                              INFO: Allocating KV cache with shape: (24, 4, 8, 8)
2025-04-11T04:23:18.5154708Z                              consisting of 24 blocks.                           
2025-04-11T04:23:18.5154990Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.5155136Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.5156279Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.5156452Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.5156586Z _________________________________ test_engine __________________________________
2025-04-11T04:23:18.5156590Z 
2025-04-11T04:23:18.5156682Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.5157283Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.5157377Z 
2025-04-11T04:23:18.5157482Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.5157566Z         try_count = 0
2025-04-11T04:23:18.5157665Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.5157746Z             max_try, int
2025-04-11T04:23:18.5157893Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.5157965Z     
2025-04-11T04:23:18.5158079Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.5158154Z             try:
2025-04-11T04:23:18.5158239Z                 try_count += 1
2025-04-11T04:23:18.5158335Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.5158414Z                 return ret
2025-04-11T04:23:18.5158511Z             except exception_type as e:
2025-04-11T04:23:18.5158615Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.5158807Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.5158924Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.5159071Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.5159230Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.5159312Z                     continue
2025-04-11T04:23:18.5159394Z                 else:
2025-04-11T04:23:18.5159623Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.5159715Z >                   raise e
2025-04-11T04:23:18.5159719Z 
2025-04-11T04:23:18.5159812Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.5159921Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5160061Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.5160152Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.5160295Z tests/test_infer/test_streamingllm.py:117: in test_engine
2025-04-11T04:23:18.5160456Z     spawn(run_dist, 1, func_to_run=check_streamingllm, ret=result_list)
2025-04-11T04:23:18.5160559Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.5160655Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.5160910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.5161088Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.5161369Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.5161457Z     while not context.join():
2025-04-11T04:23:18.5161563Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5161567Z 
2025-04-11T04:23:18.5161861Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8477e50>
2025-04-11T04:23:18.5161947Z timeout = None
2025-04-11T04:23:18.5161951Z 
2025-04-11T04:23:18.5162047Z     def join(self, timeout=None):
2025-04-11T04:23:18.5162177Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.5162249Z     
2025-04-11T04:23:18.5162398Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.5162543Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.5162709Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.5162801Z         of the first process exiting.
2025-04-11T04:23:18.5162872Z     
2025-04-11T04:23:18.5163021Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.5163156Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.5163229Z     
2025-04-11T04:23:18.5163306Z         Args:
2025-04-11T04:23:18.5163444Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.5163522Z         """
2025-04-11T04:23:18.5163658Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.5163841Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.5163921Z             return True
2025-04-11T04:23:18.5163999Z     
2025-04-11T04:23:18.5164132Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.5164247Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.5164344Z             self.sentinels.keys(),
2025-04-11T04:23:18.5164431Z             timeout=timeout,
2025-04-11T04:23:18.5164508Z         )
2025-04-11T04:23:18.5164578Z     
2025-04-11T04:23:18.5164661Z         error_index = None
2025-04-11T04:23:18.5164752Z         for sentinel in ready:
2025-04-11T04:23:18.5164857Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.5164963Z             process = self.processes[index]
2025-04-11T04:23:18.5165045Z             process.join()
2025-04-11T04:23:18.5165137Z             if process.exitcode != 0:
2025-04-11T04:23:18.5165226Z                 error_index = index
2025-04-11T04:23:18.5165304Z                 break
2025-04-11T04:23:18.5165377Z     
2025-04-11T04:23:18.5165468Z         # Return if there was no error.
2025-04-11T04:23:18.5165558Z         if error_index is None:
2025-04-11T04:23:18.5165691Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.5165785Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.5165862Z     
2025-04-11T04:23:18.5166001Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.5166102Z         for process in self.processes:
2025-04-11T04:23:18.5166190Z             if process.is_alive():
2025-04-11T04:23:18.5166281Z                 process.terminate()
2025-04-11T04:23:18.5166370Z             process.join()
2025-04-11T04:23:18.5166442Z     
2025-04-11T04:23:18.5166589Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.5166703Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.5166816Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.5166936Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.5167018Z             if exitcode < 0:
2025-04-11T04:23:18.5167127Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.5167230Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5167384Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.5167478Z                     error_index=error_index,
2025-04-11T04:23:18.5167578Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5167668Z                     exit_code=exitcode,
2025-04-11T04:23:18.5167753Z                     signal_name=name,
2025-04-11T04:23:18.5167831Z                 )
2025-04-11T04:23:18.5167989Z             else:
2025-04-11T04:23:18.5168100Z                 raise ProcessExitedException(
2025-04-11T04:23:18.5168263Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.5168359Z                     error_index=error_index,
2025-04-11T04:23:18.5168463Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.5168549Z                     exit_code=exitcode,
2025-04-11T04:23:18.5168625Z                 )
2025-04-11T04:23:18.5168693Z     
2025-04-11T04:23:18.5168832Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.5169013Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.5169106Z         msg += original_trace
2025-04-11T04:23:18.5169288Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.5169451Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.5169537Z E       
2025-04-11T04:23:18.5169664Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.5169773Z E       Traceback (most recent call last):
2025-04-11T04:23:18.5170074Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.5170269Z E           fn(i, *args)
2025-04-11T04:23:18.5170509Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
2025-04-11T04:23:18.5170608Z E           ret[rank] = func_to_run(**kwargs)
2025-04-11T04:23:18.5170866Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
2025-04-11T04:23:18.5170944Z E           ).cuda()
2025-04-11T04:23:18.5171231Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.5171333Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.5171600Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.5171726Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.5171999Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.5172098Z E           module._apply(fn)
2025-04-11T04:23:18.5172364Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.5172455Z E           module._apply(fn)
2025-04-11T04:23:18.5172720Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.5172820Z E           param_applied = fn(param)
2025-04-11T04:23:18.5173092Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.5173210Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.5173324Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5173606Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5173751Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5173909Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5173913Z 
2025-04-11T04:23:18.5174217Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.5174367Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.5174519Z [04/11/25 04:15:36] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.5174649Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.5174847Z                              :75 launch                                         
2025-04-11T04:23:18.5174994Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.5175120Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.5175323Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.5175467Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.5176617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.5176787Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.5177493Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.5177667Z   warnings.warn(
2025-04-11T04:23:18.5178515Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.5178599Z   warnings.warn(
2025-04-11T04:23:18.5178745Z ________________________ test_flash_decoding_attention _________________________
2025-04-11T04:23:18.5178756Z 
2025-04-11T04:23:18.5178839Z args = (), kwargs = {}
2025-04-11T04:23:18.5178843Z 
2025-04-11T04:23:18.5178943Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.5179045Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.5179156Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.5179281Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.5179385Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.5179479Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.5179488Z 
2025-04-11T04:23:18.5179583Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.5179695Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5179859Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.5179957Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.5180071Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5180075Z 
2025-04-11T04:23:18.5180154Z device = None
2025-04-11T04:23:18.5180158Z 
2025-04-11T04:23:18.5180292Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5180450Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5180523Z     
2025-04-11T04:23:18.5180608Z         Args:
2025-04-11T04:23:18.5180782Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5180957Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5181064Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5181144Z         """
2025-04-11T04:23:18.5181222Z         _lazy_init()
2025-04-11T04:23:18.5181317Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5181428Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5181533Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5181917Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5182064Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5182226Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5182240Z 
2025-04-11T04:23:18.5182480Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5182629Z ______________________ test_vllm_flash_decoding_attention ______________________
2025-04-11T04:23:18.5182633Z 
2025-04-11T04:23:18.5182721Z args = (), kwargs = {}
2025-04-11T04:23:18.5182725Z 
2025-04-11T04:23:18.5182816Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.5182914Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.5183021Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.5183139Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.5183240Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.5183332Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.5183336Z 
2025-04-11T04:23:18.5183435Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.5183542Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5183793Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.5183888Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.5183998Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5184002Z 
2025-04-11T04:23:18.5184081Z device = None
2025-04-11T04:23:18.5184085Z 
2025-04-11T04:23:18.5184201Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5184357Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5184430Z     
2025-04-11T04:23:18.5184511Z         Args:
2025-04-11T04:23:18.5184678Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5184854Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5184965Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5185038Z         """
2025-04-11T04:23:18.5185125Z         _lazy_init()
2025-04-11T04:23:18.5185217Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5185329Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5185432Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5185722Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5185860Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5186077Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5186081Z 
2025-04-11T04:23:18.5186330Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5186488Z _____________________ test_get_cos_and_sin[dtype0-64-64-4] _____________________
2025-04-11T04:23:18.5186491Z 
2025-04-11T04:23:18.5186651Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float16
2025-04-11T04:23:18.5186659Z 
2025-04-11T04:23:18.5186770Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5186887Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T04:23:18.5186994Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T04:23:18.5187168Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5187316Z     def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T04:23:18.5187415Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T04:23:18.5187610Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T04:23:18.5187712Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5188090Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5188230Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5188394Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5188402Z 
2025-04-11T04:23:18.5188640Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
2025-04-11T04:23:18.5188783Z _____________________ test_get_cos_and_sin[dtype1-64-64-4] _____________________
2025-04-11T04:23:18.5188791Z 
2025-04-11T04:23:18.5188935Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T04:23:18.5188939Z 
2025-04-11T04:23:18.5189046Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5189165Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T04:23:18.5189270Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T04:23:18.5189435Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5189586Z     def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T04:23:18.5189695Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T04:23:18.5189876Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T04:23:18.5190192Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5190484Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5190617Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5190780Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5190784Z 
2025-04-11T04:23:18.5190958Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
2025-04-11T04:23:18.5191110Z ____________________ test_kv_cache_memcopy[True-16-8-16-4] _____________________
2025-04-11T04:23:18.5191114Z 
2025-04-11T04:23:18.5191268Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5191358Z same_context_len = True
2025-04-11T04:23:18.5191362Z 
2025-04-11T04:23:18.5191464Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5191597Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5191742Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5191853Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5191998Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5192093Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5192176Z         bsz: int,
2025-04-11T04:23:18.5192261Z         block_size: int,
2025-04-11T04:23:18.5192355Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5192445Z         num_kv_heads: int,
2025-04-11T04:23:18.5192532Z         same_context_len: bool,
2025-04-11T04:23:18.5192611Z     ):
2025-04-11T04:23:18.5192840Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5192844Z 
2025-04-11T04:23:18.5192999Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5193120Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5193123Z 
2025-04-11T04:23:18.5193269Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5193359Z same_context_len = True
2025-04-11T04:23:18.5193363Z 
2025-04-11T04:23:18.5193457Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5193537Z         bsz: int,
2025-04-11T04:23:18.5193618Z         block_size: int,
2025-04-11T04:23:18.5193708Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5193795Z         num_kv_heads: int,
2025-04-11T04:23:18.5193881Z         same_context_len: bool,
2025-04-11T04:23:18.5193958Z     ):
2025-04-11T04:23:18.5194045Z         torch.manual_seed(123)
2025-04-11T04:23:18.5194121Z     
2025-04-11T04:23:18.5194433Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5194556Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5194656Z         dtype = torch.float16
2025-04-11T04:23:18.5194749Z         device = get_current_device()
2025-04-11T04:23:18.5194825Z     
2025-04-11T04:23:18.5194907Z         if same_context_len:
2025-04-11T04:23:18.5195138Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5195251Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5195537Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5195693Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5195857Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5195865Z 
2025-04-11T04:23:18.5196065Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5196212Z ____________________ test_kv_cache_memcopy[True-16-8-16-7] _____________________
2025-04-11T04:23:18.5196303Z 
2025-04-11T04:23:18.5196456Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5196540Z same_context_len = True
2025-04-11T04:23:18.5196544Z 
2025-04-11T04:23:18.5196646Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5196779Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5196920Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5197038Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5197175Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5197269Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5197344Z         bsz: int,
2025-04-11T04:23:18.5197429Z         block_size: int,
2025-04-11T04:23:18.5197526Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5197607Z         num_kv_heads: int,
2025-04-11T04:23:18.5197696Z         same_context_len: bool,
2025-04-11T04:23:18.5197772Z     ):
2025-04-11T04:23:18.5197995Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5198005Z 
2025-04-11T04:23:18.5198157Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5198265Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5198269Z 
2025-04-11T04:23:18.5198416Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5198501Z same_context_len = True
2025-04-11T04:23:18.5198505Z 
2025-04-11T04:23:18.5198603Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5198678Z         bsz: int,
2025-04-11T04:23:18.5198762Z         block_size: int,
2025-04-11T04:23:18.5198854Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5198936Z         num_kv_heads: int,
2025-04-11T04:23:18.5199024Z         same_context_len: bool,
2025-04-11T04:23:18.5199098Z     ):
2025-04-11T04:23:18.5199185Z         torch.manual_seed(123)
2025-04-11T04:23:18.5199259Z     
2025-04-11T04:23:18.5199456Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5199584Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5199672Z         dtype = torch.float16
2025-04-11T04:23:18.5199769Z         device = get_current_device()
2025-04-11T04:23:18.5199841Z     
2025-04-11T04:23:18.5199924Z         if same_context_len:
2025-04-11T04:23:18.5200157Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5200261Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5200636Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5200776Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5200941Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5200948Z 
2025-04-11T04:23:18.5201127Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5201276Z ____________________ test_kv_cache_memcopy[True-16-8-16-32] ____________________
2025-04-11T04:23:18.5201280Z 
2025-04-11T04:23:18.5201429Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5201512Z same_context_len = True
2025-04-11T04:23:18.5201521Z 
2025-04-11T04:23:18.5201625Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5201748Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5201891Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5202005Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5202147Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5202235Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5202399Z         bsz: int,
2025-04-11T04:23:18.5202487Z         block_size: int,
2025-04-11T04:23:18.5202578Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5202664Z         num_kv_heads: int,
2025-04-11T04:23:18.5202748Z         same_context_len: bool,
2025-04-11T04:23:18.5202821Z     ):
2025-04-11T04:23:18.5203051Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5203055Z 
2025-04-11T04:23:18.5203204Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5203318Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5203322Z 
2025-04-11T04:23:18.5203464Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5203556Z same_context_len = True
2025-04-11T04:23:18.5203559Z 
2025-04-11T04:23:18.5203649Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5203729Z         bsz: int,
2025-04-11T04:23:18.5203814Z         block_size: int,
2025-04-11T04:23:18.5203903Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5203990Z         num_kv_heads: int,
2025-04-11T04:23:18.5204074Z         same_context_len: bool,
2025-04-11T04:23:18.5204152Z     ):
2025-04-11T04:23:18.5204239Z         torch.manual_seed(123)
2025-04-11T04:23:18.5204310Z     
2025-04-11T04:23:18.5204515Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5204632Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5204726Z         dtype = torch.float16
2025-04-11T04:23:18.5204818Z         device = get_current_device()
2025-04-11T04:23:18.5204894Z     
2025-04-11T04:23:18.5204979Z         if same_context_len:
2025-04-11T04:23:18.5205210Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5205319Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5205611Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5205750Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5205906Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5205910Z 
2025-04-11T04:23:18.5206096Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5206239Z ____________________ test_kv_cache_memcopy[True-16-8-32-4] _____________________
2025-04-11T04:23:18.5206243Z 
2025-04-11T04:23:18.5206391Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5206477Z same_context_len = True
2025-04-11T04:23:18.5206481Z 
2025-04-11T04:23:18.5206670Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5206802Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5206940Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5207059Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5207195Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5207287Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5207365Z         bsz: int,
2025-04-11T04:23:18.5207446Z         block_size: int,
2025-04-11T04:23:18.5207542Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5207623Z         num_kv_heads: int,
2025-04-11T04:23:18.5207714Z         same_context_len: bool,
2025-04-11T04:23:18.5207785Z     ):
2025-04-11T04:23:18.5208010Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5208014Z 
2025-04-11T04:23:18.5208175Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5208285Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5208289Z 
2025-04-11T04:23:18.5208435Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5208600Z same_context_len = True
2025-04-11T04:23:18.5208604Z 
2025-04-11T04:23:18.5208700Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5208777Z         bsz: int,
2025-04-11T04:23:18.5208857Z         block_size: int,
2025-04-11T04:23:18.5208950Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5209032Z         num_kv_heads: int,
2025-04-11T04:23:18.5209122Z         same_context_len: bool,
2025-04-11T04:23:18.5209194Z     ):
2025-04-11T04:23:18.5209285Z         torch.manual_seed(123)
2025-04-11T04:23:18.5209357Z     
2025-04-11T04:23:18.5209556Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5209684Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5209771Z         dtype = torch.float16
2025-04-11T04:23:18.5209866Z         device = get_current_device()
2025-04-11T04:23:18.5209937Z     
2025-04-11T04:23:18.5210025Z         if same_context_len:
2025-04-11T04:23:18.5210253Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5210357Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5210648Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5210785Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5210949Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5210953Z 
2025-04-11T04:23:18.5211130Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5211282Z ____________________ test_kv_cache_memcopy[True-16-8-32-7] _____________________
2025-04-11T04:23:18.5211285Z 
2025-04-11T04:23:18.5211426Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5211511Z same_context_len = True
2025-04-11T04:23:18.5211515Z 
2025-04-11T04:23:18.5211626Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5211748Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5211895Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5212007Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5212150Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5212236Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5212311Z         bsz: int,
2025-04-11T04:23:18.5212395Z         block_size: int,
2025-04-11T04:23:18.5212483Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5212570Z         num_kv_heads: int,
2025-04-11T04:23:18.5212736Z         same_context_len: bool,
2025-04-11T04:23:18.5212811Z     ):
2025-04-11T04:23:18.5213044Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5213052Z 
2025-04-11T04:23:18.5213203Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5213317Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5213321Z 
2025-04-11T04:23:18.5213461Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5213548Z same_context_len = True
2025-04-11T04:23:18.5213552Z 
2025-04-11T04:23:18.5213644Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5213724Z         bsz: int,
2025-04-11T04:23:18.5213803Z         block_size: int,
2025-04-11T04:23:18.5213890Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5213975Z         num_kv_heads: int,
2025-04-11T04:23:18.5214057Z         same_context_len: bool,
2025-04-11T04:23:18.5214136Z     ):
2025-04-11T04:23:18.5214222Z         torch.manual_seed(123)
2025-04-11T04:23:18.5214293Z     
2025-04-11T04:23:18.5214495Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5214702Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5214802Z         dtype = torch.float16
2025-04-11T04:23:18.5214893Z         device = get_current_device()
2025-04-11T04:23:18.5214962Z     
2025-04-11T04:23:18.5215051Z         if same_context_len:
2025-04-11T04:23:18.5215286Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5215397Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5215679Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5215818Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5215980Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5215984Z 
2025-04-11T04:23:18.5216164Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5216314Z ____________________ test_kv_cache_memcopy[True-16-8-32-32] ____________________
2025-04-11T04:23:18.5216318Z 
2025-04-11T04:23:18.5216465Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5216556Z same_context_len = True
2025-04-11T04:23:18.5216560Z 
2025-04-11T04:23:18.5216664Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5216804Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5216945Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5217064Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5217200Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5217292Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5217375Z         bsz: int,
2025-04-11T04:23:18.5217459Z         block_size: int,
2025-04-11T04:23:18.5217556Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5217642Z         num_kv_heads: int,
2025-04-11T04:23:18.5217737Z         same_context_len: bool,
2025-04-11T04:23:18.5217811Z     ):
2025-04-11T04:23:18.5218032Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5218036Z 
2025-04-11T04:23:18.5218198Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5218309Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5218313Z 
2025-04-11T04:23:18.5218460Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5218543Z same_context_len = True
2025-04-11T04:23:18.5218546Z 
2025-04-11T04:23:18.5218644Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5218827Z         bsz: int,
2025-04-11T04:23:18.5218912Z         block_size: int,
2025-04-11T04:23:18.5219007Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5219088Z         num_kv_heads: int,
2025-04-11T04:23:18.5219185Z         same_context_len: bool,
2025-04-11T04:23:18.5219259Z     ):
2025-04-11T04:23:18.5219347Z         torch.manual_seed(123)
2025-04-11T04:23:18.5219427Z     
2025-04-11T04:23:18.5219623Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5219747Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5219836Z         dtype = torch.float16
2025-04-11T04:23:18.5219935Z         device = get_current_device()
2025-04-11T04:23:18.5220007Z     
2025-04-11T04:23:18.5220092Z         if same_context_len:
2025-04-11T04:23:18.5220325Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5220436Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5220729Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5220867Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5221121Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5221125Z 
2025-04-11T04:23:18.5221304Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5221449Z ____________________ test_kv_cache_memcopy[True-16-8-64-4] _____________________
2025-04-11T04:23:18.5221458Z 
2025-04-11T04:23:18.5221601Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5221684Z same_context_len = True
2025-04-11T04:23:18.5221688Z 
2025-04-11T04:23:18.5221803Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5221927Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5222076Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5222187Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5222330Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5222419Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5222494Z         bsz: int,
2025-04-11T04:23:18.5222579Z         block_size: int,
2025-04-11T04:23:18.5222666Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5222751Z         num_kv_heads: int,
2025-04-11T04:23:18.5222837Z         same_context_len: bool,
2025-04-11T04:23:18.5222908Z     ):
2025-04-11T04:23:18.5223138Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5223142Z 
2025-04-11T04:23:18.5223293Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5223407Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5223414Z 
2025-04-11T04:23:18.5223558Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5223647Z same_context_len = True
2025-04-11T04:23:18.5223650Z 
2025-04-11T04:23:18.5223743Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5223819Z         bsz: int,
2025-04-11T04:23:18.5223904Z         block_size: int,
2025-04-11T04:23:18.5223993Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5224087Z         num_kv_heads: int,
2025-04-11T04:23:18.5224179Z         same_context_len: bool,
2025-04-11T04:23:18.5224263Z     ):
2025-04-11T04:23:18.5224355Z         torch.manual_seed(123)
2025-04-11T04:23:18.5224426Z     
2025-04-11T04:23:18.5224629Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5224743Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5224831Z         dtype = torch.float16
2025-04-11T04:23:18.5224922Z         device = get_current_device()
2025-04-11T04:23:18.5225087Z     
2025-04-11T04:23:18.5225182Z         if same_context_len:
2025-04-11T04:23:18.5225406Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5225522Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5225807Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5225947Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5226107Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5226111Z 
2025-04-11T04:23:18.5226293Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5226438Z ____________________ test_kv_cache_memcopy[True-16-8-64-7] _____________________
2025-04-11T04:23:18.5226441Z 
2025-04-11T04:23:18.5226587Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5226676Z same_context_len = True
2025-04-11T04:23:18.5226680Z 
2025-04-11T04:23:18.5226787Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5226916Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5227139Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5227252Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5227386Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5227472Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5227556Z         bsz: int,
2025-04-11T04:23:18.5227637Z         block_size: int,
2025-04-11T04:23:18.5227730Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5227810Z         num_kv_heads: int,
2025-04-11T04:23:18.5227893Z         same_context_len: bool,
2025-04-11T04:23:18.5227971Z     ):
2025-04-11T04:23:18.5228192Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5228196Z 
2025-04-11T04:23:18.5228351Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5228505Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5228513Z 
2025-04-11T04:23:18.5228665Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5228746Z same_context_len = True
2025-04-11T04:23:18.5228750Z 
2025-04-11T04:23:18.5228847Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5228924Z         bsz: int,
2025-04-11T04:23:18.5229005Z         block_size: int,
2025-04-11T04:23:18.5229100Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5229183Z         num_kv_heads: int,
2025-04-11T04:23:18.5229272Z         same_context_len: bool,
2025-04-11T04:23:18.5229345Z     ):
2025-04-11T04:23:18.5229433Z         torch.manual_seed(123)
2025-04-11T04:23:18.5229514Z     
2025-04-11T04:23:18.5229716Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5229843Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5229930Z         dtype = torch.float16
2025-04-11T04:23:18.5230035Z         device = get_current_device()
2025-04-11T04:23:18.5230108Z     
2025-04-11T04:23:18.5230196Z         if same_context_len:
2025-04-11T04:23:18.5230426Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5230534Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5230818Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5230952Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5231117Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5231121Z 
2025-04-11T04:23:18.5231392Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5231542Z ____________________ test_kv_cache_memcopy[True-16-8-64-32] ____________________
2025-04-11T04:23:18.5231547Z 
2025-04-11T04:23:18.5231708Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5231792Z same_context_len = True
2025-04-11T04:23:18.5231796Z 
2025-04-11T04:23:18.5231907Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5232030Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5232175Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5232289Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5232428Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5232524Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5232601Z         bsz: int,
2025-04-11T04:23:18.5232691Z         block_size: int,
2025-04-11T04:23:18.5232783Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5232872Z         num_kv_heads: int,
2025-04-11T04:23:18.5232957Z         same_context_len: bool,
2025-04-11T04:23:18.5233031Z     ):
2025-04-11T04:23:18.5233258Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5233355Z 
2025-04-11T04:23:18.5233511Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5233628Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5233632Z 
2025-04-11T04:23:18.5233779Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5233887Z same_context_len = True
2025-04-11T04:23:18.5233897Z 
2025-04-11T04:23:18.5234005Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5234086Z         bsz: int,
2025-04-11T04:23:18.5234186Z         block_size: int,
2025-04-11T04:23:18.5234272Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5234361Z         num_kv_heads: int,
2025-04-11T04:23:18.5234447Z         same_context_len: bool,
2025-04-11T04:23:18.5234519Z     ):
2025-04-11T04:23:18.5234610Z         torch.manual_seed(123)
2025-04-11T04:23:18.5234683Z     
2025-04-11T04:23:18.5234888Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5235006Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5235100Z         dtype = torch.float16
2025-04-11T04:23:18.5235189Z         device = get_current_device()
2025-04-11T04:23:18.5235261Z     
2025-04-11T04:23:18.5235350Z         if same_context_len:
2025-04-11T04:23:18.5235574Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5235685Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5235971Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5236114Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5236273Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5236280Z 
2025-04-11T04:23:18.5236457Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5236607Z ____________________ test_kv_cache_memcopy[True-16-32-16-4] ____________________
2025-04-11T04:23:18.5236611Z 
2025-04-11T04:23:18.5236757Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5236846Z same_context_len = True
2025-04-11T04:23:18.5236850Z 
2025-04-11T04:23:18.5236957Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5237086Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5237226Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5237345Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5237566Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5237656Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5237739Z         bsz: int,
2025-04-11T04:23:18.5237823Z         block_size: int,
2025-04-11T04:23:18.5237914Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5237993Z         num_kv_heads: int,
2025-04-11T04:23:18.5238078Z         same_context_len: bool,
2025-04-11T04:23:18.5238158Z     ):
2025-04-11T04:23:18.5238376Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5238380Z 
2025-04-11T04:23:18.5238533Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5238642Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5238646Z 
2025-04-11T04:23:18.5238796Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5238878Z same_context_len = True
2025-04-11T04:23:18.5238885Z 
2025-04-11T04:23:18.5238976Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5239059Z         bsz: int,
2025-04-11T04:23:18.5239138Z         block_size: int,
2025-04-11T04:23:18.5239351Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5239432Z         num_kv_heads: int,
2025-04-11T04:23:18.5239521Z         same_context_len: bool,
2025-04-11T04:23:18.5239594Z     ):
2025-04-11T04:23:18.5239679Z         torch.manual_seed(123)
2025-04-11T04:23:18.5239756Z     
2025-04-11T04:23:18.5239953Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5240072Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5240158Z         dtype = torch.float16
2025-04-11T04:23:18.5240248Z         device = get_current_device()
2025-04-11T04:23:18.5240327Z     
2025-04-11T04:23:18.5240411Z         if same_context_len:
2025-04-11T04:23:18.5240646Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5240752Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5241035Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5241174Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5241331Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5241335Z 
2025-04-11T04:23:18.5241518Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5241667Z ____________________ test_kv_cache_memcopy[True-16-32-16-7] ____________________
2025-04-11T04:23:18.5241670Z 
2025-04-11T04:23:18.5241825Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5241907Z same_context_len = True
2025-04-11T04:23:18.5241911Z 
2025-04-11T04:23:18.5242027Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5242151Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5242295Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5242413Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5242551Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5242646Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5242727Z         bsz: int,
2025-04-11T04:23:18.5242811Z         block_size: int,
2025-04-11T04:23:18.5242900Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5242983Z         num_kv_heads: int,
2025-04-11T04:23:18.5243075Z         same_context_len: bool,
2025-04-11T04:23:18.5243150Z     ):
2025-04-11T04:23:18.5243379Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5243384Z 
2025-04-11T04:23:18.5243534Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5243803Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5243808Z 
2025-04-11T04:23:18.5243954Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5244037Z same_context_len = True
2025-04-11T04:23:18.5244047Z 
2025-04-11T04:23:18.5244138Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5244215Z         bsz: int,
2025-04-11T04:23:18.5244301Z         block_size: int,
2025-04-11T04:23:18.5244388Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5244475Z         num_kv_heads: int,
2025-04-11T04:23:18.5244558Z         same_context_len: bool,
2025-04-11T04:23:18.5244632Z     ):
2025-04-11T04:23:18.5244724Z         torch.manual_seed(123)
2025-04-11T04:23:18.5244796Z     
2025-04-11T04:23:18.5244998Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5245114Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5245207Z         dtype = torch.float16
2025-04-11T04:23:18.5245302Z         device = get_current_device()
2025-04-11T04:23:18.5245382Z     
2025-04-11T04:23:18.5245482Z         if same_context_len:
2025-04-11T04:23:18.5245802Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5245939Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5246233Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5246371Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5246539Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5246543Z 
2025-04-11T04:23:18.5246725Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5246888Z ___________________ test_kv_cache_memcopy[True-16-32-16-32] ____________________
2025-04-11T04:23:18.5246892Z 
2025-04-11T04:23:18.5247044Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5247137Z same_context_len = True
2025-04-11T04:23:18.5247143Z 
2025-04-11T04:23:18.5247252Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5247384Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5247527Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5247642Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5247790Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5247882Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5247971Z         bsz: int,
2025-04-11T04:23:18.5248056Z         block_size: int,
2025-04-11T04:23:18.5248151Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5248237Z         num_kv_heads: int,
2025-04-11T04:23:18.5248327Z         same_context_len: bool,
2025-04-11T04:23:18.5248413Z     ):
2025-04-11T04:23:18.5248640Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5248644Z 
2025-04-11T04:23:18.5248802Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5248920Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5248924Z 
2025-04-11T04:23:18.5249078Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5249165Z same_context_len = True
2025-04-11T04:23:18.5249169Z 
2025-04-11T04:23:18.5249265Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5249351Z         bsz: int,
2025-04-11T04:23:18.5249434Z         block_size: int,
2025-04-11T04:23:18.5249529Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5249613Z         num_kv_heads: int,
2025-04-11T04:23:18.5249700Z         same_context_len: bool,
2025-04-11T04:23:18.5249783Z     ):
2025-04-11T04:23:18.5249963Z         torch.manual_seed(123)
2025-04-11T04:23:18.5250043Z     
2025-04-11T04:23:18.5250244Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5250367Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5250456Z         dtype = torch.float16
2025-04-11T04:23:18.5250546Z         device = get_current_device()
2025-04-11T04:23:18.5250625Z     
2025-04-11T04:23:18.5250708Z         if same_context_len:
2025-04-11T04:23:18.5250934Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5251041Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5251326Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5251461Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5251621Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5251625Z 
2025-04-11T04:23:18.5251808Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5252039Z ____________________ test_kv_cache_memcopy[True-16-32-32-4] ____________________
2025-04-11T04:23:18.5252042Z 
2025-04-11T04:23:18.5252194Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5252276Z same_context_len = True
2025-04-11T04:23:18.5252280Z 
2025-04-11T04:23:18.5252392Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5252516Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5252656Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5252773Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5252909Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5253000Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5253078Z         bsz: int,
2025-04-11T04:23:18.5253164Z         block_size: int,
2025-04-11T04:23:18.5253251Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5253333Z         num_kv_heads: int,
2025-04-11T04:23:18.5253425Z         same_context_len: bool,
2025-04-11T04:23:18.5253497Z     ):
2025-04-11T04:23:18.5253725Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5253729Z 
2025-04-11T04:23:18.5253881Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5253996Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5254000Z 
2025-04-11T04:23:18.5254141Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5254222Z same_context_len = True
2025-04-11T04:23:18.5254226Z 
2025-04-11T04:23:18.5254322Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5254399Z         bsz: int,
2025-04-11T04:23:18.5254486Z         block_size: int,
2025-04-11T04:23:18.5254573Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5254660Z         num_kv_heads: int,
2025-04-11T04:23:18.5254742Z         same_context_len: bool,
2025-04-11T04:23:18.5254817Z     ):
2025-04-11T04:23:18.5254908Z         torch.manual_seed(123)
2025-04-11T04:23:18.5254979Z     
2025-04-11T04:23:18.5255182Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5255298Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5255384Z         dtype = torch.float16
2025-04-11T04:23:18.5255481Z         device = get_current_device()
2025-04-11T04:23:18.5255553Z     
2025-04-11T04:23:18.5255643Z         if same_context_len:
2025-04-11T04:23:18.5255863Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5255974Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5256339Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5256477Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5256647Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5256651Z 
2025-04-11T04:23:18.5256824Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5256991Z ____________________ test_kv_cache_memcopy[True-16-32-32-7] ____________________
2025-04-11T04:23:18.5256995Z 
2025-04-11T04:23:18.5257145Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5257239Z same_context_len = True
2025-04-11T04:23:18.5257243Z 
2025-04-11T04:23:18.5257348Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5257478Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5257623Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5257734Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5257875Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5258055Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5258141Z         bsz: int,
2025-04-11T04:23:18.5258222Z         block_size: int,
2025-04-11T04:23:18.5258313Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5258399Z         num_kv_heads: int,
2025-04-11T04:23:18.5258485Z         same_context_len: bool,
2025-04-11T04:23:18.5258563Z     ):
2025-04-11T04:23:18.5258785Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5258789Z 
2025-04-11T04:23:18.5258941Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5259051Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5259055Z 
2025-04-11T04:23:18.5259205Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5259288Z same_context_len = True
2025-04-11T04:23:18.5259291Z 
2025-04-11T04:23:18.5259384Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5259474Z         bsz: int,
2025-04-11T04:23:18.5259560Z         block_size: int,
2025-04-11T04:23:18.5259660Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5259742Z         num_kv_heads: int,
2025-04-11T04:23:18.5259825Z         same_context_len: bool,
2025-04-11T04:23:18.5259908Z     ):
2025-04-11T04:23:18.5259994Z         torch.manual_seed(123)
2025-04-11T04:23:18.5260070Z     
2025-04-11T04:23:18.5260267Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5260387Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5260474Z         dtype = torch.float16
2025-04-11T04:23:18.5260564Z         device = get_current_device()
2025-04-11T04:23:18.5260641Z     
2025-04-11T04:23:18.5260727Z         if same_context_len:
2025-04-11T04:23:18.5260956Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5261063Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5261350Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5261490Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5261646Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5261650Z 
2025-04-11T04:23:18.5261834Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5261980Z ___________________ test_kv_cache_memcopy[True-16-32-32-32] ____________________
2025-04-11T04:23:18.5261984Z 
2025-04-11T04:23:18.5262139Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5262310Z same_context_len = True
2025-04-11T04:23:18.5262315Z 
2025-04-11T04:23:18.5262425Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5262549Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5262694Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5262814Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5262952Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5263044Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5263121Z         bsz: int,
2025-04-11T04:23:18.5263205Z         block_size: int,
2025-04-11T04:23:18.5263294Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5263375Z         num_kv_heads: int,
2025-04-11T04:23:18.5263463Z         same_context_len: bool,
2025-04-11T04:23:18.5263536Z     ):
2025-04-11T04:23:18.5263762Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5263770Z 
2025-04-11T04:23:18.5263921Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5264035Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5264142Z 
2025-04-11T04:23:18.5264291Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5264375Z same_context_len = True
2025-04-11T04:23:18.5264379Z 
2025-04-11T04:23:18.5264478Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5264555Z         bsz: int,
2025-04-11T04:23:18.5264640Z         block_size: int,
2025-04-11T04:23:18.5264728Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5264808Z         num_kv_heads: int,
2025-04-11T04:23:18.5264897Z         same_context_len: bool,
2025-04-11T04:23:18.5264969Z     ):
2025-04-11T04:23:18.5265059Z         torch.manual_seed(123)
2025-04-11T04:23:18.5265130Z     
2025-04-11T04:23:18.5265335Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5265453Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5265540Z         dtype = torch.float16
2025-04-11T04:23:18.5265638Z         device = get_current_device()
2025-04-11T04:23:18.5265717Z     
2025-04-11T04:23:18.5265808Z         if same_context_len:
2025-04-11T04:23:18.5266033Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5266139Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5266428Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5266564Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5266729Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5266733Z 
2025-04-11T04:23:18.5266911Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5267063Z ____________________ test_kv_cache_memcopy[True-16-32-64-4] ____________________
2025-04-11T04:23:18.5267067Z 
2025-04-11T04:23:18.5267209Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5267300Z same_context_len = True
2025-04-11T04:23:18.5267304Z 
2025-04-11T04:23:18.5267410Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5267532Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5267679Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5267791Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5267932Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5268021Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5268099Z         bsz: int,
2025-04-11T04:23:18.5268180Z         block_size: int,
2025-04-11T04:23:18.5268271Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5268486Z         num_kv_heads: int,
2025-04-11T04:23:18.5268581Z         same_context_len: bool,
2025-04-11T04:23:18.5268659Z     ):
2025-04-11T04:23:18.5268884Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5268892Z 
2025-04-11T04:23:18.5269050Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5269161Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5269165Z 
2025-04-11T04:23:18.5269309Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5269400Z same_context_len = True
2025-04-11T04:23:18.5269404Z 
2025-04-11T04:23:18.5269494Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5269576Z         bsz: int,
2025-04-11T04:23:18.5269654Z         block_size: int,
2025-04-11T04:23:18.5269744Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5269824Z         num_kv_heads: int,
2025-04-11T04:23:18.5269911Z         same_context_len: bool,
2025-04-11T04:23:18.5269994Z     ):
2025-04-11T04:23:18.5270081Z         torch.manual_seed(123)
2025-04-11T04:23:18.5270158Z     
2025-04-11T04:23:18.5270356Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5270577Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5270671Z         dtype = torch.float16
2025-04-11T04:23:18.5270759Z         device = get_current_device()
2025-04-11T04:23:18.5270837Z     
2025-04-11T04:23:18.5270919Z         if same_context_len:
2025-04-11T04:23:18.5271147Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5271253Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5271533Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5271679Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5271838Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5271842Z 
2025-04-11T04:23:18.5272025Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5272171Z ____________________ test_kv_cache_memcopy[True-16-32-64-7] ____________________
2025-04-11T04:23:18.5272175Z 
2025-04-11T04:23:18.5272321Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5272404Z same_context_len = True
2025-04-11T04:23:18.5272408Z 
2025-04-11T04:23:18.5272518Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5272638Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5272777Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5272896Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5273038Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5273132Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5273210Z         bsz: int,
2025-04-11T04:23:18.5273290Z         block_size: int,
2025-04-11T04:23:18.5273389Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5273470Z         num_kv_heads: int,
2025-04-11T04:23:18.5273560Z         same_context_len: bool,
2025-04-11T04:23:18.5273633Z     ):
2025-04-11T04:23:18.5273858Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5273862Z 
2025-04-11T04:23:18.5274013Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5274125Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5274134Z 
2025-04-11T04:23:18.5274276Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5274355Z same_context_len = True
2025-04-11T04:23:18.5274359Z 
2025-04-11T04:23:18.5274551Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5274633Z         bsz: int,
2025-04-11T04:23:18.5274720Z         block_size: int,
2025-04-11T04:23:18.5274808Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5274892Z         num_kv_heads: int,
2025-04-11T04:23:18.5274983Z         same_context_len: bool,
2025-04-11T04:23:18.5275056Z     ):
2025-04-11T04:23:18.5275146Z         torch.manual_seed(123)
2025-04-11T04:23:18.5275217Z     
2025-04-11T04:23:18.5275415Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5275537Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5275623Z         dtype = torch.float16
2025-04-11T04:23:18.5275719Z         device = get_current_device()
2025-04-11T04:23:18.5275790Z     
2025-04-11T04:23:18.5275879Z         if same_context_len:
2025-04-11T04:23:18.5276105Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5276208Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5276500Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5276725Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5276889Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5276893Z 
2025-04-11T04:23:18.5277067Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5277221Z ___________________ test_kv_cache_memcopy[True-16-32-64-32] ____________________
2025-04-11T04:23:18.5277224Z 
2025-04-11T04:23:18.5277370Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5277457Z same_context_len = True
2025-04-11T04:23:18.5277460Z 
2025-04-11T04:23:18.5277564Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5277691Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5277839Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5277959Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5278113Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5278200Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5278293Z         bsz: int,
2025-04-11T04:23:18.5278393Z         block_size: int,
2025-04-11T04:23:18.5278487Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5278576Z         num_kv_heads: int,
2025-04-11T04:23:18.5278660Z         same_context_len: bool,
2025-04-11T04:23:18.5278739Z     ):
2025-04-11T04:23:18.5278961Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5278965Z 
2025-04-11T04:23:18.5279116Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5279238Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5279242Z 
2025-04-11T04:23:18.5279385Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5279478Z same_context_len = True
2025-04-11T04:23:18.5279482Z 
2025-04-11T04:23:18.5279574Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5279654Z         bsz: int,
2025-04-11T04:23:18.5279734Z         block_size: int,
2025-04-11T04:23:18.5279827Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5279908Z         num_kv_heads: int,
2025-04-11T04:23:18.5279993Z         same_context_len: bool,
2025-04-11T04:23:18.5280070Z     ):
2025-04-11T04:23:18.5280153Z         torch.manual_seed(123)
2025-04-11T04:23:18.5280228Z     
2025-04-11T04:23:18.5280422Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5280537Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5280630Z         dtype = torch.float16
2025-04-11T04:23:18.5280802Z         device = get_current_device()
2025-04-11T04:23:18.5280882Z     
2025-04-11T04:23:18.5280965Z         if same_context_len:
2025-04-11T04:23:18.5281190Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5281304Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5281584Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5281722Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5281881Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5281885Z 
2025-04-11T04:23:18.5282062Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T04:23:18.5282211Z ____________________ test_kv_cache_memcopy[False-16-8-16-4] ____________________
2025-04-11T04:23:18.5282215Z 
2025-04-11T04:23:18.5282371Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5282455Z same_context_len = False
2025-04-11T04:23:18.5282459Z 
2025-04-11T04:23:18.5282654Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5282784Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5282924Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5283039Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5283175Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5283265Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5283342Z         bsz: int,
2025-04-11T04:23:18.5283420Z         block_size: int,
2025-04-11T04:23:18.5283515Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5283595Z         num_kv_heads: int,
2025-04-11T04:23:18.5283683Z         same_context_len: bool,
2025-04-11T04:23:18.5283757Z     ):
2025-04-11T04:23:18.5283981Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5283992Z 
2025-04-11T04:23:18.5284142Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5284255Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5284258Z 
2025-04-11T04:23:18.5284409Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5284490Z same_context_len = False
2025-04-11T04:23:18.5284494Z 
2025-04-11T04:23:18.5284592Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5284668Z         bsz: int,
2025-04-11T04:23:18.5284756Z         block_size: int,
2025-04-11T04:23:18.5284845Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5284925Z         num_kv_heads: int,
2025-04-11T04:23:18.5285013Z         same_context_len: bool,
2025-04-11T04:23:18.5285086Z     ):
2025-04-11T04:23:18.5285175Z         torch.manual_seed(123)
2025-04-11T04:23:18.5285252Z     
2025-04-11T04:23:18.5285450Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5285574Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5285662Z         dtype = torch.float16
2025-04-11T04:23:18.5285757Z         device = get_current_device()
2025-04-11T04:23:18.5285829Z     
2025-04-11T04:23:18.5285916Z         if same_context_len:
2025-04-11T04:23:18.5286143Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5286218Z         else:
2025-04-11T04:23:18.5286460Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5286569Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5286851Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5287083Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5287251Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5287259Z 
2025-04-11T04:23:18.5287436Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5287582Z ____________________ test_kv_cache_memcopy[False-16-8-16-7] ____________________
2025-04-11T04:23:18.5287592Z 
2025-04-11T04:23:18.5287737Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5287820Z same_context_len = False
2025-04-11T04:23:18.5287824Z 
2025-04-11T04:23:18.5287934Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5288059Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5288201Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5288312Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5288458Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5288546Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5288623Z         bsz: int,
2025-04-11T04:23:18.5288710Z         block_size: int,
2025-04-11T04:23:18.5288987Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5289074Z         num_kv_heads: int,
2025-04-11T04:23:18.5289160Z         same_context_len: bool,
2025-04-11T04:23:18.5289232Z     ):
2025-04-11T04:23:18.5289461Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5289465Z 
2025-04-11T04:23:18.5289625Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5289739Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5289742Z 
2025-04-11T04:23:18.5289893Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5289984Z same_context_len = False
2025-04-11T04:23:18.5289987Z 
2025-04-11T04:23:18.5290095Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5290170Z         bsz: int,
2025-04-11T04:23:18.5290256Z         block_size: int,
2025-04-11T04:23:18.5290346Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5290444Z         num_kv_heads: int,
2025-04-11T04:23:18.5290530Z         same_context_len: bool,
2025-04-11T04:23:18.5290609Z     ):
2025-04-11T04:23:18.5290694Z         torch.manual_seed(123)
2025-04-11T04:23:18.5290767Z     
2025-04-11T04:23:18.5290972Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5291087Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5291178Z         dtype = torch.float16
2025-04-11T04:23:18.5291269Z         device = get_current_device()
2025-04-11T04:23:18.5291340Z     
2025-04-11T04:23:18.5291430Z         if same_context_len:
2025-04-11T04:23:18.5291659Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5291740Z         else:
2025-04-11T04:23:18.5291978Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5292092Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5292371Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5292509Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5292675Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5292678Z 
2025-04-11T04:23:18.5292853Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5293006Z ___________________ test_kv_cache_memcopy[False-16-8-16-32] ____________________
2025-04-11T04:23:18.5293010Z 
2025-04-11T04:23:18.5293248Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5293340Z same_context_len = False
2025-04-11T04:23:18.5293343Z 
2025-04-11T04:23:18.5293449Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5293580Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5293721Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5293834Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5293976Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5294064Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5294145Z         bsz: int,
2025-04-11T04:23:18.5294225Z         block_size: int,
2025-04-11T04:23:18.5294315Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5294403Z         num_kv_heads: int,
2025-04-11T04:23:18.5294487Z         same_context_len: bool,
2025-04-11T04:23:18.5294566Z     ):
2025-04-11T04:23:18.5294790Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5294794Z 
2025-04-11T04:23:18.5294953Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5295062Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5295151Z 
2025-04-11T04:23:18.5295303Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5295387Z same_context_len = False
2025-04-11T04:23:18.5295391Z 
2025-04-11T04:23:18.5295483Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5295565Z         bsz: int,
2025-04-11T04:23:18.5295644Z         block_size: int,
2025-04-11T04:23:18.5295736Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5295815Z         num_kv_heads: int,
2025-04-11T04:23:18.5295899Z         same_context_len: bool,
2025-04-11T04:23:18.5295978Z     ):
2025-04-11T04:23:18.5296063Z         torch.manual_seed(123)
2025-04-11T04:23:18.5296138Z     
2025-04-11T04:23:18.5296337Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5296457Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5296546Z         dtype = torch.float16
2025-04-11T04:23:18.5296640Z         device = get_current_device()
2025-04-11T04:23:18.5296717Z     
2025-04-11T04:23:18.5296799Z         if same_context_len:
2025-04-11T04:23:18.5297023Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5297097Z         else:
2025-04-11T04:23:18.5297333Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5297443Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5297720Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5297864Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5298024Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5298028Z 
2025-04-11T04:23:18.5298211Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5298359Z ____________________ test_kv_cache_memcopy[False-16-8-32-4] ____________________
2025-04-11T04:23:18.5298363Z 
2025-04-11T04:23:18.5298510Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5298592Z same_context_len = False
2025-04-11T04:23:18.5298596Z 
2025-04-11T04:23:18.5298698Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5298826Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5298963Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5299083Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5299221Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5299392Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5299472Z         bsz: int,
2025-04-11T04:23:18.5299553Z         block_size: int,
2025-04-11T04:23:18.5299647Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5299730Z         num_kv_heads: int,
2025-04-11T04:23:18.5299820Z         same_context_len: bool,
2025-04-11T04:23:18.5299892Z     ):
2025-04-11T04:23:18.5300114Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5300124Z 
2025-04-11T04:23:18.5300274Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5300382Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5300386Z 
2025-04-11T04:23:18.5300538Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5300620Z same_context_len = False
2025-04-11T04:23:18.5300623Z 
2025-04-11T04:23:18.5300727Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5300808Z         bsz: int,
2025-04-11T04:23:18.5300895Z         block_size: int,
2025-04-11T04:23:18.5300982Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5301066Z         num_kv_heads: int,
2025-04-11T04:23:18.5301246Z         same_context_len: bool,
2025-04-11T04:23:18.5301331Z     ):
2025-04-11T04:23:18.5301446Z         torch.manual_seed(123)
2025-04-11T04:23:18.5301517Z     
2025-04-11T04:23:18.5301715Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5301838Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5301925Z         dtype = torch.float16
2025-04-11T04:23:18.5302023Z         device = get_current_device()
2025-04-11T04:23:18.5302095Z     
2025-04-11T04:23:18.5302180Z         if same_context_len:
2025-04-11T04:23:18.5302424Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5302503Z         else:
2025-04-11T04:23:18.5302757Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5302867Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5303155Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5303288Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5303453Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5303457Z 
2025-04-11T04:23:18.5303636Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5303783Z ____________________ test_kv_cache_memcopy[False-16-8-32-7] ____________________
2025-04-11T04:23:18.5303793Z 
2025-04-11T04:23:18.5303941Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5304024Z same_context_len = False
2025-04-11T04:23:18.5304031Z 
2025-04-11T04:23:18.5304147Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5304273Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5304423Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5304538Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5304675Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5304769Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5304847Z         bsz: int,
2025-04-11T04:23:18.5304933Z         block_size: int,
2025-04-11T04:23:18.5305023Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5305111Z         num_kv_heads: int,
2025-04-11T04:23:18.5305197Z         same_context_len: bool,
2025-04-11T04:23:18.5305268Z     ):
2025-04-11T04:23:18.5305498Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5305502Z 
2025-04-11T04:23:18.5305765Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5305885Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5305892Z 
2025-04-11T04:23:18.5306039Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5306130Z same_context_len = False
2025-04-11T04:23:18.5306133Z 
2025-04-11T04:23:18.5306226Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5306305Z         bsz: int,
2025-04-11T04:23:18.5306398Z         block_size: int,
2025-04-11T04:23:18.5306486Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5306580Z         num_kv_heads: int,
2025-04-11T04:23:18.5306666Z         same_context_len: bool,
2025-04-11T04:23:18.5306746Z     ):
2025-04-11T04:23:18.5306833Z         torch.manual_seed(123)
2025-04-11T04:23:18.5306906Z     
2025-04-11T04:23:18.5307110Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5307229Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5307325Z         dtype = torch.float16
2025-04-11T04:23:18.5307415Z         device = get_current_device()
2025-04-11T04:23:18.5307580Z     
2025-04-11T04:23:18.5307678Z         if same_context_len:
2025-04-11T04:23:18.5307905Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5307988Z         else:
2025-04-11T04:23:18.5308225Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5308340Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5308681Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5308816Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5308987Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5308991Z 
2025-04-11T04:23:18.5309168Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5309327Z ___________________ test_kv_cache_memcopy[False-16-8-32-32] ____________________
2025-04-11T04:23:18.5309332Z 
2025-04-11T04:23:18.5309476Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5309564Z same_context_len = False
2025-04-11T04:23:18.5309567Z 
2025-04-11T04:23:18.5309676Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5309808Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5309950Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5310064Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5310212Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5310300Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5310388Z         bsz: int,
2025-04-11T04:23:18.5310468Z         block_size: int,
2025-04-11T04:23:18.5310556Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5310645Z         num_kv_heads: int,
2025-04-11T04:23:18.5310736Z         same_context_len: bool,
2025-04-11T04:23:18.5310816Z     ):
2025-04-11T04:23:18.5311041Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5311045Z 
2025-04-11T04:23:18.5311203Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5311313Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5311317Z 
2025-04-11T04:23:18.5311466Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5311550Z same_context_len = False
2025-04-11T04:23:18.5311554Z 
2025-04-11T04:23:18.5311645Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5311734Z         bsz: int,
2025-04-11T04:23:18.5311919Z         block_size: int,
2025-04-11T04:23:18.5312020Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5312104Z         num_kv_heads: int,
2025-04-11T04:23:18.5312190Z         same_context_len: bool,
2025-04-11T04:23:18.5312276Z     ):
2025-04-11T04:23:18.5312363Z         torch.manual_seed(123)
2025-04-11T04:23:18.5312444Z     
2025-04-11T04:23:18.5312647Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5312787Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5312874Z         dtype = torch.float16
2025-04-11T04:23:18.5312975Z         device = get_current_device()
2025-04-11T04:23:18.5313063Z     
2025-04-11T04:23:18.5313148Z         if same_context_len:
2025-04-11T04:23:18.5313380Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5313455Z         else:
2025-04-11T04:23:18.5313693Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5313816Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5314103Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5314337Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5314498Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5314502Z 
2025-04-11T04:23:18.5314683Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5314830Z ____________________ test_kv_cache_memcopy[False-16-8-64-4] ____________________
2025-04-11T04:23:18.5314833Z 
2025-04-11T04:23:18.5314985Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5315069Z same_context_len = False
2025-04-11T04:23:18.5315072Z 
2025-04-11T04:23:18.5315184Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5315319Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5315459Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5315584Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5315726Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5315819Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5315897Z         bsz: int,
2025-04-11T04:23:18.5315977Z         block_size: int,
2025-04-11T04:23:18.5316072Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5316154Z         num_kv_heads: int,
2025-04-11T04:23:18.5316244Z         same_context_len: bool,
2025-04-11T04:23:18.5316317Z     ):
2025-04-11T04:23:18.5316537Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5316549Z 
2025-04-11T04:23:18.5316703Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5316815Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5316819Z 
2025-04-11T04:23:18.5316971Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5317062Z same_context_len = False
2025-04-11T04:23:18.5317065Z 
2025-04-11T04:23:18.5317164Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5317241Z         bsz: int,
2025-04-11T04:23:18.5317327Z         block_size: int,
2025-04-11T04:23:18.5317418Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5317501Z         num_kv_heads: int,
2025-04-11T04:23:18.5317593Z         same_context_len: bool,
2025-04-11T04:23:18.5317667Z     ):
2025-04-11T04:23:18.5317758Z         torch.manual_seed(123)
2025-04-11T04:23:18.5317830Z     
2025-04-11T04:23:18.5318029Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5318155Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5318326Z         dtype = torch.float16
2025-04-11T04:23:18.5318426Z         device = get_current_device()
2025-04-11T04:23:18.5318499Z     
2025-04-11T04:23:18.5318584Z         if same_context_len:
2025-04-11T04:23:18.5318818Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5318893Z         else:
2025-04-11T04:23:18.5319132Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5319238Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5319524Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5319660Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5319827Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5319830Z 
2025-04-11T04:23:18.5320017Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5320165Z ____________________ test_kv_cache_memcopy[False-16-8-64-7] ____________________
2025-04-11T04:23:18.5320266Z 
2025-04-11T04:23:18.5320411Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5320495Z same_context_len = False
2025-04-11T04:23:18.5320499Z 
2025-04-11T04:23:18.5320614Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5320738Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5320886Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5321000Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5321140Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5321233Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5321309Z         bsz: int,
2025-04-11T04:23:18.5321397Z         block_size: int,
2025-04-11T04:23:18.5321489Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5321576Z         num_kv_heads: int,
2025-04-11T04:23:18.5321660Z         same_context_len: bool,
2025-04-11T04:23:18.5321738Z     ):
2025-04-11T04:23:18.5321971Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5321975Z 
2025-04-11T04:23:18.5322132Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5322247Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5322251Z 
2025-04-11T04:23:18.5322393Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5322483Z same_context_len = False
2025-04-11T04:23:18.5322486Z 
2025-04-11T04:23:18.5322580Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5322658Z         bsz: int,
2025-04-11T04:23:18.5322750Z         block_size: int,
2025-04-11T04:23:18.5322843Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5322932Z         num_kv_heads: int,
2025-04-11T04:23:18.5323019Z         same_context_len: bool,
2025-04-11T04:23:18.5323098Z     ):
2025-04-11T04:23:18.5323186Z         torch.manual_seed(123)
2025-04-11T04:23:18.5323268Z     
2025-04-11T04:23:18.5323479Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5323600Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5323695Z         dtype = torch.float16
2025-04-11T04:23:18.5323788Z         device = get_current_device()
2025-04-11T04:23:18.5323859Z     
2025-04-11T04:23:18.5323951Z         if same_context_len:
2025-04-11T04:23:18.5324175Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5324273Z         else:
2025-04-11T04:23:18.5324526Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5324729Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5325012Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5325152Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5325320Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5325323Z 
2025-04-11T04:23:18.5325501Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5325658Z ___________________ test_kv_cache_memcopy[False-16-8-64-32] ____________________
2025-04-11T04:23:18.5325661Z 
2025-04-11T04:23:18.5325806Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5325895Z same_context_len = False
2025-04-11T04:23:18.5325899Z 
2025-04-11T04:23:18.5326007Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5326146Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5326289Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5326401Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5326628Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5326717Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5326803Z         bsz: int,
2025-04-11T04:23:18.5326885Z         block_size: int,
2025-04-11T04:23:18.5326974Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5327065Z         num_kv_heads: int,
2025-04-11T04:23:18.5327151Z         same_context_len: bool,
2025-04-11T04:23:18.5327233Z     ):
2025-04-11T04:23:18.5327455Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5327459Z 
2025-04-11T04:23:18.5327617Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5327732Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5327735Z 
2025-04-11T04:23:18.5327886Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T04:23:18.5327968Z same_context_len = False
2025-04-11T04:23:18.5327976Z 
2025-04-11T04:23:18.5328071Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5328153Z         bsz: int,
2025-04-11T04:23:18.5328234Z         block_size: int,
2025-04-11T04:23:18.5328333Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5328415Z         num_kv_heads: int,
2025-04-11T04:23:18.5328501Z         same_context_len: bool,
2025-04-11T04:23:18.5328583Z     ):
2025-04-11T04:23:18.5328669Z         torch.manual_seed(123)
2025-04-11T04:23:18.5328747Z     
2025-04-11T04:23:18.5328944Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5329063Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5329149Z         dtype = torch.float16
2025-04-11T04:23:18.5329249Z         device = get_current_device()
2025-04-11T04:23:18.5329329Z     
2025-04-11T04:23:18.5329412Z         if same_context_len:
2025-04-11T04:23:18.5329642Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5329721Z         else:
2025-04-11T04:23:18.5329956Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5330069Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5330351Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5330492Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5330652Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5330656Z 
2025-04-11T04:23:18.5330918Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5331072Z ___________________ test_kv_cache_memcopy[False-16-32-16-4] ____________________
2025-04-11T04:23:18.5331076Z 
2025-04-11T04:23:18.5331228Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5331321Z same_context_len = False
2025-04-11T04:23:18.5331324Z 
2025-04-11T04:23:18.5331429Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5331560Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5331702Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5331821Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5331956Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5332050Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5332127Z         bsz: int,
2025-04-11T04:23:18.5332208Z         block_size: int,
2025-04-11T04:23:18.5332305Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5332392Z         num_kv_heads: int,
2025-04-11T04:23:18.5332485Z         same_context_len: bool,
2025-04-11T04:23:18.5332560Z     ):
2025-04-11T04:23:18.5332780Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5332896Z 
2025-04-11T04:23:18.5333051Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5333164Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5333167Z 
2025-04-11T04:23:18.5333318Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5333403Z same_context_len = False
2025-04-11T04:23:18.5333407Z 
2025-04-11T04:23:18.5333511Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5333590Z         bsz: int,
2025-04-11T04:23:18.5333675Z         block_size: int,
2025-04-11T04:23:18.5333764Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5333846Z         num_kv_heads: int,
2025-04-11T04:23:18.5333942Z         same_context_len: bool,
2025-04-11T04:23:18.5334016Z     ):
2025-04-11T04:23:18.5334111Z         torch.manual_seed(123)
2025-04-11T04:23:18.5334184Z     
2025-04-11T04:23:18.5334385Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5334509Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5334596Z         dtype = torch.float16
2025-04-11T04:23:18.5334699Z         device = get_current_device()
2025-04-11T04:23:18.5334771Z     
2025-04-11T04:23:18.5334859Z         if same_context_len:
2025-04-11T04:23:18.5335091Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5335168Z         else:
2025-04-11T04:23:18.5335416Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5335535Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5335827Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5335962Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5336129Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5336133Z 
2025-04-11T04:23:18.5336311Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5336460Z ___________________ test_kv_cache_memcopy[False-16-32-16-7] ____________________
2025-04-11T04:23:18.5336463Z 
2025-04-11T04:23:18.5336614Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5336702Z same_context_len = False
2025-04-11T04:23:18.5336706Z 
2025-04-11T04:23:18.5336818Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5336942Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5337172Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5337291Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5337428Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5337526Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5337603Z         bsz: int,
2025-04-11T04:23:18.5337689Z         block_size: int,
2025-04-11T04:23:18.5337778Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5337862Z         num_kv_heads: int,
2025-04-11T04:23:18.5337947Z         same_context_len: bool,
2025-04-11T04:23:18.5338019Z     ):
2025-04-11T04:23:18.5338247Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5338251Z 
2025-04-11T04:23:18.5338399Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5338515Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5338519Z 
2025-04-11T04:23:18.5338663Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5338750Z same_context_len = False
2025-04-11T04:23:18.5338754Z 
2025-04-11T04:23:18.5338931Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5339009Z         bsz: int,
2025-04-11T04:23:18.5339095Z         block_size: int,
2025-04-11T04:23:18.5339183Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5339270Z         num_kv_heads: int,
2025-04-11T04:23:18.5339352Z         same_context_len: bool,
2025-04-11T04:23:18.5339426Z     ):
2025-04-11T04:23:18.5339516Z         torch.manual_seed(123)
2025-04-11T04:23:18.5339589Z     
2025-04-11T04:23:18.5339788Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5339903Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5339992Z         dtype = torch.float16
2025-04-11T04:23:18.5340081Z         device = get_current_device()
2025-04-11T04:23:18.5340156Z     
2025-04-11T04:23:18.5340244Z         if same_context_len:
2025-04-11T04:23:18.5340466Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5340550Z         else:
2025-04-11T04:23:18.5340780Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5340890Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5341168Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5341304Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5341468Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5341472Z 
2025-04-11T04:23:18.5341649Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5341805Z ___________________ test_kv_cache_memcopy[False-16-32-16-32] ___________________
2025-04-11T04:23:18.5341808Z 
2025-04-11T04:23:18.5341952Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5342043Z same_context_len = False
2025-04-11T04:23:18.5342047Z 
2025-04-11T04:23:18.5342150Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5342278Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5342417Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5342528Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5342671Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5342757Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5342838Z         bsz: int,
2025-04-11T04:23:18.5342916Z         block_size: int,
2025-04-11T04:23:18.5343003Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5343090Z         num_kv_heads: int,
2025-04-11T04:23:18.5343261Z         same_context_len: bool,
2025-04-11T04:23:18.5343341Z     ):
2025-04-11T04:23:18.5343566Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5343574Z 
2025-04-11T04:23:18.5343731Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5343843Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5343848Z 
2025-04-11T04:23:18.5343993Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5344082Z same_context_len = False
2025-04-11T04:23:18.5344085Z 
2025-04-11T04:23:18.5344178Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5344260Z         bsz: int,
2025-04-11T04:23:18.5344337Z         block_size: int,
2025-04-11T04:23:18.5344432Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5344513Z         num_kv_heads: int,
2025-04-11T04:23:18.5344596Z         same_context_len: bool,
2025-04-11T04:23:18.5344679Z     ):
2025-04-11T04:23:18.5344765Z         torch.manual_seed(123)
2025-04-11T04:23:18.5344842Z     
2025-04-11T04:23:18.5345037Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5345236Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5345328Z         dtype = torch.float16
2025-04-11T04:23:18.5345424Z         device = get_current_device()
2025-04-11T04:23:18.5345506Z     
2025-04-11T04:23:18.5345588Z         if same_context_len:
2025-04-11T04:23:18.5345833Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5345908Z         else:
2025-04-11T04:23:18.5346145Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5346257Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5346541Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5346685Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5346848Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5346852Z 
2025-04-11T04:23:18.5347037Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5347184Z ___________________ test_kv_cache_memcopy[False-16-32-32-4] ____________________
2025-04-11T04:23:18.5347187Z 
2025-04-11T04:23:18.5347334Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5347416Z same_context_len = False
2025-04-11T04:23:18.5347420Z 
2025-04-11T04:23:18.5347524Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5347654Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5347792Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5347912Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5348051Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5348142Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5348238Z         bsz: int,
2025-04-11T04:23:18.5348317Z         block_size: int,
2025-04-11T04:23:18.5348441Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5348524Z         num_kv_heads: int,
2025-04-11T04:23:18.5348612Z         same_context_len: bool,
2025-04-11T04:23:18.5348685Z     ):
2025-04-11T04:23:18.5348904Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5348914Z 
2025-04-11T04:23:18.5349069Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5349178Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5349182Z 
2025-04-11T04:23:18.5349425Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5349512Z same_context_len = False
2025-04-11T04:23:18.5349516Z 
2025-04-11T04:23:18.5349611Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5349690Z         bsz: int,
2025-04-11T04:23:18.5349773Z         block_size: int,
2025-04-11T04:23:18.5349861Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5349940Z         num_kv_heads: int,
2025-04-11T04:23:18.5350029Z         same_context_len: bool,
2025-04-11T04:23:18.5350100Z     ):
2025-04-11T04:23:18.5350188Z         torch.manual_seed(123)
2025-04-11T04:23:18.5350260Z     
2025-04-11T04:23:18.5350453Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5350574Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5350662Z         dtype = torch.float16
2025-04-11T04:23:18.5350757Z         device = get_current_device()
2025-04-11T04:23:18.5350829Z     
2025-04-11T04:23:18.5350913Z         if same_context_len:
2025-04-11T04:23:18.5351145Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5351219Z         else:
2025-04-11T04:23:18.5351552Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5351659Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5351945Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5352079Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5352238Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5352247Z 
2025-04-11T04:23:18.5352419Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5352566Z ___________________ test_kv_cache_memcopy[False-16-32-32-7] ____________________
2025-04-11T04:23:18.5352573Z 
2025-04-11T04:23:18.5352721Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5352802Z same_context_len = False
2025-04-11T04:23:18.5352809Z 
2025-04-11T04:23:18.5352919Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5353043Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5353187Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5353298Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5353435Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5353526Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5353600Z         bsz: int,
2025-04-11T04:23:18.5353684Z         block_size: int,
2025-04-11T04:23:18.5353773Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5353859Z         num_kv_heads: int,
2025-04-11T04:23:18.5353943Z         same_context_len: bool,
2025-04-11T04:23:18.5354015Z     ):
2025-04-11T04:23:18.5354246Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5354250Z 
2025-04-11T04:23:18.5354401Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5354519Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5354523Z 
2025-04-11T04:23:18.5354665Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5354753Z same_context_len = False
2025-04-11T04:23:18.5354757Z 
2025-04-11T04:23:18.5354849Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5354924Z         bsz: int,
2025-04-11T04:23:18.5355009Z         block_size: int,
2025-04-11T04:23:18.5355096Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5355180Z         num_kv_heads: int,
2025-04-11T04:23:18.5355265Z         same_context_len: bool,
2025-04-11T04:23:18.5355338Z     ):
2025-04-11T04:23:18.5355505Z         torch.manual_seed(123)
2025-04-11T04:23:18.5355580Z     
2025-04-11T04:23:18.5355782Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5355897Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5355993Z         dtype = torch.float16
2025-04-11T04:23:18.5356084Z         device = get_current_device()
2025-04-11T04:23:18.5356155Z     
2025-04-11T04:23:18.5356242Z         if same_context_len:
2025-04-11T04:23:18.5356464Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5356543Z         else:
2025-04-11T04:23:18.5356774Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5356885Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5357171Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5357309Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5357476Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5357577Z 
2025-04-11T04:23:18.5357765Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5357924Z ___________________ test_kv_cache_memcopy[False-16-32-32-32] ___________________
2025-04-11T04:23:18.5357928Z 
2025-04-11T04:23:18.5358074Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5358164Z same_context_len = False
2025-04-11T04:23:18.5358168Z 
2025-04-11T04:23:18.5358271Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5358400Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5358538Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5358653Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5358794Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5358881Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5358965Z         bsz: int,
2025-04-11T04:23:18.5359049Z         block_size: int,
2025-04-11T04:23:18.5359137Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5359222Z         num_kv_heads: int,
2025-04-11T04:23:18.5359307Z         same_context_len: bool,
2025-04-11T04:23:18.5359386Z     ):
2025-04-11T04:23:18.5359605Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5359609Z 
2025-04-11T04:23:18.5359763Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5359873Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5359877Z 
2025-04-11T04:23:18.5360020Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5360111Z same_context_len = False
2025-04-11T04:23:18.5360114Z 
2025-04-11T04:23:18.5360206Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5360286Z         bsz: int,
2025-04-11T04:23:18.5360365Z         block_size: int,
2025-04-11T04:23:18.5360459Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5360540Z         num_kv_heads: int,
2025-04-11T04:23:18.5360622Z         same_context_len: bool,
2025-04-11T04:23:18.5360699Z     ):
2025-04-11T04:23:18.5360785Z         torch.manual_seed(123)
2025-04-11T04:23:18.5360860Z     
2025-04-11T04:23:18.5361054Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5361168Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5361259Z         dtype = torch.float16
2025-04-11T04:23:18.5361348Z         device = get_current_device()
2025-04-11T04:23:18.5361423Z     
2025-04-11T04:23:18.5361505Z         if same_context_len:
2025-04-11T04:23:18.5361808Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5361886Z         else:
2025-04-11T04:23:18.5362117Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5362232Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5362508Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5362648Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5362806Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5362809Z 
2025-04-11T04:23:18.5362991Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5363137Z ___________________ test_kv_cache_memcopy[False-16-32-64-4] ____________________
2025-04-11T04:23:18.5363141Z 
2025-04-11T04:23:18.5363289Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5363372Z same_context_len = False
2025-04-11T04:23:18.5363376Z 
2025-04-11T04:23:18.5363479Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5363710Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5363849Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5363965Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5364104Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5364193Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5364270Z         bsz: int,
2025-04-11T04:23:18.5364350Z         block_size: int,
2025-04-11T04:23:18.5364442Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5364522Z         num_kv_heads: int,
2025-04-11T04:23:18.5364610Z         same_context_len: bool,
2025-04-11T04:23:18.5364683Z     ):
2025-04-11T04:23:18.5364907Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5364911Z 
2025-04-11T04:23:18.5365071Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5365186Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5365189Z 
2025-04-11T04:23:18.5365337Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5365419Z same_context_len = False
2025-04-11T04:23:18.5365423Z 
2025-04-11T04:23:18.5365519Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5365595Z         bsz: int,
2025-04-11T04:23:18.5365674Z         block_size: int,
2025-04-11T04:23:18.5365768Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5365847Z         num_kv_heads: int,
2025-04-11T04:23:18.5365937Z         same_context_len: bool,
2025-04-11T04:23:18.5366010Z     ):
2025-04-11T04:23:18.5366100Z         torch.manual_seed(123)
2025-04-11T04:23:18.5366171Z     
2025-04-11T04:23:18.5366370Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5366493Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5366583Z         dtype = torch.float16
2025-04-11T04:23:18.5366679Z         device = get_current_device()
2025-04-11T04:23:18.5366750Z     
2025-04-11T04:23:18.5366831Z         if same_context_len:
2025-04-11T04:23:18.5367059Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5367133Z         else:
2025-04-11T04:23:18.5367365Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5367469Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5367752Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5367980Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5368140Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5368151Z 
2025-04-11T04:23:18.5368331Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5368476Z ___________________ test_kv_cache_memcopy[False-16-32-64-7] ____________________
2025-04-11T04:23:18.5368480Z 
2025-04-11T04:23:18.5368635Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5368732Z same_context_len = False
2025-04-11T04:23:18.5368735Z 
2025-04-11T04:23:18.5368852Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5368977Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5369130Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5369243Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5369383Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5369478Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5369554Z         bsz: int,
2025-04-11T04:23:18.5369639Z         block_size: int,
2025-04-11T04:23:18.5369812Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5369900Z         num_kv_heads: int,
2025-04-11T04:23:18.5369986Z         same_context_len: bool,
2025-04-11T04:23:18.5370059Z     ):
2025-04-11T04:23:18.5370286Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5370290Z 
2025-04-11T04:23:18.5370439Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5370555Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5370559Z 
2025-04-11T04:23:18.5370700Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5370787Z same_context_len = False
2025-04-11T04:23:18.5370791Z 
2025-04-11T04:23:18.5370887Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5370962Z         bsz: int,
2025-04-11T04:23:18.5371050Z         block_size: int,
2025-04-11T04:23:18.5371136Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5371225Z         num_kv_heads: int,
2025-04-11T04:23:18.5371310Z         same_context_len: bool,
2025-04-11T04:23:18.5371381Z     ):
2025-04-11T04:23:18.5371473Z         torch.manual_seed(123)
2025-04-11T04:23:18.5371543Z     
2025-04-11T04:23:18.5371744Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5371860Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5371951Z         dtype = torch.float16
2025-04-11T04:23:18.5372040Z         device = get_current_device()
2025-04-11T04:23:18.5372111Z     
2025-04-11T04:23:18.5372197Z         if same_context_len:
2025-04-11T04:23:18.5372418Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5372499Z         else:
2025-04-11T04:23:18.5372731Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5372840Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5373126Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5373261Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5373422Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5373426Z 
2025-04-11T04:23:18.5373602Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5373754Z ___________________ test_kv_cache_memcopy[False-16-32-64-32] ___________________
2025-04-11T04:23:18.5373758Z 
2025-04-11T04:23:18.5373909Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5374081Z same_context_len = False
2025-04-11T04:23:18.5374085Z 
2025-04-11T04:23:18.5374193Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T04:23:18.5374318Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.5374465Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T04:23:18.5374575Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.5374716Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5374804Z     def test_kv_cache_memcopy(
2025-04-11T04:23:18.5374886Z         bsz: int,
2025-04-11T04:23:18.5374966Z         block_size: int,
2025-04-11T04:23:18.5375053Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5375141Z         num_kv_heads: int,
2025-04-11T04:23:18.5375224Z         same_context_len: bool,
2025-04-11T04:23:18.5375299Z     ):
2025-04-11T04:23:18.5375516Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5375523Z 
2025-04-11T04:23:18.5375677Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T04:23:18.5375789Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5375878Z 
2025-04-11T04:23:18.5376025Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T04:23:18.5376111Z same_context_len = False
2025-04-11T04:23:18.5376115Z 
2025-04-11T04:23:18.5376205Z     def run_context_copy_kv_to_cache(
2025-04-11T04:23:18.5376287Z         bsz: int,
2025-04-11T04:23:18.5376366Z         block_size: int,
2025-04-11T04:23:18.5376459Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5376538Z         num_kv_heads: int,
2025-04-11T04:23:18.5376620Z         same_context_len: bool,
2025-04-11T04:23:18.5376698Z     ):
2025-04-11T04:23:18.5376783Z         torch.manual_seed(123)
2025-04-11T04:23:18.5376861Z     
2025-04-11T04:23:18.5377058Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T04:23:18.5377172Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T04:23:18.5377264Z         dtype = torch.float16
2025-04-11T04:23:18.5377354Z         device = get_current_device()
2025-04-11T04:23:18.5377433Z     
2025-04-11T04:23:18.5377516Z         if same_context_len:
2025-04-11T04:23:18.5377740Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T04:23:18.5377815Z         else:
2025-04-11T04:23:18.5378047Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T04:23:18.5378160Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5378439Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5378579Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5378740Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5378743Z 
2025-04-11T04:23:18.5378926Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T04:23:18.5379066Z ___________________________ test_rms_layernorm[64-2] ___________________________
2025-04-11T04:23:18.5379070Z 
2025-04-11T04:23:18.5379152Z M = 2, N = 64
2025-04-11T04:23:18.5379156Z 
2025-04-11T04:23:18.5379265Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5379385Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5379493Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5379579Z         torch.manual_seed(123)
2025-04-11T04:23:18.5379677Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5379780Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5379784Z 
2025-04-11T04:23:18.5379939Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5380151Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5380415Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T04:23:18.5380527Z     with torch.cuda.device(device):
2025-04-11T04:23:18.5380636Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5380640Z 
2025-04-11T04:23:18.5380760Z self = <torch.cuda.device object at 0x7fb5b8706890>
2025-04-11T04:23:18.5380763Z 
2025-04-11T04:23:18.5380845Z     def __enter__(self):
2025-04-11T04:23:18.5380989Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T04:23:18.5381095Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5381388Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5381531Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5381695Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5381699Z 
2025-04-11T04:23:18.5381952Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T04:23:18.5382176Z ___________________________ test_rms_layernorm[64-4] ___________________________
2025-04-11T04:23:18.5382180Z 
2025-04-11T04:23:18.5382263Z M = 4, N = 64
2025-04-11T04:23:18.5382266Z 
2025-04-11T04:23:18.5382374Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5382499Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5382598Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5382684Z         torch.manual_seed(123)
2025-04-11T04:23:18.5382781Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5382873Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5382878Z 
2025-04-11T04:23:18.5383030Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5383140Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5383144Z 
2025-04-11T04:23:18.5383227Z device = None
2025-04-11T04:23:18.5383231Z 
2025-04-11T04:23:18.5383354Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5383510Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5383586Z     
2025-04-11T04:23:18.5383661Z         Args:
2025-04-11T04:23:18.5383836Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5384006Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5384122Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5384196Z         """
2025-04-11T04:23:18.5384275Z         _lazy_init()
2025-04-11T04:23:18.5384378Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5384480Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5384592Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5384871Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5385014Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5385172Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5385176Z 
2025-04-11T04:23:18.5385416Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5385557Z ___________________________ test_rms_layernorm[64-8] ___________________________
2025-04-11T04:23:18.5385561Z 
2025-04-11T04:23:18.5385635Z M = 8, N = 64
2025-04-11T04:23:18.5385639Z 
2025-04-11T04:23:18.5385752Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5385868Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5385969Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5386227Z         torch.manual_seed(123)
2025-04-11T04:23:18.5386321Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5386419Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5386423Z 
2025-04-11T04:23:18.5386578Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5386696Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5386700Z 
2025-04-11T04:23:18.5386779Z device = None
2025-04-11T04:23:18.5386783Z 
2025-04-11T04:23:18.5386910Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5387061Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5387139Z     
2025-04-11T04:23:18.5387213Z         Args:
2025-04-11T04:23:18.5387383Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5387563Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5387675Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5387776Z         """
2025-04-11T04:23:18.5387857Z         _lazy_init()
2025-04-11T04:23:18.5387968Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5388170Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5388274Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5388599Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5388736Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5388899Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5388903Z 
2025-04-11T04:23:18.5389142Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5389288Z __________________________ test_rms_layernorm[64-16] ___________________________
2025-04-11T04:23:18.5389295Z 
2025-04-11T04:23:18.5389371Z M = 16, N = 64
2025-04-11T04:23:18.5389376Z 
2025-04-11T04:23:18.5389480Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5389604Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5389704Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5389795Z         torch.manual_seed(123)
2025-04-11T04:23:18.5389883Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5389993Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5389997Z 
2025-04-11T04:23:18.5390151Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5390263Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5390275Z 
2025-04-11T04:23:18.5390355Z device = None
2025-04-11T04:23:18.5390359Z 
2025-04-11T04:23:18.5390480Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5390640Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5390711Z     
2025-04-11T04:23:18.5390788Z         Args:
2025-04-11T04:23:18.5390959Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5391130Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5391243Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5391317Z         """
2025-04-11T04:23:18.5391406Z         _lazy_init()
2025-04-11T04:23:18.5391500Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5391606Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5391710Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5391991Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5392131Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5392380Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5392385Z 
2025-04-11T04:23:18.5392630Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5392773Z __________________________ test_rms_layernorm[128-2] ___________________________
2025-04-11T04:23:18.5392777Z 
2025-04-11T04:23:18.5392856Z M = 2, N = 128
2025-04-11T04:23:18.5392860Z 
2025-04-11T04:23:18.5392969Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5393111Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5393244Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5393336Z         torch.manual_seed(123)
2025-04-11T04:23:18.5393438Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5393528Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5393532Z 
2025-04-11T04:23:18.5393685Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5393807Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5393811Z 
2025-04-11T04:23:18.5393894Z device = None
2025-04-11T04:23:18.5393898Z 
2025-04-11T04:23:18.5394016Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5394261Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5394339Z     
2025-04-11T04:23:18.5394413Z         Args:
2025-04-11T04:23:18.5394585Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5394751Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5394862Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5394935Z         """
2025-04-11T04:23:18.5395013Z         _lazy_init()
2025-04-11T04:23:18.5395115Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5395216Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5395327Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5395606Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5395748Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5395905Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5395909Z 
2025-04-11T04:23:18.5396145Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5396289Z __________________________ test_rms_layernorm[128-4] ___________________________
2025-04-11T04:23:18.5396293Z 
2025-04-11T04:23:18.5396368Z M = 4, N = 128
2025-04-11T04:23:18.5396372Z 
2025-04-11T04:23:18.5396481Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5396598Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5396700Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5396790Z         torch.manual_seed(123)
2025-04-11T04:23:18.5396877Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5396975Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5396979Z 
2025-04-11T04:23:18.5397130Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5397245Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5397248Z 
2025-04-11T04:23:18.5397326Z device = None
2025-04-11T04:23:18.5397330Z 
2025-04-11T04:23:18.5397450Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5397596Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5397670Z     
2025-04-11T04:23:18.5397742Z         Args:
2025-04-11T04:23:18.5397907Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5398078Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5398264Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5398344Z         """
2025-04-11T04:23:18.5398424Z         _lazy_init()
2025-04-11T04:23:18.5398523Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5398634Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5398737Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5399023Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5399160Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5399324Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5399328Z 
2025-04-11T04:23:18.5399565Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5399708Z __________________________ test_rms_layernorm[128-8] ___________________________
2025-04-11T04:23:18.5399715Z 
2025-04-11T04:23:18.5399790Z M = 8, N = 128
2025-04-11T04:23:18.5399794Z 
2025-04-11T04:23:18.5399899Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5400024Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5400206Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5400303Z         torch.manual_seed(123)
2025-04-11T04:23:18.5400397Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5400500Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5400505Z 
2025-04-11T04:23:18.5400653Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5400762Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5400766Z 
2025-04-11T04:23:18.5400850Z device = None
2025-04-11T04:23:18.5400854Z 
2025-04-11T04:23:18.5400972Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5401129Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5401200Z     
2025-04-11T04:23:18.5401279Z         Args:
2025-04-11T04:23:18.5401444Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5401616Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5401726Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5401800Z         """
2025-04-11T04:23:18.5401887Z         _lazy_init()
2025-04-11T04:23:18.5401983Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5402090Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5402197Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5402478Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5402617Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5402776Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5402780Z 
2025-04-11T04:23:18.5403025Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5403168Z __________________________ test_rms_layernorm[128-16] __________________________
2025-04-11T04:23:18.5403172Z 
2025-04-11T04:23:18.5403251Z M = 16, N = 128
2025-04-11T04:23:18.5403255Z 
2025-04-11T04:23:18.5403360Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5403480Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5403577Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5403664Z         torch.manual_seed(123)
2025-04-11T04:23:18.5403758Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5403849Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5403853Z 
2025-04-11T04:23:18.5404008Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5404219Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5404224Z 
2025-04-11T04:23:18.5404309Z device = None
2025-04-11T04:23:18.5404313Z 
2025-04-11T04:23:18.5404432Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5404585Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5404663Z     
2025-04-11T04:23:18.5404735Z         Args:
2025-04-11T04:23:18.5404904Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5405071Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5405179Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5405253Z         """
2025-04-11T04:23:18.5405332Z         _lazy_init()
2025-04-11T04:23:18.5405430Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5405527Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5405638Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5405922Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5406137Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5406299Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5406303Z 
2025-04-11T04:23:18.5406539Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5406679Z __________________________ test_rms_layernorm[512-2] ___________________________
2025-04-11T04:23:18.5406683Z 
2025-04-11T04:23:18.5406759Z M = 2, N = 512
2025-04-11T04:23:18.5406763Z 
2025-04-11T04:23:18.5406874Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5406989Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5407092Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5407180Z         torch.manual_seed(123)
2025-04-11T04:23:18.5407266Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5407359Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5407363Z 
2025-04-11T04:23:18.5407514Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5407629Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5407632Z 
2025-04-11T04:23:18.5407711Z device = None
2025-04-11T04:23:18.5407715Z 
2025-04-11T04:23:18.5407837Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5407985Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5408058Z     
2025-04-11T04:23:18.5408137Z         Args:
2025-04-11T04:23:18.5408302Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5408473Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5408582Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5408664Z         """
2025-04-11T04:23:18.5408741Z         _lazy_init()
2025-04-11T04:23:18.5408834Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5408943Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5409048Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5409332Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5409468Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5409631Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5409635Z 
2025-04-11T04:23:18.5409869Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5410004Z __________________________ test_rms_layernorm[512-4] ___________________________
2025-04-11T04:23:18.5410096Z 
2025-04-11T04:23:18.5410176Z M = 4, N = 512
2025-04-11T04:23:18.5410180Z 
2025-04-11T04:23:18.5410285Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5410410Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5410512Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5410604Z         torch.manual_seed(123)
2025-04-11T04:23:18.5410690Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5410780Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5410788Z 
2025-04-11T04:23:18.5410935Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5411042Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5411046Z 
2025-04-11T04:23:18.5411127Z device = None
2025-04-11T04:23:18.5411131Z 
2025-04-11T04:23:18.5411247Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5411404Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5411475Z     
2025-04-11T04:23:18.5411551Z         Args:
2025-04-11T04:23:18.5411717Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5411970Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5412086Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5412161Z         """
2025-04-11T04:23:18.5412246Z         _lazy_init()
2025-04-11T04:23:18.5412340Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5412440Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5412554Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5412841Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5412978Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5413146Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5413150Z 
2025-04-11T04:23:18.5413390Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5413528Z __________________________ test_rms_layernorm[512-8] ___________________________
2025-04-11T04:23:18.5413532Z 
2025-04-11T04:23:18.5413611Z M = 8, N = 512
2025-04-11T04:23:18.5413615Z 
2025-04-11T04:23:18.5413720Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5413834Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5413941Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5414027Z         torch.manual_seed(123)
2025-04-11T04:23:18.5414122Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5414214Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5414218Z 
2025-04-11T04:23:18.5414369Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5414480Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5414484Z 
2025-04-11T04:23:18.5414563Z device = None
2025-04-11T04:23:18.5414572Z 
2025-04-11T04:23:18.5414689Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5414840Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5414918Z     
2025-04-11T04:23:18.5414991Z         Args:
2025-04-11T04:23:18.5415162Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5415327Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5415432Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5415511Z         """
2025-04-11T04:23:18.5415589Z         _lazy_init()
2025-04-11T04:23:18.5415687Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5415785Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5415979Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5416262Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5416400Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5416566Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5416570Z 
2025-04-11T04:23:18.5416804Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5416944Z __________________________ test_rms_layernorm[512-16] __________________________
2025-04-11T04:23:18.5416948Z 
2025-04-11T04:23:18.5417023Z M = 16, N = 512
2025-04-11T04:23:18.5417027Z 
2025-04-11T04:23:18.5417136Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5417250Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5417352Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5417440Z         torch.manual_seed(123)
2025-04-11T04:23:18.5417527Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5417622Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5417705Z 
2025-04-11T04:23:18.5417859Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5417971Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5417975Z 
2025-04-11T04:23:18.5418052Z device = None
2025-04-11T04:23:18.5418056Z 
2025-04-11T04:23:18.5418176Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5418323Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5418393Z     
2025-04-11T04:23:18.5418471Z         Args:
2025-04-11T04:23:18.5418635Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5418801Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5418907Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5418985Z         """
2025-04-11T04:23:18.5419063Z         _lazy_init()
2025-04-11T04:23:18.5419156Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5419263Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5419364Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5419652Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5419786Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5419943Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5419953Z 
2025-04-11T04:23:18.5420187Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5420324Z __________________________ test_rms_layernorm[5120-2] __________________________
2025-04-11T04:23:18.5420331Z 
2025-04-11T04:23:18.5420415Z M = 2, N = 5120
2025-04-11T04:23:18.5420418Z 
2025-04-11T04:23:18.5420522Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5420647Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5420743Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5420834Z         torch.manual_seed(123)
2025-04-11T04:23:18.5420922Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5421012Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5421016Z 
2025-04-11T04:23:18.5421171Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5421280Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5421284Z 
2025-04-11T04:23:18.5421364Z device = None
2025-04-11T04:23:18.5421368Z 
2025-04-11T04:23:18.5421484Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5421722Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5421796Z     
2025-04-11T04:23:18.5421870Z         Args:
2025-04-11T04:23:18.5422041Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5422208Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5422374Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5422461Z         """
2025-04-11T04:23:18.5422558Z         _lazy_init()
2025-04-11T04:23:18.5422653Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5422751Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5422862Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5423143Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5423285Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5423448Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5423452Z 
2025-04-11T04:23:18.5423693Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5423954Z __________________________ test_rms_layernorm[5120-4] __________________________
2025-04-11T04:23:18.5423957Z 
2025-04-11T04:23:18.5424040Z M = 4, N = 5120
2025-04-11T04:23:18.5424044Z 
2025-04-11T04:23:18.5424148Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5424265Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5424370Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5424456Z         torch.manual_seed(123)
2025-04-11T04:23:18.5424553Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5424644Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5424648Z 
2025-04-11T04:23:18.5424802Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5424913Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5424917Z 
2025-04-11T04:23:18.5424996Z device = None
2025-04-11T04:23:18.5425000Z 
2025-04-11T04:23:18.5425124Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5425276Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5425357Z     
2025-04-11T04:23:18.5425430Z         Args:
2025-04-11T04:23:18.5425603Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5425766Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5425870Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5425953Z         """
2025-04-11T04:23:18.5426032Z         _lazy_init()
2025-04-11T04:23:18.5426129Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5426228Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5426335Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5426621Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5426759Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5426917Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5426921Z 
2025-04-11T04:23:18.5427156Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5427295Z __________________________ test_rms_layernorm[5120-8] __________________________
2025-04-11T04:23:18.5427298Z 
2025-04-11T04:23:18.5427373Z M = 8, N = 5120
2025-04-11T04:23:18.5427377Z 
2025-04-11T04:23:18.5427487Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5427604Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5427700Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5427880Z         torch.manual_seed(123)
2025-04-11T04:23:18.5427971Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5428065Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5428075Z 
2025-04-11T04:23:18.5428223Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5428335Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5428338Z 
2025-04-11T04:23:18.5428441Z device = None
2025-04-11T04:23:18.5428445Z 
2025-04-11T04:23:18.5428572Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5428721Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5428793Z     
2025-04-11T04:23:18.5428872Z         Args:
2025-04-11T04:23:18.5429038Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5429208Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5429314Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5429387Z         """
2025-04-11T04:23:18.5429469Z         _lazy_init()
2025-04-11T04:23:18.5429562Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5429761Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5429866Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5430148Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5430280Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5430435Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5430445Z 
2025-04-11T04:23:18.5430680Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5430820Z _________________________ test_rms_layernorm[5120-16] __________________________
2025-04-11T04:23:18.5430824Z 
2025-04-11T04:23:18.5430906Z M = 16, N = 5120
2025-04-11T04:23:18.5430910Z 
2025-04-11T04:23:18.5431015Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.5431140Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T04:23:18.5431238Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T04:23:18.5431328Z         torch.manual_seed(123)
2025-04-11T04:23:18.5431417Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5431505Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5431509Z 
2025-04-11T04:23:18.5431662Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T04:23:18.5431769Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5431773Z 
2025-04-11T04:23:18.5431855Z device = None
2025-04-11T04:23:18.5431858Z 
2025-04-11T04:23:18.5431975Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5432133Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5432203Z     
2025-04-11T04:23:18.5432277Z         Args:
2025-04-11T04:23:18.5432449Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5432618Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5432726Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5432801Z         """
2025-04-11T04:23:18.5432885Z         _lazy_init()
2025-04-11T04:23:18.5432978Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5433076Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5433183Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5433465Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5433603Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5433853Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5433858Z 
2025-04-11T04:23:18.5434099Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5434250Z ____________________ test_rotary_emb[dtype0-64-16-32-64-4] _____________________
2025-04-11T04:23:18.5434254Z 
2025-04-11T04:23:18.5434415Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float16
2025-04-11T04:23:18.5434425Z 
2025-04-11T04:23:18.5434536Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5434641Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.5434753Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.5434856Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T04:23:18.5434961Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.5435127Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5435265Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T04:23:18.5435360Z         torch.manual_seed(10)
2025-04-11T04:23:18.5435455Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.5435653Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.5435785Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5435917Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5435991Z     
2025-04-11T04:23:18.5436160Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5436236Z     
2025-04-11T04:23:18.5436334Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5436409Z     
2025-04-11T04:23:18.5436502Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.5436622Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.5436730Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5436830Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5436920Z         cos_2 = cos[:, : D // 2]
2025-04-11T04:23:18.5437001Z         sin_2 = sin[:, : D // 2]
2025-04-11T04:23:18.5437123Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5437256Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.5437469Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.5437602Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5437673Z     
2025-04-11T04:23:18.5437758Z         # create data
2025-04-11T04:23:18.5437840Z         block_size = 32
2025-04-11T04:23:18.5438009Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T04:23:18.5438101Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5438246Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.5438354Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5438642Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5438789Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5438949Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5438953Z 
2025-04-11T04:23:18.5439154Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T04:23:18.5439300Z ____________________ test_rotary_emb[dtype0-64-32-32-64-4] _____________________
2025-04-11T04:23:18.5439304Z 
2025-04-11T04:23:18.5439452Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float16
2025-04-11T04:23:18.5439462Z 
2025-04-11T04:23:18.5439569Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5439673Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.5439774Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.5439956Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T04:23:18.5440062Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.5440226Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5440365Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T04:23:18.5440459Z         torch.manual_seed(10)
2025-04-11T04:23:18.5440553Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.5440660Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.5440785Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5440914Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5440988Z     
2025-04-11T04:23:18.5441155Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5441230Z     
2025-04-11T04:23:18.5441325Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5441401Z     
2025-04-11T04:23:18.5441498Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.5441617Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.5441721Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5441911Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5442002Z         cos_2 = cos[:, : D // 2]
2025-04-11T04:23:18.5442085Z         sin_2 = sin[:, : D // 2]
2025-04-11T04:23:18.5442208Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5442339Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.5442549Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.5442676Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5442747Z     
2025-04-11T04:23:18.5442834Z         # create data
2025-04-11T04:23:18.5442913Z         block_size = 32
2025-04-11T04:23:18.5443084Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T04:23:18.5443179Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5443316Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.5443431Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5443714Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5443853Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5444013Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5444017Z 
2025-04-11T04:23:18.5444217Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T04:23:18.5444361Z ____________________ test_rotary_emb[dtype1-64-16-32-64-4] _____________________
2025-04-11T04:23:18.5444365Z 
2025-04-11T04:23:18.5444511Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float32
2025-04-11T04:23:18.5444525Z 
2025-04-11T04:23:18.5444633Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5444738Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.5444839Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.5444944Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T04:23:18.5445046Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.5445207Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5445342Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T04:23:18.5445437Z         torch.manual_seed(10)
2025-04-11T04:23:18.5445532Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.5445638Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.5445766Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5445907Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5445982Z     
2025-04-11T04:23:18.5446229Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5446333Z     
2025-04-11T04:23:18.5446429Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5446511Z     
2025-04-11T04:23:18.5446604Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.5446721Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.5446825Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5446920Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5447008Z         cos_2 = cos[:, : D // 2]
2025-04-11T04:23:18.5447089Z         sin_2 = sin[:, : D // 2]
2025-04-11T04:23:18.5447210Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5447337Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.5447543Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.5447673Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5447746Z     
2025-04-11T04:23:18.5447834Z         # create data
2025-04-11T04:23:18.5447915Z         block_size = 32
2025-04-11T04:23:18.5448079Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T04:23:18.5448279Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5448413Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.5448518Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5448802Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5448940Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5449097Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5449101Z 
2025-04-11T04:23:18.5449299Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T04:23:18.5449448Z ____________________ test_rotary_emb[dtype1-64-32-32-64-4] _____________________
2025-04-11T04:23:18.5449451Z 
2025-04-11T04:23:18.5449599Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float32
2025-04-11T04:23:18.5449612Z 
2025-04-11T04:23:18.5449718Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.5449819Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.5449918Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.5450019Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T04:23:18.5450118Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.5450276Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T04:23:18.5450410Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T04:23:18.5450502Z         torch.manual_seed(10)
2025-04-11T04:23:18.5450595Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.5450706Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.5450828Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5450955Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5451031Z     
2025-04-11T04:23:18.5451196Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5451276Z     
2025-04-11T04:23:18.5451367Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5451442Z     
2025-04-11T04:23:18.5451535Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.5451649Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.5451752Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5451847Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.5451935Z         cos_2 = cos[:, : D // 2]
2025-04-11T04:23:18.5452015Z         sin_2 = sin[:, : D // 2]
2025-04-11T04:23:18.5452133Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5452347Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.5452556Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.5452685Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5452756Z     
2025-04-11T04:23:18.5452839Z         # create data
2025-04-11T04:23:18.5452920Z         block_size = 32
2025-04-11T04:23:18.5453081Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T04:23:18.5453181Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.5453316Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.5453425Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5453707Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5453844Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5454004Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5454008Z 
2025-04-11T04:23:18.5454204Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T04:23:18.5454433Z _____________________ test_silu_and_mul[dtype0-11008-64-2] _____________________
2025-04-11T04:23:18.5454437Z 
2025-04-11T04:23:18.5454579Z SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float32
2025-04-11T04:23:18.5454590Z 
2025-04-11T04:23:18.5454696Z     @pytest.mark.parametrize("SHAPE_X", [2])
2025-04-11T04:23:18.5454798Z     @pytest.mark.parametrize("SHAPE_Y", [64])
2025-04-11T04:23:18.5454910Z     @pytest.mark.parametrize("SHAPE_Z", [11008])
2025-04-11T04:23:18.5455069Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
2025-04-11T04:23:18.5455208Z     def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
2025-04-11T04:23:18.5455295Z         torch.manual_seed(5)
2025-04-11T04:23:18.5455387Z         device = get_current_device()
2025-04-11T04:23:18.5455572Z >       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
2025-04-11T04:23:18.5455674Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5455957Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5456088Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5456249Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5456253Z 
2025-04-11T04:23:18.5456423Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
2025-04-11T04:23:18.5456577Z _____________________ test_silu_and_mul[dtype1-11008-64-2] _____________________
2025-04-11T04:23:18.5456581Z 
2025-04-11T04:23:18.5456718Z SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float16
2025-04-11T04:23:18.5456722Z 
2025-04-11T04:23:18.5456829Z     @pytest.mark.parametrize("SHAPE_X", [2])
2025-04-11T04:23:18.5456938Z     @pytest.mark.parametrize("SHAPE_Y", [64])
2025-04-11T04:23:18.5457046Z     @pytest.mark.parametrize("SHAPE_Z", [11008])
2025-04-11T04:23:18.5457213Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
2025-04-11T04:23:18.5457352Z     def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
2025-04-11T04:23:18.5457440Z         torch.manual_seed(5)
2025-04-11T04:23:18.5457531Z         device = get_current_device()
2025-04-11T04:23:18.5457715Z >       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
2025-04-11T04:23:18.5457827Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5458115Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5458258Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5458503Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5458508Z 
2025-04-11T04:23:18.5458681Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
2025-04-11T04:23:18.5458854Z _____________ test_context_attention[True-False-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.5458861Z 
2025-04-11T04:23:18.5459019Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5459170Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5459258Z use_new_kcache_layout = True
2025-04-11T04:23:18.5459262Z 
2025-04-11T04:23:18.5459472Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5459575Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5459697Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5459839Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5459964Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5460082Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5460221Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5460364Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5460604Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5460701Z     def test_context_attention(
2025-04-11T04:23:18.5460778Z         bsz: int,
2025-04-11T04:23:18.5460865Z         block_size: int,
2025-04-11T04:23:18.5460956Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5461041Z         num_attn_heads: int,
2025-04-11T04:23:18.5461128Z         kv_group_num: int,
2025-04-11T04:23:18.5461215Z         same_context_len: bool,
2025-04-11T04:23:18.5461306Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5461396Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5461470Z     ):
2025-04-11T04:23:18.5461591Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5461791Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5461984Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5462163Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5462331Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5462408Z             return
2025-04-11T04:23:18.5462481Z     
2025-04-11T04:23:18.5462571Z         torch.manual_seed(123)
2025-04-11T04:23:18.5462671Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5462765Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5462858Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5462861Z 
2025-04-11T04:23:18.5463026Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5463147Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5463396Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T04:23:18.5463496Z     with torch.cuda.device(device):
2025-04-11T04:23:18.5463611Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5463615Z 
2025-04-11T04:23:18.5463737Z self = <torch.cuda.device object at 0x7fb5b84e9bd0>
2025-04-11T04:23:18.5463741Z 
2025-04-11T04:23:18.5463824Z     def __enter__(self):
2025-04-11T04:23:18.5463959Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T04:23:18.5464062Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5464341Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5464477Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5464724Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5464729Z 
2025-04-11T04:23:18.5464972Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T04:23:18.5465144Z _____________ test_context_attention[True-False-True-1-16-8-16-32] _____________
2025-04-11T04:23:18.5465148Z 
2025-04-11T04:23:18.5465305Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5465452Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5465545Z use_new_kcache_layout = True
2025-04-11T04:23:18.5465549Z 
2025-04-11T04:23:18.5465747Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5465851Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5465973Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5466111Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5466233Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5466347Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5466490Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5466708Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5466862Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5466958Z     def test_context_attention(
2025-04-11T04:23:18.5467038Z         bsz: int,
2025-04-11T04:23:18.5467125Z         block_size: int,
2025-04-11T04:23:18.5467215Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5467298Z         num_attn_heads: int,
2025-04-11T04:23:18.5467386Z         kv_group_num: int,
2025-04-11T04:23:18.5467478Z         same_context_len: bool,
2025-04-11T04:23:18.5467568Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5467663Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5467741Z     ):
2025-04-11T04:23:18.5467858Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5468057Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5468246Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5468482Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5468661Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5468738Z             return
2025-04-11T04:23:18.5468818Z     
2025-04-11T04:23:18.5468907Z         torch.manual_seed(123)
2025-04-11T04:23:18.5469005Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5469100Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5469193Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5469198Z 
2025-04-11T04:23:18.5469375Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5469489Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5469493Z 
2025-04-11T04:23:18.5469578Z device = None
2025-04-11T04:23:18.5469582Z 
2025-04-11T04:23:18.5469702Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5469861Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5469940Z     
2025-04-11T04:23:18.5470017Z         Args:
2025-04-11T04:23:18.5470194Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5470364Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5470479Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5470554Z         """
2025-04-11T04:23:18.5470631Z         _lazy_init()
2025-04-11T04:23:18.5470733Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5470834Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5471060Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5471350Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5471493Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5471657Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5471661Z 
2025-04-11T04:23:18.5471897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5472068Z _____________ test_context_attention[True-False-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.5472072Z 
2025-04-11T04:23:18.5472223Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5472380Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5472469Z use_new_kcache_layout = True
2025-04-11T04:23:18.5472472Z 
2025-04-11T04:23:18.5472679Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5472785Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5472904Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5473145Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5473266Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5473386Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5473522Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5473662Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5473815Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5473903Z     def test_context_attention(
2025-04-11T04:23:18.5473985Z         bsz: int,
2025-04-11T04:23:18.5474068Z         block_size: int,
2025-04-11T04:23:18.5474166Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5474255Z         num_attn_heads: int,
2025-04-11T04:23:18.5474337Z         kv_group_num: int,
2025-04-11T04:23:18.5474426Z         same_context_len: bool,
2025-04-11T04:23:18.5474511Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5474611Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5474684Z     ):
2025-04-11T04:23:18.5474804Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5474995Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5475177Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5475358Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5475523Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5475605Z             return
2025-04-11T04:23:18.5475685Z     
2025-04-11T04:23:18.5475781Z         torch.manual_seed(123)
2025-04-11T04:23:18.5475889Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5475980Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5476079Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5476086Z 
2025-04-11T04:23:18.5476262Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5476377Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5476381Z 
2025-04-11T04:23:18.5476461Z device = None
2025-04-11T04:23:18.5476465Z 
2025-04-11T04:23:18.5476587Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5476739Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5476811Z     
2025-04-11T04:23:18.5476890Z         Args:
2025-04-11T04:23:18.5477057Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5477317Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5477426Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5477507Z         """
2025-04-11T04:23:18.5477586Z         _lazy_init()
2025-04-11T04:23:18.5477685Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5477792Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5477897Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5478185Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5478325Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5478490Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5478494Z 
2025-04-11T04:23:18.5478739Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5478911Z _____________ test_context_attention[True-False-True-1-16-8-32-32] _____________
2025-04-11T04:23:18.5478915Z 
2025-04-11T04:23:18.5479074Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5479309Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5479409Z use_new_kcache_layout = True
2025-04-11T04:23:18.5479413Z 
2025-04-11T04:23:18.5479615Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5479724Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5479842Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5479984Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5480100Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5480215Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5480358Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5480499Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5480655Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5480744Z     def test_context_attention(
2025-04-11T04:23:18.5480824Z         bsz: int,
2025-04-11T04:23:18.5480913Z         block_size: int,
2025-04-11T04:23:18.5481001Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5481091Z         num_attn_heads: int,
2025-04-11T04:23:18.5481174Z         kv_group_num: int,
2025-04-11T04:23:18.5481263Z         same_context_len: bool,
2025-04-11T04:23:18.5481346Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5481433Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5481515Z     ):
2025-04-11T04:23:18.5481626Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5481824Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5482010Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5482183Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5482352Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5482432Z             return
2025-04-11T04:23:18.5482508Z     
2025-04-11T04:23:18.5482595Z         torch.manual_seed(123)
2025-04-11T04:23:18.5482700Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5482788Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5482876Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5482880Z 
2025-04-11T04:23:18.5483051Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5483162Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5483166Z 
2025-04-11T04:23:18.5483246Z device = None
2025-04-11T04:23:18.5483249Z 
2025-04-11T04:23:18.5483566Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5483727Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5483802Z     
2025-04-11T04:23:18.5483876Z         Args:
2025-04-11T04:23:18.5484055Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5484221Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5484331Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5484406Z         """
2025-04-11T04:23:18.5484490Z         _lazy_init()
2025-04-11T04:23:18.5484583Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5484685Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5484795Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5485080Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5485228Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5485390Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5485479Z 
2025-04-11T04:23:18.5485735Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5485912Z _____________ test_context_attention[True-False-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.5485916Z 
2025-04-11T04:23:18.5486070Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5486227Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5486316Z use_new_kcache_layout = True
2025-04-11T04:23:18.5486319Z 
2025-04-11T04:23:18.5486522Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5486626Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5486750Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5486891Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5487014Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5487129Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5487264Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5487404Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5487555Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5487647Z     def test_context_attention(
2025-04-11T04:23:18.5487724Z         bsz: int,
2025-04-11T04:23:18.5487808Z         block_size: int,
2025-04-11T04:23:18.5487901Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5487981Z         num_attn_heads: int,
2025-04-11T04:23:18.5488069Z         kv_group_num: int,
2025-04-11T04:23:18.5488151Z         same_context_len: bool,
2025-04-11T04:23:18.5488243Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5488331Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5488405Z     ):
2025-04-11T04:23:18.5488517Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5488712Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5488901Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5489074Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5489249Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5489325Z             return
2025-04-11T04:23:18.5489398Z     
2025-04-11T04:23:18.5489490Z         torch.manual_seed(123)
2025-04-11T04:23:18.5489588Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5489679Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5489768Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5489858Z 
2025-04-11T04:23:18.5490029Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5490144Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5490152Z 
2025-04-11T04:23:18.5490230Z device = None
2025-04-11T04:23:18.5490234Z 
2025-04-11T04:23:18.5490355Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5490505Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5490582Z     
2025-04-11T04:23:18.5490660Z         Args:
2025-04-11T04:23:18.5490831Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5491007Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5491118Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5491199Z         """
2025-04-11T04:23:18.5491282Z         _lazy_init()
2025-04-11T04:23:18.5491389Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5491497Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5491607Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5491978Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5492117Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5492283Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5492287Z 
2025-04-11T04:23:18.5492523Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5492699Z ____________ test_context_attention[True-False-True-1-16-16-16-32] _____________
2025-04-11T04:23:18.5492703Z 
2025-04-11T04:23:18.5492856Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5493011Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5493098Z use_new_kcache_layout = True
2025-04-11T04:23:18.5493102Z 
2025-04-11T04:23:18.5493303Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5493416Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5493531Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5493675Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5493790Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5493908Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5494043Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5494178Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5494337Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5494426Z     def test_context_attention(
2025-04-11T04:23:18.5494512Z         bsz: int,
2025-04-11T04:23:18.5494595Z         block_size: int,
2025-04-11T04:23:18.5494685Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5494773Z         num_attn_heads: int,
2025-04-11T04:23:18.5494858Z         kv_group_num: int,
2025-04-11T04:23:18.5494961Z         same_context_len: bool,
2025-04-11T04:23:18.5495044Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5495142Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5495215Z     ):
2025-04-11T04:23:18.5495325Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5495531Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5495714Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5495894Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5496160Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5496246Z             return
2025-04-11T04:23:18.5496321Z     
2025-04-11T04:23:18.5496407Z         torch.manual_seed(123)
2025-04-11T04:23:18.5496518Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5496607Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5496699Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5496703Z 
2025-04-11T04:23:18.5496870Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5496987Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5496991Z 
2025-04-11T04:23:18.5497068Z device = None
2025-04-11T04:23:18.5497072Z 
2025-04-11T04:23:18.5497188Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5497346Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5497419Z     
2025-04-11T04:23:18.5497496Z         Args:
2025-04-11T04:23:18.5497669Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5497843Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5498039Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5498112Z         """
2025-04-11T04:23:18.5498198Z         _lazy_init()
2025-04-11T04:23:18.5498291Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5498396Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5498500Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5498780Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5498921Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5499079Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5499083Z 
2025-04-11T04:23:18.5499330Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5499499Z _____________ test_context_attention[True-False-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.5499506Z 
2025-04-11T04:23:18.5499662Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5499805Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5499896Z use_new_kcache_layout = True
2025-04-11T04:23:18.5499899Z 
2025-04-11T04:23:18.5500098Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5500200Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5500320Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5500457Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5500575Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5500688Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5500827Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5500960Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5501112Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5501201Z     def test_context_attention(
2025-04-11T04:23:18.5501275Z         bsz: int,
2025-04-11T04:23:18.5501358Z         block_size: int,
2025-04-11T04:23:18.5501444Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5501530Z         num_attn_heads: int,
2025-04-11T04:23:18.5501611Z         kv_group_num: int,
2025-04-11T04:23:18.5501693Z         same_context_len: bool,
2025-04-11T04:23:18.5501780Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5501865Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5501944Z     ):
2025-04-11T04:23:18.5502052Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5502326Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5502513Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5502689Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5502858Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5502935Z             return
2025-04-11T04:23:18.5503009Z     
2025-04-11T04:23:18.5503096Z         torch.manual_seed(123)
2025-04-11T04:23:18.5503192Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5503284Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5503371Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5503375Z 
2025-04-11T04:23:18.5503542Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5503652Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5503660Z 
2025-04-11T04:23:18.5503739Z device = None
2025-04-11T04:23:18.5503743Z 
2025-04-11T04:23:18.5503858Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5504090Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5504167Z     
2025-04-11T04:23:18.5504240Z         Args:
2025-04-11T04:23:18.5504408Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5504572Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5504680Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5504769Z         """
2025-04-11T04:23:18.5504846Z         _lazy_init()
2025-04-11T04:23:18.5504956Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5505055Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5505169Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5505462Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5505600Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5505760Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5505764Z 
2025-04-11T04:23:18.5506005Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5506178Z ____________ test_context_attention[True-False-True-1-16-16-32-32] _____________
2025-04-11T04:23:18.5506181Z 
2025-04-11T04:23:18.5506333Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5506481Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5506567Z use_new_kcache_layout = True
2025-04-11T04:23:18.5506571Z 
2025-04-11T04:23:18.5506775Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5506879Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5506998Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5507136Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5507252Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5507366Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5507501Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5507638Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5507789Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5507875Z     def test_context_attention(
2025-04-11T04:23:18.5507955Z         bsz: int,
2025-04-11T04:23:18.5508035Z         block_size: int,
2025-04-11T04:23:18.5508127Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5508207Z         num_attn_heads: int,
2025-04-11T04:23:18.5508371Z         kv_group_num: int,
2025-04-11T04:23:18.5508490Z         same_context_len: bool,
2025-04-11T04:23:18.5508576Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5508667Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5508743Z     ):
2025-04-11T04:23:18.5508856Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5509048Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5509227Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5509398Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5509561Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5509641Z             return
2025-04-11T04:23:18.5509713Z     
2025-04-11T04:23:18.5509802Z         torch.manual_seed(123)
2025-04-11T04:23:18.5509902Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5509990Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5510084Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5510088Z 
2025-04-11T04:23:18.5510359Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5510476Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5510480Z 
2025-04-11T04:23:18.5510559Z device = None
2025-04-11T04:23:18.5510563Z 
2025-04-11T04:23:18.5510685Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5510831Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5510902Z     
2025-04-11T04:23:18.5510982Z         Args:
2025-04-11T04:23:18.5511151Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5511321Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5511431Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5511511Z         """
2025-04-11T04:23:18.5511588Z         _lazy_init()
2025-04-11T04:23:18.5511682Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5511791Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5511894Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5512178Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5512313Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5512474Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5512478Z 
2025-04-11T04:23:18.5512714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5512891Z _____________ test_context_attention[True-False-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.5512904Z 
2025-04-11T04:23:18.5513062Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5513207Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5513299Z use_new_kcache_layout = True
2025-04-11T04:23:18.5513303Z 
2025-04-11T04:23:18.5513503Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5513610Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5513725Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5513865Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5513980Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5514092Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5514236Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5514371Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5514613Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5514703Z     def test_context_attention(
2025-04-11T04:23:18.5514779Z         bsz: int,
2025-04-11T04:23:18.5514867Z         block_size: int,
2025-04-11T04:23:18.5514959Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5515044Z         num_attn_heads: int,
2025-04-11T04:23:18.5515126Z         kv_group_num: int,
2025-04-11T04:23:18.5515214Z         same_context_len: bool,
2025-04-11T04:23:18.5515296Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5515385Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5515461Z     ):
2025-04-11T04:23:18.5515571Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5515764Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5515944Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5516122Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5516285Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5516463Z             return
2025-04-11T04:23:18.5516542Z     
2025-04-11T04:23:18.5516627Z         torch.manual_seed(123)
2025-04-11T04:23:18.5516728Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5516813Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5516901Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5516905Z 
2025-04-11T04:23:18.5517077Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5517189Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5517193Z 
2025-04-11T04:23:18.5517274Z device = None
2025-04-11T04:23:18.5517277Z 
2025-04-11T04:23:18.5517393Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5517548Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5517620Z     
2025-04-11T04:23:18.5517693Z         Args:
2025-04-11T04:23:18.5517861Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5518029Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5518136Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5518207Z         """
2025-04-11T04:23:18.5518287Z         _lazy_init()
2025-04-11T04:23:18.5518379Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5518478Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5518585Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5518867Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5519009Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5519170Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5519173Z 
2025-04-11T04:23:18.5519421Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5519590Z _____________ test_context_attention[True-False-True-4-16-8-16-32] _____________
2025-04-11T04:23:18.5519594Z 
2025-04-11T04:23:18.5519750Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5519896Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5519981Z use_new_kcache_layout = True
2025-04-11T04:23:18.5519985Z 
2025-04-11T04:23:18.5520193Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5520296Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5520414Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5520636Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5520757Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5520869Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5521010Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5521147Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5521299Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5521388Z     def test_context_attention(
2025-04-11T04:23:18.5521463Z         bsz: int,
2025-04-11T04:23:18.5521551Z         block_size: int,
2025-04-11T04:23:18.5521639Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5521719Z         num_attn_heads: int,
2025-04-11T04:23:18.5521807Z         kv_group_num: int,
2025-04-11T04:23:18.5521892Z         same_context_len: bool,
2025-04-11T04:23:18.5522020Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5522107Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5522182Z     ):
2025-04-11T04:23:18.5522296Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5522484Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5522763Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5522932Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5523107Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5523180Z             return
2025-04-11T04:23:18.5523250Z     
2025-04-11T04:23:18.5523347Z         torch.manual_seed(123)
2025-04-11T04:23:18.5523451Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5523540Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5523629Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5523633Z 
2025-04-11T04:23:18.5523804Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5523915Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5523919Z 
2025-04-11T04:23:18.5523999Z device = None
2025-04-11T04:23:18.5524004Z 
2025-04-11T04:23:18.5524122Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5524270Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5524341Z     
2025-04-11T04:23:18.5524413Z         Args:
2025-04-11T04:23:18.5524581Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5524746Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5524848Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5524923Z         """
2025-04-11T04:23:18.5524999Z         _lazy_init()
2025-04-11T04:23:18.5525096Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5525200Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5525303Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5525588Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5525729Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5525894Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5525898Z 
2025-04-11T04:23:18.5526137Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5526306Z _____________ test_context_attention[True-False-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.5526310Z 
2025-04-11T04:23:18.5526460Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5526610Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5526780Z use_new_kcache_layout = True
2025-04-11T04:23:18.5526785Z 
2025-04-11T04:23:18.5526985Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5527094Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5527209Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5527353Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5527467Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5527581Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5527715Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5527846Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5528000Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5528086Z     def test_context_attention(
2025-04-11T04:23:18.5528165Z         bsz: int,
2025-04-11T04:23:18.5528249Z         block_size: int,
2025-04-11T04:23:18.5528339Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5528419Z         num_attn_heads: int,
2025-04-11T04:23:18.5528500Z         kv_group_num: int,
2025-04-11T04:23:18.5528673Z         same_context_len: bool,
2025-04-11T04:23:18.5528755Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5528843Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5528915Z     ):
2025-04-11T04:23:18.5529022Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5529219Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5529404Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5529577Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5529741Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5529821Z             return
2025-04-11T04:23:18.5529891Z     
2025-04-11T04:23:18.5529975Z         torch.manual_seed(123)
2025-04-11T04:23:18.5530076Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5530166Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5530259Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5530262Z 
2025-04-11T04:23:18.5530428Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5530541Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5530545Z 
2025-04-11T04:23:18.5530621Z device = None
2025-04-11T04:23:18.5530624Z 
2025-04-11T04:23:18.5530738Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5530890Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5530958Z     
2025-04-11T04:23:18.5531033Z         Args:
2025-04-11T04:23:18.5531201Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5531370Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5531473Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5531548Z         """
2025-04-11T04:23:18.5531630Z         _lazy_init()
2025-04-11T04:23:18.5531722Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5531825Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5531929Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5532210Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5532355Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5532515Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5532519Z 
2025-04-11T04:23:18.5532862Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5533039Z _____________ test_context_attention[True-False-True-4-16-8-32-32] _____________
2025-04-11T04:23:18.5533042Z 
2025-04-11T04:23:18.5533211Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5533360Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5533450Z use_new_kcache_layout = True
2025-04-11T04:23:18.5533454Z 
2025-04-11T04:23:18.5533650Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5533755Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5533869Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5534007Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5534125Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5534235Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5534376Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5534509Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5534661Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5534840Z     def test_context_attention(
2025-04-11T04:23:18.5534917Z         bsz: int,
2025-04-11T04:23:18.5535000Z         block_size: int,
2025-04-11T04:23:18.5535088Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5535177Z         num_attn_heads: int,
2025-04-11T04:23:18.5535259Z         kv_group_num: int,
2025-04-11T04:23:18.5535342Z         same_context_len: bool,
2025-04-11T04:23:18.5535431Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5535516Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5535592Z     ):
2025-04-11T04:23:18.5535702Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5535896Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5536086Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5536255Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5536424Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5536497Z             return
2025-04-11T04:23:18.5536569Z     
2025-04-11T04:23:18.5536652Z         torch.manual_seed(123)
2025-04-11T04:23:18.5536747Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5536838Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5536926Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5536929Z 
2025-04-11T04:23:18.5537097Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5537207Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5537211Z 
2025-04-11T04:23:18.5537290Z device = None
2025-04-11T04:23:18.5537297Z 
2025-04-11T04:23:18.5537411Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5537559Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5537635Z     
2025-04-11T04:23:18.5537706Z         Args:
2025-04-11T04:23:18.5537872Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5538034Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5538139Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5538215Z         """
2025-04-11T04:23:18.5538292Z         _lazy_init()
2025-04-11T04:23:18.5538389Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5538488Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5538595Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5538957Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5539098Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5539257Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5539265Z 
2025-04-11T04:23:18.5539503Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5539671Z _____________ test_context_attention[True-False-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.5539675Z 
2025-04-11T04:23:18.5539825Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5539975Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5540060Z use_new_kcache_layout = True
2025-04-11T04:23:18.5540064Z 
2025-04-11T04:23:18.5540265Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5540369Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5540488Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5540625Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5540844Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5540959Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5541094Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5541232Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5541382Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5541472Z     def test_context_attention(
2025-04-11T04:23:18.5541549Z         bsz: int,
2025-04-11T04:23:18.5541628Z         block_size: int,
2025-04-11T04:23:18.5541719Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5541800Z         num_attn_heads: int,
2025-04-11T04:23:18.5541892Z         kv_group_num: int,
2025-04-11T04:23:18.5541985Z         same_context_len: bool,
2025-04-11T04:23:18.5542068Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5542167Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5542239Z     ):
2025-04-11T04:23:18.5542354Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5542545Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5542726Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5542900Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5543064Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5543142Z             return
2025-04-11T04:23:18.5543214Z     
2025-04-11T04:23:18.5543301Z         torch.manual_seed(123)
2025-04-11T04:23:18.5543399Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5543487Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5543583Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5543587Z 
2025-04-11T04:23:18.5543755Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5543875Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5543879Z 
2025-04-11T04:23:18.5543954Z device = None
2025-04-11T04:23:18.5543958Z 
2025-04-11T04:23:18.5544075Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5544226Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5544298Z     
2025-04-11T04:23:18.5544370Z         Args:
2025-04-11T04:23:18.5544536Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5544737Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5544841Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5545032Z         """
2025-04-11T04:23:18.5545113Z         _lazy_init()
2025-04-11T04:23:18.5545212Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5545316Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5545421Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5545703Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5545837Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5545997Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5546001Z 
2025-04-11T04:23:18.5546237Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5546409Z ____________ test_context_attention[True-False-True-4-16-16-16-32] _____________
2025-04-11T04:23:18.5546413Z 
2025-04-11T04:23:18.5546568Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5546712Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5546806Z use_new_kcache_layout = True
2025-04-11T04:23:18.5546891Z 
2025-04-11T04:23:18.5547102Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5547213Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5547326Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5547467Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5547582Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5547692Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5547831Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5547964Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5548120Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5548207Z     def test_context_attention(
2025-04-11T04:23:18.5548288Z         bsz: int,
2025-04-11T04:23:18.5548366Z         block_size: int,
2025-04-11T04:23:18.5548488Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5548576Z         num_attn_heads: int,
2025-04-11T04:23:18.5548657Z         kv_group_num: int,
2025-04-11T04:23:18.5548746Z         same_context_len: bool,
2025-04-11T04:23:18.5548830Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5548918Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5548995Z     ):
2025-04-11T04:23:18.5549105Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5549299Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5549480Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5549653Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5549814Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5549897Z             return
2025-04-11T04:23:18.5549988Z     
2025-04-11T04:23:18.5550071Z         torch.manual_seed(123)
2025-04-11T04:23:18.5550182Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5550269Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5550356Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5550364Z 
2025-04-11T04:23:18.5550538Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5550649Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5550653Z 
2025-04-11T04:23:18.5550733Z device = None
2025-04-11T04:23:18.5550737Z 
2025-04-11T04:23:18.5550852Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5551003Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5551178Z     
2025-04-11T04:23:18.5551257Z         Args:
2025-04-11T04:23:18.5551425Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5551591Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5551700Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5551773Z         """
2025-04-11T04:23:18.5551852Z         _lazy_init()
2025-04-11T04:23:18.5551945Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5552044Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5552152Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5552442Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5552582Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5552742Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5552746Z 
2025-04-11T04:23:18.5552986Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5553249Z _____________ test_context_attention[True-False-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.5553253Z 
2025-04-11T04:23:18.5553408Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5553553Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5553640Z use_new_kcache_layout = True
2025-04-11T04:23:18.5553648Z 
2025-04-11T04:23:18.5553845Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5553947Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5554065Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5554200Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5554322Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5554434Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5554567Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5554707Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5554858Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5554953Z     def test_context_attention(
2025-04-11T04:23:18.5555028Z         bsz: int,
2025-04-11T04:23:18.5555116Z         block_size: int,
2025-04-11T04:23:18.5555207Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5555291Z         num_attn_heads: int,
2025-04-11T04:23:18.5555386Z         kv_group_num: int,
2025-04-11T04:23:18.5555470Z         same_context_len: bool,
2025-04-11T04:23:18.5555566Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5555652Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5555723Z     ):
2025-04-11T04:23:18.5555849Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5556054Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5556237Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5556417Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5556583Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5556662Z             return
2025-04-11T04:23:18.5556734Z     
2025-04-11T04:23:18.5556822Z         torch.manual_seed(123)
2025-04-11T04:23:18.5556919Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5557021Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5557108Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5557112Z 
2025-04-11T04:23:18.5557288Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5557491Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5557495Z 
2025-04-11T04:23:18.5557576Z device = None
2025-04-11T04:23:18.5557583Z 
2025-04-11T04:23:18.5557711Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5557859Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5557943Z     
2025-04-11T04:23:18.5558015Z         Args:
2025-04-11T04:23:18.5558190Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5558366Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5558469Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5558547Z         """
2025-04-11T04:23:18.5558623Z         _lazy_init()
2025-04-11T04:23:18.5558718Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5558817Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5558926Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5559210Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5559442Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5559603Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5559607Z 
2025-04-11T04:23:18.5559844Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5560014Z ____________ test_context_attention[True-False-True-4-16-16-32-32] _____________
2025-04-11T04:23:18.5560017Z 
2025-04-11T04:23:18.5560169Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5560323Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5560417Z use_new_kcache_layout = True
2025-04-11T04:23:18.5560421Z 
2025-04-11T04:23:18.5560632Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5560734Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5560852Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5560993Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5561105Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5561218Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5561353Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5561489Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5561637Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5561727Z     def test_context_attention(
2025-04-11T04:23:18.5561806Z         bsz: int,
2025-04-11T04:23:18.5561886Z         block_size: int,
2025-04-11T04:23:18.5561981Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5562065Z         num_attn_heads: int,
2025-04-11T04:23:18.5562145Z         kv_group_num: int,
2025-04-11T04:23:18.5562236Z         same_context_len: bool,
2025-04-11T04:23:18.5562323Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5562414Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5562486Z     ):
2025-04-11T04:23:18.5562593Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5562787Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5562966Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5563139Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5563300Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5563377Z             return
2025-04-11T04:23:18.5563448Z     
2025-04-11T04:23:18.5563637Z         torch.manual_seed(123)
2025-04-11T04:23:18.5563741Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5563828Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5563927Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5563931Z 
2025-04-11T04:23:18.5564098Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5564211Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5564215Z 
2025-04-11T04:23:18.5564293Z device = None
2025-04-11T04:23:18.5564297Z 
2025-04-11T04:23:18.5564416Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5564565Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5564637Z     
2025-04-11T04:23:18.5564719Z         Args:
2025-04-11T04:23:18.5564885Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5565060Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5565166Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5565239Z         """
2025-04-11T04:23:18.5565408Z         _lazy_init()
2025-04-11T04:23:18.5565502Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5565603Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5565705Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5565990Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5566125Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5566283Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5566295Z 
2025-04-11T04:23:18.5566537Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5566705Z _____________ test_context_attention[True-False-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.5566709Z 
2025-04-11T04:23:18.5566862Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5567014Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5567105Z use_new_kcache_layout = True
2025-04-11T04:23:18.5567109Z 
2025-04-11T04:23:18.5567316Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5567437Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5567556Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5567694Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5567812Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5567927Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5568066Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5568203Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5568355Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5568445Z     def test_context_attention(
2025-04-11T04:23:18.5568520Z         bsz: int,
2025-04-11T04:23:18.5568603Z         block_size: int,
2025-04-11T04:23:18.5568694Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5568780Z         num_attn_heads: int,
2025-04-11T04:23:18.5568861Z         kv_group_num: int,
2025-04-11T04:23:18.5568944Z         same_context_len: bool,
2025-04-11T04:23:18.5569030Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5569116Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5569190Z     ):
2025-04-11T04:23:18.5569300Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5569495Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5569762Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5569933Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5570103Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5570177Z             return
2025-04-11T04:23:18.5570253Z     
2025-04-11T04:23:18.5570339Z         torch.manual_seed(123)
2025-04-11T04:23:18.5570441Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5570529Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5570616Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5570620Z 
2025-04-11T04:23:18.5570789Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5570898Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5570902Z 
2025-04-11T04:23:18.5570981Z device = None
2025-04-11T04:23:18.5570985Z 
2025-04-11T04:23:18.5571100Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5571253Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5571324Z     
2025-04-11T04:23:18.5571483Z         Args:
2025-04-11T04:23:18.5571653Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5571817Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5571927Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5572002Z         """
2025-04-11T04:23:18.5572083Z         _lazy_init()
2025-04-11T04:23:18.5572178Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5572280Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5572389Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5572676Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5572825Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5572985Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5572992Z 
2025-04-11T04:23:18.5573239Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5573408Z ____________ test_context_attention[True-False-False-1-16-8-16-32] _____________
2025-04-11T04:23:18.5573412Z 
2025-04-11T04:23:18.5573565Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5573714Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5573801Z use_new_kcache_layout = True
2025-04-11T04:23:18.5573805Z 
2025-04-11T04:23:18.5574008Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5574111Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5574233Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5574371Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5574488Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5574608Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5574743Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5574882Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5575031Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5575128Z     def test_context_attention(
2025-04-11T04:23:18.5575210Z         bsz: int,
2025-04-11T04:23:18.5575288Z         block_size: int,
2025-04-11T04:23:18.5575385Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5575466Z         num_attn_heads: int,
2025-04-11T04:23:18.5575551Z         kv_group_num: int,
2025-04-11T04:23:18.5575636Z         same_context_len: bool,
2025-04-11T04:23:18.5575805Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5575899Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5575973Z     ):
2025-04-11T04:23:18.5576084Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5576280Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5576462Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5576631Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5576792Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5576870Z             return
2025-04-11T04:23:18.5576942Z     
2025-04-11T04:23:18.5577029Z         torch.manual_seed(123)
2025-04-11T04:23:18.5577128Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5577218Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5577311Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5577314Z 
2025-04-11T04:23:18.5577479Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5577592Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5577771Z 
2025-04-11T04:23:18.5577850Z device = None
2025-04-11T04:23:18.5577854Z 
2025-04-11T04:23:18.5577985Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5578135Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5578209Z     
2025-04-11T04:23:18.5578282Z         Args:
2025-04-11T04:23:18.5578452Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5578619Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5578723Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5578798Z         """
2025-04-11T04:23:18.5578878Z         _lazy_init()
2025-04-11T04:23:18.5578978Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5579079Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5579182Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5579472Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5579608Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5579772Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5579776Z 
2025-04-11T04:23:18.5580011Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5580178Z _____________ test_context_attention[True-False-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.5580182Z 
2025-04-11T04:23:18.5580330Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5580483Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5580570Z use_new_kcache_layout = True
2025-04-11T04:23:18.5580574Z 
2025-04-11T04:23:18.5580769Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5580877Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5580989Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5581129Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5581243Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5581356Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5581489Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5581621Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5581775Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5581947Z     def test_context_attention(
2025-04-11T04:23:18.5582030Z         bsz: int,
2025-04-11T04:23:18.5582111Z         block_size: int,
2025-04-11T04:23:18.5582199Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5582289Z         num_attn_heads: int,
2025-04-11T04:23:18.5582372Z         kv_group_num: int,
2025-04-11T04:23:18.5582460Z         same_context_len: bool,
2025-04-11T04:23:18.5582543Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5582629Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5582704Z     ):
2025-04-11T04:23:18.5582814Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5583029Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5583216Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5583396Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5583564Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5583641Z             return
2025-04-11T04:23:18.5583712Z     
2025-04-11T04:23:18.5583798Z         torch.manual_seed(123)
2025-04-11T04:23:18.5583993Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5584081Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5584175Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5584179Z 
2025-04-11T04:23:18.5584346Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5584455Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5584463Z 
2025-04-11T04:23:18.5584539Z device = None
2025-04-11T04:23:18.5584543Z 
2025-04-11T04:23:18.5584658Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5584809Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5584880Z     
2025-04-11T04:23:18.5584961Z         Args:
2025-04-11T04:23:18.5585129Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5585290Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5585403Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5585476Z         """
2025-04-11T04:23:18.5585557Z         _lazy_init()
2025-04-11T04:23:18.5585650Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5585754Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5585858Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5586139Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5586276Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5586434Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5586438Z 
2025-04-11T04:23:18.5586682Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5586849Z ____________ test_context_attention[True-False-False-1-16-8-32-32] _____________
2025-04-11T04:23:18.5586857Z 
2025-04-11T04:23:18.5587010Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5587154Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5587244Z use_new_kcache_layout = True
2025-04-11T04:23:18.5587248Z 
2025-04-11T04:23:18.5587445Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5587549Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5587666Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5587802Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5587927Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5588147Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5588298Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5588484Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5588646Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5588737Z     def test_context_attention(
2025-04-11T04:23:18.5588813Z         bsz: int,
2025-04-11T04:23:18.5588903Z         block_size: int,
2025-04-11T04:23:18.5588993Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5589074Z         num_attn_heads: int,
2025-04-11T04:23:18.5589161Z         kv_group_num: int,
2025-04-11T04:23:18.5589244Z         same_context_len: bool,
2025-04-11T04:23:18.5589331Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5589416Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5589492Z     ):
2025-04-11T04:23:18.5589601Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5589795Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5589979Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5590243Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5590410Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5590484Z             return
2025-04-11T04:23:18.5590559Z     
2025-04-11T04:23:18.5590643Z         torch.manual_seed(123)
2025-04-11T04:23:18.5590740Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5590832Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5590920Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5590924Z 
2025-04-11T04:23:18.5591091Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5591207Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5591211Z 
2025-04-11T04:23:18.5591290Z device = None
2025-04-11T04:23:18.5591294Z 
2025-04-11T04:23:18.5591407Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5591558Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5591634Z     
2025-04-11T04:23:18.5591708Z         Args:
2025-04-11T04:23:18.5591879Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5592045Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5592154Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5592228Z         """
2025-04-11T04:23:18.5592306Z         _lazy_init()
2025-04-11T04:23:18.5592405Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5592503Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5592611Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5592899Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5593046Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5593218Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5593222Z 
2025-04-11T04:23:18.5593469Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5593640Z ____________ test_context_attention[True-False-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.5593644Z 
2025-04-11T04:23:18.5593795Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5593952Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5594038Z use_new_kcache_layout = True
2025-04-11T04:23:18.5594041Z 
2025-04-11T04:23:18.5594344Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5594453Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5594566Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5594711Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5594825Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5594942Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5595075Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5595214Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5595363Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5595449Z     def test_context_attention(
2025-04-11T04:23:18.5595528Z         bsz: int,
2025-04-11T04:23:18.5595610Z         block_size: int,
2025-04-11T04:23:18.5595702Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5595788Z         num_attn_heads: int,
2025-04-11T04:23:18.5595870Z         kv_group_num: int,
2025-04-11T04:23:18.5595959Z         same_context_len: bool,
2025-04-11T04:23:18.5596044Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5596220Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5596293Z     ):
2025-04-11T04:23:18.5596406Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5596598Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5596779Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5596955Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5597118Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5597198Z             return
2025-04-11T04:23:18.5597270Z     
2025-04-11T04:23:18.5597356Z         torch.manual_seed(123)
2025-04-11T04:23:18.5597457Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5597543Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5597635Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5597638Z 
2025-04-11T04:23:18.5597812Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5597934Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5597938Z 
2025-04-11T04:23:18.5598015Z device = None
2025-04-11T04:23:18.5598019Z 
2025-04-11T04:23:18.5598144Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5598292Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5598362Z     
2025-04-11T04:23:18.5598436Z         Args:
2025-04-11T04:23:18.5598601Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5598769Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5598875Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5598950Z         """
2025-04-11T04:23:18.5599027Z         _lazy_init()
2025-04-11T04:23:18.5599120Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5599225Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5599327Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5599614Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5599747Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5599908Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5599912Z 
2025-04-11T04:23:18.5600148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5600318Z ____________ test_context_attention[True-False-False-1-16-16-16-32] ____________
2025-04-11T04:23:18.5600409Z 
2025-04-11T04:23:18.5600564Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5600708Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5600805Z use_new_kcache_layout = True
2025-04-11T04:23:18.5600809Z 
2025-04-11T04:23:18.5601005Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5601111Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5601226Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5601367Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5601480Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5601591Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5601731Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5601867Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5602022Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5602108Z     def test_context_attention(
2025-04-11T04:23:18.5602287Z         bsz: int,
2025-04-11T04:23:18.5602377Z         block_size: int,
2025-04-11T04:23:18.5602468Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5602551Z         num_attn_heads: int,
2025-04-11T04:23:18.5602632Z         kv_group_num: int,
2025-04-11T04:23:18.5602727Z         same_context_len: bool,
2025-04-11T04:23:18.5602818Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5602913Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5602987Z     ):
2025-04-11T04:23:18.5603098Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5603301Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5603479Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5603650Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5603817Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5603895Z             return
2025-04-11T04:23:18.5603969Z     
2025-04-11T04:23:18.5604053Z         torch.manual_seed(123)
2025-04-11T04:23:18.5604151Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5604239Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5604326Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5604330Z 
2025-04-11T04:23:18.5604497Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5604610Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5604614Z 
2025-04-11T04:23:18.5604693Z device = None
2025-04-11T04:23:18.5604697Z 
2025-04-11T04:23:18.5604810Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5604966Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5605037Z     
2025-04-11T04:23:18.5605107Z         Args:
2025-04-11T04:23:18.5605276Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5605444Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5605554Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5605626Z         """
2025-04-11T04:23:18.5605705Z         _lazy_init()
2025-04-11T04:23:18.5605798Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5605894Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5606002Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5606284Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5606510Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5606671Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5606675Z 
2025-04-11T04:23:18.5606915Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5607086Z ____________ test_context_attention[True-False-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.5607089Z 
2025-04-11T04:23:18.5607244Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5607392Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5607476Z use_new_kcache_layout = True
2025-04-11T04:23:18.5607480Z 
2025-04-11T04:23:18.5607690Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5607791Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5607911Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5608056Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5608173Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5608284Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5608518Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5608657Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5608808Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5608897Z     def test_context_attention(
2025-04-11T04:23:18.5608974Z         bsz: int,
2025-04-11T04:23:18.5609053Z         block_size: int,
2025-04-11T04:23:18.5609144Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5609224Z         num_attn_heads: int,
2025-04-11T04:23:18.5609310Z         kv_group_num: int,
2025-04-11T04:23:18.5609393Z         same_context_len: bool,
2025-04-11T04:23:18.5609481Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5609569Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5609641Z     ):
2025-04-11T04:23:18.5609756Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5609948Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5610137Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5610305Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5610472Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5610546Z             return
2025-04-11T04:23:18.5610617Z     
2025-04-11T04:23:18.5610704Z         torch.manual_seed(123)
2025-04-11T04:23:18.5610800Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5610889Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5610976Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5610980Z 
2025-04-11T04:23:18.5611149Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5611264Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5611272Z 
2025-04-11T04:23:18.5611348Z device = None
2025-04-11T04:23:18.5611352Z 
2025-04-11T04:23:18.5611471Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5611619Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5611692Z     
2025-04-11T04:23:18.5611764Z         Args:
2025-04-11T04:23:18.5611929Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5612096Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5612198Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5612274Z         """
2025-04-11T04:23:18.5612350Z         _lazy_init()
2025-04-11T04:23:18.5612539Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5612653Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5612756Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5613053Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5613192Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5613355Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5613359Z 
2025-04-11T04:23:18.5613599Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5613771Z ____________ test_context_attention[True-False-False-1-16-16-32-32] ____________
2025-04-11T04:23:18.5613775Z 
2025-04-11T04:23:18.5613925Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5614079Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5614164Z use_new_kcache_layout = True
2025-04-11T04:23:18.5614168Z 
2025-04-11T04:23:18.5614366Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5614558Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5614673Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5614816Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5614931Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5615045Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5615180Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5615316Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5615468Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5615556Z     def test_context_attention(
2025-04-11T04:23:18.5615637Z         bsz: int,
2025-04-11T04:23:18.5615721Z         block_size: int,
2025-04-11T04:23:18.5615813Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5615894Z         num_attn_heads: int,
2025-04-11T04:23:18.5615976Z         kv_group_num: int,
2025-04-11T04:23:18.5616067Z         same_context_len: bool,
2025-04-11T04:23:18.5616149Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5616241Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5616312Z     ):
2025-04-11T04:23:18.5616421Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5616615Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5616794Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5616967Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5617130Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5617212Z             return
2025-04-11T04:23:18.5617285Z     
2025-04-11T04:23:18.5617368Z         torch.manual_seed(123)
2025-04-11T04:23:18.5617468Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5617560Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5617660Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5617663Z 
2025-04-11T04:23:18.5617830Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5617944Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5617948Z 
2025-04-11T04:23:18.5618027Z device = None
2025-04-11T04:23:18.5618030Z 
2025-04-11T04:23:18.5618145Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5618297Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5618366Z     
2025-04-11T04:23:18.5618441Z         Args:
2025-04-11T04:23:18.5618691Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5618862Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5618966Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5619043Z         """
2025-04-11T04:23:18.5619123Z         _lazy_init()
2025-04-11T04:23:18.5619218Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5619320Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5619424Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5619710Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5619849Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5620007Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5620011Z 
2025-04-11T04:23:18.5620257Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5620423Z _____________ test_context_attention[True-False-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.5620426Z 
2025-04-11T04:23:18.5620666Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5620811Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5620904Z use_new_kcache_layout = True
2025-04-11T04:23:18.5620908Z 
2025-04-11T04:23:18.5621105Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5621206Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5621327Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5621463Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5621579Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5621691Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5621832Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5621964Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5622117Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5622204Z     def test_context_attention(
2025-04-11T04:23:18.5622278Z         bsz: int,
2025-04-11T04:23:18.5622362Z         block_size: int,
2025-04-11T04:23:18.5622450Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5622534Z         num_attn_heads: int,
2025-04-11T04:23:18.5622616Z         kv_group_num: int,
2025-04-11T04:23:18.5622698Z         same_context_len: bool,
2025-04-11T04:23:18.5622788Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5622873Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5622949Z     ):
2025-04-11T04:23:18.5623057Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5623258Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5623454Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5623625Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5623797Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5623870Z             return
2025-04-11T04:23:18.5623945Z     
2025-04-11T04:23:18.5624027Z         torch.manual_seed(123)
2025-04-11T04:23:18.5624123Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5624215Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5624303Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5624307Z 
2025-04-11T04:23:18.5624476Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5624588Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5624592Z 
2025-04-11T04:23:18.5624754Z device = None
2025-04-11T04:23:18.5624758Z 
2025-04-11T04:23:18.5624878Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5625027Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5625109Z     
2025-04-11T04:23:18.5625181Z         Args:
2025-04-11T04:23:18.5625351Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5625519Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5625627Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5625704Z         """
2025-04-11T04:23:18.5625780Z         _lazy_init()
2025-04-11T04:23:18.5625882Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5625983Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5626091Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5626380Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5626522Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5626762Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5626766Z 
2025-04-11T04:23:18.5627004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5627177Z ____________ test_context_attention[True-False-False-4-16-8-16-32] _____________
2025-04-11T04:23:18.5627181Z 
2025-04-11T04:23:18.5627330Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5627478Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5627563Z use_new_kcache_layout = True
2025-04-11T04:23:18.5627567Z 
2025-04-11T04:23:18.5627767Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5627875Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5627992Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5628128Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5628244Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5628358Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5628523Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5628665Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5628813Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5628900Z     def test_context_attention(
2025-04-11T04:23:18.5628980Z         bsz: int,
2025-04-11T04:23:18.5629059Z         block_size: int,
2025-04-11T04:23:18.5629150Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5629234Z         num_attn_heads: int,
2025-04-11T04:23:18.5629319Z         kv_group_num: int,
2025-04-11T04:23:18.5629406Z         same_context_len: bool,
2025-04-11T04:23:18.5629489Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5629582Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5629659Z     ):
2025-04-11T04:23:18.5629775Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5629969Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5630149Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5630322Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5630489Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5630570Z             return
2025-04-11T04:23:18.5630640Z     
2025-04-11T04:23:18.5630731Z         torch.manual_seed(123)
2025-04-11T04:23:18.5630830Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5631013Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5631110Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5631114Z 
2025-04-11T04:23:18.5631284Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5631402Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5631406Z 
2025-04-11T04:23:18.5631481Z device = None
2025-04-11T04:23:18.5631486Z 
2025-04-11T04:23:18.5631610Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5631762Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5631831Z     
2025-04-11T04:23:18.5631908Z         Args:
2025-04-11T04:23:18.5632078Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5632246Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5632354Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5632431Z         """
2025-04-11T04:23:18.5632509Z         _lazy_init()
2025-04-11T04:23:18.5632602Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5632707Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5632926Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5633223Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5633366Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5633538Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5633542Z 
2025-04-11T04:23:18.5633818Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5633986Z _____________ test_context_attention[True-False-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.5633994Z 
2025-04-11T04:23:18.5634145Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5634290Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5634383Z use_new_kcache_layout = True
2025-04-11T04:23:18.5634390Z 
2025-04-11T04:23:18.5634587Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5634694Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5634808Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5634948Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5635063Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5635175Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5635313Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5635447Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5635603Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5635690Z     def test_context_attention(
2025-04-11T04:23:18.5635768Z         bsz: int,
2025-04-11T04:23:18.5635856Z         block_size: int,
2025-04-11T04:23:18.5635952Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5636037Z         num_attn_heads: int,
2025-04-11T04:23:18.5636118Z         kv_group_num: int,
2025-04-11T04:23:18.5636204Z         same_context_len: bool,
2025-04-11T04:23:18.5636288Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5636379Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5636457Z     ):
2025-04-11T04:23:18.5636568Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5636765Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5636945Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5637206Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5637372Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5637448Z             return
2025-04-11T04:23:18.5637531Z     
2025-04-11T04:23:18.5637619Z         torch.manual_seed(123)
2025-04-11T04:23:18.5637720Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5637812Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5637900Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5637904Z 
2025-04-11T04:23:18.5638078Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5638189Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5638193Z 
2025-04-11T04:23:18.5638286Z device = None
2025-04-11T04:23:18.5638290Z 
2025-04-11T04:23:18.5638429Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5638594Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5638665Z     
2025-04-11T04:23:18.5638738Z         Args:
2025-04-11T04:23:18.5638912Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5639167Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5639276Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5639348Z         """
2025-04-11T04:23:18.5639429Z         _lazy_init()
2025-04-11T04:23:18.5639524Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5639625Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5639732Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5640011Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5640152Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5640313Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5640317Z 
2025-04-11T04:23:18.5640556Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5640730Z ____________ test_context_attention[True-False-False-4-16-8-32-32] _____________
2025-04-11T04:23:18.5640734Z 
2025-04-11T04:23:18.5640887Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5641032Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5641117Z use_new_kcache_layout = True
2025-04-11T04:23:18.5641121Z 
2025-04-11T04:23:18.5641320Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5641424Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5641544Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5641681Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5641804Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5641917Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5642051Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5642191Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5642343Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5642432Z     def test_context_attention(
2025-04-11T04:23:18.5642508Z         bsz: int,
2025-04-11T04:23:18.5642592Z         block_size: int,
2025-04-11T04:23:18.5642680Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5642760Z         num_attn_heads: int,
2025-04-11T04:23:18.5642846Z         kv_group_num: int,
2025-04-11T04:23:18.5642928Z         same_context_len: bool,
2025-04-11T04:23:18.5643014Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5643100Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5643174Z     ):
2025-04-11T04:23:18.5643409Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5643609Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5643804Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5643973Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5644149Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5644223Z             return
2025-04-11T04:23:18.5644294Z     
2025-04-11T04:23:18.5644384Z         torch.manual_seed(123)
2025-04-11T04:23:18.5644479Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5644568Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5644656Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5644659Z 
2025-04-11T04:23:18.5644834Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5644944Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5644948Z 
2025-04-11T04:23:18.5645025Z device = None
2025-04-11T04:23:18.5645123Z 
2025-04-11T04:23:18.5645247Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5645396Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5645473Z     
2025-04-11T04:23:18.5645545Z         Args:
2025-04-11T04:23:18.5645716Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5645882Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5645988Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5646067Z         """
2025-04-11T04:23:18.5646145Z         _lazy_init()
2025-04-11T04:23:18.5646245Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5646348Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5646450Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5646737Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5646879Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5647043Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5647047Z 
2025-04-11T04:23:18.5647286Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5647457Z ____________ test_context_attention[True-False-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.5647460Z 
2025-04-11T04:23:18.5647611Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5647761Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5647847Z use_new_kcache_layout = True
2025-04-11T04:23:18.5647854Z 
2025-04-11T04:23:18.5648052Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5648166Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5648285Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5648437Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5648558Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5648675Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5648810Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5648944Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5649098Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5649185Z     def test_context_attention(
2025-04-11T04:23:18.5649266Z         bsz: int,
2025-04-11T04:23:18.5649348Z         block_size: int,
2025-04-11T04:23:18.5649522Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5649609Z         num_attn_heads: int,
2025-04-11T04:23:18.5649691Z         kv_group_num: int,
2025-04-11T04:23:18.5649780Z         same_context_len: bool,
2025-04-11T04:23:18.5649868Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5649957Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5650029Z     ):
2025-04-11T04:23:18.5650137Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5650333Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5650512Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5650702Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5650868Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5650946Z             return
2025-04-11T04:23:18.5651022Z     
2025-04-11T04:23:18.5651107Z         torch.manual_seed(123)
2025-04-11T04:23:18.5651212Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5651300Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5651488Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5651492Z 
2025-04-11T04:23:18.5651662Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5651776Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5651780Z 
2025-04-11T04:23:18.5651855Z device = None
2025-04-11T04:23:18.5651859Z 
2025-04-11T04:23:18.5651973Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5652126Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5652195Z     
2025-04-11T04:23:18.5652271Z         Args:
2025-04-11T04:23:18.5652438Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5652610Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5652715Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5652787Z         """
2025-04-11T04:23:18.5652893Z         _lazy_init()
2025-04-11T04:23:18.5652995Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5653108Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5653215Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5653505Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5653641Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5653801Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5653805Z 
2025-04-11T04:23:18.5654052Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5654227Z ____________ test_context_attention[True-False-False-4-16-16-16-32] ____________
2025-04-11T04:23:18.5654230Z 
2025-04-11T04:23:18.5654388Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5654536Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5654624Z use_new_kcache_layout = True
2025-04-11T04:23:18.5654627Z 
2025-04-11T04:23:18.5654829Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5654938Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5655051Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5655188Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5655304Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5655414Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5655652Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5655789Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5655941Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5656033Z     def test_context_attention(
2025-04-11T04:23:18.5656113Z         bsz: int,
2025-04-11T04:23:18.5656199Z         block_size: int,
2025-04-11T04:23:18.5656289Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5656379Z         num_attn_heads: int,
2025-04-11T04:23:18.5656463Z         kv_group_num: int,
2025-04-11T04:23:18.5656550Z         same_context_len: bool,
2025-04-11T04:23:18.5656641Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5656732Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5656810Z     ):
2025-04-11T04:23:18.5656921Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5657114Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5657305Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5657479Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5657732Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5657819Z             return
2025-04-11T04:23:18.5657903Z     
2025-04-11T04:23:18.5657994Z         torch.manual_seed(123)
2025-04-11T04:23:18.5658092Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5658191Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5658280Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5658284Z 
2025-04-11T04:23:18.5658461Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5658570Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5658574Z 
2025-04-11T04:23:18.5658655Z device = None
2025-04-11T04:23:18.5658658Z 
2025-04-11T04:23:18.5658775Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5658925Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5658997Z     
2025-04-11T04:23:18.5659073Z         Args:
2025-04-11T04:23:18.5659240Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5659402Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5659509Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5659582Z         """
2025-04-11T04:23:18.5659659Z         _lazy_init()
2025-04-11T04:23:18.5659757Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5659855Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5659960Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5660243Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5660381Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5660539Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5660547Z 
2025-04-11T04:23:18.5660782Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5660954Z ____________ test_context_attention[True-False-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.5660958Z 
2025-04-11T04:23:18.5661107Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5661254Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5661336Z use_new_kcache_layout = True
2025-04-11T04:23:18.5661340Z 
2025-04-11T04:23:18.5661539Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5661641Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5661844Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5661982Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5662096Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5662214Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5662349Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5662483Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5662631Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5662718Z     def test_context_attention(
2025-04-11T04:23:18.5662794Z         bsz: int,
2025-04-11T04:23:18.5662879Z         block_size: int,
2025-04-11T04:23:18.5662975Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5663056Z         num_attn_heads: int,
2025-04-11T04:23:18.5663142Z         kv_group_num: int,
2025-04-11T04:23:18.5663222Z         same_context_len: bool,
2025-04-11T04:23:18.5663307Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5663425Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5663497Z     ):
2025-04-11T04:23:18.5663609Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5663905Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5664087Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5664257Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5664421Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5664501Z             return
2025-04-11T04:23:18.5664571Z     
2025-04-11T04:23:18.5664657Z         torch.manual_seed(123)
2025-04-11T04:23:18.5664755Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5664844Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5664940Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5664944Z 
2025-04-11T04:23:18.5665111Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5665225Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5665232Z 
2025-04-11T04:23:18.5665308Z device = None
2025-04-11T04:23:18.5665312Z 
2025-04-11T04:23:18.5665428Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5665575Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5665647Z     
2025-04-11T04:23:18.5665719Z         Args:
2025-04-11T04:23:18.5665885Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5666054Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5666157Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5666234Z         """
2025-04-11T04:23:18.5666314Z         _lazy_init()
2025-04-11T04:23:18.5666407Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5666506Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5666613Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5666896Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5667029Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5667189Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5667193Z 
2025-04-11T04:23:18.5667430Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5667602Z ____________ test_context_attention[True-False-False-4-16-16-32-32] ____________
2025-04-11T04:23:18.5667605Z 
2025-04-11T04:23:18.5667757Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5668000Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.5668095Z use_new_kcache_layout = True
2025-04-11T04:23:18.5668099Z 
2025-04-11T04:23:18.5668298Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5668406Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5668556Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5668696Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5668809Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5668920Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5669060Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5669193Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5669345Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5669433Z     def test_context_attention(
2025-04-11T04:23:18.5669513Z         bsz: int,
2025-04-11T04:23:18.5669601Z         block_size: int,
2025-04-11T04:23:18.5669698Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5669894Z         num_attn_heads: int,
2025-04-11T04:23:18.5669976Z         kv_group_num: int,
2025-04-11T04:23:18.5670062Z         same_context_len: bool,
2025-04-11T04:23:18.5670143Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5670230Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5670305Z     ):
2025-04-11T04:23:18.5670414Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5670610Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5670788Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5670962Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5671127Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5671201Z             return
2025-04-11T04:23:18.5671279Z     
2025-04-11T04:23:18.5671371Z         torch.manual_seed(123)
2025-04-11T04:23:18.5671473Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5671559Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5671654Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5671658Z 
2025-04-11T04:23:18.5671825Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5671936Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5671940Z 
2025-04-11T04:23:18.5672020Z device = None
2025-04-11T04:23:18.5672023Z 
2025-04-11T04:23:18.5672139Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5672292Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5672363Z     
2025-04-11T04:23:18.5672443Z         Args:
2025-04-11T04:23:18.5672609Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5672773Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5672886Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5672957Z         """
2025-04-11T04:23:18.5676957Z         _lazy_init()
2025-04-11T04:23:18.5677051Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5677152Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5677254Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5677535Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5677675Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5677837Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5677947Z 
2025-04-11T04:23:18.5678198Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5678364Z _____________ test_context_attention[False-True-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.5678372Z 
2025-04-11T04:23:18.5678524Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5678668Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5678761Z use_new_kcache_layout = False
2025-04-11T04:23:18.5678765Z 
2025-04-11T04:23:18.5678962Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5679066Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5679187Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5679323Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5679445Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5679556Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5679702Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5680018Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5680168Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5680272Z     def test_context_attention(
2025-04-11T04:23:18.5680347Z         bsz: int,
2025-04-11T04:23:18.5680431Z         block_size: int,
2025-04-11T04:23:18.5680518Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5680598Z         num_attn_heads: int,
2025-04-11T04:23:18.5680689Z         kv_group_num: int,
2025-04-11T04:23:18.5680770Z         same_context_len: bool,
2025-04-11T04:23:18.5680855Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5680964Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5681037Z     ):
2025-04-11T04:23:18.5681143Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5681337Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5681524Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5681694Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5681858Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5681932Z             return
2025-04-11T04:23:18.5682008Z     
2025-04-11T04:23:18.5682092Z         torch.manual_seed(123)
2025-04-11T04:23:18.5682188Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5682281Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5682369Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5682373Z 
2025-04-11T04:23:18.5682540Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5682654Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5682657Z 
2025-04-11T04:23:18.5682738Z device = None
2025-04-11T04:23:18.5682742Z 
2025-04-11T04:23:18.5682857Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5683007Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5683082Z     
2025-04-11T04:23:18.5683154Z         Args:
2025-04-11T04:23:18.5683322Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5683486Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5683595Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5683666Z         """
2025-04-11T04:23:18.5683743Z         _lazy_init()
2025-04-11T04:23:18.5683839Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5683937Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5684156Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5684440Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5684578Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5684740Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5684743Z 
2025-04-11T04:23:18.5684984Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5685154Z _____________ test_context_attention[False-True-True-1-16-8-16-32] _____________
2025-04-11T04:23:18.5685158Z 
2025-04-11T04:23:18.5685307Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5685453Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5685544Z use_new_kcache_layout = False
2025-04-11T04:23:18.5685548Z 
2025-04-11T04:23:18.5685752Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5685857Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5685972Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5686196Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5686312Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5686428Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5686566Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5686715Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5686871Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5686962Z     def test_context_attention(
2025-04-11T04:23:18.5687042Z         bsz: int,
2025-04-11T04:23:18.5687122Z         block_size: int,
2025-04-11T04:23:18.5687213Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5687300Z         num_attn_heads: int,
2025-04-11T04:23:18.5687387Z         kv_group_num: int,
2025-04-11T04:23:18.5687473Z         same_context_len: bool,
2025-04-11T04:23:18.5687561Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5687661Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5687733Z     ):
2025-04-11T04:23:18.5687846Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5688036Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5688219Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5688394Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5688554Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5688635Z             return
2025-04-11T04:23:18.5688708Z     
2025-04-11T04:23:18.5688801Z         torch.manual_seed(123)
2025-04-11T04:23:18.5688900Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5688990Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5689085Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5689092Z 
2025-04-11T04:23:18.5689257Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5689371Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5689375Z 
2025-04-11T04:23:18.5689451Z device = None
2025-04-11T04:23:18.5689455Z 
2025-04-11T04:23:18.5689573Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5689721Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5689792Z     
2025-04-11T04:23:18.5689869Z         Args:
2025-04-11T04:23:18.5690033Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5690294Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5690403Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5690482Z         """
2025-04-11T04:23:18.5690560Z         _lazy_init()
2025-04-11T04:23:18.5690659Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5690762Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5690869Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5691155Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5691290Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5691451Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5691455Z 
2025-04-11T04:23:18.5691695Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5691866Z _____________ test_context_attention[False-True-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.5691873Z 
2025-04-11T04:23:18.5692025Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5692169Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5692343Z use_new_kcache_layout = False
2025-04-11T04:23:18.5692348Z 
2025-04-11T04:23:18.5692548Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5692657Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5692772Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5692915Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5693030Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5693142Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5693280Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5693419Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5693573Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5693662Z     def test_context_attention(
2025-04-11T04:23:18.5693740Z         bsz: int,
2025-04-11T04:23:18.5693825Z         block_size: int,
2025-04-11T04:23:18.5693913Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5694000Z         num_attn_heads: int,
2025-04-11T04:23:18.5694083Z         kv_group_num: int,
2025-04-11T04:23:18.5694172Z         same_context_len: bool,
2025-04-11T04:23:18.5694256Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5694344Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5694419Z     ):
2025-04-11T04:23:18.5694528Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5694722Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5694908Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5695081Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5695249Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5695327Z             return
2025-04-11T04:23:18.5695403Z     
2025-04-11T04:23:18.5695489Z         torch.manual_seed(123)
2025-04-11T04:23:18.5695592Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5695684Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5695774Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5695777Z 
2025-04-11T04:23:18.5695949Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5696071Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5696075Z 
2025-04-11T04:23:18.5696157Z device = None
2025-04-11T04:23:18.5696161Z 
2025-04-11T04:23:18.5696279Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5696517Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5696596Z     
2025-04-11T04:23:18.5696672Z         Args:
2025-04-11T04:23:18.5696852Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5697018Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5697131Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5697207Z         """
2025-04-11T04:23:18.5697293Z         _lazy_init()
2025-04-11T04:23:18.5697393Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5697500Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5697614Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5697903Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5698050Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5698214Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5698217Z 
2025-04-11T04:23:18.5698576Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5698746Z _____________ test_context_attention[False-True-True-1-16-8-32-32] _____________
2025-04-11T04:23:18.5698750Z 
2025-04-11T04:23:18.5698904Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5699053Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5699142Z use_new_kcache_layout = False
2025-04-11T04:23:18.5699146Z 
2025-04-11T04:23:18.5699352Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5699457Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5699580Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5699722Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5699842Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5699956Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5700095Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5700233Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5700387Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5700478Z     def test_context_attention(
2025-04-11T04:23:18.5700555Z         bsz: int,
2025-04-11T04:23:18.5700638Z         block_size: int,
2025-04-11T04:23:18.5700731Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5700814Z         num_attn_heads: int,
2025-04-11T04:23:18.5700901Z         kv_group_num: int,
2025-04-11T04:23:18.5700986Z         same_context_len: bool,
2025-04-11T04:23:18.5701075Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5701167Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5701240Z     ):
2025-04-11T04:23:18.5701365Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5737804Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5738100Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5738273Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5738447Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5738522Z             return
2025-04-11T04:23:18.5738595Z     
2025-04-11T04:23:18.5738711Z         torch.manual_seed(123)
2025-04-11T04:23:18.5738819Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5738910Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5739001Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5739005Z 
2025-04-11T04:23:18.5739302Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5739425Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5739432Z 
2025-04-11T04:23:18.5739513Z device = None
2025-04-11T04:23:18.5739526Z 
2025-04-11T04:23:18.5739643Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5739802Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5739879Z     
2025-04-11T04:23:18.5739959Z         Args:
2025-04-11T04:23:18.5740151Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5740349Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5740475Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5740549Z         """
2025-04-11T04:23:18.5740627Z         _lazy_init()
2025-04-11T04:23:18.5740729Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5740829Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5740944Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5741348Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5741493Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5741657Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5741661Z 
2025-04-11T04:23:18.5741904Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5742076Z _____________ test_context_attention[False-True-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.5742079Z 
2025-04-11T04:23:18.5742241Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5742390Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5742484Z use_new_kcache_layout = False
2025-04-11T04:23:18.5742488Z 
2025-04-11T04:23:18.5742705Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5742811Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5742929Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5743073Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5743188Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5743306Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5743442Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5743582Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5743734Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5743822Z     def test_context_attention(
2025-04-11T04:23:18.5743913Z         bsz: int,
2025-04-11T04:23:18.5743996Z         block_size: int,
2025-04-11T04:23:18.5744097Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5744181Z         num_attn_heads: int,
2025-04-11T04:23:18.5744275Z         kv_group_num: int,
2025-04-11T04:23:18.5744358Z         same_context_len: bool,
2025-04-11T04:23:18.5744442Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5744533Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5744615Z     ):
2025-04-11T04:23:18.5744727Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5744929Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5745118Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5745306Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5745579Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5745663Z             return
2025-04-11T04:23:18.5745743Z     
2025-04-11T04:23:18.5745833Z         torch.manual_seed(123)
2025-04-11T04:23:18.5745931Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5746023Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5746119Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5746123Z 
2025-04-11T04:23:18.5746292Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5746407Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5746411Z 
2025-04-11T04:23:18.5746488Z device = None
2025-04-11T04:23:18.5746492Z 
2025-04-11T04:23:18.5746613Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5746761Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5746833Z     
2025-04-11T04:23:18.5746909Z         Args:
2025-04-11T04:23:18.5747080Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5747247Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5747435Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5747513Z         """
2025-04-11T04:23:18.5747589Z         _lazy_init()
2025-04-11T04:23:18.5747685Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5747790Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5747895Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5748183Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5748337Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5748565Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5748570Z 
2025-04-11T04:23:18.5748825Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5749005Z ____________ test_context_attention[False-True-True-1-16-16-16-32] _____________
2025-04-11T04:23:18.5749015Z 
2025-04-11T04:23:18.5749169Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5749311Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5749402Z use_new_kcache_layout = False
2025-04-11T04:23:18.5749406Z 
2025-04-11T04:23:18.5749604Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5749708Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5749826Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5749966Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5750082Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5750199Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5750340Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5750472Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5750630Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5750718Z     def test_context_attention(
2025-04-11T04:23:18.5750797Z         bsz: int,
2025-04-11T04:23:18.5750878Z         block_size: int,
2025-04-11T04:23:18.5750967Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5751054Z         num_attn_heads: int,
2025-04-11T04:23:18.5751136Z         kv_group_num: int,
2025-04-11T04:23:18.5751224Z         same_context_len: bool,
2025-04-11T04:23:18.5751307Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5751394Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5751470Z     ):
2025-04-11T04:23:18.5751580Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5751870Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5752055Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5752242Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5752407Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5752488Z             return
2025-04-11T04:23:18.5752566Z     
2025-04-11T04:23:18.5752651Z         torch.manual_seed(123)
2025-04-11T04:23:18.5752752Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5752840Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5752933Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5752936Z 
2025-04-11T04:23:18.5753109Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5753223Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5753230Z 
2025-04-11T04:23:18.5753312Z device = None
2025-04-11T04:23:18.5753315Z 
2025-04-11T04:23:18.5753441Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5753593Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5753762Z     
2025-04-11T04:23:18.5753846Z         Args:
2025-04-11T04:23:18.5754025Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5754189Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5754297Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5754370Z         """
2025-04-11T04:23:18.5754450Z         _lazy_init()
2025-04-11T04:23:18.5754544Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5754644Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5754751Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5755034Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5755171Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5755329Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5755333Z 
2025-04-11T04:23:18.5755574Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5755740Z _____________ test_context_attention[False-True-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.5755744Z 
2025-04-11T04:23:18.5755899Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5756042Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5756130Z use_new_kcache_layout = False
2025-04-11T04:23:18.5756134Z 
2025-04-11T04:23:18.5756334Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5756440Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5756559Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5756695Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5756815Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5756926Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5757061Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5757197Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5757346Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5757436Z     def test_context_attention(
2025-04-11T04:23:18.5757513Z         bsz: int,
2025-04-11T04:23:18.5757597Z         block_size: int,
2025-04-11T04:23:18.5757684Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5757765Z         num_attn_heads: int,
2025-04-11T04:23:18.5757921Z         kv_group_num: int,
2025-04-11T04:23:18.5758009Z         same_context_len: bool,
2025-04-11T04:23:18.5758096Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5758183Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5758259Z     ):
2025-04-11T04:23:18.5758374Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5758565Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5758748Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5758918Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5759082Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5759156Z             return
2025-04-11T04:23:18.5759227Z     
2025-04-11T04:23:18.5759314Z         torch.manual_seed(123)
2025-04-11T04:23:18.5759415Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5759504Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5759593Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5759597Z 
2025-04-11T04:23:18.5759766Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5759964Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5759968Z 
2025-04-11T04:23:18.5760046Z device = None
2025-04-11T04:23:18.5760050Z 
2025-04-11T04:23:18.5760185Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5760341Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5760417Z     
2025-04-11T04:23:18.5760490Z         Args:
2025-04-11T04:23:18.5760660Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5760826Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5760934Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5761011Z         """
2025-04-11T04:23:18.5761090Z         _lazy_init()
2025-04-11T04:23:18.5761189Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5761293Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5761397Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5761681Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5761815Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5761975Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5761979Z 
2025-04-11T04:23:18.5762220Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5762392Z ____________ test_context_attention[False-True-True-1-16-16-32-32] _____________
2025-04-11T04:23:18.5762396Z 
2025-04-11T04:23:18.5762560Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5762703Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5762797Z use_new_kcache_layout = False
2025-04-11T04:23:18.5762801Z 
2025-04-11T04:23:18.5762999Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5763105Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5763218Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5763358Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5763472Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5763586Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5763721Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5763852Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5764105Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5764197Z     def test_context_attention(
2025-04-11T04:23:18.5764276Z         bsz: int,
2025-04-11T04:23:18.5764362Z         block_size: int,
2025-04-11T04:23:18.5764453Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5764541Z         num_attn_heads: int,
2025-04-11T04:23:18.5764623Z         kv_group_num: int,
2025-04-11T04:23:18.5764714Z         same_context_len: bool,
2025-04-11T04:23:18.5764805Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5764906Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5764992Z     ):
2025-04-11T04:23:18.5765103Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5765315Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5765505Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5765715Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5765888Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5766078Z             return
2025-04-11T04:23:18.5766154Z     
2025-04-11T04:23:18.5766240Z         torch.manual_seed(123)
2025-04-11T04:23:18.5766342Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5766429Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5766522Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5766526Z 
2025-04-11T04:23:18.5766693Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5766802Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5766809Z 
2025-04-11T04:23:18.5766886Z device = None
2025-04-11T04:23:18.5766890Z 
2025-04-11T04:23:18.5767005Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5767160Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5767230Z     
2025-04-11T04:23:18.5767308Z         Args:
2025-04-11T04:23:18.5767473Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5767640Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5767745Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5767818Z         """
2025-04-11T04:23:18.5767898Z         _lazy_init()
2025-04-11T04:23:18.5767992Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5768095Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5768197Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5768476Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5768613Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5768773Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5768777Z 
2025-04-11T04:23:18.5769021Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5769189Z _____________ test_context_attention[False-True-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.5769193Z 
2025-04-11T04:23:18.5769346Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5769503Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5769604Z use_new_kcache_layout = False
2025-04-11T04:23:18.5769608Z 
2025-04-11T04:23:18.5769820Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5769923Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5770052Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5770275Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5770401Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5770515Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5770659Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5770795Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5770948Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5771039Z     def test_context_attention(
2025-04-11T04:23:18.5771116Z         bsz: int,
2025-04-11T04:23:18.5771198Z         block_size: int,
2025-04-11T04:23:18.5771287Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5771368Z         num_attn_heads: int,
2025-04-11T04:23:18.5771455Z         kv_group_num: int,
2025-04-11T04:23:18.5771540Z         same_context_len: bool,
2025-04-11T04:23:18.5771626Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5771714Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5771794Z     ):
2025-04-11T04:23:18.5771904Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5772096Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5772368Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5772538Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5772706Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5772781Z             return
2025-04-11T04:23:18.5772857Z     
2025-04-11T04:23:18.5772943Z         torch.manual_seed(123)
2025-04-11T04:23:18.5773041Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5773134Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5773223Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5773227Z 
2025-04-11T04:23:18.5773402Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5773513Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5773517Z 
2025-04-11T04:23:18.5773597Z device = None
2025-04-11T04:23:18.5773606Z 
2025-04-11T04:23:18.5773724Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5773873Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5773948Z     
2025-04-11T04:23:18.5774021Z         Args:
2025-04-11T04:23:18.5774192Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5774361Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5774476Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5774562Z         """
2025-04-11T04:23:18.5774643Z         _lazy_init()
2025-04-11T04:23:18.5774749Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5774855Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5774982Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5775289Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5775430Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5775593Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5775597Z 
2025-04-11T04:23:18.5775835Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5776003Z _____________ test_context_attention[False-True-True-4-16-8-16-32] _____________
2025-04-11T04:23:18.5776007Z 
2025-04-11T04:23:18.5776156Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5776304Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5776476Z use_new_kcache_layout = False
2025-04-11T04:23:18.5776481Z 
2025-04-11T04:23:18.5776685Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5776790Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5776910Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5777052Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5777168Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5777285Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5777422Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5777559Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5777712Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5777802Z     def test_context_attention(
2025-04-11T04:23:18.5777882Z         bsz: int,
2025-04-11T04:23:18.5777964Z         block_size: int,
2025-04-11T04:23:18.5778061Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5778144Z         num_attn_heads: int,
2025-04-11T04:23:18.5778228Z         kv_group_num: int,
2025-04-11T04:23:18.5778314Z         same_context_len: bool,
2025-04-11T04:23:18.5778483Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5778575Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5778649Z     ):
2025-04-11T04:23:18.5778762Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5778956Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5779137Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5779314Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5779476Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5779555Z             return
2025-04-11T04:23:18.5779634Z     
2025-04-11T04:23:18.5779723Z         torch.manual_seed(123)
2025-04-11T04:23:18.5779822Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5779909Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5780031Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5780035Z 
2025-04-11T04:23:18.5780210Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5780324Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5780329Z 
2025-04-11T04:23:18.5780408Z device = None
2025-04-11T04:23:18.5780412Z 
2025-04-11T04:23:18.5780536Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5780686Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5780757Z     
2025-04-11T04:23:18.5780837Z         Args:
2025-04-11T04:23:18.5781011Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5781191Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5781297Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5781384Z         """
2025-04-11T04:23:18.5781461Z         _lazy_init()
2025-04-11T04:23:18.5781557Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5781660Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5781762Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5782048Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5782182Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5782345Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5782349Z 
2025-04-11T04:23:18.5782672Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5782840Z _____________ test_context_attention[False-True-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.5782847Z 
2025-04-11T04:23:18.5782999Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5783146Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5783237Z use_new_kcache_layout = False
2025-04-11T04:23:18.5783241Z 
2025-04-11T04:23:18.5783437Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5783544Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5783659Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5783801Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5783917Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5784028Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5784170Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5784303Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5784457Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5784627Z     def test_context_attention(
2025-04-11T04:23:18.5784704Z         bsz: int,
2025-04-11T04:23:18.5784795Z         block_size: int,
2025-04-11T04:23:18.5784891Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5784977Z         num_attn_heads: int,
2025-04-11T04:23:18.5785059Z         kv_group_num: int,
2025-04-11T04:23:18.5785148Z         same_context_len: bool,
2025-04-11T04:23:18.5785237Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5785325Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5785408Z     ):
2025-04-11T04:23:18.5785518Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5785711Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5785895Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5786063Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5786235Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5786316Z             return
2025-04-11T04:23:18.5786407Z     
2025-04-11T04:23:18.5786496Z         torch.manual_seed(123)
2025-04-11T04:23:18.5786597Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5786683Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5786771Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5786775Z 
2025-04-11T04:23:18.5786945Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5787055Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5787058Z 
2025-04-11T04:23:18.5787140Z device = None
2025-04-11T04:23:18.5787143Z 
2025-04-11T04:23:18.5787262Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5787415Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5787489Z     
2025-04-11T04:23:18.5787562Z         Args:
2025-04-11T04:23:18.5787728Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5787893Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5788001Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5788074Z         """
2025-04-11T04:23:18.5788155Z         _lazy_init()
2025-04-11T04:23:18.5788251Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5788348Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5788505Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5788917Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5789057Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5789216Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5789223Z 
2025-04-11T04:23:18.5789471Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5789639Z _____________ test_context_attention[False-True-True-4-16-8-32-32] _____________
2025-04-11T04:23:18.5789643Z 
2025-04-11T04:23:18.5789797Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5789948Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5790035Z use_new_kcache_layout = False
2025-04-11T04:23:18.5790039Z 
2025-04-11T04:23:18.5790256Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5790360Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5790480Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5790632Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5790751Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5791002Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5791138Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5791286Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5791437Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5791527Z     def test_context_attention(
2025-04-11T04:23:18.5791604Z         bsz: int,
2025-04-11T04:23:18.5791694Z         block_size: int,
2025-04-11T04:23:18.5791782Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5791865Z         num_attn_heads: int,
2025-04-11T04:23:18.5791950Z         kv_group_num: int,
2025-04-11T04:23:18.5792037Z         same_context_len: bool,
2025-04-11T04:23:18.5792125Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5792213Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5792287Z     ):
2025-04-11T04:23:18.5792401Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5792593Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5792778Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5792948Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5793115Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5793189Z             return
2025-04-11T04:23:18.5793261Z     
2025-04-11T04:23:18.5793349Z         torch.manual_seed(123)
2025-04-11T04:23:18.5793446Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5793536Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5793629Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5793633Z 
2025-04-11T04:23:18.5793799Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5793916Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5793920Z 
2025-04-11T04:23:18.5793997Z device = None
2025-04-11T04:23:18.5794001Z 
2025-04-11T04:23:18.5794121Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5794268Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5794342Z     
2025-04-11T04:23:18.5794415Z         Args:
2025-04-11T04:23:18.5794585Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5794751Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5794854Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5794943Z         """
2025-04-11T04:23:18.5795114Z         _lazy_init()
2025-04-11T04:23:18.5795229Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5795336Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5795446Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5795736Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5795874Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5796033Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5796037Z 
2025-04-11T04:23:18.5796270Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5796438Z _____________ test_context_attention[False-True-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.5796441Z 
2025-04-11T04:23:18.5796597Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5796741Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5796828Z use_new_kcache_layout = False
2025-04-11T04:23:18.5796831Z 
2025-04-11T04:23:18.5797112Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5797219Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5797336Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5797477Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5797592Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5797708Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5797842Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5797977Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5798131Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5798221Z     def test_context_attention(
2025-04-11T04:23:18.5798298Z         bsz: int,
2025-04-11T04:23:18.5798380Z         block_size: int,
2025-04-11T04:23:18.5798472Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5798558Z         num_attn_heads: int,
2025-04-11T04:23:18.5798640Z         kv_group_num: int,
2025-04-11T04:23:18.5798727Z         same_context_len: bool,
2025-04-11T04:23:18.5798810Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5798908Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5798988Z     ):
2025-04-11T04:23:18.5799108Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5799303Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5799482Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5799656Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5799821Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5799898Z             return
2025-04-11T04:23:18.5799970Z     
2025-04-11T04:23:18.5800059Z         torch.manual_seed(123)
2025-04-11T04:23:18.5800160Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5800247Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5800338Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5800342Z 
2025-04-11T04:23:18.5800511Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5800623Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5800627Z 
2025-04-11T04:23:18.5800704Z device = None
2025-04-11T04:23:18.5800707Z 
2025-04-11T04:23:18.5800824Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5800976Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5801048Z     
2025-04-11T04:23:18.5801211Z         Args:
2025-04-11T04:23:18.5801380Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5801556Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5801663Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5801758Z         """
2025-04-11T04:23:18.5801840Z         _lazy_init()
2025-04-11T04:23:18.5801934Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5802037Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5802141Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5802424Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5802557Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5802721Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5802725Z 
2025-04-11T04:23:18.5802964Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5803132Z ____________ test_context_attention[False-True-True-4-16-16-16-32] _____________
2025-04-11T04:23:18.5803216Z 
2025-04-11T04:23:18.5803373Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5803512Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5803603Z use_new_kcache_layout = False
2025-04-11T04:23:18.5803607Z 
2025-04-11T04:23:18.5803805Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5803911Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5804028Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5804163Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5804285Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5804400Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5804537Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5804675Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5804828Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5804915Z     def test_context_attention(
2025-04-11T04:23:18.5805004Z         bsz: int,
2025-04-11T04:23:18.5805103Z         block_size: int,
2025-04-11T04:23:18.5805192Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5805282Z         num_attn_heads: int,
2025-04-11T04:23:18.5805370Z         kv_group_num: int,
2025-04-11T04:23:18.5805456Z         same_context_len: bool,
2025-04-11T04:23:18.5805555Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5805641Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5805719Z     ):
2025-04-11T04:23:18.5805830Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5806022Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5806205Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5806379Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5806544Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5806618Z             return
2025-04-11T04:23:18.5806693Z     
2025-04-11T04:23:18.5806780Z         torch.manual_seed(123)
2025-04-11T04:23:18.5806880Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5806973Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5807062Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5807066Z 
2025-04-11T04:23:18.5807239Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5807439Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5807443Z 
2025-04-11T04:23:18.5807528Z device = None
2025-04-11T04:23:18.5807532Z 
2025-04-11T04:23:18.5807650Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5807806Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5807879Z     
2025-04-11T04:23:18.5807953Z         Args:
2025-04-11T04:23:18.5808122Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5808288Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5808399Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5808472Z         """
2025-04-11T04:23:18.5808551Z         _lazy_init()
2025-04-11T04:23:18.5808650Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5808751Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5808862Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5809144Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5809375Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5809541Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5809546Z 
2025-04-11T04:23:18.5809794Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5809963Z _____________ test_context_attention[False-True-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.5809967Z 
2025-04-11T04:23:18.5810119Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5810270Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5810363Z use_new_kcache_layout = False
2025-04-11T04:23:18.5810367Z 
2025-04-11T04:23:18.5810571Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5810676Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5810804Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5810945Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5811059Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5811184Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5811331Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5811479Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5811630Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5811721Z     def test_context_attention(
2025-04-11T04:23:18.5811797Z         bsz: int,
2025-04-11T04:23:18.5811878Z         block_size: int,
2025-04-11T04:23:18.5811970Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5812054Z         num_attn_heads: int,
2025-04-11T04:23:18.5812138Z         kv_group_num: int,
2025-04-11T04:23:18.5812222Z         same_context_len: bool,
2025-04-11T04:23:18.5812307Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5812404Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5812477Z     ):
2025-04-11T04:23:18.5812592Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5812782Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5812965Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5813132Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5813295Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5813374Z             return
2025-04-11T04:23:18.5813446Z     
2025-04-11T04:23:18.5813735Z         torch.manual_seed(123)
2025-04-11T04:23:18.5813840Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5813929Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5814025Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5814035Z 
2025-04-11T04:23:18.5814202Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5814317Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5814321Z 
2025-04-11T04:23:18.5814400Z device = None
2025-04-11T04:23:18.5814404Z 
2025-04-11T04:23:18.5814526Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5814675Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5814751Z     
2025-04-11T04:23:18.5814825Z         Args:
2025-04-11T04:23:18.5814992Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5815165Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5815269Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5815347Z         """
2025-04-11T04:23:18.5815426Z         _lazy_init()
2025-04-11T04:23:18.5815605Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5815709Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5815813Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5816121Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5816260Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5816421Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5816425Z 
2025-04-11T04:23:18.5816671Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5816851Z ____________ test_context_attention[False-True-True-4-16-16-32-32] _____________
2025-04-11T04:23:18.5816854Z 
2025-04-11T04:23:18.5817022Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5817168Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T04:23:18.5817268Z use_new_kcache_layout = False
2025-04-11T04:23:18.5817272Z 
2025-04-11T04:23:18.5817481Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5817595Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5817711Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5817851Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5817967Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5818082Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5818222Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5818359Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5818514Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5818601Z     def test_context_attention(
2025-04-11T04:23:18.5818682Z         bsz: int,
2025-04-11T04:23:18.5818763Z         block_size: int,
2025-04-11T04:23:18.5818853Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5818940Z         num_attn_heads: int,
2025-04-11T04:23:18.5819021Z         kv_group_num: int,
2025-04-11T04:23:18.5819107Z         same_context_len: bool,
2025-04-11T04:23:18.5819192Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5819281Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5819359Z     ):
2025-04-11T04:23:18.5819468Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5819665Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5819936Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5820120Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5820293Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5820371Z             return
2025-04-11T04:23:18.5820453Z     
2025-04-11T04:23:18.5820542Z         torch.manual_seed(123)
2025-04-11T04:23:18.5820643Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5820731Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5820821Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5820829Z 
2025-04-11T04:23:18.5820997Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5821111Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5821114Z 
2025-04-11T04:23:18.5821196Z device = None
2025-04-11T04:23:18.5821200Z 
2025-04-11T04:23:18.5821319Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5821474Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5821545Z     
2025-04-11T04:23:18.5821624Z         Args:
2025-04-11T04:23:18.5821875Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5822040Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5822151Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5822225Z         """
2025-04-11T04:23:18.5822308Z         _lazy_init()
2025-04-11T04:23:18.5822402Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5822503Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5822613Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5822899Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5823045Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5823209Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5823213Z 
2025-04-11T04:23:18.5823472Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5823640Z _____________ test_context_attention[False-True-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.5823644Z 
2025-04-11T04:23:18.5823799Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5823946Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5824035Z use_new_kcache_layout = False
2025-04-11T04:23:18.5824042Z 
2025-04-11T04:23:18.5824243Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5824347Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5824473Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5824612Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5824733Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5824850Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5824987Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5825120Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5825271Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5825362Z     def test_context_attention(
2025-04-11T04:23:18.5825439Z         bsz: int,
2025-04-11T04:23:18.5825523Z         block_size: int,
2025-04-11T04:23:18.5825612Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5825695Z         num_attn_heads: int,
2025-04-11T04:23:18.5825780Z         kv_group_num: int,
2025-04-11T04:23:18.5825864Z         same_context_len: bool,
2025-04-11T04:23:18.5825950Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5826138Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5826214Z     ):
2025-04-11T04:23:18.5826330Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5826529Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5826717Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5826887Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5827053Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5827126Z             return
2025-04-11T04:23:18.5827198Z     
2025-04-11T04:23:18.5827290Z         torch.manual_seed(123)
2025-04-11T04:23:18.5827388Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5827481Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5827573Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5827580Z 
2025-04-11T04:23:18.5827749Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5827861Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5827947Z 
2025-04-11T04:23:18.5828028Z device = None
2025-04-11T04:23:18.5828044Z 
2025-04-11T04:23:18.5828166Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5828315Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5828402Z     
2025-04-11T04:23:18.5828518Z         Args:
2025-04-11T04:23:18.5828688Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5828850Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5828954Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5829034Z         """
2025-04-11T04:23:18.5829110Z         _lazy_init()
2025-04-11T04:23:18.5829212Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5829313Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5829420Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5829707Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5829845Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5830016Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5830019Z 
2025-04-11T04:23:18.5830262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5830453Z ____________ test_context_attention[False-True-False-1-16-8-16-32] _____________
2025-04-11T04:23:18.5830457Z 
2025-04-11T04:23:18.5830614Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5830775Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5830870Z use_new_kcache_layout = False
2025-04-11T04:23:18.5830874Z 
2025-04-11T04:23:18.5831081Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5831187Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5831303Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5831443Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5831558Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5831675Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5831811Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5831947Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5832097Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5832185Z     def test_context_attention(
2025-04-11T04:23:18.5832358Z         bsz: int,
2025-04-11T04:23:18.5832442Z         block_size: int,
2025-04-11T04:23:18.5832536Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5832618Z         num_attn_heads: int,
2025-04-11T04:23:18.5832704Z         kv_group_num: int,
2025-04-11T04:23:18.5832795Z         same_context_len: bool,
2025-04-11T04:23:18.5832878Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5832970Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5833043Z     ):
2025-04-11T04:23:18.5833156Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5833346Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5833525Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5833700Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5833866Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5833942Z             return
2025-04-11T04:23:18.5834017Z     
2025-04-11T04:23:18.5834103Z         torch.manual_seed(123)
2025-04-11T04:23:18.5834318Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5834406Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5834504Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5834507Z 
2025-04-11T04:23:18.5834674Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5834795Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5834798Z 
2025-04-11T04:23:18.5834884Z device = None
2025-04-11T04:23:18.5834888Z 
2025-04-11T04:23:18.5835018Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5835172Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5835247Z     
2025-04-11T04:23:18.5835327Z         Args:
2025-04-11T04:23:18.5835495Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5835664Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5835774Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5835846Z         """
2025-04-11T04:23:18.5835926Z         _lazy_init()
2025-04-11T04:23:18.5836021Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5836126Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5836229Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5836518Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5836653Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5836814Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5836821Z 
2025-04-11T04:23:18.5837066Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5837234Z _____________ test_context_attention[False-True-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.5837242Z 
2025-04-11T04:23:18.5837395Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5837541Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5837646Z use_new_kcache_layout = False
2025-04-11T04:23:18.5837650Z 
2025-04-11T04:23:18.5837850Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5837958Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5838075Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5838219Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5838345Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5838544Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5838686Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5838821Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5838977Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5839064Z     def test_context_attention(
2025-04-11T04:23:18.5839140Z         bsz: int,
2025-04-11T04:23:18.5839228Z         block_size: int,
2025-04-11T04:23:18.5839316Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5839402Z         num_attn_heads: int,
2025-04-11T04:23:18.5839484Z         kv_group_num: int,
2025-04-11T04:23:18.5839566Z         same_context_len: bool,
2025-04-11T04:23:18.5839653Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5839742Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5839827Z     ):
2025-04-11T04:23:18.5839942Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5840142Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5840331Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5840610Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5840778Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5840857Z             return
2025-04-11T04:23:18.5840934Z     
2025-04-11T04:23:18.5841021Z         torch.manual_seed(123)
2025-04-11T04:23:18.5841130Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5841221Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5841312Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5841316Z 
2025-04-11T04:23:18.5841486Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5841597Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5841604Z 
2025-04-11T04:23:18.5841685Z device = None
2025-04-11T04:23:18.5841689Z 
2025-04-11T04:23:18.5841806Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5841958Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5842033Z     
2025-04-11T04:23:18.5842106Z         Args:
2025-04-11T04:23:18.5842273Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5842437Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5842557Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5842630Z         """
2025-04-11T04:23:18.5842711Z         _lazy_init()
2025-04-11T04:23:18.5842814Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5842916Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5843024Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5843312Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5843450Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5843611Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5843615Z 
2025-04-11T04:23:18.5843858Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5844026Z ____________ test_context_attention[False-True-False-1-16-8-32-32] _____________
2025-04-11T04:23:18.5844030Z 
2025-04-11T04:23:18.5844179Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5844331Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5844425Z use_new_kcache_layout = False
2025-04-11T04:23:18.5844429Z 
2025-04-11T04:23:18.5844720Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5844829Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5844954Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5845094Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5845220Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5845335Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5845472Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5845609Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5845760Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5845850Z     def test_context_attention(
2025-04-11T04:23:18.5845925Z         bsz: int,
2025-04-11T04:23:18.5846007Z         block_size: int,
2025-04-11T04:23:18.5846100Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5846183Z         num_attn_heads: int,
2025-04-11T04:23:18.5846275Z         kv_group_num: int,
2025-04-11T04:23:18.5846360Z         same_context_len: bool,
2025-04-11T04:23:18.5846443Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5846535Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5846693Z     ):
2025-04-11T04:23:18.5846809Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5847006Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5847191Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5847372Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5847543Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5847629Z             return
2025-04-11T04:23:18.5847701Z     
2025-04-11T04:23:18.5847790Z         torch.manual_seed(123)
2025-04-11T04:23:18.5847893Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5847984Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5848074Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5848078Z 
2025-04-11T04:23:18.5848244Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5848366Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5848370Z 
2025-04-11T04:23:18.5848450Z device = None
2025-04-11T04:23:18.5848454Z 
2025-04-11T04:23:18.5848576Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5848726Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5848800Z     
2025-04-11T04:23:18.5848875Z         Args:
2025-04-11T04:23:18.5849044Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5849216Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5849328Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5849417Z         """
2025-04-11T04:23:18.5849495Z         _lazy_init()
2025-04-11T04:23:18.5849594Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5849700Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5849807Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5850096Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5850232Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5850392Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5850396Z 
2025-04-11T04:23:18.5850636Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5850809Z ____________ test_context_attention[False-True-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.5850894Z 
2025-04-11T04:23:18.5851048Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5851197Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5851290Z use_new_kcache_layout = False
2025-04-11T04:23:18.5851295Z 
2025-04-11T04:23:18.5851495Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5851601Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5851716Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5851855Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5851969Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5852084Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5852227Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5852368Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5852524Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5852612Z     def test_context_attention(
2025-04-11T04:23:18.5852691Z         bsz: int,
2025-04-11T04:23:18.5852858Z         block_size: int,
2025-04-11T04:23:18.5852950Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5853038Z         num_attn_heads: int,
2025-04-11T04:23:18.5853120Z         kv_group_num: int,
2025-04-11T04:23:18.5853208Z         same_context_len: bool,
2025-04-11T04:23:18.5853291Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5853382Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5853457Z     ):
2025-04-11T04:23:18.5853566Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5853765Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5853943Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5854117Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5854282Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5854367Z             return
2025-04-11T04:23:18.5854440Z     
2025-04-11T04:23:18.5854527Z         torch.manual_seed(123)
2025-04-11T04:23:18.5854629Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5854717Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5854816Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5854820Z 
2025-04-11T04:23:18.5854991Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5855103Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5855118Z 
2025-04-11T04:23:18.5855195Z device = None
2025-04-11T04:23:18.5855199Z 
2025-04-11T04:23:18.5855316Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5855479Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5855550Z     
2025-04-11T04:23:18.5855630Z         Args:
2025-04-11T04:23:18.5855796Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5855963Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5856071Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5856143Z         """
2025-04-11T04:23:18.5856224Z         _lazy_init()
2025-04-11T04:23:18.5856318Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5856421Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5856526Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5856807Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5856945Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5857187Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5857191Z 
2025-04-11T04:23:18.5857440Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5857612Z ____________ test_context_attention[False-True-False-1-16-16-16-32] ____________
2025-04-11T04:23:18.5857616Z 
2025-04-11T04:23:18.5857770Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5857917Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5858007Z use_new_kcache_layout = False
2025-04-11T04:23:18.5858011Z 
2025-04-11T04:23:18.5858208Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5858312Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5858435Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5858575Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5858694Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5858810Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5859050Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5859185Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5859334Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5859425Z     def test_context_attention(
2025-04-11T04:23:18.5859500Z         bsz: int,
2025-04-11T04:23:18.5859584Z         block_size: int,
2025-04-11T04:23:18.5859673Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5859755Z         num_attn_heads: int,
2025-04-11T04:23:18.5859841Z         kv_group_num: int,
2025-04-11T04:23:18.5859927Z         same_context_len: bool,
2025-04-11T04:23:18.5860014Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5860101Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5860182Z     ):
2025-04-11T04:23:18.5860294Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5860491Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5860686Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5860855Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5861021Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5861095Z             return
2025-04-11T04:23:18.5861178Z     
2025-04-11T04:23:18.5861264Z         torch.manual_seed(123)
2025-04-11T04:23:18.5861362Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5861453Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5861541Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5861546Z 
2025-04-11T04:23:18.5861719Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5861829Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5861833Z 
2025-04-11T04:23:18.5861919Z device = None
2025-04-11T04:23:18.5861923Z 
2025-04-11T04:23:18.5862037Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5862183Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5862259Z     
2025-04-11T04:23:18.5862333Z         Args:
2025-04-11T04:23:18.5862503Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5862664Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5862772Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5862845Z         """
2025-04-11T04:23:18.5862922Z         _lazy_init()
2025-04-11T04:23:18.5863019Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5863202Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5863323Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5863616Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5863755Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5863917Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5863921Z 
2025-04-11T04:23:18.5864159Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5864333Z ____________ test_context_attention[False-True-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.5864336Z 
2025-04-11T04:23:18.5864484Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5864634Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5864725Z use_new_kcache_layout = False
2025-04-11T04:23:18.5864729Z 
2025-04-11T04:23:18.5864930Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5865122Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5865240Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5865382Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5865496Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5865615Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5865755Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5865897Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5866049Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5866141Z     def test_context_attention(
2025-04-11T04:23:18.5866223Z         bsz: int,
2025-04-11T04:23:18.5866308Z         block_size: int,
2025-04-11T04:23:18.5866402Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5866485Z         num_attn_heads: int,
2025-04-11T04:23:18.5866566Z         kv_group_num: int,
2025-04-11T04:23:18.5866658Z         same_context_len: bool,
2025-04-11T04:23:18.5866743Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5866835Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5866908Z     ):
2025-04-11T04:23:18.5867025Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5867217Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5867397Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5867570Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5867733Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5867816Z             return
2025-04-11T04:23:18.5867890Z     
2025-04-11T04:23:18.5867977Z         torch.manual_seed(123)
2025-04-11T04:23:18.5868075Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5868166Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5868259Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5868263Z 
2025-04-11T04:23:18.5868468Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5868583Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5868588Z 
2025-04-11T04:23:18.5868665Z device = None
2025-04-11T04:23:18.5868669Z 
2025-04-11T04:23:18.5868788Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5868938Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5869008Z     
2025-04-11T04:23:18.5869085Z         Args:
2025-04-11T04:23:18.5869341Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5869515Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5869620Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5869700Z         """
2025-04-11T04:23:18.5869780Z         _lazy_init()
2025-04-11T04:23:18.5869874Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5869978Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5870080Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5870364Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5870500Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5870667Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5870671Z 
2025-04-11T04:23:18.5870916Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5871093Z ____________ test_context_attention[False-True-False-1-16-16-32-32] ____________
2025-04-11T04:23:18.5871096Z 
2025-04-11T04:23:18.5871251Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5871496Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5871593Z use_new_kcache_layout = False
2025-04-11T04:23:18.5871597Z 
2025-04-11T04:23:18.5871793Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5871899Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5872020Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5872157Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5872277Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5872389Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5872531Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5872665Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5872814Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5872905Z     def test_context_attention(
2025-04-11T04:23:18.5872982Z         bsz: int,
2025-04-11T04:23:18.5873066Z         block_size: int,
2025-04-11T04:23:18.5873155Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5873240Z         num_attn_heads: int,
2025-04-11T04:23:18.5873321Z         kv_group_num: int,
2025-04-11T04:23:18.5873405Z         same_context_len: bool,
2025-04-11T04:23:18.5873491Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5873577Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5873653Z     ):
2025-04-11T04:23:18.5873763Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5873960Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5874141Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5874311Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5874481Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5874554Z             return
2025-04-11T04:23:18.5874629Z     
2025-04-11T04:23:18.5874715Z         torch.manual_seed(123)
2025-04-11T04:23:18.5874814Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5874904Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5874994Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5874997Z 
2025-04-11T04:23:18.5875167Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5875275Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5875279Z 
2025-04-11T04:23:18.5875361Z device = None
2025-04-11T04:23:18.5875449Z 
2025-04-11T04:23:18.5875567Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5875720Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5875797Z     
2025-04-11T04:23:18.5875872Z         Args:
2025-04-11T04:23:18.5876045Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5876215Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5876327Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5876402Z         """
2025-04-11T04:23:18.5876490Z         _lazy_init()
2025-04-11T04:23:18.5876589Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5876691Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5876797Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5877084Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5877223Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5877382Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5877469Z 
2025-04-11T04:23:18.5877716Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5877883Z _____________ test_context_attention[False-True-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.5877887Z 
2025-04-11T04:23:18.5878040Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5878189Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5878277Z use_new_kcache_layout = False
2025-04-11T04:23:18.5878280Z 
2025-04-11T04:23:18.5878484Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5878590Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5878712Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5878849Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5878973Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5879085Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5879218Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5879355Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5879505Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5879596Z     def test_context_attention(
2025-04-11T04:23:18.5879672Z         bsz: int,
2025-04-11T04:23:18.5879754Z         block_size: int,
2025-04-11T04:23:18.5879846Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5879929Z         num_attn_heads: int,
2025-04-11T04:23:18.5880015Z         kv_group_num: int,
2025-04-11T04:23:18.5880103Z         same_context_len: bool,
2025-04-11T04:23:18.5880191Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5880278Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5880350Z     ):
2025-04-11T04:23:18.5880467Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5880658Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5880844Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5881013Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5881179Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5881252Z             return
2025-04-11T04:23:18.5881325Z     
2025-04-11T04:23:18.5881429Z         torch.manual_seed(123)
2025-04-11T04:23:18.5881529Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5881730Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5881826Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5881830Z 
2025-04-11T04:23:18.5882000Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5882117Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5882121Z 
2025-04-11T04:23:18.5882197Z device = None
2025-04-11T04:23:18.5882201Z 
2025-04-11T04:23:18.5882321Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5882468Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5882542Z     
2025-04-11T04:23:18.5882616Z         Args:
2025-04-11T04:23:18.5882780Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5882943Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5883049Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5883129Z         """
2025-04-11T04:23:18.5883205Z         _lazy_init()
2025-04-11T04:23:18.5883302Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5883402Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5883609Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5883894Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5884026Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5884187Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5884191Z 
2025-04-11T04:23:18.5884432Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5884603Z ____________ test_context_attention[False-True-False-4-16-8-16-32] _____________
2025-04-11T04:23:18.5884607Z 
2025-04-11T04:23:18.5884760Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5884909Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5884996Z use_new_kcache_layout = False
2025-04-11T04:23:18.5885003Z 
2025-04-11T04:23:18.5885198Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5885306Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5885422Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5885561Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5885675Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5885790Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5885924Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5886056Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5886212Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5886299Z     def test_context_attention(
2025-04-11T04:23:18.5886377Z         bsz: int,
2025-04-11T04:23:18.5886459Z         block_size: int,
2025-04-11T04:23:18.5886552Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5886645Z         num_attn_heads: int,
2025-04-11T04:23:18.5886729Z         kv_group_num: int,
2025-04-11T04:23:18.5886824Z         same_context_len: bool,
2025-04-11T04:23:18.5886911Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5887001Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5887074Z     ):
2025-04-11T04:23:18.5887185Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5887382Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5887564Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5887828Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5887994Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5888073Z             return
2025-04-11T04:23:18.5888148Z     
2025-04-11T04:23:18.5888234Z         torch.manual_seed(123)
2025-04-11T04:23:18.5888337Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5888425Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5888517Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5888520Z 
2025-04-11T04:23:18.5888688Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5888797Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5888805Z 
2025-04-11T04:23:18.5888882Z device = None
2025-04-11T04:23:18.5888887Z 
2025-04-11T04:23:18.5889001Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5889154Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5889230Z     
2025-04-11T04:23:18.5889307Z         Args:
2025-04-11T04:23:18.5889472Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5889635Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5889828Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5889903Z         """
2025-04-11T04:23:18.5889985Z         _lazy_init()
2025-04-11T04:23:18.5890079Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5890183Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5890286Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5890569Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5890706Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5890869Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5890873Z 
2025-04-11T04:23:18.5891117Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5891289Z _____________ test_context_attention[False-True-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.5891293Z 
2025-04-11T04:23:18.5891447Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5891593Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5891685Z use_new_kcache_layout = False
2025-04-11T04:23:18.5891689Z 
2025-04-11T04:23:18.5891894Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5892007Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5892139Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5892280Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5892400Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5892511Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5892650Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5892788Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5892938Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5893029Z     def test_context_attention(
2025-04-11T04:23:18.5893104Z         bsz: int,
2025-04-11T04:23:18.5893188Z         block_size: int,
2025-04-11T04:23:18.5893276Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5893357Z         num_attn_heads: int,
2025-04-11T04:23:18.5893442Z         kv_group_num: int,
2025-04-11T04:23:18.5893525Z         same_context_len: bool,
2025-04-11T04:23:18.5893612Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5893698Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5893773Z     ):
2025-04-11T04:23:18.5893964Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5894159Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5894343Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5894515Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5894680Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5894754Z             return
2025-04-11T04:23:18.5894828Z     
2025-04-11T04:23:18.5894913Z         torch.manual_seed(123)
2025-04-11T04:23:18.5895011Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5895103Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5895194Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5895198Z 
2025-04-11T04:23:18.5895368Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5895480Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5895484Z 
2025-04-11T04:23:18.5895564Z device = None
2025-04-11T04:23:18.5895567Z 
2025-04-11T04:23:18.5895771Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5895922Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5895997Z     
2025-04-11T04:23:18.5896073Z         Args:
2025-04-11T04:23:18.5896242Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5896407Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5896514Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5896587Z         """
2025-04-11T04:23:18.5896664Z         _lazy_init()
2025-04-11T04:23:18.5896762Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5896861Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5896971Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5897251Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5897388Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5897550Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5897554Z 
2025-04-11T04:23:18.5897790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5897963Z ____________ test_context_attention[False-True-False-4-16-8-32-32] _____________
2025-04-11T04:23:18.5897967Z 
2025-04-11T04:23:18.5898116Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5898265Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5898353Z use_new_kcache_layout = False
2025-04-11T04:23:18.5898356Z 
2025-04-11T04:23:18.5898558Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5898660Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5898779Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5898919Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5899034Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5899148Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5899285Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5899423Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5899572Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5899659Z     def test_context_attention(
2025-04-11T04:23:18.5899738Z         bsz: int,
2025-04-11T04:23:18.5899820Z         block_size: int,
2025-04-11T04:23:18.5899997Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5900085Z         num_attn_heads: int,
2025-04-11T04:23:18.5900166Z         kv_group_num: int,
2025-04-11T04:23:18.5900256Z         same_context_len: bool,
2025-04-11T04:23:18.5900347Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5900440Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5900513Z     ):
2025-04-11T04:23:18.5900630Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5900823Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5901004Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5901180Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5901346Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5901424Z             return
2025-04-11T04:23:18.5901499Z     
2025-04-11T04:23:18.5901598Z         torch.manual_seed(123)
2025-04-11T04:23:18.5901702Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5901790Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5901981Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5901985Z 
2025-04-11T04:23:18.5902154Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5902269Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5902273Z 
2025-04-11T04:23:18.5902351Z device = None
2025-04-11T04:23:18.5902355Z 
2025-04-11T04:23:18.5902475Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5902624Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5902696Z     
2025-04-11T04:23:18.5902774Z         Args:
2025-04-11T04:23:18.5902939Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5903109Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5903216Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5903292Z         """
2025-04-11T04:23:18.5903372Z         _lazy_init()
2025-04-11T04:23:18.5903468Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5903571Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5903676Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5903958Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5904094Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5904257Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5904261Z 
2025-04-11T04:23:18.5904496Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5904667Z ____________ test_context_attention[False-True-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.5904671Z 
2025-04-11T04:23:18.5904825Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5904974Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5905064Z use_new_kcache_layout = False
2025-04-11T04:23:18.5905068Z 
2025-04-11T04:23:18.5905268Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5905373Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5905489Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5905629Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5905743Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5905856Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5905996Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5906231Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5906388Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5906480Z     def test_context_attention(
2025-04-11T04:23:18.5906556Z         bsz: int,
2025-04-11T04:23:18.5906640Z         block_size: int,
2025-04-11T04:23:18.5906729Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5906815Z         num_attn_heads: int,
2025-04-11T04:23:18.5906898Z         kv_group_num: int,
2025-04-11T04:23:18.5906985Z         same_context_len: bool,
2025-04-11T04:23:18.5907070Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5907157Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5907233Z     ):
2025-04-11T04:23:18.5907342Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5907538Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5907723Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5907893Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5908143Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5908218Z             return
2025-04-11T04:23:18.5908295Z     
2025-04-11T04:23:18.5908379Z         torch.manual_seed(123)
2025-04-11T04:23:18.5908518Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5908609Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5908700Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5908704Z 
2025-04-11T04:23:18.5908873Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5908985Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5908989Z 
2025-04-11T04:23:18.5909070Z device = None
2025-04-11T04:23:18.5909074Z 
2025-04-11T04:23:18.5909194Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5909345Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5909419Z     
2025-04-11T04:23:18.5909496Z         Args:
2025-04-11T04:23:18.5909666Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5909830Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5909939Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5910012Z         """
2025-04-11T04:23:18.5910091Z         _lazy_init()
2025-04-11T04:23:18.5910187Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5910287Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5910395Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5910676Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5910825Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5910984Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5910991Z 
2025-04-11T04:23:18.5911237Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5911408Z ____________ test_context_attention[False-True-False-4-16-16-16-32] ____________
2025-04-11T04:23:18.5911412Z 
2025-04-11T04:23:18.5911574Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5911720Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5911807Z use_new_kcache_layout = False
2025-04-11T04:23:18.5911811Z 
2025-04-11T04:23:18.5912012Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5912118Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5912417Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5912558Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5912675Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5912791Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5912926Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5913064Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5913213Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5913303Z     def test_context_attention(
2025-04-11T04:23:18.5913378Z         bsz: int,
2025-04-11T04:23:18.5913461Z         block_size: int,
2025-04-11T04:23:18.5913556Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5913639Z         num_attn_heads: int,
2025-04-11T04:23:18.5913725Z         kv_group_num: int,
2025-04-11T04:23:18.5913810Z         same_context_len: bool,
2025-04-11T04:23:18.5913900Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5913988Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5914062Z     ):
2025-04-11T04:23:18.5914183Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5914476Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5914672Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5914847Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5915018Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5915095Z             return
2025-04-11T04:23:18.5915173Z     
2025-04-11T04:23:18.5915272Z         torch.manual_seed(123)
2025-04-11T04:23:18.5915374Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5915470Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5915566Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5915570Z 
2025-04-11T04:23:18.5915740Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5915858Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5915865Z 
2025-04-11T04:23:18.5915947Z device = None
2025-04-11T04:23:18.5915951Z 
2025-04-11T04:23:18.5916073Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5916225Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5916303Z     
2025-04-11T04:23:18.5916378Z         Args:
2025-04-11T04:23:18.5916549Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5916733Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5916841Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5916920Z         """
2025-04-11T04:23:18.5917004Z         _lazy_init()
2025-04-11T04:23:18.5917105Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5917210Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5917317Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5917610Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5917748Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5917913Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5917918Z 
2025-04-11T04:23:18.5918159Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5918332Z ____________ test_context_attention[False-True-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.5918336Z 
2025-04-11T04:23:18.5918491Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5918725Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5918817Z use_new_kcache_layout = False
2025-04-11T04:23:18.5918821Z 
2025-04-11T04:23:18.5919021Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5919134Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5919250Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5919393Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5919507Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5919622Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5919758Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5919892Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5920046Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5920139Z     def test_context_attention(
2025-04-11T04:23:18.5920224Z         bsz: int,
2025-04-11T04:23:18.5920309Z         block_size: int,
2025-04-11T04:23:18.5920398Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5920573Z         num_attn_heads: int,
2025-04-11T04:23:18.5920658Z         kv_group_num: int,
2025-04-11T04:23:18.5920750Z         same_context_len: bool,
2025-04-11T04:23:18.5920834Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5920928Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5921006Z     ):
2025-04-11T04:23:18.5921116Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5921318Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5921501Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5921676Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5921842Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5921919Z             return
2025-04-11T04:23:18.5921991Z     
2025-04-11T04:23:18.5922077Z         torch.manual_seed(123)
2025-04-11T04:23:18.5922183Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5922272Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5922367Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5922371Z 
2025-04-11T04:23:18.5922540Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5922652Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5922659Z 
2025-04-11T04:23:18.5922736Z device = None
2025-04-11T04:23:18.5922740Z 
2025-04-11T04:23:18.5922856Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5923024Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5923094Z     
2025-04-11T04:23:18.5923171Z         Args:
2025-04-11T04:23:18.5923341Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5923507Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5923620Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5923692Z         """
2025-04-11T04:23:18.5923774Z         _lazy_init()
2025-04-11T04:23:18.5923869Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5923973Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5924076Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5924362Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5924500Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5924659Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5924662Z 
2025-04-11T04:23:18.5924993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5925164Z ____________ test_context_attention[False-True-False-4-16-16-32-32] ____________
2025-04-11T04:23:18.5925172Z 
2025-04-11T04:23:18.5925329Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5925475Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T04:23:18.5925566Z use_new_kcache_layout = False
2025-04-11T04:23:18.5925570Z 
2025-04-11T04:23:18.5925770Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5925871Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5925993Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5926132Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5926259Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5926383Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5926524Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5926666Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5926921Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5927021Z     def test_context_attention(
2025-04-11T04:23:18.5927096Z         bsz: int,
2025-04-11T04:23:18.5927187Z         block_size: int,
2025-04-11T04:23:18.5927276Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5927360Z         num_attn_heads: int,
2025-04-11T04:23:18.5927451Z         kv_group_num: int,
2025-04-11T04:23:18.5927534Z         same_context_len: bool,
2025-04-11T04:23:18.5927627Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5927715Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5927791Z     ):
2025-04-11T04:23:18.5927905Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5928102Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5928287Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5928460Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5928632Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5928708Z             return
2025-04-11T04:23:18.5928785Z     
2025-04-11T04:23:18.5928876Z         torch.manual_seed(123)
2025-04-11T04:23:18.5928975Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5929074Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5929166Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5929169Z 
2025-04-11T04:23:18.5929340Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5929460Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5929464Z 
2025-04-11T04:23:18.5929547Z device = None
2025-04-11T04:23:18.5929551Z 
2025-04-11T04:23:18.5929673Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5929827Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5929904Z     
2025-04-11T04:23:18.5929982Z         Args:
2025-04-11T04:23:18.5930151Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5930321Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5930429Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5930501Z         """
2025-04-11T04:23:18.5930578Z         _lazy_init()
2025-04-11T04:23:18.5930675Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5930776Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5930962Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5931243Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5931378Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5931545Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5931549Z 
2025-04-11T04:23:18.5931790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5931963Z _____________ test_context_attention[False-False-True-1-16-8-16-7] _____________
2025-04-11T04:23:18.5931967Z 
2025-04-11T04:23:18.5932117Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5932269Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5932362Z use_new_kcache_layout = False
2025-04-11T04:23:18.5932366Z 
2025-04-11T04:23:18.5932575Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5932681Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5932800Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5933035Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5933154Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5933271Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5933406Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5933543Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5933695Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5933781Z     def test_context_attention(
2025-04-11T04:23:18.5933861Z         bsz: int,
2025-04-11T04:23:18.5933942Z         block_size: int,
2025-04-11T04:23:18.5934034Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5934120Z         num_attn_heads: int,
2025-04-11T04:23:18.5934204Z         kv_group_num: int,
2025-04-11T04:23:18.5934290Z         same_context_len: bool,
2025-04-11T04:23:18.5934374Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5934473Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5934546Z     ):
2025-04-11T04:23:18.5934658Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5934851Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5935030Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5935204Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5935366Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5935445Z             return
2025-04-11T04:23:18.5935517Z     
2025-04-11T04:23:18.5935607Z         torch.manual_seed(123)
2025-04-11T04:23:18.5935708Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5935795Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5935888Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5935892Z 
2025-04-11T04:23:18.5936063Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5936177Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5936180Z 
2025-04-11T04:23:18.5936258Z device = None
2025-04-11T04:23:18.5936262Z 
2025-04-11T04:23:18.5936382Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5936530Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5936600Z     
2025-04-11T04:23:18.5936679Z         Args:
2025-04-11T04:23:18.5936847Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5937013Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5937201Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5937281Z         """
2025-04-11T04:23:18.5937368Z         _lazy_init()
2025-04-11T04:23:18.5937463Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5937573Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5937677Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5937963Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5938098Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5938262Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5938268Z 
2025-04-11T04:23:18.5938509Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5938683Z ____________ test_context_attention[False-False-True-1-16-8-16-32] _____________
2025-04-11T04:23:18.5938691Z 
2025-04-11T04:23:18.5938842Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5938990Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5939167Z use_new_kcache_layout = False
2025-04-11T04:23:18.5939171Z 
2025-04-11T04:23:18.5939374Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5939479Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5939595Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5939735Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5939850Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5939962Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5940103Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5940244Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5940398Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5940484Z     def test_context_attention(
2025-04-11T04:23:18.5940565Z         bsz: int,
2025-04-11T04:23:18.5940651Z         block_size: int,
2025-04-11T04:23:18.5940740Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5940827Z         num_attn_heads: int,
2025-04-11T04:23:18.5940911Z         kv_group_num: int,
2025-04-11T04:23:18.5941001Z         same_context_len: bool,
2025-04-11T04:23:18.5941085Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5941174Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5941251Z     ):
2025-04-11T04:23:18.5941361Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5941556Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5941737Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5941912Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5942083Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5942162Z             return
2025-04-11T04:23:18.5942238Z     
2025-04-11T04:23:18.5942324Z         torch.manual_seed(123)
2025-04-11T04:23:18.5942424Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5942512Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5942601Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5942605Z 
2025-04-11T04:23:18.5942779Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5942889Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5942893Z 
2025-04-11T04:23:18.5942973Z device = None
2025-04-11T04:23:18.5942977Z 
2025-04-11T04:23:18.5943093Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5943331Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5943406Z     
2025-04-11T04:23:18.5943483Z         Args:
2025-04-11T04:23:18.5943664Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5943832Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5943945Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5944019Z         """
2025-04-11T04:23:18.5944101Z         _lazy_init()
2025-04-11T04:23:18.5944202Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5944301Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5944408Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5944688Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5944830Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5944989Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5944993Z 
2025-04-11T04:23:18.5945235Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5945510Z _____________ test_context_attention[False-False-True-1-16-8-32-7] _____________
2025-04-11T04:23:18.5945515Z 
2025-04-11T04:23:18.5945671Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5945818Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5945906Z use_new_kcache_layout = False
2025-04-11T04:23:18.5945910Z 
2025-04-11T04:23:18.5946118Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5946222Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5946342Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5946482Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5946602Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5946717Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5946857Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5946998Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5947149Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5947241Z     def test_context_attention(
2025-04-11T04:23:18.5947317Z         bsz: int,
2025-04-11T04:23:18.5947399Z         block_size: int,
2025-04-11T04:23:18.5947492Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5947575Z         num_attn_heads: int,
2025-04-11T04:23:18.5947662Z         kv_group_num: int,
2025-04-11T04:23:18.5947746Z         same_context_len: bool,
2025-04-11T04:23:18.5947834Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5947926Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5947999Z     ):
2025-04-11T04:23:18.5948113Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5948305Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5948530Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5948701Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5948869Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5948944Z             return
2025-04-11T04:23:18.5949018Z     
2025-04-11T04:23:18.5949109Z         torch.manual_seed(123)
2025-04-11T04:23:18.5949208Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5949300Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5949388Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5949392Z 
2025-04-11T04:23:18.5949715Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5949839Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5949846Z 
2025-04-11T04:23:18.5949926Z device = None
2025-04-11T04:23:18.5949930Z 
2025-04-11T04:23:18.5950049Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5950199Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5950275Z     
2025-04-11T04:23:18.5950349Z         Args:
2025-04-11T04:23:18.5950514Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5950684Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5950788Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5950863Z         """
2025-04-11T04:23:18.5950941Z         _lazy_init()
2025-04-11T04:23:18.5951042Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5951143Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5951245Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5951532Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5951776Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5951939Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5951943Z 
2025-04-11T04:23:18.5952183Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5952356Z ____________ test_context_attention[False-False-True-1-16-8-32-32] _____________
2025-04-11T04:23:18.5952359Z 
2025-04-11T04:23:18.5952509Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5952657Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5952747Z use_new_kcache_layout = False
2025-04-11T04:23:18.5952751Z 
2025-04-11T04:23:18.5952950Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5953062Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5953176Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5953317Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5953433Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5953551Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5953689Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5953826Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5953985Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5954071Z     def test_context_attention(
2025-04-11T04:23:18.5954157Z         bsz: int,
2025-04-11T04:23:18.5954241Z         block_size: int,
2025-04-11T04:23:18.5954331Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5954423Z         num_attn_heads: int,
2025-04-11T04:23:18.5954505Z         kv_group_num: int,
2025-04-11T04:23:18.5954596Z         same_context_len: bool,
2025-04-11T04:23:18.5954680Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5954771Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5954844Z     ):
2025-04-11T04:23:18.5954953Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5955148Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5955328Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5955498Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5955660Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5955827Z             return
2025-04-11T04:23:18.5955904Z     
2025-04-11T04:23:18.5955993Z         torch.manual_seed(123)
2025-04-11T04:23:18.5956097Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5956191Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5956284Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5956288Z 
2025-04-11T04:23:18.5956456Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5956566Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5956574Z 
2025-04-11T04:23:18.5956651Z device = None
2025-04-11T04:23:18.5956654Z 
2025-04-11T04:23:18.5956770Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5956923Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5956995Z     
2025-04-11T04:23:18.5957072Z         Args:
2025-04-11T04:23:18.5957242Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5957409Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5957514Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5957671Z         """
2025-04-11T04:23:18.5957755Z         _lazy_init()
2025-04-11T04:23:18.5957852Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5957959Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5958065Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5958349Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5958488Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5958646Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5958649Z 
2025-04-11T04:23:18.5958895Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5959063Z ____________ test_context_attention[False-False-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.5959066Z 
2025-04-11T04:23:18.5959226Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5959373Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5959464Z use_new_kcache_layout = False
2025-04-11T04:23:18.5959468Z 
2025-04-11T04:23:18.5959667Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5959770Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5959888Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5960025Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5960143Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5960255Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5960399Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5960535Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5960691Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5960785Z     def test_context_attention(
2025-04-11T04:23:18.5960861Z         bsz: int,
2025-04-11T04:23:18.5960945Z         block_size: int,
2025-04-11T04:23:18.5961034Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5961116Z         num_attn_heads: int,
2025-04-11T04:23:18.5961202Z         kv_group_num: int,
2025-04-11T04:23:18.5961288Z         same_context_len: bool,
2025-04-11T04:23:18.5961375Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5961462Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5961536Z     ):
2025-04-11T04:23:18.5961647Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5961928Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5962116Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5962288Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5962459Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5962533Z             return
2025-04-11T04:23:18.5962611Z     
2025-04-11T04:23:18.5962697Z         torch.manual_seed(123)
2025-04-11T04:23:18.5962796Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5962887Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5962977Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5962980Z 
2025-04-11T04:23:18.5963151Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5963261Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5963265Z 
2025-04-11T04:23:18.5963348Z device = None
2025-04-11T04:23:18.5963351Z 
2025-04-11T04:23:18.5963468Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5963622Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5963788Z     
2025-04-11T04:23:18.5963869Z         Args:
2025-04-11T04:23:18.5964047Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5964217Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5964335Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5964411Z         """
2025-04-11T04:23:18.5964494Z         _lazy_init()
2025-04-11T04:23:18.5964603Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5964707Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5964819Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5965110Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5965248Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5965416Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5965420Z 
2025-04-11T04:23:18.5965664Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5965840Z ____________ test_context_attention[False-False-True-1-16-16-16-32] ____________
2025-04-11T04:23:18.5965844Z 
2025-04-11T04:23:18.5965999Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5966152Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5966246Z use_new_kcache_layout = False
2025-04-11T04:23:18.5966250Z 
2025-04-11T04:23:18.5966454Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5966567Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5966686Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5966832Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5966955Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5967075Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5967214Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5967354Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5967509Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5967600Z     def test_context_attention(
2025-04-11T04:23:18.5967686Z         bsz: int,
2025-04-11T04:23:18.5967770Z         block_size: int,
2025-04-11T04:23:18.5967867Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5967956Z         num_attn_heads: int,
2025-04-11T04:23:18.5968047Z         kv_group_num: int,
2025-04-11T04:23:18.5968223Z         same_context_len: bool,
2025-04-11T04:23:18.5968313Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5968403Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5968480Z     ):
2025-04-11T04:23:18.5968594Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5968788Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5968968Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5969139Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5969301Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5969378Z             return
2025-04-11T04:23:18.5969450Z     
2025-04-11T04:23:18.5969540Z         torch.manual_seed(123)
2025-04-11T04:23:18.5969638Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5969729Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5969824Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5969827Z 
2025-04-11T04:23:18.5969993Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5970193Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5970197Z 
2025-04-11T04:23:18.5970275Z device = None
2025-04-11T04:23:18.5970280Z 
2025-04-11T04:23:18.5970398Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5970549Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5970621Z     
2025-04-11T04:23:18.5970697Z         Args:
2025-04-11T04:23:18.5970861Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5971028Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5971138Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5971215Z         """
2025-04-11T04:23:18.5971291Z         _lazy_init()
2025-04-11T04:23:18.5971386Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5971490Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5971599Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5971889Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5972033Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5972208Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5972211Z 
2025-04-11T04:23:18.5972455Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5972633Z ____________ test_context_attention[False-False-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.5972643Z 
2025-04-11T04:23:18.5972797Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5972942Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5973047Z use_new_kcache_layout = False
2025-04-11T04:23:18.5973055Z 
2025-04-11T04:23:18.5973254Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5973360Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5973478Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5973618Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5973734Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5973847Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5973990Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5974125Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5974378Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5974470Z     def test_context_attention(
2025-04-11T04:23:18.5974545Z         bsz: int,
2025-04-11T04:23:18.5974631Z         block_size: int,
2025-04-11T04:23:18.5974724Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5974810Z         num_attn_heads: int,
2025-04-11T04:23:18.5974892Z         kv_group_num: int,
2025-04-11T04:23:18.5974979Z         same_context_len: bool,
2025-04-11T04:23:18.5975061Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5975148Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5975224Z     ):
2025-04-11T04:23:18.5975332Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5975530Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5975712Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5975885Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5976052Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5976127Z             return
2025-04-11T04:23:18.5976291Z     
2025-04-11T04:23:18.5976379Z         torch.manual_seed(123)
2025-04-11T04:23:18.5976480Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5976568Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5976658Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5976662Z 
2025-04-11T04:23:18.5976832Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5976943Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5976946Z 
2025-04-11T04:23:18.5977028Z device = None
2025-04-11T04:23:18.5977033Z 
2025-04-11T04:23:18.5977151Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5977307Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5977379Z     
2025-04-11T04:23:18.5977454Z         Args:
2025-04-11T04:23:18.5977623Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5977790Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5977899Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5977973Z         """
2025-04-11T04:23:18.5978053Z         _lazy_init()
2025-04-11T04:23:18.5978149Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5978250Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5978357Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5978636Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5978773Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5978933Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5978937Z 
2025-04-11T04:23:18.5979181Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5979354Z ____________ test_context_attention[False-False-True-1-16-16-32-32] ____________
2025-04-11T04:23:18.5979358Z 
2025-04-11T04:23:18.5979512Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.5979660Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5979747Z use_new_kcache_layout = False
2025-04-11T04:23:18.5979751Z 
2025-04-11T04:23:18.5979953Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5980057Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5980176Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5980313Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5980516Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5980631Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5980768Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5980911Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5981061Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5981150Z     def test_context_attention(
2025-04-11T04:23:18.5981226Z         bsz: int,
2025-04-11T04:23:18.5981307Z         block_size: int,
2025-04-11T04:23:18.5981400Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5981482Z         num_attn_heads: int,
2025-04-11T04:23:18.5981569Z         kv_group_num: int,
2025-04-11T04:23:18.5981653Z         same_context_len: bool,
2025-04-11T04:23:18.5981742Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5981832Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5981904Z     ):
2025-04-11T04:23:18.5982020Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5982214Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5982487Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5982657Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5982825Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5982909Z             return
2025-04-11T04:23:18.5982982Z     
2025-04-11T04:23:18.5983072Z         torch.manual_seed(123)
2025-04-11T04:23:18.5983171Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5983268Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5983358Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5983363Z 
2025-04-11T04:23:18.5983533Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5983653Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5983657Z 
2025-04-11T04:23:18.5983736Z device = None
2025-04-11T04:23:18.5983745Z 
2025-04-11T04:23:18.5983876Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5984027Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5984102Z     
2025-04-11T04:23:18.5984178Z         Args:
2025-04-11T04:23:18.5984343Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5984513Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5984618Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5984694Z         """
2025-04-11T04:23:18.5984771Z         _lazy_init()
2025-04-11T04:23:18.5984868Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5984973Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5985078Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5985368Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5985505Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5985668Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5985671Z 
2025-04-11T04:23:18.5985909Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5986080Z _____________ test_context_attention[False-False-True-4-16-8-16-7] _____________
2025-04-11T04:23:18.5986083Z 
2025-04-11T04:23:18.5986233Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5986383Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5986470Z use_new_kcache_layout = False
2025-04-11T04:23:18.5986560Z 
2025-04-11T04:23:18.5986761Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5986870Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5986990Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5987130Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5987246Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5987363Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5987498Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5987629Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5987782Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5987870Z     def test_context_attention(
2025-04-11T04:23:18.5987949Z         bsz: int,
2025-04-11T04:23:18.5988030Z         block_size: int,
2025-04-11T04:23:18.5988122Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5988208Z         num_attn_heads: int,
2025-04-11T04:23:18.5988290Z         kv_group_num: int,
2025-04-11T04:23:18.5988377Z         same_context_len: bool,
2025-04-11T04:23:18.5988580Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5988671Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5988744Z     ):
2025-04-11T04:23:18.5988852Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5989048Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5989230Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5989404Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5989567Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5989645Z             return
2025-04-11T04:23:18.5989721Z     
2025-04-11T04:23:18.5989807Z         torch.manual_seed(123)
2025-04-11T04:23:18.5989908Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5989996Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5990090Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5990094Z 
2025-04-11T04:23:18.5990261Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5990372Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5990379Z 
2025-04-11T04:23:18.5990456Z device = None
2025-04-11T04:23:18.5990460Z 
2025-04-11T04:23:18.5990574Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5990728Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5990799Z     
2025-04-11T04:23:18.5990875Z         Args:
2025-04-11T04:23:18.5991041Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5991213Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5991319Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5991391Z         """
2025-04-11T04:23:18.5991476Z         _lazy_init()
2025-04-11T04:23:18.5991572Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5991677Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5991781Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5992065Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5992203Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5992361Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5992365Z 
2025-04-11T04:23:18.5992609Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5992871Z ____________ test_context_attention[False-False-True-4-16-8-16-32] _____________
2025-04-11T04:23:18.5992876Z 
2025-04-11T04:23:18.5993031Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5993180Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5993270Z use_new_kcache_layout = False
2025-04-11T04:23:18.5993275Z 
2025-04-11T04:23:18.5993473Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.5993576Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.5993695Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.5993832Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.5993950Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.5994063Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.5994205Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.5994339Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.5994489Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.5994673Z     def test_context_attention(
2025-04-11T04:23:18.5994748Z         bsz: int,
2025-04-11T04:23:18.5994834Z         block_size: int,
2025-04-11T04:23:18.5994923Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.5995012Z         num_attn_heads: int,
2025-04-11T04:23:18.5995099Z         kv_group_num: int,
2025-04-11T04:23:18.5995183Z         same_context_len: bool,
2025-04-11T04:23:18.5995278Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.5995370Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.5995446Z     ):
2025-04-11T04:23:18.5995558Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.5995755Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.5995943Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.5996114Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.5996284Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.5996358Z             return
2025-04-11T04:23:18.5996432Z     
2025-04-11T04:23:18.5996517Z         torch.manual_seed(123)
2025-04-11T04:23:18.5996614Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.5996705Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.5996795Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.5996799Z 
2025-04-11T04:23:18.5996968Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.5997078Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.5997082Z 
2025-04-11T04:23:18.5997162Z device = None
2025-04-11T04:23:18.5997166Z 
2025-04-11T04:23:18.5997284Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.5997433Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5997507Z     
2025-04-11T04:23:18.5997583Z         Args:
2025-04-11T04:23:18.5997751Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.5997917Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.5998024Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.5998095Z         """
2025-04-11T04:23:18.5998171Z         _lazy_init()
2025-04-11T04:23:18.5998268Z         with torch.cuda.device(device):
2025-04-11T04:23:18.5998368Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.5998476Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.5998862Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.5999003Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.5999162Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5999170Z 
2025-04-11T04:23:18.5999405Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.5999575Z _____________ test_context_attention[False-False-True-4-16-8-32-7] _____________
2025-04-11T04:23:18.5999579Z 
2025-04-11T04:23:18.5999731Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.5999881Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.5999969Z use_new_kcache_layout = False
2025-04-11T04:23:18.5999973Z 
2025-04-11T04:23:18.6000173Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6000276Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6000397Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6000534Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6000648Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6000847Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6000983Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6001122Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6001272Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6001360Z     def test_context_attention(
2025-04-11T04:23:18.6001438Z         bsz: int,
2025-04-11T04:23:18.6001519Z         block_size: int,
2025-04-11T04:23:18.6001609Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6001693Z         num_attn_heads: int,
2025-04-11T04:23:18.6001779Z         kv_group_num: int,
2025-04-11T04:23:18.6001861Z         same_context_len: bool,
2025-04-11T04:23:18.6001948Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6002039Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6002111Z     ):
2025-04-11T04:23:18.6002223Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6002416Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6002598Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6002773Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6002937Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6003016Z             return
2025-04-11T04:23:18.6003087Z     
2025-04-11T04:23:18.6003176Z         torch.manual_seed(123)
2025-04-11T04:23:18.6003274Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6003361Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6003458Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6003461Z 
2025-04-11T04:23:18.6003628Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6003743Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6003750Z 
2025-04-11T04:23:18.6003828Z device = None
2025-04-11T04:23:18.6003832Z 
2025-04-11T04:23:18.6003950Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6004100Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6004171Z     
2025-04-11T04:23:18.6004248Z         Args:
2025-04-11T04:23:18.6004412Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6004582Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6004686Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6004761Z         """
2025-04-11T04:23:18.6004941Z         _lazy_init()
2025-04-11T04:23:18.6005039Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6005142Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6005250Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6005537Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6005673Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6005833Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6005837Z 
2025-04-11T04:23:18.6006080Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6006249Z ____________ test_context_attention[False-False-True-4-16-8-32-32] _____________
2025-04-11T04:23:18.6006257Z 
2025-04-11T04:23:18.6006408Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6006556Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6006650Z use_new_kcache_layout = False
2025-04-11T04:23:18.6006654Z 
2025-04-11T04:23:18.6007029Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6007137Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6007254Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6007406Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6007529Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6007643Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6007792Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6007929Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6008089Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6008180Z     def test_context_attention(
2025-04-11T04:23:18.6008256Z         bsz: int,
2025-04-11T04:23:18.6008340Z         block_size: int,
2025-04-11T04:23:18.6008429Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6008518Z         num_attn_heads: int,
2025-04-11T04:23:18.6008601Z         kv_group_num: int,
2025-04-11T04:23:18.6008688Z         same_context_len: bool,
2025-04-11T04:23:18.6008770Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6008857Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6008933Z     ):
2025-04-11T04:23:18.6009041Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6009235Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6009417Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6009585Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6009750Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6009824Z             return
2025-04-11T04:23:18.6009900Z     
2025-04-11T04:23:18.6009985Z         torch.manual_seed(123)
2025-04-11T04:23:18.6010089Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6010177Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6010267Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6010271Z 
2025-04-11T04:23:18.6010439Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6010548Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6010551Z 
2025-04-11T04:23:18.6010631Z device = None
2025-04-11T04:23:18.6010635Z 
2025-04-11T04:23:18.6010747Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6010899Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6010970Z     
2025-04-11T04:23:18.6011129Z         Args:
2025-04-11T04:23:18.6011301Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6011468Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6011582Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6011655Z         """
2025-04-11T04:23:18.6011735Z         _lazy_init()
2025-04-11T04:23:18.6011830Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6011932Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6012040Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6012321Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6012459Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6012617Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6012624Z 
2025-04-11T04:23:18.6012866Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6013034Z ____________ test_context_attention[False-False-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.6013121Z 
2025-04-11T04:23:18.6013278Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6013423Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6013509Z use_new_kcache_layout = False
2025-04-11T04:23:18.6013513Z 
2025-04-11T04:23:18.6013715Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6013817Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6013934Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6014070Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6014191Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6014302Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6014438Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6014578Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6014728Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6014819Z     def test_context_attention(
2025-04-11T04:23:18.6014895Z         bsz: int,
2025-04-11T04:23:18.6014976Z         block_size: int,
2025-04-11T04:23:18.6015067Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6015151Z         num_attn_heads: int,
2025-04-11T04:23:18.6015237Z         kv_group_num: int,
2025-04-11T04:23:18.6015321Z         same_context_len: bool,
2025-04-11T04:23:18.6015408Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6015495Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6015567Z     ):
2025-04-11T04:23:18.6015679Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6015873Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6016057Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6016228Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6016393Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6016469Z             return
2025-04-11T04:23:18.6016546Z     
2025-04-11T04:23:18.6016634Z         torch.manual_seed(123)
2025-04-11T04:23:18.6016734Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6016833Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6016921Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6016924Z 
2025-04-11T04:23:18.6017091Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6017296Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6017300Z 
2025-04-11T04:23:18.6017380Z device = None
2025-04-11T04:23:18.6017384Z 
2025-04-11T04:23:18.6017504Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6017658Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6017738Z     
2025-04-11T04:23:18.6017811Z         Args:
2025-04-11T04:23:18.6017979Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6018148Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6018253Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6018330Z         """
2025-04-11T04:23:18.6018408Z         _lazy_init()
2025-04-11T04:23:18.6018506Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6018609Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6018715Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6018999Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6019240Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6019404Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6019408Z 
2025-04-11T04:23:18.6019646Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6019818Z ____________ test_context_attention[False-False-True-4-16-16-16-32] ____________
2025-04-11T04:23:18.6019823Z 
2025-04-11T04:23:18.6019975Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6020122Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6020210Z use_new_kcache_layout = False
2025-04-11T04:23:18.6020214Z 
2025-04-11T04:23:18.6020416Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6020524Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6020641Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6020785Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6020900Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6021016Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6021154Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6021289Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6021446Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6021534Z     def test_context_attention(
2025-04-11T04:23:18.6021613Z         bsz: int,
2025-04-11T04:23:18.6021695Z         block_size: int,
2025-04-11T04:23:18.6021788Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6021876Z         num_attn_heads: int,
2025-04-11T04:23:18.6021961Z         kv_group_num: int,
2025-04-11T04:23:18.6022048Z         same_context_len: bool,
2025-04-11T04:23:18.6022131Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6022228Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6022300Z     ):
2025-04-11T04:23:18.6022408Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6022604Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6022784Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6022957Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6023120Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6023198Z             return
2025-04-11T04:23:18.6023271Z     
2025-04-11T04:23:18.6023441Z         torch.manual_seed(123)
2025-04-11T04:23:18.6023546Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6023636Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6023730Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6023740Z 
2025-04-11T04:23:18.6023907Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6024020Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6024024Z 
2025-04-11T04:23:18.6024103Z device = None
2025-04-11T04:23:18.6024107Z 
2025-04-11T04:23:18.6024223Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6024376Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6024446Z     
2025-04-11T04:23:18.6024523Z         Args:
2025-04-11T04:23:18.6024692Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6024866Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6024972Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6025047Z         """
2025-04-11T04:23:18.6025134Z         _lazy_init()
2025-04-11T04:23:18.6025313Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6025421Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6025529Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6025826Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6025965Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6026123Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6026126Z 
2025-04-11T04:23:18.6026366Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6026540Z ____________ test_context_attention[False-False-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.6026543Z 
2025-04-11T04:23:18.6026698Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6026843Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6026936Z use_new_kcache_layout = False
2025-04-11T04:23:18.6026940Z 
2025-04-11T04:23:18.6027137Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6027240Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6027358Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6027494Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6027613Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6027725Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6027863Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6028001Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6028153Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6028245Z     def test_context_attention(
2025-04-11T04:23:18.6028326Z         bsz: int,
2025-04-11T04:23:18.6028443Z         block_size: int,
2025-04-11T04:23:18.6028533Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6028620Z         num_attn_heads: int,
2025-04-11T04:23:18.6028709Z         kv_group_num: int,
2025-04-11T04:23:18.6028793Z         same_context_len: bool,
2025-04-11T04:23:18.6028880Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6028968Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6029043Z     ):
2025-04-11T04:23:18.6029154Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6029346Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6029624Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6029799Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6029968Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6030046Z             return
2025-04-11T04:23:18.6030121Z     
2025-04-11T04:23:18.6030207Z         torch.manual_seed(123)
2025-04-11T04:23:18.6030308Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6030400Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6030491Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6030494Z 
2025-04-11T04:23:18.6030665Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6030776Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6030779Z 
2025-04-11T04:23:18.6030860Z device = None
2025-04-11T04:23:18.6030864Z 
2025-04-11T04:23:18.6030981Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6031134Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6031208Z     
2025-04-11T04:23:18.6031280Z         Args:
2025-04-11T04:23:18.6031544Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6031714Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6031826Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6031901Z         """
2025-04-11T04:23:18.6031979Z         _lazy_init()
2025-04-11T04:23:18.6032083Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6032184Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6032290Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6032575Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6032720Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6032878Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6032882Z 
2025-04-11T04:23:18.6033124Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6033295Z ____________ test_context_attention[False-False-True-4-16-16-32-32] ____________
2025-04-11T04:23:18.6033299Z 
2025-04-11T04:23:18.6033450Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6033599Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T04:23:18.6033687Z use_new_kcache_layout = False
2025-04-11T04:23:18.6033691Z 
2025-04-11T04:23:18.6033892Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6033997Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6034114Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6034255Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6034371Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6034489Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6034630Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6034767Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6034918Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6035005Z     def test_context_attention(
2025-04-11T04:23:18.6035084Z         bsz: int,
2025-04-11T04:23:18.6035165Z         block_size: int,
2025-04-11T04:23:18.6035256Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6035338Z         num_attn_heads: int,
2025-04-11T04:23:18.6035422Z         kv_group_num: int,
2025-04-11T04:23:18.6035508Z         same_context_len: bool,
2025-04-11T04:23:18.6035592Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6035765Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6035839Z     ):
2025-04-11T04:23:18.6035952Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6036144Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6036328Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6036498Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6036659Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6036737Z             return
2025-04-11T04:23:18.6036808Z     
2025-04-11T04:23:18.6036897Z         torch.manual_seed(123)
2025-04-11T04:23:18.6036994Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6037082Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6037176Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6037183Z 
2025-04-11T04:23:18.6037350Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6037463Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6037550Z 
2025-04-11T04:23:18.6037631Z device = None
2025-04-11T04:23:18.6037635Z 
2025-04-11T04:23:18.6037754Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6037904Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6037975Z     
2025-04-11T04:23:18.6038054Z         Args:
2025-04-11T04:23:18.6038218Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6038385Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6038488Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6038565Z         """
2025-04-11T04:23:18.6038642Z         _lazy_init()
2025-04-11T04:23:18.6038740Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6038850Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6038952Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6039248Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6039386Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6039555Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6039558Z 
2025-04-11T04:23:18.6039796Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6039961Z ____________ test_context_attention[False-False-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.6039969Z 
2025-04-11T04:23:18.6040119Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6040270Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6040362Z use_new_kcache_layout = False
2025-04-11T04:23:18.6040366Z 
2025-04-11T04:23:18.6040563Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6040674Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6040789Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6040929Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6041043Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6041155Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6041293Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6041424Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6041576Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6041662Z     def test_context_attention(
2025-04-11T04:23:18.6041818Z         bsz: int,
2025-04-11T04:23:18.6041908Z         block_size: int,
2025-04-11T04:23:18.6042000Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6042085Z         num_attn_heads: int,
2025-04-11T04:23:18.6042173Z         kv_group_num: int,
2025-04-11T04:23:18.6042259Z         same_context_len: bool,
2025-04-11T04:23:18.6042343Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6042429Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6042505Z     ):
2025-04-11T04:23:18.6042615Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6042814Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6042998Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6043168Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6043339Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6043414Z             return
2025-04-11T04:23:18.6043490Z     
2025-04-11T04:23:18.6043576Z         torch.manual_seed(123)
2025-04-11T04:23:18.6043677Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6043872Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6043962Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6043966Z 
2025-04-11T04:23:18.6044137Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6044247Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6044251Z 
2025-04-11T04:23:18.6044333Z device = None
2025-04-11T04:23:18.6044337Z 
2025-04-11T04:23:18.6044453Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6044603Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6044674Z     
2025-04-11T04:23:18.6044746Z         Args:
2025-04-11T04:23:18.6044919Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6045080Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6045190Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6045263Z         """
2025-04-11T04:23:18.6045343Z         _lazy_init()
2025-04-11T04:23:18.6045436Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6045538Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6045645Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6045927Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6046065Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6046225Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6046229Z 
2025-04-11T04:23:18.6046471Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6046640Z ____________ test_context_attention[False-False-False-1-16-8-16-32] ____________
2025-04-11T04:23:18.6046647Z 
2025-04-11T04:23:18.6046800Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6046947Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6047035Z use_new_kcache_layout = False
2025-04-11T04:23:18.6047039Z 
2025-04-11T04:23:18.6047239Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6047341Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6047459Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6047594Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6047711Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6047907Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6048047Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6048187Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6048338Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6048430Z     def test_context_attention(
2025-04-11T04:23:18.6048506Z         bsz: int,
2025-04-11T04:23:18.6048589Z         block_size: int,
2025-04-11T04:23:18.6048680Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6048764Z         num_attn_heads: int,
2025-04-11T04:23:18.6048850Z         kv_group_num: int,
2025-04-11T04:23:18.6048935Z         same_context_len: bool,
2025-04-11T04:23:18.6049022Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6049108Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6049180Z     ):
2025-04-11T04:23:18.6049292Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6049486Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6049669Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6049924Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6050088Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6050164Z             return
2025-04-11T04:23:18.6050236Z     
2025-04-11T04:23:18.6050327Z         torch.manual_seed(123)
2025-04-11T04:23:18.6050426Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6050516Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6050605Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6050609Z 
2025-04-11T04:23:18.6050776Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6050893Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6050901Z 
2025-04-11T04:23:18.6050978Z device = None
2025-04-11T04:23:18.6050982Z 
2025-04-11T04:23:18.6051108Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6051258Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6051336Z     
2025-04-11T04:23:18.6051409Z         Args:
2025-04-11T04:23:18.6051577Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6051759Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6051868Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6051956Z         """
2025-04-11T04:23:18.6052033Z         _lazy_init()
2025-04-11T04:23:18.6052131Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6052242Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6052345Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6052638Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6052774Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6052943Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6052947Z 
2025-04-11T04:23:18.6053186Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6053361Z ____________ test_context_attention[False-False-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.6053365Z 
2025-04-11T04:23:18.6053517Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6053666Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6053754Z use_new_kcache_layout = False
2025-04-11T04:23:18.6053758Z 
2025-04-11T04:23:18.6054042Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6054150Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6054266Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6054407Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6054524Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6054638Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6054771Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6054903Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6055057Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6055142Z     def test_context_attention(
2025-04-11T04:23:18.6055221Z         bsz: int,
2025-04-11T04:23:18.6055302Z         block_size: int,
2025-04-11T04:23:18.6055396Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6055477Z         num_attn_heads: int,
2025-04-11T04:23:18.6055562Z         kv_group_num: int,
2025-04-11T04:23:18.6055647Z         same_context_len: bool,
2025-04-11T04:23:18.6055731Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6055820Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6055977Z     ):
2025-04-11T04:23:18.6056088Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6056288Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6056471Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6056644Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6056807Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6056885Z             return
2025-04-11T04:23:18.6056956Z     
2025-04-11T04:23:18.6057042Z         torch.manual_seed(123)
2025-04-11T04:23:18.6057148Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6057236Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6057330Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6057334Z 
2025-04-11T04:23:18.6057505Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6057622Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6057626Z 
2025-04-11T04:23:18.6057704Z device = None
2025-04-11T04:23:18.6057708Z 
2025-04-11T04:23:18.6057825Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6057978Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6058048Z     
2025-04-11T04:23:18.6058125Z         Args:
2025-04-11T04:23:18.6058292Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6058458Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6058566Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6058639Z         """
2025-04-11T04:23:18.6058719Z         _lazy_init()
2025-04-11T04:23:18.6058814Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6058921Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6059027Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6059309Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6059448Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6059608Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6059612Z 
2025-04-11T04:23:18.6059856Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6060024Z ____________ test_context_attention[False-False-False-1-16-8-32-32] ____________
2025-04-11T04:23:18.6060028Z 
2025-04-11T04:23:18.6060262Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6060412Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6060509Z use_new_kcache_layout = False
2025-04-11T04:23:18.6060513Z 
2025-04-11T04:23:18.6060710Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6060814Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6060932Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6061068Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6061186Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6061298Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6061436Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6061569Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6061720Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6061813Z     def test_context_attention(
2025-04-11T04:23:18.6061890Z         bsz: int,
2025-04-11T04:23:18.6062075Z         block_size: int,
2025-04-11T04:23:18.6062168Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6062264Z         num_attn_heads: int,
2025-04-11T04:23:18.6062348Z         kv_group_num: int,
2025-04-11T04:23:18.6062434Z         same_context_len: bool,
2025-04-11T04:23:18.6062530Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6062619Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6062695Z     ):
2025-04-11T04:23:18.6062805Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6062999Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6063184Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6063359Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6063526Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6063603Z             return
2025-04-11T04:23:18.6063678Z     
2025-04-11T04:23:18.6063763Z         torch.manual_seed(123)
2025-04-11T04:23:18.6063861Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6063955Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6064046Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6064050Z 
2025-04-11T04:23:18.6064219Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6064330Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6064334Z 
2025-04-11T04:23:18.6064413Z device = None
2025-04-11T04:23:18.6064417Z 
2025-04-11T04:23:18.6064535Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6064690Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6064764Z     
2025-04-11T04:23:18.6064837Z         Args:
2025-04-11T04:23:18.6065009Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6065177Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6065284Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6065358Z         """
2025-04-11T04:23:18.6065434Z         _lazy_init()
2025-04-11T04:23:18.6065533Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6065633Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6065741Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6066024Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6066165Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6066443Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6066448Z 
2025-04-11T04:23:18.6066692Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6066867Z ____________ test_context_attention[False-False-False-1-16-16-16-7] ____________
2025-04-11T04:23:18.6066871Z 
2025-04-11T04:23:18.6067022Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6067171Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6067260Z use_new_kcache_layout = False
2025-04-11T04:23:18.6067264Z 
2025-04-11T04:23:18.6067467Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6067571Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6067688Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6067829Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6067945Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6068060Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6068280Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6068443Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6068591Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6068681Z     def test_context_attention(
2025-04-11T04:23:18.6068760Z         bsz: int,
2025-04-11T04:23:18.6068842Z         block_size: int,
2025-04-11T04:23:18.6068933Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6069015Z         num_attn_heads: int,
2025-04-11T04:23:18.6069101Z         kv_group_num: int,
2025-04-11T04:23:18.6069183Z         same_context_len: bool,
2025-04-11T04:23:18.6069267Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6069356Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6069432Z     ):
2025-04-11T04:23:18.6069544Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6069737Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6069919Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6070089Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6070249Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6070327Z             return
2025-04-11T04:23:18.6070398Z     
2025-04-11T04:23:18.6070486Z         torch.manual_seed(123)
2025-04-11T04:23:18.6070583Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6070670Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6070763Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6070767Z 
2025-04-11T04:23:18.6070936Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6071049Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6071053Z 
2025-04-11T04:23:18.6071131Z device = None
2025-04-11T04:23:18.6071139Z 
2025-04-11T04:23:18.6071258Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6071405Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6071476Z     
2025-04-11T04:23:18.6071553Z         Args:
2025-04-11T04:23:18.6071718Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6071883Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6071986Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6072063Z         """
2025-04-11T04:23:18.6072139Z         _lazy_init()
2025-04-11T04:23:18.6072234Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6072431Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6072539Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6072824Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6072962Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6073124Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6073128Z 
2025-04-11T04:23:18.6073365Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6073536Z ___________ test_context_attention[False-False-False-1-16-16-16-32] ____________
2025-04-11T04:23:18.6073544Z 
2025-04-11T04:23:18.6073695Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6073839Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6073935Z use_new_kcache_layout = False
2025-04-11T04:23:18.6073939Z 
2025-04-11T04:23:18.6074142Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6074346Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6074464Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6074605Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6074719Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6074832Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6074972Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6075108Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6075260Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6075346Z     def test_context_attention(
2025-04-11T04:23:18.6075421Z         bsz: int,
2025-04-11T04:23:18.6075509Z         block_size: int,
2025-04-11T04:23:18.6075597Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6075683Z         num_attn_heads: int,
2025-04-11T04:23:18.6075765Z         kv_group_num: int,
2025-04-11T04:23:18.6075854Z         same_context_len: bool,
2025-04-11T04:23:18.6075939Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6076027Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6076103Z     ):
2025-04-11T04:23:18.6076211Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6076406Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6076588Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6076763Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6076926Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6077003Z             return
2025-04-11T04:23:18.6077079Z     
2025-04-11T04:23:18.6077164Z         torch.manual_seed(123)
2025-04-11T04:23:18.6077264Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6077356Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6077445Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6077449Z 
2025-04-11T04:23:18.6077619Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6077728Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6077732Z 
2025-04-11T04:23:18.6077811Z device = None
2025-04-11T04:23:18.6077814Z 
2025-04-11T04:23:18.6077929Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6078083Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6078153Z     
2025-04-11T04:23:18.6078226Z         Args:
2025-04-11T04:23:18.6078484Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6078651Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6078761Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6078839Z         """
2025-04-11T04:23:18.6078919Z         _lazy_init()
2025-04-11T04:23:18.6079017Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6079119Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6079227Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6079509Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6079647Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6079804Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6079807Z 
2025-04-11T04:23:18.6080050Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6080219Z ____________ test_context_attention[False-False-False-1-16-16-32-7] ____________
2025-04-11T04:23:18.6080222Z 
2025-04-11T04:23:18.6080378Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6080612Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6080700Z use_new_kcache_layout = False
2025-04-11T04:23:18.6080704Z 
2025-04-11T04:23:18.6080907Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6081010Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6081129Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6081265Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6081383Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6081496Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6081636Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6081774Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6081924Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6082022Z     def test_context_attention(
2025-04-11T04:23:18.6082098Z         bsz: int,
2025-04-11T04:23:18.6082180Z         block_size: int,
2025-04-11T04:23:18.6082273Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6082357Z         num_attn_heads: int,
2025-04-11T04:23:18.6082444Z         kv_group_num: int,
2025-04-11T04:23:18.6082528Z         same_context_len: bool,
2025-04-11T04:23:18.6082615Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6082702Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6082774Z     ):
2025-04-11T04:23:18.6082888Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6083083Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6083269Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6083440Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6083608Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6083682Z             return
2025-04-11T04:23:18.6083753Z     
2025-04-11T04:23:18.6083842Z         torch.manual_seed(123)
2025-04-11T04:23:18.6083941Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6084032Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6084124Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6084127Z 
2025-04-11T04:23:18.6084294Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6084407Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6084411Z 
2025-04-11T04:23:18.6084488Z device = None
2025-04-11T04:23:18.6084577Z 
2025-04-11T04:23:18.6084701Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6084850Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6084929Z     
2025-04-11T04:23:18.6085012Z         Args:
2025-04-11T04:23:18.6085179Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6085354Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6085460Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6085543Z         """
2025-04-11T04:23:18.6085623Z         _lazy_init()
2025-04-11T04:23:18.6085723Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6085824Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6085930Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6086220Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6086356Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6086518Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6086606Z 
2025-04-11T04:23:18.6086845Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6087024Z ___________ test_context_attention[False-False-False-1-16-16-32-32] ____________
2025-04-11T04:23:18.6087027Z 
2025-04-11T04:23:18.6087180Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6087328Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6087418Z use_new_kcache_layout = False
2025-04-11T04:23:18.6087421Z 
2025-04-11T04:23:18.6087616Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6087727Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6087842Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6087984Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6088102Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6088216Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6088351Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6088485Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6088640Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6088728Z     def test_context_attention(
2025-04-11T04:23:18.6088805Z         bsz: int,
2025-04-11T04:23:18.6088887Z         block_size: int,
2025-04-11T04:23:18.6088978Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6089060Z         num_attn_heads: int,
2025-04-11T04:23:18.6089141Z         kv_group_num: int,
2025-04-11T04:23:18.6089233Z         same_context_len: bool,
2025-04-11T04:23:18.6089316Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6089407Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6089479Z     ):
2025-04-11T04:23:18.6089592Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6089788Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6089968Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6090141Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6090303Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6090379Z             return
2025-04-11T04:23:18.6090451Z     
2025-04-11T04:23:18.6090536Z         torch.manual_seed(123)
2025-04-11T04:23:18.6090639Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6090822Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6090920Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6090924Z 
2025-04-11T04:23:18.6091093Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6091210Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6091214Z 
2025-04-11T04:23:18.6091292Z device = None
2025-04-11T04:23:18.6091296Z 
2025-04-11T04:23:18.6091411Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6091566Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6091637Z     
2025-04-11T04:23:18.6091714Z         Args:
2025-04-11T04:23:18.6091881Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6092047Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6092153Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6092228Z         """
2025-04-11T04:23:18.6092308Z         _lazy_init()
2025-04-11T04:23:18.6092403Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6092507Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6092697Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6092977Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6093115Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6093273Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6093277Z 
2025-04-11T04:23:18.6093517Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6093684Z ____________ test_context_attention[False-False-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.6093687Z 
2025-04-11T04:23:18.6093844Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6093990Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6094083Z use_new_kcache_layout = False
2025-04-11T04:23:18.6094090Z 
2025-04-11T04:23:18.6094288Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6094391Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6094511Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6094648Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6094765Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6094875Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6095013Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6095145Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6095297Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6095388Z     def test_context_attention(
2025-04-11T04:23:18.6095465Z         bsz: int,
2025-04-11T04:23:18.6095549Z         block_size: int,
2025-04-11T04:23:18.6095643Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6095728Z         num_attn_heads: int,
2025-04-11T04:23:18.6095808Z         kv_group_num: int,
2025-04-11T04:23:18.6095891Z         same_context_len: bool,
2025-04-11T04:23:18.6095977Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6096064Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6096138Z     ):
2025-04-11T04:23:18.6096247Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6096439Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6096623Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6096874Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6097046Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6097125Z             return
2025-04-11T04:23:18.6097210Z     
2025-04-11T04:23:18.6097296Z         torch.manual_seed(123)
2025-04-11T04:23:18.6097398Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6097490Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6097579Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6097583Z 
2025-04-11T04:23:18.6097762Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6097873Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6097877Z 
2025-04-11T04:23:18.6097956Z device = None
2025-04-11T04:23:18.6097960Z 
2025-04-11T04:23:18.6098075Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6098225Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6098303Z     
2025-04-11T04:23:18.6098376Z         Args:
2025-04-11T04:23:18.6098545Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6098709Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6098902Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6098975Z         """
2025-04-11T04:23:18.6099053Z         _lazy_init()
2025-04-11T04:23:18.6099153Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6099254Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6099363Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6099650Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6099790Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6099954Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6099958Z 
2025-04-11T04:23:18.6100197Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6100379Z ____________ test_context_attention[False-False-False-4-16-8-16-32] ____________
2025-04-11T04:23:18.6100382Z 
2025-04-11T04:23:18.6100535Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6100687Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6100775Z use_new_kcache_layout = False
2025-04-11T04:23:18.6100779Z 
2025-04-11T04:23:18.6100983Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6101086Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6101204Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6101341Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6101462Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6101580Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6101716Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6101857Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6102008Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6102095Z     def test_context_attention(
2025-04-11T04:23:18.6102175Z         bsz: int,
2025-04-11T04:23:18.6102259Z         block_size: int,
2025-04-11T04:23:18.6102352Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6102437Z         num_attn_heads: int,
2025-04-11T04:23:18.6102523Z         kv_group_num: int,
2025-04-11T04:23:18.6102606Z         same_context_len: bool,
2025-04-11T04:23:18.6102690Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6102782Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6102855Z     ):
2025-04-11T04:23:18.6103059Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6103255Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6103434Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6103610Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6103771Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6103852Z             return
2025-04-11T04:23:18.6103925Z     
2025-04-11T04:23:18.6104012Z         torch.manual_seed(123)
2025-04-11T04:23:18.6104111Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6104199Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6104293Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6104297Z 
2025-04-11T04:23:18.6104464Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6104583Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6104587Z 
2025-04-11T04:23:18.6104664Z device = None
2025-04-11T04:23:18.6104668Z 
2025-04-11T04:23:18.6104967Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6105115Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6105186Z     
2025-04-11T04:23:18.6105264Z         Args:
2025-04-11T04:23:18.6105432Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6105604Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6105708Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6105784Z         """
2025-04-11T04:23:18.6105862Z         _lazy_init()
2025-04-11T04:23:18.6105958Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6106065Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6106173Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6106460Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6106602Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6106765Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6106769Z 
2025-04-11T04:23:18.6107008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6107177Z ____________ test_context_attention[False-False-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.6107184Z 
2025-04-11T04:23:18.6107334Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6107479Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6107571Z use_new_kcache_layout = False
2025-04-11T04:23:18.6107575Z 
2025-04-11T04:23:18.6107777Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6107885Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6108005Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6108147Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6108262Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6108376Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6108549Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6108684Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6108835Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6108921Z     def test_context_attention(
2025-04-11T04:23:18.6108996Z         bsz: int,
2025-04-11T04:23:18.6109082Z         block_size: int,
2025-04-11T04:23:18.6109264Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6109367Z         num_attn_heads: int,
2025-04-11T04:23:18.6109450Z         kv_group_num: int,
2025-04-11T04:23:18.6109538Z         same_context_len: bool,
2025-04-11T04:23:18.6109628Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6109720Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6109798Z     ):
2025-04-11T04:23:18.6109908Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6110105Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6110287Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6110466Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6110629Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6110703Z             return
2025-04-11T04:23:18.6110784Z     
2025-04-11T04:23:18.6110872Z         torch.manual_seed(123)
2025-04-11T04:23:18.6110972Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6111060Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6111262Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6111266Z 
2025-04-11T04:23:18.6111438Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6111547Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6111551Z 
2025-04-11T04:23:18.6111631Z device = None
2025-04-11T04:23:18.6111634Z 
2025-04-11T04:23:18.6111750Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6111903Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6111974Z     
2025-04-11T04:23:18.6112046Z         Args:
2025-04-11T04:23:18.6112211Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6112378Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6112486Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6112558Z         """
2025-04-11T04:23:18.6112640Z         _lazy_init()
2025-04-11T04:23:18.6112735Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6112835Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6112944Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6113225Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6113364Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6113521Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6113525Z 
2025-04-11T04:23:18.6113768Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6113943Z ____________ test_context_attention[False-False-False-4-16-8-32-32] ____________
2025-04-11T04:23:18.6113946Z 
2025-04-11T04:23:18.6114101Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6114251Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6114339Z use_new_kcache_layout = False
2025-04-11T04:23:18.6114343Z 
2025-04-11T04:23:18.6114544Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6114646Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6114766Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6114902Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6115020Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6115131Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6115265Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6115488Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6115641Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6115736Z     def test_context_attention(
2025-04-11T04:23:18.6115814Z         bsz: int,
2025-04-11T04:23:18.6115898Z         block_size: int,
2025-04-11T04:23:18.6115988Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6116071Z         num_attn_heads: int,
2025-04-11T04:23:18.6116158Z         kv_group_num: int,
2025-04-11T04:23:18.6116242Z         same_context_len: bool,
2025-04-11T04:23:18.6116330Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6116417Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6116489Z     ):
2025-04-11T04:23:18.6116604Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6116796Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6116988Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6117160Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6117418Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6117493Z             return
2025-04-11T04:23:18.6117571Z     
2025-04-11T04:23:18.6117668Z         torch.manual_seed(123)
2025-04-11T04:23:18.6117766Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6117857Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6117947Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6117951Z 
2025-04-11T04:23:18.6118117Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6118233Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6118237Z 
2025-04-11T04:23:18.6118315Z device = None
2025-04-11T04:23:18.6118319Z 
2025-04-11T04:23:18.6118441Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6118589Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6118664Z     
2025-04-11T04:23:18.6118741Z         Args:
2025-04-11T04:23:18.6118910Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6119074Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6119180Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6119266Z         """
2025-04-11T04:23:18.6119343Z         _lazy_init()
2025-04-11T04:23:18.6119444Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6119545Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6119649Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6119932Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6120069Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6120231Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6120237Z 
2025-04-11T04:23:18.6120478Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6120648Z ____________ test_context_attention[False-False-False-4-16-16-16-7] ____________
2025-04-11T04:23:18.6120652Z 
2025-04-11T04:23:18.6120802Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6120950Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6121038Z use_new_kcache_layout = False
2025-04-11T04:23:18.6121041Z 
2025-04-11T04:23:18.6121242Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6121348Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6121547Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6121691Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6121807Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6121926Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6122062Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6122197Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6122351Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6122439Z     def test_context_attention(
2025-04-11T04:23:18.6122519Z         bsz: int,
2025-04-11T04:23:18.6122601Z         block_size: int,
2025-04-11T04:23:18.6122694Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6122776Z         num_attn_heads: int,
2025-04-11T04:23:18.6122860Z         kv_group_num: int,
2025-04-11T04:23:18.6122948Z         same_context_len: bool,
2025-04-11T04:23:18.6123033Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6123124Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6123196Z     ):
2025-04-11T04:23:18.6123305Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6123605Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6123787Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6123962Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6124127Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6124206Z             return
2025-04-11T04:23:18.6124279Z     
2025-04-11T04:23:18.6124366Z         torch.manual_seed(123)
2025-04-11T04:23:18.6124468Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6124556Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6124653Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6124657Z 
2025-04-11T04:23:18.6124825Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6124941Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6124948Z 
2025-04-11T04:23:18.6125025Z device = None
2025-04-11T04:23:18.6125029Z 
2025-04-11T04:23:18.6125145Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6125303Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6125376Z     
2025-04-11T04:23:18.6125454Z         Args:
2025-04-11T04:23:18.6125622Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6125790Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6125896Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6125968Z         """
2025-04-11T04:23:18.6126052Z         _lazy_init()
2025-04-11T04:23:18.6126148Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6126252Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6126358Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6126647Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6126790Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6126950Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6126954Z 
2025-04-11T04:23:18.6127202Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6127378Z ___________ test_context_attention[False-False-False-4-16-16-16-32] ____________
2025-04-11T04:23:18.6127382Z 
2025-04-11T04:23:18.6127538Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6127771Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6127867Z use_new_kcache_layout = False
2025-04-11T04:23:18.6127872Z 
2025-04-11T04:23:18.6128072Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6128183Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6128301Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6128442Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6128562Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6128677Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6128818Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6128953Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6129102Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6129199Z     def test_context_attention(
2025-04-11T04:23:18.6129276Z         bsz: int,
2025-04-11T04:23:18.6129363Z         block_size: int,
2025-04-11T04:23:18.6129453Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6129629Z         num_attn_heads: int,
2025-04-11T04:23:18.6129712Z         kv_group_num: int,
2025-04-11T04:23:18.6129801Z         same_context_len: bool,
2025-04-11T04:23:18.6129894Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6129987Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6130065Z     ):
2025-04-11T04:23:18.6130183Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6130374Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6130568Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6130738Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6130908Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6130982Z             return
2025-04-11T04:23:18.6131056Z     
2025-04-11T04:23:18.6131142Z         torch.manual_seed(123)
2025-04-11T04:23:18.6131245Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6131339Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6131430Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6131434Z 
2025-04-11T04:23:18.6131602Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6131713Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6131717Z 
2025-04-11T04:23:18.6131801Z device = None
2025-04-11T04:23:18.6131805Z 
2025-04-11T04:23:18.6131924Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6132071Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6132146Z     
2025-04-11T04:23:18.6132220Z         Args:
2025-04-11T04:23:18.6132392Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6132561Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6132675Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6132747Z         """
2025-04-11T04:23:18.6132824Z         _lazy_init()
2025-04-11T04:23:18.6132924Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6133024Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6133133Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6133414Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6133549Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6133707Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6133793Z 
2025-04-11T04:23:18.6134036Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6134209Z ____________ test_context_attention[False-False-False-4-16-16-32-7] ____________
2025-04-11T04:23:18.6134217Z 
2025-04-11T04:23:18.6134368Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6134516Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6134604Z use_new_kcache_layout = False
2025-04-11T04:23:18.6134608Z 
2025-04-11T04:23:18.6134812Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6134914Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6135034Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6135172Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6135288Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6135409Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6135544Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6135684Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6135933Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6136019Z     def test_context_attention(
2025-04-11T04:23:18.6136100Z         bsz: int,
2025-04-11T04:23:18.6136182Z         block_size: int,
2025-04-11T04:23:18.6136272Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6136357Z         num_attn_heads: int,
2025-04-11T04:23:18.6136445Z         kv_group_num: int,
2025-04-11T04:23:18.6136528Z         same_context_len: bool,
2025-04-11T04:23:18.6136611Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6136702Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6136776Z     ):
2025-04-11T04:23:18.6136888Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6137084Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6137263Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6137443Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6137606Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6137685Z             return
2025-04-11T04:23:18.6137758Z     
2025-04-11T04:23:18.6137846Z         torch.manual_seed(123)
2025-04-11T04:23:18.6137945Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6138034Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6138128Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6138132Z 
2025-04-11T04:23:18.6138298Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6138417Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6138420Z 
2025-04-11T04:23:18.6138497Z device = None
2025-04-11T04:23:18.6138501Z 
2025-04-11T04:23:18.6138620Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6138772Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6138843Z     
2025-04-11T04:23:18.6138920Z         Args:
2025-04-11T04:23:18.6139086Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6139253Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6139356Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6139432Z         """
2025-04-11T04:23:18.6139511Z         _lazy_init()
2025-04-11T04:23:18.6139603Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6139709Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6139896Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6140203Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6140338Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6140503Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6140507Z 
2025-04-11T04:23:18.6140746Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6140922Z ___________ test_context_attention[False-False-False-4-16-16-32-32] ____________
2025-04-11T04:23:18.6140930Z 
2025-04-11T04:23:18.6141083Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6141229Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T04:23:18.6141323Z use_new_kcache_layout = False
2025-04-11T04:23:18.6141327Z 
2025-04-11T04:23:18.6141530Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6141639Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.6141755Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6141985Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6142111Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6142230Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6142386Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6142530Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6142690Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6142778Z     def test_context_attention(
2025-04-11T04:23:18.6142860Z         bsz: int,
2025-04-11T04:23:18.6142944Z         block_size: int,
2025-04-11T04:23:18.6143033Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6143124Z         num_attn_heads: int,
2025-04-11T04:23:18.6143207Z         kv_group_num: int,
2025-04-11T04:23:18.6143294Z         same_context_len: bool,
2025-04-11T04:23:18.6143377Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6143473Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6143550Z     ):
2025-04-11T04:23:18.6143660Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6143858Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6144036Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6144210Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6144373Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6144446Z             return
2025-04-11T04:23:18.6144524Z     
2025-04-11T04:23:18.6144609Z         torch.manual_seed(123)
2025-04-11T04:23:18.6144716Z         # It's necessary to clear cache here.
2025-04-11T04:23:18.6144807Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6144898Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6144908Z 
2025-04-11T04:23:18.6145082Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T04:23:18.6145196Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6145199Z 
2025-04-11T04:23:18.6145281Z device = None
2025-04-11T04:23:18.6145285Z 
2025-04-11T04:23:18.6145402Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6145551Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6145624Z     
2025-04-11T04:23:18.6145696Z         Args:
2025-04-11T04:23:18.6145863Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6146026Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6146222Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6146300Z         """
2025-04-11T04:23:18.6146382Z         _lazy_init()
2025-04-11T04:23:18.6146477Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6146583Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6146694Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6146976Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6147117Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6147274Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6147278Z 
2025-04-11T04:23:18.6147516Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6147683Z ______________ test_flash_decoding[True-False-1-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6147687Z 
2025-04-11T04:23:18.6147840Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6148003Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6148177Z use_new_kcache_layout = True
2025-04-11T04:23:18.6148181Z 
2025-04-11T04:23:18.6148386Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6148522Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6148645Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6148785Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6148905Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6149017Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6149154Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6149268Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6149404Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6149558Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6149647Z     def test_flash_decoding(
2025-04-11T04:23:18.6149729Z         bsz: int,
2025-04-11T04:23:18.6149813Z         block_size: int,
2025-04-11T04:23:18.6149904Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6149991Z         num_attn_heads: int,
2025-04-11T04:23:18.6150075Z         kv_group_num: int,
2025-04-11T04:23:18.6150162Z         same_context_len: bool,
2025-04-11T04:23:18.6150240Z         q_len: int,
2025-04-11T04:23:18.6150323Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6150414Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6150485Z     ):
2025-04-11T04:23:18.6150598Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6150796Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6150983Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6151155Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6151324Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6151488Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6151561Z     
2025-04-11T04:23:18.6151650Z         torch.manual_seed(123)
2025-04-11T04:23:18.6151739Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6151830Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6151837Z 
2025-04-11T04:23:18.6151993Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6152104Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6152108Z 
2025-04-11T04:23:18.6152190Z device = None
2025-04-11T04:23:18.6152285Z 
2025-04-11T04:23:18.6152404Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6152558Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6152634Z     
2025-04-11T04:23:18.6152712Z         Args:
2025-04-11T04:23:18.6152881Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6153048Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6153156Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6153229Z         """
2025-04-11T04:23:18.6153319Z         _lazy_init()
2025-04-11T04:23:18.6153422Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6153529Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6153650Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6153938Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6154080Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6154240Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6154338Z 
2025-04-11T04:23:18.6154581Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6154746Z _____________ test_flash_decoding[True-False-1-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6154750Z 
2025-04-11T04:23:18.6154904Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6155066Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6155155Z use_new_kcache_layout = True
2025-04-11T04:23:18.6155159Z 
2025-04-11T04:23:18.6155359Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6155468Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6155589Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6155728Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6155850Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6155961Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6156097Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6156203Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6156337Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6156492Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6156577Z     def test_flash_decoding(
2025-04-11T04:23:18.6156656Z         bsz: int,
2025-04-11T04:23:18.6156739Z         block_size: int,
2025-04-11T04:23:18.6156829Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6156918Z         num_attn_heads: int,
2025-04-11T04:23:18.6157004Z         kv_group_num: int,
2025-04-11T04:23:18.6157092Z         same_context_len: bool,
2025-04-11T04:23:18.6157168Z         q_len: int,
2025-04-11T04:23:18.6157253Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6157347Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6157421Z     ):
2025-04-11T04:23:18.6157533Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6157723Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6157905Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6158073Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6158235Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6158396Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6158468Z     
2025-04-11T04:23:18.6158652Z         torch.manual_seed(123)
2025-04-11T04:23:18.6158745Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6158840Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6158844Z 
2025-04-11T04:23:18.6159005Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6159118Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6159122Z 
2025-04-11T04:23:18.6159203Z device = None
2025-04-11T04:23:18.6159207Z 
2025-04-11T04:23:18.6159323Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6159477Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6159548Z     
2025-04-11T04:23:18.6159624Z         Args:
2025-04-11T04:23:18.6159791Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6159955Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6160070Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6160142Z         """
2025-04-11T04:23:18.6160223Z         _lazy_init()
2025-04-11T04:23:18.6160317Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6160504Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6160615Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6160902Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6161045Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6161206Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6161210Z 
2025-04-11T04:23:18.6161454Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6161625Z ______________ test_flash_decoding[True-False-1-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6161632Z 
2025-04-11T04:23:18.6161788Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6161957Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6162053Z use_new_kcache_layout = True
2025-04-11T04:23:18.6162060Z 
2025-04-11T04:23:18.6162261Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6162368Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6162492Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6162633Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6162755Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6162873Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6163012Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6163126Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6163267Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6163424Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6163516Z     def test_flash_decoding(
2025-04-11T04:23:18.6163599Z         bsz: int,
2025-04-11T04:23:18.6163684Z         block_size: int,
2025-04-11T04:23:18.6163778Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6163871Z         num_attn_heads: int,
2025-04-11T04:23:18.6163956Z         kv_group_num: int,
2025-04-11T04:23:18.6164050Z         same_context_len: bool,
2025-04-11T04:23:18.6164135Z         q_len: int,
2025-04-11T04:23:18.6164229Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6164327Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6164408Z     ):
2025-04-11T04:23:18.6164524Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6164726Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6164999Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6165171Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6165342Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6165506Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6165578Z     
2025-04-11T04:23:18.6165666Z         torch.manual_seed(123)
2025-04-11T04:23:18.6165754Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6165847Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6165851Z 
2025-04-11T04:23:18.6166008Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6166120Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6166124Z 
2025-04-11T04:23:18.6166204Z device = None
2025-04-11T04:23:18.6166208Z 
2025-04-11T04:23:18.6166327Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6166481Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6166639Z     
2025-04-11T04:23:18.6166720Z         Args:
2025-04-11T04:23:18.6166890Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6167054Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6167165Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6167238Z         """
2025-04-11T04:23:18.6167319Z         _lazy_init()
2025-04-11T04:23:18.6167414Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6167518Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6167622Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6167906Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6168045Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6168200Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6168207Z 
2025-04-11T04:23:18.6168451Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6168615Z _____________ test_flash_decoding[True-False-1-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6168619Z 
2025-04-11T04:23:18.6168770Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6168934Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6169025Z use_new_kcache_layout = True
2025-04-11T04:23:18.6169029Z 
2025-04-11T04:23:18.6169229Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6169335Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6169454Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6169590Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6169713Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6169825Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6169965Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6170067Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6170200Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6170351Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6170436Z     def test_flash_decoding(
2025-04-11T04:23:18.6170514Z         bsz: int,
2025-04-11T04:23:18.6170596Z         block_size: int,
2025-04-11T04:23:18.6170684Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6170771Z         num_attn_heads: int,
2025-04-11T04:23:18.6170935Z         kv_group_num: int,
2025-04-11T04:23:18.6171029Z         same_context_len: bool,
2025-04-11T04:23:18.6171105Z         q_len: int,
2025-04-11T04:23:18.6171188Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6171283Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6171357Z     ):
2025-04-11T04:23:18.6171470Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6171663Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6171850Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6172021Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6172183Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6172346Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6172418Z     
2025-04-11T04:23:18.6172511Z         torch.manual_seed(123)
2025-04-11T04:23:18.6172601Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6172697Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6172700Z 
2025-04-11T04:23:18.6172937Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6173048Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6173056Z 
2025-04-11T04:23:18.6173133Z device = None
2025-04-11T04:23:18.6173137Z 
2025-04-11T04:23:18.6173254Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6173407Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6173479Z     
2025-04-11T04:23:18.6173556Z         Args:
2025-04-11T04:23:18.6173721Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6173886Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6173998Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6174071Z         """
2025-04-11T04:23:18.6174153Z         _lazy_init()
2025-04-11T04:23:18.6174252Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6174367Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6174477Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6174765Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6174904Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6175064Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6175068Z 
2025-04-11T04:23:18.6175311Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6175477Z _____________ test_flash_decoding[True-False-1-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6175485Z 
2025-04-11T04:23:18.6175639Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6175802Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6175896Z use_new_kcache_layout = True
2025-04-11T04:23:18.6175900Z 
2025-04-11T04:23:18.6176098Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6176202Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6176321Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6176459Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6176576Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6176687Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6176828Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6176929Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6177148Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6177304Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6177394Z     def test_flash_decoding(
2025-04-11T04:23:18.6177474Z         bsz: int,
2025-04-11T04:23:18.6177558Z         block_size: int,
2025-04-11T04:23:18.6177647Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6177734Z         num_attn_heads: int,
2025-04-11T04:23:18.6177816Z         kv_group_num: int,
2025-04-11T04:23:18.6177905Z         same_context_len: bool,
2025-04-11T04:23:18.6177982Z         q_len: int,
2025-04-11T04:23:18.6178069Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6178156Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6178229Z     ):
2025-04-11T04:23:18.6178341Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6178532Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6178719Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6178890Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6179145Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6179304Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6179377Z     
2025-04-11T04:23:18.6179468Z         torch.manual_seed(123)
2025-04-11T04:23:18.6179556Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6179650Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6179654Z 
2025-04-11T04:23:18.6179809Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6179920Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6179927Z 
2025-04-11T04:23:18.6180006Z device = None
2025-04-11T04:23:18.6180010Z 
2025-04-11T04:23:18.6180130Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6180286Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6180361Z     
2025-04-11T04:23:18.6180438Z         Args:
2025-04-11T04:23:18.6180604Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6180772Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6180876Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6180948Z         """
2025-04-11T04:23:18.6181030Z         _lazy_init()
2025-04-11T04:23:18.6181123Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6181226Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6181330Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6181621Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6181759Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6181916Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6181923Z 
2025-04-11T04:23:18.6182172Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6182338Z _____________ test_flash_decoding[True-False-1-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6182342Z 
2025-04-11T04:23:18.6182498Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6182663Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6182754Z use_new_kcache_layout = True
2025-04-11T04:23:18.6182758Z 
2025-04-11T04:23:18.6182959Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6183063Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6183283Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6183426Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6183544Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6183660Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6183801Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6183905Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6184040Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6184192Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6184279Z     def test_flash_decoding(
2025-04-11T04:23:18.6184359Z         bsz: int,
2025-04-11T04:23:18.6184441Z         block_size: int,
2025-04-11T04:23:18.6184539Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6184622Z         num_attn_heads: int,
2025-04-11T04:23:18.6184717Z         kv_group_num: int,
2025-04-11T04:23:18.6184810Z         same_context_len: bool,
2025-04-11T04:23:18.6184885Z         q_len: int,
2025-04-11T04:23:18.6184983Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6185176Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6185252Z     ):
2025-04-11T04:23:18.6185375Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6185568Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6185758Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6185931Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6186100Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6186258Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6186331Z     
2025-04-11T04:23:18.6186427Z         torch.manual_seed(123)
2025-04-11T04:23:18.6186515Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6186608Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6186612Z 
2025-04-11T04:23:18.6186773Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6186888Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6186892Z 
2025-04-11T04:23:18.6186971Z device = None
2025-04-11T04:23:18.6186975Z 
2025-04-11T04:23:18.6187093Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6187250Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6187322Z     
2025-04-11T04:23:18.6187400Z         Args:
2025-04-11T04:23:18.6187570Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6187738Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6187844Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6187919Z         """
2025-04-11T04:23:18.6188001Z         _lazy_init()
2025-04-11T04:23:18.6188096Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6188205Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6188309Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6188791Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6188931Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6189089Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6189093Z 
2025-04-11T04:23:18.6189338Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6189502Z _____________ test_flash_decoding[True-False-1-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6189601Z 
2025-04-11T04:23:18.6189760Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6189927Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6190023Z use_new_kcache_layout = True
2025-04-11T04:23:18.6190026Z 
2025-04-11T04:23:18.6190224Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6190331Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6190447Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6190583Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6190702Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6190817Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6190959Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6191062Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6191198Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6191351Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6191537Z     def test_flash_decoding(
2025-04-11T04:23:18.6191618Z         bsz: int,
2025-04-11T04:23:18.6191701Z         block_size: int,
2025-04-11T04:23:18.6191795Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6191878Z         num_attn_heads: int,
2025-04-11T04:23:18.6191960Z         kv_group_num: int,
2025-04-11T04:23:18.6192051Z         same_context_len: bool,
2025-04-11T04:23:18.6192127Z         q_len: int,
2025-04-11T04:23:18.6192215Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6192301Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6192373Z     ):
2025-04-11T04:23:18.6192488Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6192682Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6192871Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6193044Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6193214Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6193375Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6193446Z     
2025-04-11T04:23:18.6193537Z         torch.manual_seed(123)
2025-04-11T04:23:18.6193625Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6193718Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6193722Z 
2025-04-11T04:23:18.6193877Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6193990Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6193994Z 
2025-04-11T04:23:18.6194073Z device = None
2025-04-11T04:23:18.6194077Z 
2025-04-11T04:23:18.6194196Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6194349Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6194421Z     
2025-04-11T04:23:18.6194502Z         Args:
2025-04-11T04:23:18.6194666Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6194834Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6194938Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6195011Z         """
2025-04-11T04:23:18.6195095Z         _lazy_init()
2025-04-11T04:23:18.6195189Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6195294Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6195399Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6195771Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6195910Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6196066Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6196073Z 
2025-04-11T04:23:18.6196314Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6196480Z _____________ test_flash_decoding[True-False-1-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6196484Z 
2025-04-11T04:23:18.6196640Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6196802Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6196894Z use_new_kcache_layout = True
2025-04-11T04:23:18.6196898Z 
2025-04-11T04:23:18.6197094Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6197204Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6197327Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6197464Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6197585Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6197787Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6197927Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6198031Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6198174Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6198322Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6198406Z     def test_flash_decoding(
2025-04-11T04:23:18.6198486Z         bsz: int,
2025-04-11T04:23:18.6198569Z         block_size: int,
2025-04-11T04:23:18.6198666Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6198747Z         num_attn_heads: int,
2025-04-11T04:23:18.6198833Z         kv_group_num: int,
2025-04-11T04:23:18.6198923Z         same_context_len: bool,
2025-04-11T04:23:18.6198999Z         q_len: int,
2025-04-11T04:23:18.6199086Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6199175Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6199252Z     ):
2025-04-11T04:23:18.6199368Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6199561Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6199746Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6199915Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6200080Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6200236Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6200308Z     
2025-04-11T04:23:18.6200401Z         torch.manual_seed(123)
2025-04-11T04:23:18.6200488Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6200580Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6200584Z 
2025-04-11T04:23:18.6200742Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6200853Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6200857Z 
2025-04-11T04:23:18.6200935Z device = None
2025-04-11T04:23:18.6200938Z 
2025-04-11T04:23:18.6201058Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6201206Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6201276Z     
2025-04-11T04:23:18.6201355Z         Args:
2025-04-11T04:23:18.6201518Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6201685Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6201956Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6202032Z         """
2025-04-11T04:23:18.6202114Z         _lazy_init()
2025-04-11T04:23:18.6202209Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6202319Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6202422Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6202709Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6202845Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6203000Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6203004Z 
2025-04-11T04:23:18.6203247Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6203411Z ______________ test_flash_decoding[True-False-1-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6203418Z 
2025-04-11T04:23:18.6203572Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6203736Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6203923Z use_new_kcache_layout = True
2025-04-11T04:23:18.6203928Z 
2025-04-11T04:23:18.6204128Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6204236Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6204351Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6204490Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6204609Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6204721Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6204861Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6204963Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6205103Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6205254Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6205338Z     def test_flash_decoding(
2025-04-11T04:23:18.6205422Z         bsz: int,
2025-04-11T04:23:18.6205504Z         block_size: int,
2025-04-11T04:23:18.6205597Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6205678Z         num_attn_heads: int,
2025-04-11T04:23:18.6205762Z         kv_group_num: int,
2025-04-11T04:23:18.6205852Z         same_context_len: bool,
2025-04-11T04:23:18.6205927Z         q_len: int,
2025-04-11T04:23:18.6206014Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6206102Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6206175Z     ):
2025-04-11T04:23:18.6206289Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6206481Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6206667Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6206838Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6207008Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6207165Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6207241Z     
2025-04-11T04:23:18.6207328Z         torch.manual_seed(123)
2025-04-11T04:23:18.6207415Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6207509Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6207513Z 
2025-04-11T04:23:18.6207666Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6207781Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6207785Z 
2025-04-11T04:23:18.6207863Z device = None
2025-04-11T04:23:18.6207867Z 
2025-04-11T04:23:18.6208066Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6208218Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6208290Z     
2025-04-11T04:23:18.6208372Z         Args:
2025-04-11T04:23:18.6208539Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6208707Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6208814Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6208888Z         """
2025-04-11T04:23:18.6208970Z         _lazy_init()
2025-04-11T04:23:18.6209064Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6209169Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6209277Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6209572Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6209711Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6209874Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6209970Z 
2025-04-11T04:23:18.6210224Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6210391Z _____________ test_flash_decoding[True-False-1-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6210394Z 
2025-04-11T04:23:18.6210553Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6210717Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6210809Z use_new_kcache_layout = True
2025-04-11T04:23:18.6210813Z 
2025-04-11T04:23:18.6211017Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6211126Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6211245Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6211381Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6211506Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6211622Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6211762Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6211863Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6212001Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6212153Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6212241Z     def test_flash_decoding(
2025-04-11T04:23:18.6212325Z         bsz: int,
2025-04-11T04:23:18.6212408Z         block_size: int,
2025-04-11T04:23:18.6212501Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6212584Z         num_attn_heads: int,
2025-04-11T04:23:18.6212668Z         kv_group_num: int,
2025-04-11T04:23:18.6212762Z         same_context_len: bool,
2025-04-11T04:23:18.6212838Z         q_len: int,
2025-04-11T04:23:18.6212928Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6213017Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6213099Z     ):
2025-04-11T04:23:18.6213211Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6213404Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6213587Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6213757Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6213922Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6214083Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6214159Z     
2025-04-11T04:23:18.6214330Z         torch.manual_seed(123)
2025-04-11T04:23:18.6214422Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6214519Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6214523Z 
2025-04-11T04:23:18.6214678Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6214801Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6214805Z 
2025-04-11T04:23:18.6214883Z device = None
2025-04-11T04:23:18.6214886Z 
2025-04-11T04:23:18.6215005Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6215154Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6215225Z     
2025-04-11T04:23:18.6215305Z         Args:
2025-04-11T04:23:18.6215471Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6215641Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6215750Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6215827Z         """
2025-04-11T04:23:18.6215906Z         _lazy_init()
2025-04-11T04:23:18.6216000Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6216191Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6216297Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6216583Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6216717Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6216877Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6216881Z 
2025-04-11T04:23:18.6217119Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6217283Z ______________ test_flash_decoding[True-False-1-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6217287Z 
2025-04-11T04:23:18.6217444Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6217607Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6217703Z use_new_kcache_layout = True
2025-04-11T04:23:18.6217706Z 
2025-04-11T04:23:18.6217905Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6218012Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6218127Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6218268Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6218384Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6218498Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6218637Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6218737Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6218879Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6219028Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6219114Z     def test_flash_decoding(
2025-04-11T04:23:18.6219203Z         bsz: int,
2025-04-11T04:23:18.6219285Z         block_size: int,
2025-04-11T04:23:18.6219383Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6219470Z         num_attn_heads: int,
2025-04-11T04:23:18.6219556Z         kv_group_num: int,
2025-04-11T04:23:18.6219646Z         same_context_len: bool,
2025-04-11T04:23:18.6219721Z         q_len: int,
2025-04-11T04:23:18.6219810Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6219897Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6219973Z     ):
2025-04-11T04:23:18.6220081Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6220272Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6220543Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6220717Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6220888Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6221047Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6221125Z     
2025-04-11T04:23:18.6221212Z         torch.manual_seed(123)
2025-04-11T04:23:18.6221301Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6221397Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6221401Z 
2025-04-11T04:23:18.6221557Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6221671Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6221675Z 
2025-04-11T04:23:18.6221753Z device = None
2025-04-11T04:23:18.6221757Z 
2025-04-11T04:23:18.6221882Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6222033Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6222105Z     
2025-04-11T04:23:18.6222264Z         Args:
2025-04-11T04:23:18.6222431Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6222601Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6222705Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6222781Z         """
2025-04-11T04:23:18.6222860Z         _lazy_init()
2025-04-11T04:23:18.6222955Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6223060Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6223164Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6223453Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6223590Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6223751Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6223758Z 
2025-04-11T04:23:18.6223994Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6224157Z _____________ test_flash_decoding[True-False-1-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6224165Z 
2025-04-11T04:23:18.6224315Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6224477Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6224570Z use_new_kcache_layout = True
2025-04-11T04:23:18.6224574Z 
2025-04-11T04:23:18.6224771Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6224882Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6224999Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6225141Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6225257Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6225374Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6225516Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6225621Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6225764Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6225914Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6226001Z     def test_flash_decoding(
2025-04-11T04:23:18.6226082Z         bsz: int,
2025-04-11T04:23:18.6226167Z         block_size: int,
2025-04-11T04:23:18.6226264Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6226345Z         num_attn_heads: int,
2025-04-11T04:23:18.6226434Z         kv_group_num: int,
2025-04-11T04:23:18.6226610Z         same_context_len: bool,
2025-04-11T04:23:18.6226690Z         q_len: int,
2025-04-11T04:23:18.6226780Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6226871Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6226954Z     ):
2025-04-11T04:23:18.6227064Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6227255Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6227443Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6227612Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6227781Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6227937Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6228014Z     
2025-04-11T04:23:18.6228104Z         torch.manual_seed(123)
2025-04-11T04:23:18.6228193Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6228288Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6228292Z 
2025-04-11T04:23:18.6228474Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6228706Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6228710Z 
2025-04-11T04:23:18.6228788Z device = None
2025-04-11T04:23:18.6228793Z 
2025-04-11T04:23:18.6228915Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6229065Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6229147Z     
2025-04-11T04:23:18.6229227Z         Args:
2025-04-11T04:23:18.6229397Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6229574Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6229685Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6229769Z         """
2025-04-11T04:23:18.6229849Z         _lazy_init()
2025-04-11T04:23:18.6229950Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6230064Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6230168Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6230461Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6230595Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6230756Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6230760Z 
2025-04-11T04:23:18.6230998Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6231164Z _____________ test_flash_decoding[True-False-1-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6231173Z 
2025-04-11T04:23:18.6231327Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6231492Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6231586Z use_new_kcache_layout = True
2025-04-11T04:23:18.6231590Z 
2025-04-11T04:23:18.6231787Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6231895Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6232011Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6232152Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6232267Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6232378Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6232521Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6232624Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6232853Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6233006Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6233095Z     def test_flash_decoding(
2025-04-11T04:23:18.6233177Z         bsz: int,
2025-04-11T04:23:18.6233259Z         block_size: int,
2025-04-11T04:23:18.6233354Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6233437Z         num_attn_heads: int,
2025-04-11T04:23:18.6233521Z         kv_group_num: int,
2025-04-11T04:23:18.6233607Z         same_context_len: bool,
2025-04-11T04:23:18.6233684Z         q_len: int,
2025-04-11T04:23:18.6233773Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6233861Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6233939Z     ):
2025-04-11T04:23:18.6234049Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6234239Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6234428Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6234597Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6234849Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6235007Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6235083Z     
2025-04-11T04:23:18.6235170Z         torch.manual_seed(123)
2025-04-11T04:23:18.6235258Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6235352Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6235355Z 
2025-04-11T04:23:18.6235509Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6235622Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6235626Z 
2025-04-11T04:23:18.6235704Z device = None
2025-04-11T04:23:18.6235708Z 
2025-04-11T04:23:18.6235831Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6235979Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6236055Z     
2025-04-11T04:23:18.6236133Z         Args:
2025-04-11T04:23:18.6236298Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6236465Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6236570Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6236646Z         """
2025-04-11T04:23:18.6236724Z         _lazy_init()
2025-04-11T04:23:18.6236819Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6236924Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6237029Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6237316Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6237455Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6237613Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6237620Z 
2025-04-11T04:23:18.6237856Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6238026Z _____________ test_flash_decoding[True-False-1-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6238030Z 
2025-04-11T04:23:18.6238182Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6238347Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6238440Z use_new_kcache_layout = True
2025-04-11T04:23:18.6238444Z 
2025-04-11T04:23:18.6238641Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6238747Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6238946Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6239089Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6239206Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6239321Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6239461Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6239566Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6239705Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6239853Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6239942Z     def test_flash_decoding(
2025-04-11T04:23:18.6240020Z         bsz: int,
2025-04-11T04:23:18.6240102Z         block_size: int,
2025-04-11T04:23:18.6240195Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6240277Z         num_attn_heads: int,
2025-04-11T04:23:18.6240363Z         kv_group_num: int,
2025-04-11T04:23:18.6240454Z         same_context_len: bool,
2025-04-11T04:23:18.6240531Z         q_len: int,
2025-04-11T04:23:18.6240619Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6240707Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6240868Z     ):
2025-04-11T04:23:18.6240980Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6241179Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6241360Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6241530Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6241698Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6241855Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6241934Z     
2025-04-11T04:23:18.6242029Z         torch.manual_seed(123)
2025-04-11T04:23:18.6242123Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6242220Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6242223Z 
2025-04-11T04:23:18.6242379Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6242505Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6242509Z 
2025-04-11T04:23:18.6242587Z device = None
2025-04-11T04:23:18.6242591Z 
2025-04-11T04:23:18.6242719Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6242869Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6242943Z     
2025-04-11T04:23:18.6243017Z         Args:
2025-04-11T04:23:18.6243184Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6243354Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6243463Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6243539Z         """
2025-04-11T04:23:18.6243617Z         _lazy_init()
2025-04-11T04:23:18.6243713Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6243821Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6243927Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6244214Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6244348Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6244515Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6244518Z 
2025-04-11T04:23:18.6244758Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6244926Z _____________ test_flash_decoding[True-False-1-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6244930Z 
2025-04-11T04:23:18.6245182Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6245348Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6245445Z use_new_kcache_layout = True
2025-04-11T04:23:18.6245449Z 
2025-04-11T04:23:18.6245646Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6245755Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6245869Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6246010Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6246128Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6246239Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6246380Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6246482Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6246623Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6246772Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6246861Z     def test_flash_decoding(
2025-04-11T04:23:18.6247024Z         bsz: int,
2025-04-11T04:23:18.6247109Z         block_size: int,
2025-04-11T04:23:18.6247202Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6247285Z         num_attn_heads: int,
2025-04-11T04:23:18.6247371Z         kv_group_num: int,
2025-04-11T04:23:18.6247457Z         same_context_len: bool,
2025-04-11T04:23:18.6247533Z         q_len: int,
2025-04-11T04:23:18.6247622Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6247710Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6247786Z     ):
2025-04-11T04:23:18.6247896Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6248093Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6248276Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6248445Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6248615Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6248772Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6248848Z     
2025-04-11T04:23:18.6248936Z         torch.manual_seed(123)
2025-04-11T04:23:18.6249030Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6249120Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6249124Z 
2025-04-11T04:23:18.6249278Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6249392Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6249396Z 
2025-04-11T04:23:18.6249473Z device = None
2025-04-11T04:23:18.6249477Z 
2025-04-11T04:23:18.6249600Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6249748Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6249823Z     
2025-04-11T04:23:18.6249897Z         Args:
2025-04-11T04:23:18.6250065Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6250232Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6250337Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6250413Z         """
2025-04-11T04:23:18.6250490Z         _lazy_init()
2025-04-11T04:23:18.6250590Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6250692Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6250795Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6251081Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6251310Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6251475Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6251483Z 
2025-04-11T04:23:18.6251722Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6251895Z _____________ test_flash_decoding[True-False-1-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6251898Z 
2025-04-11T04:23:18.6252049Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6252216Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6252305Z use_new_kcache_layout = True
2025-04-11T04:23:18.6252308Z 
2025-04-11T04:23:18.6252504Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6252611Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6252730Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6252871Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6252985Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6253188Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6253324Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6253425Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6253565Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6253714Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6253804Z     def test_flash_decoding(
2025-04-11T04:23:18.6253881Z         bsz: int,
2025-04-11T04:23:18.6253964Z         block_size: int,
2025-04-11T04:23:18.6254062Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6254149Z         num_attn_heads: int,
2025-04-11T04:23:18.6254240Z         kv_group_num: int,
2025-04-11T04:23:18.6254328Z         same_context_len: bool,
2025-04-11T04:23:18.6254415Z         q_len: int,
2025-04-11T04:23:18.6254499Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6254586Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6254665Z     ):
2025-04-11T04:23:18.6254774Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6254969Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6255149Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6255318Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6255484Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6255641Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6255720Z     
2025-04-11T04:23:18.6255805Z         torch.manual_seed(123)
2025-04-11T04:23:18.6255898Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6255990Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6255994Z 
2025-04-11T04:23:18.6256148Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6256266Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6256270Z 
2025-04-11T04:23:18.6256347Z device = None
2025-04-11T04:23:18.6256351Z 
2025-04-11T04:23:18.6256468Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6256615Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6256688Z     
2025-04-11T04:23:18.6256761Z         Args:
2025-04-11T04:23:18.6256937Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6257111Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6257298Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6257384Z         """
2025-04-11T04:23:18.6257462Z         _lazy_init()
2025-04-11T04:23:18.6257560Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6257667Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6257773Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6258062Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6258198Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6258360Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6258364Z 
2025-04-11T04:23:18.6258601Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6258772Z _____________ test_flash_decoding[True-False-1-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6258776Z 
2025-04-11T04:23:18.6258929Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6259096Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6259268Z use_new_kcache_layout = True
2025-04-11T04:23:18.6259273Z 
2025-04-11T04:23:18.6259476Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6259586Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6259704Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6259848Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6259963Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6260078Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6260215Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6260319Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6260462Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6260613Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6260703Z     def test_flash_decoding(
2025-04-11T04:23:18.6260783Z         bsz: int,
2025-04-11T04:23:18.6260864Z         block_size: int,
2025-04-11T04:23:18.6260957Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6261038Z         num_attn_heads: int,
2025-04-11T04:23:18.6261123Z         kv_group_num: int,
2025-04-11T04:23:18.6261208Z         same_context_len: bool,
2025-04-11T04:23:18.6261287Z         q_len: int,
2025-04-11T04:23:18.6261372Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6261459Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6261536Z     ):
2025-04-11T04:23:18.6261645Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6261841Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6262030Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6262203Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6262382Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6262543Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6262620Z     
2025-04-11T04:23:18.6262705Z         torch.manual_seed(123)
2025-04-11T04:23:18.6262798Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6262889Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6262893Z 
2025-04-11T04:23:18.6263049Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6263163Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6263166Z 
2025-04-11T04:23:18.6263244Z device = None
2025-04-11T04:23:18.6263248Z 
2025-04-11T04:23:18.6263457Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6263607Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6263683Z     
2025-04-11T04:23:18.6263759Z         Args:
2025-04-11T04:23:18.6263932Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6264099Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6264203Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6264280Z         """
2025-04-11T04:23:18.6264358Z         _lazy_init()
2025-04-11T04:23:18.6264456Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6264558Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6264663Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6264951Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6265090Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6265253Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6265339Z 
2025-04-11T04:23:18.6265583Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6265756Z _____________ test_flash_decoding[True-False-1-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.6265760Z 
2025-04-11T04:23:18.6265911Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6266080Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6266169Z use_new_kcache_layout = True
2025-04-11T04:23:18.6266172Z 
2025-04-11T04:23:18.6266369Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6266477Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6266597Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6266739Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6266854Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6266974Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6267111Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6267215Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6267358Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6267507Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6267595Z     def test_flash_decoding(
2025-04-11T04:23:18.6267671Z         bsz: int,
2025-04-11T04:23:18.6267759Z         block_size: int,
2025-04-11T04:23:18.6267848Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6267930Z         num_attn_heads: int,
2025-04-11T04:23:18.6268018Z         kv_group_num: int,
2025-04-11T04:23:18.6268108Z         same_context_len: bool,
2025-04-11T04:23:18.6268186Z         q_len: int,
2025-04-11T04:23:18.6268271Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6268360Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6268474Z     ):
2025-04-11T04:23:18.6268588Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6268786Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6268968Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6269142Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6269307Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6269465Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6269543Z     
2025-04-11T04:23:18.6269630Z         torch.manual_seed(123)
2025-04-11T04:23:18.6269819Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6269913Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6269918Z 
2025-04-11T04:23:18.6270078Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6270194Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6270197Z 
2025-04-11T04:23:18.6270277Z device = None
2025-04-11T04:23:18.6270282Z 
2025-04-11T04:23:18.6270403Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6270552Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6270628Z     
2025-04-11T04:23:18.6270707Z         Args:
2025-04-11T04:23:18.6270882Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6271050Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6271163Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6271247Z         """
2025-04-11T04:23:18.6271326Z         _lazy_init()
2025-04-11T04:23:18.6271423Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6271524Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6271725Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6272019Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6272155Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6272315Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6272319Z 
2025-04-11T04:23:18.6272559Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6272730Z _____________ test_flash_decoding[True-False-1-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.6272734Z 
2025-04-11T04:23:18.6272888Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6273058Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6273149Z use_new_kcache_layout = True
2025-04-11T04:23:18.6273153Z 
2025-04-11T04:23:18.6273358Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6273462Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6273578Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6273719Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6273836Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6273953Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6274089Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6274190Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6274331Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6274482Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6274572Z     def test_flash_decoding(
2025-04-11T04:23:18.6274652Z         bsz: int,
2025-04-11T04:23:18.6274737Z         block_size: int,
2025-04-11T04:23:18.6274826Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6274909Z         num_attn_heads: int,
2025-04-11T04:23:18.6274995Z         kv_group_num: int,
2025-04-11T04:23:18.6275079Z         same_context_len: bool,
2025-04-11T04:23:18.6275160Z         q_len: int,
2025-04-11T04:23:18.6275245Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6275333Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6275409Z     ):
2025-04-11T04:23:18.6275519Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6275712Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6275998Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6276174Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6276342Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6276500Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6276578Z     
2025-04-11T04:23:18.6276665Z         torch.manual_seed(123)
2025-04-11T04:23:18.6276759Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6276848Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6276852Z 
2025-04-11T04:23:18.6277007Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6277119Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6277123Z 
2025-04-11T04:23:18.6277201Z device = None
2025-04-11T04:23:18.6277204Z 
2025-04-11T04:23:18.6277327Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6277476Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6277552Z     
2025-04-11T04:23:18.6277626Z         Args:
2025-04-11T04:23:18.6277882Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6278046Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6278151Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6278228Z         """
2025-04-11T04:23:18.6278305Z         _lazy_init()
2025-04-11T04:23:18.6278402Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6278503Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6278606Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6278892Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6279032Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6279193Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6279200Z 
2025-04-11T04:23:18.6279438Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6279608Z _____________ test_flash_decoding[True-False-1-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.6279612Z 
2025-04-11T04:23:18.6279762Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6279930Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6280018Z use_new_kcache_layout = True
2025-04-11T04:23:18.6280022Z 
2025-04-11T04:23:18.6280233Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6280340Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6280469Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6280621Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6280744Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6280863Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6280996Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6281101Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6281235Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6281383Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6281472Z     def test_flash_decoding(
2025-04-11T04:23:18.6281549Z         bsz: int,
2025-04-11T04:23:18.6281632Z         block_size: int,
2025-04-11T04:23:18.6281721Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6281800Z         num_attn_heads: int,
2025-04-11T04:23:18.6281888Z         kv_group_num: int,
2025-04-11T04:23:18.6282058Z         same_context_len: bool,
2025-04-11T04:23:18.6282141Z         q_len: int,
2025-04-11T04:23:18.6282228Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6282317Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6282398Z     ):
2025-04-11T04:23:18.6282507Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6282706Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6282888Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6283061Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6283224Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6283382Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6283461Z     
2025-04-11T04:23:18.6283548Z         torch.manual_seed(123)
2025-04-11T04:23:18.6283643Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6283733Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6283737Z 
2025-04-11T04:23:18.6283893Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6284090Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6284095Z 
2025-04-11T04:23:18.6284173Z device = None
2025-04-11T04:23:18.6284179Z 
2025-04-11T04:23:18.6284296Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6284445Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6284523Z     
2025-04-11T04:23:18.6284596Z         Args:
2025-04-11T04:23:18.6284765Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6284931Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6285040Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6285116Z         """
2025-04-11T04:23:18.6285193Z         _lazy_init()
2025-04-11T04:23:18.6285293Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6285393Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6285504Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6285783Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6285918Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6286078Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6286082Z 
2025-04-11T04:23:18.6286318Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6286489Z _____________ test_flash_decoding[True-False-1-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.6286493Z 
2025-04-11T04:23:18.6286649Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6286817Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6286902Z use_new_kcache_layout = True
2025-04-11T04:23:18.6286910Z 
2025-04-11T04:23:18.6287113Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6287216Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6287333Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6287473Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6287588Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6287705Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6287842Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6287946Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6288164Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6288319Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6288408Z     def test_flash_decoding(
2025-04-11T04:23:18.6288489Z         bsz: int,
2025-04-11T04:23:18.6288577Z         block_size: int,
2025-04-11T04:23:18.6288668Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6288749Z         num_attn_heads: int,
2025-04-11T04:23:18.6288835Z         kv_group_num: int,
2025-04-11T04:23:18.6288921Z         same_context_len: bool,
2025-04-11T04:23:18.6289000Z         q_len: int,
2025-04-11T04:23:18.6289083Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6289171Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6289248Z     ):
2025-04-11T04:23:18.6289358Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6289553Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6289736Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6289909Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6290071Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6290317Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6290392Z     
2025-04-11T04:23:18.6290484Z         torch.manual_seed(123)
2025-04-11T04:23:18.6290584Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6290675Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6290679Z 
2025-04-11T04:23:18.6290844Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6290955Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6290958Z 
2025-04-11T04:23:18.6291040Z device = None
2025-04-11T04:23:18.6291044Z 
2025-04-11T04:23:18.6291164Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6291317Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6291394Z     
2025-04-11T04:23:18.6291467Z         Args:
2025-04-11T04:23:18.6291639Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6291803Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6291911Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6291984Z         """
2025-04-11T04:23:18.6292061Z         _lazy_init()
2025-04-11T04:23:18.6292160Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6292262Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6292370Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6292651Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6292786Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6292949Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6292953Z 
2025-04-11T04:23:18.6293195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6293367Z ____________ test_flash_decoding[True-False-1-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.6293371Z 
2025-04-11T04:23:18.6293520Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6293687Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6293776Z use_new_kcache_layout = True
2025-04-11T04:23:18.6293779Z 
2025-04-11T04:23:18.6293983Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6294087Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6294285Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6294431Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6294546Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6294665Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6294801Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6294908Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6295042Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6295192Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6295282Z     def test_flash_decoding(
2025-04-11T04:23:18.6295360Z         bsz: int,
2025-04-11T04:23:18.6295445Z         block_size: int,
2025-04-11T04:23:18.6295535Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6295616Z         num_attn_heads: int,
2025-04-11T04:23:18.6295703Z         kv_group_num: int,
2025-04-11T04:23:18.6295792Z         same_context_len: bool,
2025-04-11T04:23:18.6295872Z         q_len: int,
2025-04-11T04:23:18.6295957Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6296047Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6296221Z     ):
2025-04-11T04:23:18.6296333Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6296529Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6296713Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6296886Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6297047Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6297207Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6297280Z     
2025-04-11T04:23:18.6297366Z         torch.manual_seed(123)
2025-04-11T04:23:18.6297464Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6297555Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6297559Z 
2025-04-11T04:23:18.6297718Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6297832Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6297836Z 
2025-04-11T04:23:18.6297918Z device = None
2025-04-11T04:23:18.6297921Z 
2025-04-11T04:23:18.6298038Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6298186Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6298261Z     
2025-04-11T04:23:18.6298335Z         Args:
2025-04-11T04:23:18.6298505Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6298672Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6298780Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6298856Z         """
2025-04-11T04:23:18.6298935Z         _lazy_init()
2025-04-11T04:23:18.6299035Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6299135Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6299247Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6299529Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6299664Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6299827Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6299831Z 
2025-04-11T04:23:18.6300071Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6300243Z _____________ test_flash_decoding[True-False-1-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.6300247Z 
2025-04-11T04:23:18.6300573Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6300744Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6300835Z use_new_kcache_layout = True
2025-04-11T04:23:18.6300843Z 
2025-04-11T04:23:18.6301050Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6301156Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6301277Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6301418Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6301532Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6301649Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6301792Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6301896Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6302034Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6302186Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6302278Z     def test_flash_decoding(
2025-04-11T04:23:18.6302440Z         bsz: int,
2025-04-11T04:23:18.6302527Z         block_size: int,
2025-04-11T04:23:18.6302618Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6302702Z         num_attn_heads: int,
2025-04-11T04:23:18.6302787Z         kv_group_num: int,
2025-04-11T04:23:18.6302872Z         same_context_len: bool,
2025-04-11T04:23:18.6302960Z         q_len: int,
2025-04-11T04:23:18.6303044Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6303135Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6303208Z     ):
2025-04-11T04:23:18.6303317Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6303513Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6303697Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6303869Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6304031Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6304196Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6304268Z     
2025-04-11T04:23:18.6304353Z         torch.manual_seed(123)
2025-04-11T04:23:18.6304443Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6304532Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6304536Z 
2025-04-11T04:23:18.6304693Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6304802Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6304806Z 
2025-04-11T04:23:18.6304890Z device = None
2025-04-11T04:23:18.6304893Z 
2025-04-11T04:23:18.6305019Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6305173Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6305257Z     
2025-04-11T04:23:18.6305338Z         Args:
2025-04-11T04:23:18.6305514Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6305679Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6305786Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6305858Z         """
2025-04-11T04:23:18.6305936Z         _lazy_init()
2025-04-11T04:23:18.6306034Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6306134Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6306238Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6306519Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6306751Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6306912Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6306916Z 
2025-04-11T04:23:18.6307165Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6307340Z ____________ test_flash_decoding[True-False-1-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.6307344Z 
2025-04-11T04:23:18.6307495Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6307664Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6307753Z use_new_kcache_layout = True
2025-04-11T04:23:18.6307756Z 
2025-04-11T04:23:18.6307958Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6308061Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6308184Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6308323Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6308463Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6308676Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6308816Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6308926Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6309066Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6309221Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6309397Z     def test_flash_decoding(
2025-04-11T04:23:18.6309503Z         bsz: int,
2025-04-11T04:23:18.6309667Z         block_size: int,
2025-04-11T04:23:18.6309793Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6309984Z         num_attn_heads: int,
2025-04-11T04:23:18.6310143Z         kv_group_num: int,
2025-04-11T04:23:18.6310267Z         same_context_len: bool,
2025-04-11T04:23:18.6310421Z         q_len: int,
2025-04-11T04:23:18.6310541Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6310651Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6310832Z     ):
2025-04-11T04:23:18.6310975Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6311240Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6311456Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6311675Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6311880Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6312077Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6312212Z     
2025-04-11T04:23:18.6312340Z         torch.manual_seed(123)
2025-04-11T04:23:18.6312579Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6312689Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6312693Z 
2025-04-11T04:23:18.6312929Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6313076Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6313081Z 
2025-04-11T04:23:18.6313203Z device = None
2025-04-11T04:23:18.6313240Z 
2025-04-11T04:23:18.6313390Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6313570Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6313700Z     
2025-04-11T04:23:18.6313819Z         Args:
2025-04-11T04:23:18.6314058Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6314252Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6314416Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6314603Z         """
2025-04-11T04:23:18.6314718Z         _lazy_init()
2025-04-11T04:23:18.6314895Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6315025Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6315196Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6315506Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6315693Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6315902Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6315907Z 
2025-04-11T04:23:18.6316183Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6316508Z _____________ test_flash_decoding[True-False-1-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.6316512Z 
2025-04-11T04:23:18.6316695Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6316914Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6317039Z use_new_kcache_layout = True
2025-04-11T04:23:18.6317135Z 
2025-04-11T04:23:18.6317412Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6317547Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6317727Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6317896Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6318038Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6318233Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6318399Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6318561Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6318729Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6318938Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6319064Z     def test_flash_decoding(
2025-04-11T04:23:18.6319186Z         bsz: int,
2025-04-11T04:23:18.6319336Z         block_size: int,
2025-04-11T04:23:18.6319456Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6319615Z         num_attn_heads: int,
2025-04-11T04:23:18.6319717Z         kv_group_num: int,
2025-04-11T04:23:18.6319880Z         same_context_len: bool,
2025-04-11T04:23:18.6319986Z         q_len: int,
2025-04-11T04:23:18.6320191Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6320351Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6320455Z     ):
2025-04-11T04:23:18.6320624Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6320859Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6321075Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6321306Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6321509Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6321724Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6321833Z     
2025-04-11T04:23:18.6321994Z         torch.manual_seed(123)
2025-04-11T04:23:18.6322114Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6322247Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6322284Z 
2025-04-11T04:23:18.6322473Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6322600Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6322604Z 
2025-04-11T04:23:18.6322758Z device = None
2025-04-11T04:23:18.6322762Z 
2025-04-11T04:23:18.6323006Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6323223Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6323325Z     
2025-04-11T04:23:18.6323447Z         Args:
2025-04-11T04:23:18.6323656Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6323945Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6324120Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6324221Z         """
2025-04-11T04:23:18.6324358Z         _lazy_init()
2025-04-11T04:23:18.6324468Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6324651Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6324798Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6325113Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6325318Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6325504Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6325509Z 
2025-04-11T04:23:18.6325886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6326105Z _____________ test_flash_decoding[True-False-1-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.6326110Z 
2025-04-11T04:23:18.6326320Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6326512Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6326658Z use_new_kcache_layout = True
2025-04-11T04:23:18.6326662Z 
2025-04-11T04:23:18.6326877Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6327027Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6327219Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6327383Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6327560Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6327709Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6327986Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6328129Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6328291Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6328500Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6328613Z     def test_flash_decoding(
2025-04-11T04:23:18.6328751Z         bsz: int,
2025-04-11T04:23:18.6328872Z         block_size: int,
2025-04-11T04:23:18.6329038Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6329152Z         num_attn_heads: int,
2025-04-11T04:23:18.6329268Z         kv_group_num: int,
2025-04-11T04:23:18.6329431Z         same_context_len: bool,
2025-04-11T04:23:18.6329527Z         q_len: int,
2025-04-11T04:23:18.6329697Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6329819Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6329922Z     ):
2025-04-11T04:23:18.6330093Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6330328Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6330569Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6330778Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6331000Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6331189Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6331329Z     
2025-04-11T04:23:18.6331434Z         torch.manual_seed(123)
2025-04-11T04:23:18.6331730Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6331900Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6331905Z 
2025-04-11T04:23:18.6332089Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6332274Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6332278Z 
2025-04-11T04:23:18.6332389Z device = None
2025-04-11T04:23:18.6332393Z 
2025-04-11T04:23:18.6332586Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6332775Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6332878Z     
2025-04-11T04:23:18.6333026Z         Args:
2025-04-11T04:23:18.6333223Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6361738Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6361981Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6362104Z         """
2025-04-11T04:23:18.6362201Z         _lazy_init()
2025-04-11T04:23:18.6362335Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6362495Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6362850Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6363174Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6363326Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6363499Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6363506Z 
2025-04-11T04:23:18.6363775Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6363956Z _____________ test_flash_decoding[True-False-1-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.6363961Z 
2025-04-11T04:23:18.6364133Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6364305Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6364409Z use_new_kcache_layout = True
2025-04-11T04:23:18.6364417Z 
2025-04-11T04:23:18.6364641Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6364757Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6364890Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6365040Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6365165Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6365285Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6365435Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6365547Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6365699Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6365863Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6365967Z     def test_flash_decoding(
2025-04-11T04:23:18.6366055Z         bsz: int,
2025-04-11T04:23:18.6366149Z         block_size: int,
2025-04-11T04:23:18.6366253Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6366339Z         num_attn_heads: int,
2025-04-11T04:23:18.6366427Z         kv_group_num: int,
2025-04-11T04:23:18.6366522Z         same_context_len: bool,
2025-04-11T04:23:18.6366601Z         q_len: int,
2025-04-11T04:23:18.6366692Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6366782Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6366858Z     ):
2025-04-11T04:23:18.6366983Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6367184Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6367483Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6367666Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6367840Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6368009Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6368087Z     
2025-04-11T04:23:18.6368185Z         torch.manual_seed(123)
2025-04-11T04:23:18.6368280Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6368385Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6368389Z 
2025-04-11T04:23:18.6368557Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6368682Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6368686Z 
2025-04-11T04:23:18.6368768Z device = None
2025-04-11T04:23:18.6368773Z 
2025-04-11T04:23:18.6368901Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6369069Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6369143Z     
2025-04-11T04:23:18.6369224Z         Args:
2025-04-11T04:23:18.6369489Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6369665Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6369779Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6369855Z         """
2025-04-11T04:23:18.6369940Z         _lazy_init()
2025-04-11T04:23:18.6370038Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6370147Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6370255Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6370551Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6370699Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6370860Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6370864Z 
2025-04-11T04:23:18.6371124Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6371303Z _____________ test_flash_decoding[True-False-1-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.6371307Z 
2025-04-11T04:23:18.6371469Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6371637Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6371730Z use_new_kcache_layout = True
2025-04-11T04:23:18.6371734Z 
2025-04-11T04:23:18.6371938Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6372050Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6372175Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6372318Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6372443Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6372564Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6372711Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6372820Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6372959Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6373116Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6373205Z     def test_flash_decoding(
2025-04-11T04:23:18.6373289Z         bsz: int,
2025-04-11T04:23:18.6373374Z         block_size: int,
2025-04-11T04:23:18.6373473Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6373560Z         num_attn_heads: int,
2025-04-11T04:23:18.6373647Z         kv_group_num: int,
2025-04-11T04:23:18.6373743Z         same_context_len: bool,
2025-04-11T04:23:18.6373907Z         q_len: int,
2025-04-11T04:23:18.6374002Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6374095Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6374170Z     ):
2025-04-11T04:23:18.6374296Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6374496Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6374685Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6374860Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6375032Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6375191Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6375265Z     
2025-04-11T04:23:18.6395924Z         torch.manual_seed(123)
2025-04-11T04:23:18.6396052Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6396150Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6396154Z 
2025-04-11T04:23:18.6396318Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6396546Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6396550Z 
2025-04-11T04:23:18.6396632Z device = None
2025-04-11T04:23:18.6396637Z 
2025-04-11T04:23:18.6396772Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6396942Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6397018Z     
2025-04-11T04:23:18.6397100Z         Args:
2025-04-11T04:23:18.6397273Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6397451Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6397563Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6397643Z         """
2025-04-11T04:23:18.6397731Z         _lazy_init()
2025-04-11T04:23:18.6397830Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6397941Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6398055Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6398361Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6398503Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6398661Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6398665Z 
2025-04-11T04:23:18.6398917Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6399089Z _____________ test_flash_decoding[True-False-1-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.6399093Z 
2025-04-11T04:23:18.6399258Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6399428Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6399524Z use_new_kcache_layout = True
2025-04-11T04:23:18.6399532Z 
2025-04-11T04:23:18.6399735Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6399846Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6399967Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6400109Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6400229Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6400341Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6400484Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6400619Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6400852Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6401009Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6401102Z     def test_flash_decoding(
2025-04-11T04:23:18.6401182Z         bsz: int,
2025-04-11T04:23:18.6401267Z         block_size: int,
2025-04-11T04:23:18.6401364Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6401447Z         num_attn_heads: int,
2025-04-11T04:23:18.6401529Z         kv_group_num: int,
2025-04-11T04:23:18.6401619Z         same_context_len: bool,
2025-04-11T04:23:18.6401695Z         q_len: int,
2025-04-11T04:23:18.6401783Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6401871Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6401943Z     ):
2025-04-11T04:23:18.6402061Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6402252Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6402443Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6402616Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6402784Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6403042Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6403115Z     
2025-04-11T04:23:18.6403205Z         torch.manual_seed(123)
2025-04-11T04:23:18.6403294Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6403389Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6403393Z 
2025-04-11T04:23:18.6403549Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6403664Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6403668Z 
2025-04-11T04:23:18.6403745Z device = None
2025-04-11T04:23:18.6403749Z 
2025-04-11T04:23:18.6403873Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6404026Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6404097Z     
2025-04-11T04:23:18.6404173Z         Args:
2025-04-11T04:23:18.6404338Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6404511Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6404617Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6404689Z         """
2025-04-11T04:23:18.6404774Z         _lazy_init()
2025-04-11T04:23:18.6404869Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6404973Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6405076Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6405366Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6405507Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6405664Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6405668Z 
2025-04-11T04:23:18.6405919Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6406090Z ____________ test_flash_decoding[True-False-1-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.6406093Z 
2025-04-11T04:23:18.6406253Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6406419Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6406509Z use_new_kcache_layout = True
2025-04-11T04:23:18.6406512Z 
2025-04-11T04:23:18.6406711Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6406816Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6406932Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6407175Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6407296Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6407407Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6407550Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6407653Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6407792Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6407941Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6408025Z     def test_flash_decoding(
2025-04-11T04:23:18.6408104Z         bsz: int,
2025-04-11T04:23:18.6408204Z         block_size: int,
2025-04-11T04:23:18.6408303Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6408383Z         num_attn_heads: int,
2025-04-11T04:23:18.6408465Z         kv_group_num: int,
2025-04-11T04:23:18.6408552Z         same_context_len: bool,
2025-04-11T04:23:18.6408633Z         q_len: int,
2025-04-11T04:23:18.6429264Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6429358Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6429432Z     ):
2025-04-11T04:23:18.6429648Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6429847Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6430033Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6430205Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6430373Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6430529Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6430605Z     
2025-04-11T04:23:18.6430694Z         torch.manual_seed(123)
2025-04-11T04:23:18.6430784Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6430878Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6430882Z 
2025-04-11T04:23:18.6431036Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6431154Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6431158Z 
2025-04-11T04:23:18.6431236Z device = None
2025-04-11T04:23:18.6431240Z 
2025-04-11T04:23:18.6431358Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6431506Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6431576Z     
2025-04-11T04:23:18.6431653Z         Args:
2025-04-11T04:23:18.6431815Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6431980Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6432084Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6432163Z         """
2025-04-11T04:23:18.6432240Z         _lazy_init()
2025-04-11T04:23:18.6432334Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6432437Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6432543Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6432828Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6432961Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6433119Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6433123Z 
2025-04-11T04:23:18.6433362Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6433529Z _____________ test_flash_decoding[True-False-1-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.6433533Z 
2025-04-11T04:23:18.6433780Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6433948Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6434038Z use_new_kcache_layout = True
2025-04-11T04:23:18.6434046Z 
2025-04-11T04:23:18.6434243Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6434348Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6434462Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6434603Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6434719Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6434831Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6434973Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6435079Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6435219Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6435370Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6435454Z     def test_flash_decoding(
2025-04-11T04:23:18.6435533Z         bsz: int,
2025-04-11T04:23:18.6435702Z         block_size: int,
2025-04-11T04:23:18.6435794Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6435875Z         num_attn_heads: int,
2025-04-11T04:23:18.6435960Z         kv_group_num: int,
2025-04-11T04:23:18.6436046Z         same_context_len: bool,
2025-04-11T04:23:18.6436122Z         q_len: int,
2025-04-11T04:23:18.6436210Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6436298Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6436373Z     ):
2025-04-11T04:23:18.6436484Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6436680Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6436870Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6437051Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6437215Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6437380Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6437452Z     
2025-04-11T04:23:18.6437546Z         torch.manual_seed(123)
2025-04-11T04:23:18.6437635Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6437727Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6437735Z 
2025-04-11T04:23:18.6437888Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6438000Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6438004Z 
2025-04-11T04:23:18.6438084Z device = None
2025-04-11T04:23:18.6438088Z 
2025-04-11T04:23:18.6438204Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6438361Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6438431Z     
2025-04-11T04:23:18.6438509Z         Args:
2025-04-11T04:23:18.6438675Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6438842Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6438953Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6439026Z         """
2025-04-11T04:23:18.6439107Z         _lazy_init()
2025-04-11T04:23:18.6439201Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6439301Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6439409Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6439693Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6439915Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6440074Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6440078Z 
2025-04-11T04:23:18.6440322Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6440495Z ____________ test_flash_decoding[True-False-1-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.6440499Z 
2025-04-11T04:23:18.6440657Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6440818Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.6440902Z use_new_kcache_layout = True
2025-04-11T04:23:18.6440910Z 
2025-04-11T04:23:18.6441106Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6441207Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6441325Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6441464Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6441585Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6441694Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6441937Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6442043Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6442179Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6442333Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6442419Z     def test_flash_decoding(
2025-04-11T04:23:18.6442503Z         bsz: int,
2025-04-11T04:23:18.6442583Z         block_size: int,
2025-04-11T04:23:18.6442674Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6442758Z         num_attn_heads: int,
2025-04-11T04:23:18.6442842Z         kv_group_num: int,
2025-04-11T04:23:18.6442930Z         same_context_len: bool,
2025-04-11T04:23:18.6443012Z         q_len: int,
2025-04-11T04:23:18.6443095Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6443187Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6443259Z     ):
2025-04-11T04:23:18.6443374Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6443568Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6443751Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6443922Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6444083Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6444241Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6444312Z     
2025-04-11T04:23:18.6444401Z         torch.manual_seed(123)
2025-04-11T04:23:18.6444492Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6444587Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6444591Z 
2025-04-11T04:23:18.6444745Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6444861Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6444865Z 
2025-04-11T04:23:18.6444948Z device = None
2025-04-11T04:23:18.6444952Z 
2025-04-11T04:23:18.6445070Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6445223Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6445295Z     
2025-04-11T04:23:18.6445371Z         Args:
2025-04-11T04:23:18.6445535Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6445698Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6445805Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6445964Z         """
2025-04-11T04:23:18.6446047Z         _lazy_init()
2025-04-11T04:23:18.6446142Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6446244Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6446353Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6446639Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6446778Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6446934Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6446938Z 
2025-04-11T04:23:18.6447185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6447353Z ______________ test_flash_decoding[True-False-5-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6447357Z 
2025-04-11T04:23:18.6447514Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6447677Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6447765Z use_new_kcache_layout = True
2025-04-11T04:23:18.6447865Z 
2025-04-11T04:23:18.6448066Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6448167Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6448287Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6448423Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6448540Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6448651Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6448791Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6448892Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6449025Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6449181Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6449264Z     def test_flash_decoding(
2025-04-11T04:23:18.6449342Z         bsz: int,
2025-04-11T04:23:18.6449427Z         block_size: int,
2025-04-11T04:23:18.6449517Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6449601Z         num_attn_heads: int,
2025-04-11T04:23:18.6449684Z         kv_group_num: int,
2025-04-11T04:23:18.6449772Z         same_context_len: bool,
2025-04-11T04:23:18.6449850Z         q_len: int,
2025-04-11T04:23:18.6449934Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6450026Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6450099Z     ):
2025-04-11T04:23:18.6450212Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6450405Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6450592Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6450764Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6450930Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6451095Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6451168Z     
2025-04-11T04:23:18.6451256Z         torch.manual_seed(123)
2025-04-11T04:23:18.6451342Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6451434Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6451438Z 
2025-04-11T04:23:18.6451590Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6451700Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6451710Z 
2025-04-11T04:23:18.6451787Z device = None
2025-04-11T04:23:18.6451791Z 
2025-04-11T04:23:18.6451903Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6452210Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6452286Z     
2025-04-11T04:23:18.6452366Z         Args:
2025-04-11T04:23:18.6452531Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6452698Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6452806Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6452879Z         """
2025-04-11T04:23:18.6452959Z         _lazy_init()
2025-04-11T04:23:18.6453053Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6453158Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6453262Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6453542Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6453683Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6453837Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6453841Z 
2025-04-11T04:23:18.6454087Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6454340Z _____________ test_flash_decoding[True-False-5-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6454345Z 
2025-04-11T04:23:18.6454498Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6454661Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6454750Z use_new_kcache_layout = True
2025-04-11T04:23:18.6454754Z 
2025-04-11T04:23:18.6454954Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6455056Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6455176Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6455317Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6455434Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6455546Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6455687Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6455789Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6455923Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6456076Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6456160Z     def test_flash_decoding(
2025-04-11T04:23:18.6456237Z         bsz: int,
2025-04-11T04:23:18.6456318Z         block_size: int,
2025-04-11T04:23:18.6456408Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6456493Z         num_attn_heads: int,
2025-04-11T04:23:18.6456576Z         kv_group_num: int,
2025-04-11T04:23:18.6456664Z         same_context_len: bool,
2025-04-11T04:23:18.6456743Z         q_len: int,
2025-04-11T04:23:18.6456832Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6456918Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6456988Z     ):
2025-04-11T04:23:18.6457104Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6457295Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6457477Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6457647Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6457812Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6457966Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6458037Z     
2025-04-11T04:23:18.6458127Z         torch.manual_seed(123)
2025-04-11T04:23:18.6458304Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6458401Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6458405Z 
2025-04-11T04:23:18.6458558Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6458673Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6458680Z 
2025-04-11T04:23:18.6458759Z device = None
2025-04-11T04:23:18.6458763Z 
2025-04-11T04:23:18.6458879Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6459032Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6459103Z     
2025-04-11T04:23:18.6459178Z         Args:
2025-04-11T04:23:18.6459341Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6459508Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6459612Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6459688Z         """
2025-04-11T04:23:18.6459768Z         _lazy_init()
2025-04-11T04:23:18.6459862Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6459966Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6460154Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6460433Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6460571Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6460726Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6460730Z 
2025-04-11T04:23:18.6460972Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6461135Z ______________ test_flash_decoding[True-False-5-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6461139Z 
2025-04-11T04:23:18.6461297Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6461458Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6461545Z use_new_kcache_layout = True
2025-04-11T04:23:18.6461552Z 
2025-04-11T04:23:18.6461751Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6461852Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6461972Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6462109Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6462227Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6462340Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6462478Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6462580Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6462712Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6462867Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6462952Z     def test_flash_decoding(
2025-04-11T04:23:18.6463029Z         bsz: int,
2025-04-11T04:23:18.6463115Z         block_size: int,
2025-04-11T04:23:18.6463208Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6463289Z         num_attn_heads: int,
2025-04-11T04:23:18.6463369Z         kv_group_num: int,
2025-04-11T04:23:18.6463456Z         same_context_len: bool,
2025-04-11T04:23:18.6463531Z         q_len: int,
2025-04-11T04:23:18.6463620Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6463706Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6463776Z     ):
2025-04-11T04:23:18.6463890Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6464082Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6464349Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6464523Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6464687Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6464847Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6464918Z     
2025-04-11T04:23:18.6465007Z         torch.manual_seed(123)
2025-04-11T04:23:18.6465093Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6465188Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6465192Z 
2025-04-11T04:23:18.6465343Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6465456Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6465460Z 
2025-04-11T04:23:18.6465537Z device = None
2025-04-11T04:23:18.6465541Z 
2025-04-11T04:23:18.6465656Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6465815Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6465886Z     
2025-04-11T04:23:18.6465963Z         Args:
2025-04-11T04:23:18.6466128Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6466384Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6466489Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6466561Z         """
2025-04-11T04:23:18.6466643Z         _lazy_init()
2025-04-11T04:23:18.6466736Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6466842Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6466946Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6467227Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6467369Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6467524Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6467528Z 
2025-04-11T04:23:18.6467771Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6467939Z _____________ test_flash_decoding[True-False-5-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6467943Z 
2025-04-11T04:23:18.6468097Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6468257Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6468347Z use_new_kcache_layout = True
2025-04-11T04:23:18.6468351Z 
2025-04-11T04:23:18.6468610Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6468716Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6468831Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6468971Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6469089Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6469202Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6469343Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6469445Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6469576Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6469728Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6469810Z     def test_flash_decoding(
2025-04-11T04:23:18.6469889Z         bsz: int,
2025-04-11T04:23:18.6469969Z         block_size: int,
2025-04-11T04:23:18.6470063Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6470147Z         num_attn_heads: int,
2025-04-11T04:23:18.6470230Z         kv_group_num: int,
2025-04-11T04:23:18.6470316Z         same_context_len: bool,
2025-04-11T04:23:18.6470493Z         q_len: int,
2025-04-11T04:23:18.6470587Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6470675Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6470746Z     ):
2025-04-11T04:23:18.6470865Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6471057Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6471242Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6471414Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6471583Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6471740Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6471810Z     
2025-04-11T04:23:18.6471900Z         torch.manual_seed(123)
2025-04-11T04:23:18.6471986Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6472082Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6472086Z 
2025-04-11T04:23:18.6472239Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6472464Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6472468Z 
2025-04-11T04:23:18.6472545Z device = None
2025-04-11T04:23:18.6472549Z 
2025-04-11T04:23:18.6472664Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6472818Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6472890Z     
2025-04-11T04:23:18.6472966Z         Args:
2025-04-11T04:23:18.6473130Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6473295Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6473400Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6473472Z         """
2025-04-11T04:23:18.6473556Z         _lazy_init()
2025-04-11T04:23:18.6473649Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6473752Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6473858Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6474141Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6474276Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6474431Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6474435Z 
2025-04-11T04:23:18.6474676Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6474840Z _____________ test_flash_decoding[True-False-5-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6474844Z 
2025-04-11T04:23:18.6475002Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6475162Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6475250Z use_new_kcache_layout = True
2025-04-11T04:23:18.6475257Z 
2025-04-11T04:23:18.6475453Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6475559Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6475675Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6475810Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6475928Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6476038Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6476175Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6476278Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6476409Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6476649Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6476738Z     def test_flash_decoding(
2025-04-11T04:23:18.6476817Z         bsz: int,
2025-04-11T04:23:18.6476904Z         block_size: int,
2025-04-11T04:23:18.6476999Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6477080Z         num_attn_heads: int,
2025-04-11T04:23:18.6477163Z         kv_group_num: int,
2025-04-11T04:23:18.6477253Z         same_context_len: bool,
2025-04-11T04:23:18.6477329Z         q_len: int,
2025-04-11T04:23:18.6477415Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6477502Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6477574Z     ):
2025-04-11T04:23:18.6477689Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6477879Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6478061Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6478233Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6478399Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6478645Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6478718Z     
2025-04-11T04:23:18.6478808Z         torch.manual_seed(123)
2025-04-11T04:23:18.6478895Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6478991Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6478996Z 
2025-04-11T04:23:18.6479149Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6479263Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6479267Z 
2025-04-11T04:23:18.6479344Z device = None
2025-04-11T04:23:18.6479349Z 
2025-04-11T04:23:18.6479465Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6479621Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6479692Z     
2025-04-11T04:23:18.6479769Z         Args:
2025-04-11T04:23:18.6479937Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6480110Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6480215Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6480288Z         """
2025-04-11T04:23:18.6480373Z         _lazy_init()
2025-04-11T04:23:18.6480467Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6480573Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6480676Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6480966Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6481105Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6481260Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6481264Z 
2025-04-11T04:23:18.6481506Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6481677Z _____________ test_flash_decoding[True-False-5-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6481681Z 
2025-04-11T04:23:18.6481838Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6482000Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6482089Z use_new_kcache_layout = True
2025-04-11T04:23:18.6482093Z 
2025-04-11T04:23:18.6482291Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6482396Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6482511Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6482733Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6482863Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6482973Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6483115Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6483217Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6483353Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6483499Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6483583Z     def test_flash_decoding(
2025-04-11T04:23:18.6483663Z         bsz: int,
2025-04-11T04:23:18.6483745Z         block_size: int,
2025-04-11T04:23:18.6483839Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6483920Z         num_attn_heads: int,
2025-04-11T04:23:18.6484003Z         kv_group_num: int,
2025-04-11T04:23:18.6484091Z         same_context_len: bool,
2025-04-11T04:23:18.6484170Z         q_len: int,
2025-04-11T04:23:18.6484258Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6484345Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6484417Z     ):
2025-04-11T04:23:18.6484531Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6484817Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6485000Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6485169Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6485334Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6485489Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6485565Z     
2025-04-11T04:23:18.6485650Z         torch.manual_seed(123)
2025-04-11T04:23:18.6485735Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6485838Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6485842Z 
2025-04-11T04:23:18.6485996Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6486111Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6486118Z 
2025-04-11T04:23:18.6486198Z device = None
2025-04-11T04:23:18.6486202Z 
2025-04-11T04:23:18.6486321Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6486470Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6486539Z     
2025-04-11T04:23:18.6486618Z         Args:
2025-04-11T04:23:18.6486783Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6486953Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6487058Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6487129Z         """
2025-04-11T04:23:18.6487210Z         _lazy_init()
2025-04-11T04:23:18.6487304Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6487409Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6487515Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6487803Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6487938Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6488095Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6488102Z 
2025-04-11T04:23:18.6488342Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6488508Z _____________ test_flash_decoding[True-False-5-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6488512Z 
2025-04-11T04:23:18.6488750Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6488917Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6489004Z use_new_kcache_layout = True
2025-04-11T04:23:18.6489008Z 
2025-04-11T04:23:18.6489208Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6489314Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6489428Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6489564Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6489684Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6489795Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6489933Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6490034Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6490171Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6490326Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6490416Z     def test_flash_decoding(
2025-04-11T04:23:18.6490497Z         bsz: int,
2025-04-11T04:23:18.6490579Z         block_size: int,
2025-04-11T04:23:18.6490758Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6490838Z         num_attn_heads: int,
2025-04-11T04:23:18.6490920Z         kv_group_num: int,
2025-04-11T04:23:18.6491011Z         same_context_len: bool,
2025-04-11T04:23:18.6491087Z         q_len: int,
2025-04-11T04:23:18.6491174Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6491261Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6491338Z     ):
2025-04-11T04:23:18.6491448Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6491642Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6491823Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6491995Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6492160Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6492319Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6492393Z     
2025-04-11T04:23:18.6492479Z         torch.manual_seed(123)
2025-04-11T04:23:18.6492565Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6492659Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6492662Z 
2025-04-11T04:23:18.6492815Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6492929Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6492933Z 
2025-04-11T04:23:18.6493010Z device = None
2025-04-11T04:23:18.6493014Z 
2025-04-11T04:23:18.6493136Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6493289Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6493359Z     
2025-04-11T04:23:18.6493436Z         Args:
2025-04-11T04:23:18.6493602Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6493774Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6493878Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6493953Z         """
2025-04-11T04:23:18.6494032Z         _lazy_init()
2025-04-11T04:23:18.6494125Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6494230Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6494332Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6494618Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6494849Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6495009Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6495017Z 
2025-04-11T04:23:18.6495255Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6495424Z _____________ test_flash_decoding[True-False-5-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6495428Z 
2025-04-11T04:23:18.6495585Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6495748Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6495838Z use_new_kcache_layout = True
2025-04-11T04:23:18.6495842Z 
2025-04-11T04:23:18.6496040Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6496147Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6496263Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6496405Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6496524Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6496635Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6496860Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6496962Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6497100Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6497251Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6497338Z     def test_flash_decoding(
2025-04-11T04:23:18.6497419Z         bsz: int,
2025-04-11T04:23:18.6497501Z         block_size: int,
2025-04-11T04:23:18.6497594Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6497676Z         num_attn_heads: int,
2025-04-11T04:23:18.6497759Z         kv_group_num: int,
2025-04-11T04:23:18.6497848Z         same_context_len: bool,
2025-04-11T04:23:18.6497927Z         q_len: int,
2025-04-11T04:23:18.6498016Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6498104Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6498180Z     ):
2025-04-11T04:23:18.6498289Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6498484Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6498670Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6498837Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6499004Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6499159Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6499236Z     
2025-04-11T04:23:18.6499321Z         torch.manual_seed(123)
2025-04-11T04:23:18.6499407Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6499505Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6499508Z 
2025-04-11T04:23:18.6499662Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6499778Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6499786Z 
2025-04-11T04:23:18.6499865Z device = None
2025-04-11T04:23:18.6499869Z 
2025-04-11T04:23:18.6499988Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6500135Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6500208Z     
2025-04-11T04:23:18.6500290Z         Args:
2025-04-11T04:23:18.6500457Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6500623Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6500729Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6500803Z         """
2025-04-11T04:23:18.6500968Z         _lazy_init()
2025-04-11T04:23:18.6501066Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6501171Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6501279Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6501572Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6501710Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6501871Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6501875Z 
2025-04-11T04:23:18.6502117Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6502286Z ______________ test_flash_decoding[True-False-5-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6502293Z 
2025-04-11T04:23:18.6502445Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6502610Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6502699Z use_new_kcache_layout = True
2025-04-11T04:23:18.6502703Z 
2025-04-11T04:23:18.6503004Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6503110Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6503226Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6503369Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6503485Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6503597Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6503735Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6503836Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6503971Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6504122Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6504209Z     def test_flash_decoding(
2025-04-11T04:23:18.6504288Z         bsz: int,
2025-04-11T04:23:18.6504367Z         block_size: int,
2025-04-11T04:23:18.6504464Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6504545Z         num_attn_heads: int,
2025-04-11T04:23:18.6504629Z         kv_group_num: int,
2025-04-11T04:23:18.6504712Z         same_context_len: bool,
2025-04-11T04:23:18.6504787Z         q_len: int,
2025-04-11T04:23:18.6504875Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6504965Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6505043Z     ):
2025-04-11T04:23:18.6505154Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6505347Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6505531Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6505703Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6505870Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6506029Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6506103Z     
2025-04-11T04:23:18.6506188Z         torch.manual_seed(123)
2025-04-11T04:23:18.6506274Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6506369Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6506372Z 
2025-04-11T04:23:18.6506525Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6506638Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6506642Z 
2025-04-11T04:23:18.6506717Z device = None
2025-04-11T04:23:18.6506721Z 
2025-04-11T04:23:18.6506836Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6507066Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6507141Z     
2025-04-11T04:23:18.6507219Z         Args:
2025-04-11T04:23:18.6507386Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6507559Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6507663Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6507736Z         """
2025-04-11T04:23:18.6507813Z         _lazy_init()
2025-04-11T04:23:18.6507907Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6508013Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6508115Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6508399Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6508580Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6508743Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6508747Z 
2025-04-11T04:23:18.6508987Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6509250Z _____________ test_flash_decoding[True-False-5-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6509257Z 
2025-04-11T04:23:18.6509408Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6509570Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6509663Z use_new_kcache_layout = True
2025-04-11T04:23:18.6509667Z 
2025-04-11T04:23:18.6509860Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6509965Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6510077Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6510220Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6510333Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6510445Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6510587Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6510688Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6510823Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6510972Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6511061Z     def test_flash_decoding(
2025-04-11T04:23:18.6511136Z         bsz: int,
2025-04-11T04:23:18.6511218Z         block_size: int,
2025-04-11T04:23:18.6511309Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6511389Z         num_attn_heads: int,
2025-04-11T04:23:18.6511472Z         kv_group_num: int,
2025-04-11T04:23:18.6511557Z         same_context_len: bool,
2025-04-11T04:23:18.6511631Z         q_len: int,
2025-04-11T04:23:18.6511720Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6511808Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6511883Z     ):
2025-04-11T04:23:18.6511992Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6512185Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6512369Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6512537Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6512701Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6512856Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6512933Z     
2025-04-11T04:23:18.6513017Z         torch.manual_seed(123)
2025-04-11T04:23:18.6513106Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6513297Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6513301Z 
2025-04-11T04:23:18.6513458Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6513574Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6513581Z 
2025-04-11T04:23:18.6513659Z device = None
2025-04-11T04:23:18.6513663Z 
2025-04-11T04:23:18.6513783Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6513932Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6514005Z     
2025-04-11T04:23:18.6514079Z         Args:
2025-04-11T04:23:18.6514244Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6514410Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6514514Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6514589Z         """
2025-04-11T04:23:18.6514668Z         _lazy_init()
2025-04-11T04:23:18.6514762Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6514867Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6514970Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6515337Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6515472Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6515632Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6515636Z 
2025-04-11T04:23:18.6515876Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6516044Z ______________ test_flash_decoding[True-False-5-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6516049Z 
2025-04-11T04:23:18.6516199Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6516363Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6516455Z use_new_kcache_layout = True
2025-04-11T04:23:18.6516458Z 
2025-04-11T04:23:18.6516661Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6516767Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6516880Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6517020Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6517136Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6517247Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6517386Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6517485Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6517619Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6517770Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6517858Z     def test_flash_decoding(
2025-04-11T04:23:18.6517934Z         bsz: int,
2025-04-11T04:23:18.6518014Z         block_size: int,
2025-04-11T04:23:18.6518114Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6518195Z         num_attn_heads: int,
2025-04-11T04:23:18.6518279Z         kv_group_num: int,
2025-04-11T04:23:18.6518362Z         same_context_len: bool,
2025-04-11T04:23:18.6518439Z         q_len: int,
2025-04-11T04:23:18.6518526Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6518613Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6518688Z     ):
2025-04-11T04:23:18.6518798Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6518988Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6519168Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6519441Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6519612Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6519771Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6519847Z     
2025-04-11T04:23:18.6519934Z         torch.manual_seed(123)
2025-04-11T04:23:18.6520022Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6520117Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6520121Z 
2025-04-11T04:23:18.6520275Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6520388Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6520392Z 
2025-04-11T04:23:18.6520469Z device = None
2025-04-11T04:23:18.6520473Z 
2025-04-11T04:23:18.6520594Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6520747Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6520821Z     
2025-04-11T04:23:18.6520894Z         Args:
2025-04-11T04:23:18.6521058Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6521310Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6521416Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6521490Z         """
2025-04-11T04:23:18.6521569Z         _lazy_init()
2025-04-11T04:23:18.6521662Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6521766Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6521867Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6522155Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6522291Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6522457Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6522461Z 
2025-04-11T04:23:18.6522699Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6522873Z _____________ test_flash_decoding[True-False-5-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6522877Z 
2025-04-11T04:23:18.6523026Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6523188Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6523278Z use_new_kcache_layout = True
2025-04-11T04:23:18.6523282Z 
2025-04-11T04:23:18.6523480Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6523586Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6523699Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6523841Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6523955Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6524068Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6524210Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6524311Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6524451Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6524602Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6524689Z     def test_flash_decoding(
2025-04-11T04:23:18.6524765Z         bsz: int,
2025-04-11T04:23:18.6524848Z         block_size: int,
2025-04-11T04:23:18.6524942Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6525023Z         num_attn_heads: int,
2025-04-11T04:23:18.6525109Z         kv_group_num: int,
2025-04-11T04:23:18.6525192Z         same_context_len: bool,
2025-04-11T04:23:18.6525270Z         q_len: int,
2025-04-11T04:23:18.6525440Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6525529Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6525606Z     ):
2025-04-11T04:23:18.6525717Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6525914Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6526093Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6526262Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6526429Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6526583Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6526658Z     
2025-04-11T04:23:18.6526743Z         torch.manual_seed(123)
2025-04-11T04:23:18.6526833Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6526926Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6526930Z 
2025-04-11T04:23:18.6527081Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6527198Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6527287Z 
2025-04-11T04:23:18.6527367Z device = None
2025-04-11T04:23:18.6527371Z 
2025-04-11T04:23:18.6527491Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6527640Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6527713Z     
2025-04-11T04:23:18.6527789Z         Args:
2025-04-11T04:23:18.6527954Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6528119Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6528222Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6528296Z         """
2025-04-11T04:23:18.6528377Z         _lazy_init()
2025-04-11T04:23:18.6528474Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6528575Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6528678Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6528965Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6529098Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6529255Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6529259Z 
2025-04-11T04:23:18.6529496Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6529661Z _____________ test_flash_decoding[True-False-5-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6529666Z 
2025-04-11T04:23:18.6529816Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6529980Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6530069Z use_new_kcache_layout = True
2025-04-11T04:23:18.6530072Z 
2025-04-11T04:23:18.6530270Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6530380Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6530493Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6530632Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6530744Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6530857Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6530993Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6531095Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6531232Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6531469Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6531559Z     def test_flash_decoding(
2025-04-11T04:23:18.6531635Z         bsz: int,
2025-04-11T04:23:18.6531715Z         block_size: int,
2025-04-11T04:23:18.6531811Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6531892Z         num_attn_heads: int,
2025-04-11T04:23:18.6531976Z         kv_group_num: int,
2025-04-11T04:23:18.6532058Z         same_context_len: bool,
2025-04-11T04:23:18.6532133Z         q_len: int,
2025-04-11T04:23:18.6532220Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6532306Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6532382Z     ):
2025-04-11T04:23:18.6532491Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6532683Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6532861Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6533033Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6533201Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6533442Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6533517Z     
2025-04-11T04:23:18.6533602Z         torch.manual_seed(123)
2025-04-11T04:23:18.6533691Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6533781Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6533785Z 
2025-04-11T04:23:18.6533937Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6534051Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6534055Z 
2025-04-11T04:23:18.6534132Z device = None
2025-04-11T04:23:18.6534135Z 
2025-04-11T04:23:18.6534254Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6534406Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6534480Z     
2025-04-11T04:23:18.6534553Z         Args:
2025-04-11T04:23:18.6534715Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6534884Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6534989Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6535066Z         """
2025-04-11T04:23:18.6535141Z         _lazy_init()
2025-04-11T04:23:18.6535238Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6535339Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6535442Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6535725Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6535859Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6536020Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6536024Z 
2025-04-11T04:23:18.6536265Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6536434Z _____________ test_flash_decoding[True-False-5-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6536438Z 
2025-04-11T04:23:18.6536590Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6536755Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6536840Z use_new_kcache_layout = True
2025-04-11T04:23:18.6536844Z 
2025-04-11T04:23:18.6537041Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6537150Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6537265Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6537487Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6537605Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6537718Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6537856Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6537957Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6538093Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6538242Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6538329Z     def test_flash_decoding(
2025-04-11T04:23:18.6538404Z         bsz: int,
2025-04-11T04:23:18.6538482Z         block_size: int,
2025-04-11T04:23:18.6538574Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6538653Z         num_attn_heads: int,
2025-04-11T04:23:18.6538736Z         kv_group_num: int,
2025-04-11T04:23:18.6538818Z         same_context_len: bool,
2025-04-11T04:23:18.6538895Z         q_len: int,
2025-04-11T04:23:18.6538983Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6539069Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6539142Z     ):
2025-04-11T04:23:18.6539253Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6539545Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6539729Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6539901Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6540069Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6540226Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6540300Z     
2025-04-11T04:23:18.6540384Z         torch.manual_seed(123)
2025-04-11T04:23:18.6540474Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6540566Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6540570Z 
2025-04-11T04:23:18.6540723Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6540836Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6540844Z 
2025-04-11T04:23:18.6540920Z device = None
2025-04-11T04:23:18.6540924Z 
2025-04-11T04:23:18.6541042Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6541192Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6541265Z     
2025-04-11T04:23:18.6541337Z         Args:
2025-04-11T04:23:18.6541502Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6541671Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6541775Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6541851Z         """
2025-04-11T04:23:18.6541931Z         _lazy_init()
2025-04-11T04:23:18.6542028Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6542130Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6542232Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6542526Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6542661Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6542819Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6542823Z 
2025-04-11T04:23:18.6543063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6543233Z _____________ test_flash_decoding[True-False-5-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6543237Z 
2025-04-11T04:23:18.6543387Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6543635Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6543725Z use_new_kcache_layout = True
2025-04-11T04:23:18.6543730Z 
2025-04-11T04:23:18.6543932Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6544041Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6544156Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6544298Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6544412Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6544527Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6544663Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6544764Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6544902Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6545054Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6545140Z     def test_flash_decoding(
2025-04-11T04:23:18.6545215Z         bsz: int,
2025-04-11T04:23:18.6545298Z         block_size: int,
2025-04-11T04:23:18.6545547Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6545628Z         num_attn_heads: int,
2025-04-11T04:23:18.6545715Z         kv_group_num: int,
2025-04-11T04:23:18.6545798Z         same_context_len: bool,
2025-04-11T04:23:18.6545876Z         q_len: int,
2025-04-11T04:23:18.6545959Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6546046Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6546122Z     ):
2025-04-11T04:23:18.6546229Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6546425Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6546606Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6546781Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6546943Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6547105Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6547181Z     
2025-04-11T04:23:18.6547267Z         torch.manual_seed(123)
2025-04-11T04:23:18.6547358Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6547450Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6547454Z 
2025-04-11T04:23:18.6547611Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6547724Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6547728Z 
2025-04-11T04:23:18.6547804Z device = None
2025-04-11T04:23:18.6547808Z 
2025-04-11T04:23:18.6547927Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6548080Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6548157Z     
2025-04-11T04:23:18.6548229Z         Args:
2025-04-11T04:23:18.6548395Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6548590Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6548695Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6548769Z         """
2025-04-11T04:23:18.6548845Z         _lazy_init()
2025-04-11T04:23:18.6548940Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6549043Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6549144Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6549430Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6549564Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6549822Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6549827Z 
2025-04-11T04:23:18.6550065Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6550237Z _____________ test_flash_decoding[True-False-5-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6550241Z 
2025-04-11T04:23:18.6550393Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6550556Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6550643Z use_new_kcache_layout = True
2025-04-11T04:23:18.6550646Z 
2025-04-11T04:23:18.6550843Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6550950Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6551065Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6551209Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6551323Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6551440Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6551669Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6551770Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6551909Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6552061Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6552151Z     def test_flash_decoding(
2025-04-11T04:23:18.6552225Z         bsz: int,
2025-04-11T04:23:18.6552308Z         block_size: int,
2025-04-11T04:23:18.6552396Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6552475Z         num_attn_heads: int,
2025-04-11T04:23:18.6552559Z         kv_group_num: int,
2025-04-11T04:23:18.6552642Z         same_context_len: bool,
2025-04-11T04:23:18.6552720Z         q_len: int,
2025-04-11T04:23:18.6552806Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6552893Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6552969Z     ):
2025-04-11T04:23:18.6553078Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6553277Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6553454Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6553630Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6553795Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6553949Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6554023Z     
2025-04-11T04:23:18.6554106Z         torch.manual_seed(123)
2025-04-11T04:23:18.6554197Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6554289Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6554293Z 
2025-04-11T04:23:18.6554447Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6554556Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6554564Z 
2025-04-11T04:23:18.6554643Z device = None
2025-04-11T04:23:18.6554647Z 
2025-04-11T04:23:18.6554766Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6554914Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6554989Z     
2025-04-11T04:23:18.6555063Z         Args:
2025-04-11T04:23:18.6555230Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6555394Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6555497Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6555574Z         """
2025-04-11T04:23:18.6555650Z         _lazy_init()
2025-04-11T04:23:18.6555832Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6555935Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6556037Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6556325Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6556461Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6556622Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6556626Z 
2025-04-11T04:23:18.6556866Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6557033Z _____________ test_flash_decoding[True-False-5-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6557037Z 
2025-04-11T04:23:18.6557187Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6557355Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6557440Z use_new_kcache_layout = True
2025-04-11T04:23:18.6557444Z 
2025-04-11T04:23:18.6557648Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6557835Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6557954Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6558099Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6558218Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6558338Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6558477Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6558583Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6558724Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6558881Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6558971Z     def test_flash_decoding(
2025-04-11T04:23:18.6559049Z         bsz: int,
2025-04-11T04:23:18.6559138Z         block_size: int,
2025-04-11T04:23:18.6559238Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6559324Z         num_attn_heads: int,
2025-04-11T04:23:18.6559415Z         kv_group_num: int,
2025-04-11T04:23:18.6559502Z         same_context_len: bool,
2025-04-11T04:23:18.6559588Z         q_len: int,
2025-04-11T04:23:18.6559676Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6559767Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6559849Z     ):
2025-04-11T04:23:18.6559964Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6560164Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6560350Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6560530Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6560698Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6560861Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6560940Z     
2025-04-11T04:23:18.6561031Z         torch.manual_seed(123)
2025-04-11T04:23:18.6561127Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6561223Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6561227Z 
2025-04-11T04:23:18.6561388Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6561502Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6561506Z 
2025-04-11T04:23:18.6561588Z device = None
2025-04-11T04:23:18.6561596Z 
2025-04-11T04:23:18.6561715Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6561968Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6562048Z     
2025-04-11T04:23:18.6562122Z         Args:
2025-04-11T04:23:18.6562293Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6562462Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6562565Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6562642Z         """
2025-04-11T04:23:18.6562719Z         _lazy_init()
2025-04-11T04:23:18.6562817Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6562917Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6563024Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6563308Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6563444Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6563607Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6563610Z 
2025-04-11T04:23:18.6563855Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6564138Z _____________ test_flash_decoding[True-False-5-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.6564142Z 
2025-04-11T04:23:18.6564293Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6564459Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6564546Z use_new_kcache_layout = True
2025-04-11T04:23:18.6564550Z 
2025-04-11T04:23:18.6564752Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6564854Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6564970Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6565115Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6565231Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6565345Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6565484Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6565590Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6565723Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6565872Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6565961Z     def test_flash_decoding(
2025-04-11T04:23:18.6566036Z         bsz: int,
2025-04-11T04:23:18.6566117Z         block_size: int,
2025-04-11T04:23:18.6566207Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6566287Z         num_attn_heads: int,
2025-04-11T04:23:18.6566375Z         kv_group_num: int,
2025-04-11T04:23:18.6566458Z         same_context_len: bool,
2025-04-11T04:23:18.6566537Z         q_len: int,
2025-04-11T04:23:18.6566626Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6566713Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6566789Z     ):
2025-04-11T04:23:18.6566900Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6567098Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6567277Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6567448Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6567609Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6567766Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6567841Z     
2025-04-11T04:23:18.6567925Z         torch.manual_seed(123)
2025-04-11T04:23:18.6568017Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6568193Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6568198Z 
2025-04-11T04:23:18.6568356Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6568468Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6568476Z 
2025-04-11T04:23:18.6568555Z device = None
2025-04-11T04:23:18.6568562Z 
2025-04-11T04:23:18.6568675Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6568825Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6568899Z     
2025-04-11T04:23:18.6568972Z         Args:
2025-04-11T04:23:18.6569141Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6569306Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6569410Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6569487Z         """
2025-04-11T04:23:18.6569562Z         _lazy_init()
2025-04-11T04:23:18.6569664Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6569764Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6569872Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6570238Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6570372Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6570531Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6570535Z 
2025-04-11T04:23:18.6570772Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6570940Z _____________ test_flash_decoding[True-False-5-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.6570944Z 
2025-04-11T04:23:18.6571093Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6571261Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6571346Z use_new_kcache_layout = True
2025-04-11T04:23:18.6571349Z 
2025-04-11T04:23:18.6571553Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6571661Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6571773Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6571918Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6572030Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6572152Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6572288Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6572398Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6572533Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6572685Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6572773Z     def test_flash_decoding(
2025-04-11T04:23:18.6572849Z         bsz: int,
2025-04-11T04:23:18.6572934Z         block_size: int,
2025-04-11T04:23:18.6573025Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6573107Z         num_attn_heads: int,
2025-04-11T04:23:18.6573192Z         kv_group_num: int,
2025-04-11T04:23:18.6573275Z         same_context_len: bool,
2025-04-11T04:23:18.6573358Z         q_len: int,
2025-04-11T04:23:18.6573440Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6573531Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6573603Z     ):
2025-04-11T04:23:18.6573712Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6573906Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6574086Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6574349Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6574515Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6574678Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6574751Z     
2025-04-11T04:23:18.6574836Z         torch.manual_seed(123)
2025-04-11T04:23:18.6574927Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6575016Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6575020Z 
2025-04-11T04:23:18.6575176Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6575287Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6575291Z 
2025-04-11T04:23:18.6575371Z device = None
2025-04-11T04:23:18.6575375Z 
2025-04-11T04:23:18.6575489Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6575641Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6575716Z     
2025-04-11T04:23:18.6575789Z         Args:
2025-04-11T04:23:18.6575957Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6576216Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6576322Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6576395Z         """
2025-04-11T04:23:18.6576470Z         _lazy_init()
2025-04-11T04:23:18.6576567Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6576668Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6576774Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6577056Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6577189Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6577351Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6577355Z 
2025-04-11T04:23:18.6577594Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6577769Z _____________ test_flash_decoding[True-False-5-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.6577773Z 
2025-04-11T04:23:18.6577923Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6578090Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6578177Z use_new_kcache_layout = True
2025-04-11T04:23:18.6578181Z 
2025-04-11T04:23:18.6578382Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6578486Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6578600Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6578745Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6578859Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6578974Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6579109Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6579216Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6579350Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6579498Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6579587Z     def test_flash_decoding(
2025-04-11T04:23:18.6579661Z         bsz: int,
2025-04-11T04:23:18.6579746Z         block_size: int,
2025-04-11T04:23:18.6579837Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6579918Z         num_attn_heads: int,
2025-04-11T04:23:18.6580004Z         kv_group_num: int,
2025-04-11T04:23:18.6580087Z         same_context_len: bool,
2025-04-11T04:23:18.6580166Z         q_len: int,
2025-04-11T04:23:18.6580332Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6580429Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6580504Z     ):
2025-04-11T04:23:18.6580614Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6580812Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6580994Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6581167Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6581329Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6581488Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6581560Z     
2025-04-11T04:23:18.6581649Z         torch.manual_seed(123)
2025-04-11T04:23:18.6581741Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6581831Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6581839Z 
2025-04-11T04:23:18.6581998Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6582108Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6582200Z 
2025-04-11T04:23:18.6582286Z device = None
2025-04-11T04:23:18.6582290Z 
2025-04-11T04:23:18.6582407Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6582558Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6582633Z     
2025-04-11T04:23:18.6582706Z         Args:
2025-04-11T04:23:18.6582874Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6583039Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6583147Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6583224Z         """
2025-04-11T04:23:18.6583301Z         _lazy_init()
2025-04-11T04:23:18.6583404Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6583504Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6583614Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6583900Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6584042Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6584198Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6584202Z 
2025-04-11T04:23:18.6584446Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6584618Z _____________ test_flash_decoding[True-False-5-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.6584622Z 
2025-04-11T04:23:18.6584774Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6584944Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6585029Z use_new_kcache_layout = True
2025-04-11T04:23:18.6585033Z 
2025-04-11T04:23:18.6585235Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6585344Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6585463Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6585602Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6585714Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6585830Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6585965Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6586072Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6586206Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6586457Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6586549Z     def test_flash_decoding(
2025-04-11T04:23:18.6586625Z         bsz: int,
2025-04-11T04:23:18.6586709Z         block_size: int,
2025-04-11T04:23:18.6586801Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6586888Z         num_attn_heads: int,
2025-04-11T04:23:18.6586970Z         kv_group_num: int,
2025-04-11T04:23:18.6587054Z         same_context_len: bool,
2025-04-11T04:23:18.6587133Z         q_len: int,
2025-04-11T04:23:18.6587216Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6587305Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6587377Z     ):
2025-04-11T04:23:18.6587486Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6587683Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6587865Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6588041Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6588205Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6588480Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6588555Z     
2025-04-11T04:23:18.6588643Z         torch.manual_seed(123)
2025-04-11T04:23:18.6588735Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6588826Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6588829Z 
2025-04-11T04:23:18.6588985Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6589096Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6589100Z 
2025-04-11T04:23:18.6589183Z device = None
2025-04-11T04:23:18.6589186Z 
2025-04-11T04:23:18.6589301Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6589453Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6589526Z     
2025-04-11T04:23:18.6589597Z         Args:
2025-04-11T04:23:18.6589766Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6589934Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6590039Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6590110Z         """
2025-04-11T04:23:18.6590190Z         _lazy_init()
2025-04-11T04:23:18.6590287Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6590388Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6590496Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6590778Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6590916Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6591074Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6591078Z 
2025-04-11T04:23:18.6591316Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6591495Z ____________ test_flash_decoding[True-False-5-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.6591499Z 
2025-04-11T04:23:18.6591652Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6591818Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6591903Z use_new_kcache_layout = True
2025-04-11T04:23:18.6591907Z 
2025-04-11T04:23:18.6592107Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6592211Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6592328Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6592558Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6592677Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6592792Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6592928Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6593038Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6593172Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6593323Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6593409Z     def test_flash_decoding(
2025-04-11T04:23:18.6593484Z         bsz: int,
2025-04-11T04:23:18.6593568Z         block_size: int,
2025-04-11T04:23:18.6593658Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6593741Z         num_attn_heads: int,
2025-04-11T04:23:18.6593823Z         kv_group_num: int,
2025-04-11T04:23:18.6593908Z         same_context_len: bool,
2025-04-11T04:23:18.6593989Z         q_len: int,
2025-04-11T04:23:18.6594075Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6594167Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6594239Z     ):
2025-04-11T04:23:18.6594346Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6594639Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6594820Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6594992Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6595155Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6595311Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6595382Z     
2025-04-11T04:23:18.6595468Z         torch.manual_seed(123)
2025-04-11T04:23:18.6595559Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6595648Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6595655Z 
2025-04-11T04:23:18.6595818Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6595929Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6595936Z 
2025-04-11T04:23:18.6596023Z device = None
2025-04-11T04:23:18.6596027Z 
2025-04-11T04:23:18.6596143Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6596299Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6596370Z     
2025-04-11T04:23:18.6596443Z         Args:
2025-04-11T04:23:18.6596612Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6596776Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6596883Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6596956Z         """
2025-04-11T04:23:18.6597032Z         _lazy_init()
2025-04-11T04:23:18.6597133Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6597233Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6597340Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6597625Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6597762Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6597917Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6597921Z 
2025-04-11T04:23:18.6598158Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6598328Z _____________ test_flash_decoding[True-False-5-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.6598333Z 
2025-04-11T04:23:18.6598485Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6598736Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6598829Z use_new_kcache_layout = True
2025-04-11T04:23:18.6598833Z 
2025-04-11T04:23:18.6599039Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6599148Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6599271Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6599408Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6599520Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6599636Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6599768Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6599870Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6600003Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6600159Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6600241Z     def test_flash_decoding(
2025-04-11T04:23:18.6600315Z         bsz: int,
2025-04-11T04:23:18.6600398Z         block_size: int,
2025-04-11T04:23:18.6600488Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6600656Z         num_attn_heads: int,
2025-04-11T04:23:18.6600739Z         kv_group_num: int,
2025-04-11T04:23:18.6600823Z         same_context_len: bool,
2025-04-11T04:23:18.6600903Z         q_len: int,
2025-04-11T04:23:18.6600986Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6601078Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6601150Z     ):
2025-04-11T04:23:18.6601259Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6601456Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6601635Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6601811Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6601973Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6602132Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6602207Z     
2025-04-11T04:23:18.6602293Z         torch.manual_seed(123)
2025-04-11T04:23:18.6602386Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6602475Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6602479Z 
2025-04-11T04:23:18.6602637Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6602748Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6602752Z 
2025-04-11T04:23:18.6602832Z device = None
2025-04-11T04:23:18.6602836Z 
2025-04-11T04:23:18.6602951Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6603108Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6603180Z     
2025-04-11T04:23:18.6603252Z         Args:
2025-04-11T04:23:18.6603422Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6603591Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6603701Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6603773Z         """
2025-04-11T04:23:18.6603848Z         _lazy_init()
2025-04-11T04:23:18.6603947Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6604046Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6604152Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6604434Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6604571Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6604817Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6604821Z 
2025-04-11T04:23:18.6605069Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6605244Z ____________ test_flash_decoding[True-False-5-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.6605248Z 
2025-04-11T04:23:18.6605401Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6605567Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6605653Z use_new_kcache_layout = True
2025-04-11T04:23:18.6605657Z 
2025-04-11T04:23:18.6605856Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6605957Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6606073Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6606208Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6606326Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6606441Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6606576Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6606764Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6606898Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6607052Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6607134Z     def test_flash_decoding(
2025-04-11T04:23:18.6607210Z         bsz: int,
2025-04-11T04:23:18.6607296Z         block_size: int,
2025-04-11T04:23:18.6607387Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6607474Z         num_attn_heads: int,
2025-04-11T04:23:18.6607556Z         kv_group_num: int,
2025-04-11T04:23:18.6607645Z         same_context_len: bool,
2025-04-11T04:23:18.6607726Z         q_len: int,
2025-04-11T04:23:18.6607814Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6607906Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6607978Z     ):
2025-04-11T04:23:18.6608092Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6608287Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6608468Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6608645Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6608806Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6608966Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6609038Z     
2025-04-11T04:23:18.6609131Z         torch.manual_seed(123)
2025-04-11T04:23:18.6609217Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6609307Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6609317Z 
2025-04-11T04:23:18.6609476Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6609587Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6609594Z 
2025-04-11T04:23:18.6609673Z device = None
2025-04-11T04:23:18.6609677Z 
2025-04-11T04:23:18.6609791Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6609943Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6610013Z     
2025-04-11T04:23:18.6610086Z         Args:
2025-04-11T04:23:18.6610255Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6610418Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6610524Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6610595Z         """
2025-04-11T04:23:18.6610673Z         _lazy_init()
2025-04-11T04:23:18.6610867Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6610970Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6611076Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6611359Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6611497Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6611651Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6611655Z 
2025-04-11T04:23:18.6611897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6612062Z _____________ test_flash_decoding[True-False-5-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.6612066Z 
2025-04-11T04:23:18.6612214Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6612382Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6612468Z use_new_kcache_layout = True
2025-04-11T04:23:18.6612472Z 
2025-04-11T04:23:18.6612675Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6612865Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6612983Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6613120Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6613236Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6613354Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6613490Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6613600Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6613737Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6613896Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6613983Z     def test_flash_decoding(
2025-04-11T04:23:18.6614062Z         bsz: int,
2025-04-11T04:23:18.6614151Z         block_size: int,
2025-04-11T04:23:18.6614244Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6614335Z         num_attn_heads: int,
2025-04-11T04:23:18.6614421Z         kv_group_num: int,
2025-04-11T04:23:18.6614508Z         same_context_len: bool,
2025-04-11T04:23:18.6614590Z         q_len: int,
2025-04-11T04:23:18.6614677Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6614769Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6614844Z     ):
2025-04-11T04:23:18.6614959Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6615154Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6615337Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6615518Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6615684Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6615848Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6615925Z     
2025-04-11T04:23:18.6616018Z         torch.manual_seed(123)
2025-04-11T04:23:18.6616109Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6616202Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6616205Z 
2025-04-11T04:23:18.6616366Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6616481Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6616484Z 
2025-04-11T04:23:18.6616568Z device = None
2025-04-11T04:23:18.6616571Z 
2025-04-11T04:23:18.6616691Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6616847Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6617009Z     
2025-04-11T04:23:18.6617085Z         Args:
2025-04-11T04:23:18.6617253Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6617419Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6617525Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6617598Z         """
2025-04-11T04:23:18.6617678Z         _lazy_init()
2025-04-11T04:23:18.6617775Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6617874Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6617981Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6618260Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6618400Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6618558Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6618562Z 
2025-04-11T04:23:18.6618802Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6619053Z _____________ test_flash_decoding[True-False-5-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.6619057Z 
2025-04-11T04:23:18.6619213Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6619374Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6619458Z use_new_kcache_layout = True
2025-04-11T04:23:18.6619462Z 
2025-04-11T04:23:18.6619665Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6619767Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6619886Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6620022Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6620140Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6620252Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6620385Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6620494Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6620628Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6620780Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6620863Z     def test_flash_decoding(
2025-04-11T04:23:18.6620936Z         bsz: int,
2025-04-11T04:23:18.6621021Z         block_size: int,
2025-04-11T04:23:18.6621109Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6621192Z         num_attn_heads: int,
2025-04-11T04:23:18.6621273Z         kv_group_num: int,
2025-04-11T04:23:18.6621360Z         same_context_len: bool,
2025-04-11T04:23:18.6621435Z         q_len: int,
2025-04-11T04:23:18.6621516Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6621610Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6621681Z     ):
2025-04-11T04:23:18.6621793Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6621988Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6622167Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6622339Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6622501Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6622659Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6622730Z     
2025-04-11T04:23:18.6622817Z         torch.manual_seed(123)
2025-04-11T04:23:18.6622904Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6622992Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6623099Z 
2025-04-11T04:23:18.6623260Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6623369Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6623377Z 
2025-04-11T04:23:18.6623458Z device = None
2025-04-11T04:23:18.6623462Z 
2025-04-11T04:23:18.6623575Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6623729Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6623801Z     
2025-04-11T04:23:18.6623874Z         Args:
2025-04-11T04:23:18.6624044Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6624206Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6624312Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6624385Z         """
2025-04-11T04:23:18.6624463Z         _lazy_init()
2025-04-11T04:23:18.6624559Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6624657Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6624765Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6625134Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6625270Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6625425Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6625429Z 
2025-04-11T04:23:18.6625668Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6625834Z _____________ test_flash_decoding[True-False-5-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.6625838Z 
2025-04-11T04:23:18.6625991Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6626157Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6626242Z use_new_kcache_layout = True
2025-04-11T04:23:18.6626246Z 
2025-04-11T04:23:18.6626452Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6626557Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6626674Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6626811Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6626926Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6627036Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6627168Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6627273Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6627405Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6627554Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6627642Z     def test_flash_decoding(
2025-04-11T04:23:18.6627721Z         bsz: int,
2025-04-11T04:23:18.6627802Z         block_size: int,
2025-04-11T04:23:18.6627890Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6627979Z         num_attn_heads: int,
2025-04-11T04:23:18.6628060Z         kv_group_num: int,
2025-04-11T04:23:18.6628147Z         same_context_len: bool,
2025-04-11T04:23:18.6628222Z         q_len: int,
2025-04-11T04:23:18.6628305Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6628397Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6628524Z     ):
2025-04-11T04:23:18.6628637Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6628830Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6629010Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6629283Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6629449Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6629611Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6629686Z     
2025-04-11T04:23:18.6629775Z         torch.manual_seed(123)
2025-04-11T04:23:18.6629861Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6629951Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6629955Z 
2025-04-11T04:23:18.6630112Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6630223Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6630226Z 
2025-04-11T04:23:18.6630308Z device = None
2025-04-11T04:23:18.6630312Z 
2025-04-11T04:23:18.6630428Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6630579Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6630656Z     
2025-04-11T04:23:18.6630732Z         Args:
2025-04-11T04:23:18.6630897Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6631169Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6631277Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6631350Z         """
2025-04-11T04:23:18.6631430Z         _lazy_init()
2025-04-11T04:23:18.6631524Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6631623Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6631730Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6632014Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6632152Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6632311Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6632314Z 
2025-04-11T04:23:18.6632554Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6632725Z _____________ test_flash_decoding[True-False-5-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.6632729Z 
2025-04-11T04:23:18.6632882Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6633044Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6633129Z use_new_kcache_layout = True
2025-04-11T04:23:18.6633133Z 
2025-04-11T04:23:18.6633333Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6633435Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6633552Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6633690Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6633810Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6633922Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6634056Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6634165Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6634297Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6634447Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6634531Z     def test_flash_decoding(
2025-04-11T04:23:18.6634608Z         bsz: int,
2025-04-11T04:23:18.6634687Z         block_size: int,
2025-04-11T04:23:18.6634775Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6634859Z         num_attn_heads: int,
2025-04-11T04:23:18.6634940Z         kv_group_num: int,
2025-04-11T04:23:18.6635025Z         same_context_len: bool,
2025-04-11T04:23:18.6635100Z         q_len: int,
2025-04-11T04:23:18.6635182Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6635355Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6635428Z     ):
2025-04-11T04:23:18.6635541Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6635733Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6635918Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6636086Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6636244Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6636404Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6636475Z     
2025-04-11T04:23:18.6636563Z         torch.manual_seed(123)
2025-04-11T04:23:18.6636649Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6636738Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6636746Z 
2025-04-11T04:23:18.6636902Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6637011Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6637100Z 
2025-04-11T04:23:18.6637183Z device = None
2025-04-11T04:23:18.6637187Z 
2025-04-11T04:23:18.6637302Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6637453Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6637524Z     
2025-04-11T04:23:18.6637600Z         Args:
2025-04-11T04:23:18.6637767Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6637931Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6638038Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6638110Z         """
2025-04-11T04:23:18.6638191Z         _lazy_init()
2025-04-11T04:23:18.6638287Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6638387Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6638495Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6638779Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6638917Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6639073Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6639077Z 
2025-04-11T04:23:18.6639318Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6639485Z _____________ test_flash_decoding[True-False-5-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.6639489Z 
2025-04-11T04:23:18.6639644Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6639813Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6639897Z use_new_kcache_layout = True
2025-04-11T04:23:18.6639904Z 
2025-04-11T04:23:18.6640104Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6640209Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6640330Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6640466Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6640582Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6640694Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6640829Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6640935Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6641071Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6641221Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6641388Z     def test_flash_decoding(
2025-04-11T04:23:18.6641468Z         bsz: int,
2025-04-11T04:23:18.6641550Z         block_size: int,
2025-04-11T04:23:18.6641639Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6641727Z         num_attn_heads: int,
2025-04-11T04:23:18.6641807Z         kv_group_num: int,
2025-04-11T04:23:18.6641895Z         same_context_len: bool,
2025-04-11T04:23:18.6641969Z         q_len: int,
2025-04-11T04:23:18.6642054Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6642150Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6642221Z     ):
2025-04-11T04:23:18.6642334Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6642526Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6642711Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6642885Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6643047Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6643209Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6643438Z     
2025-04-11T04:23:18.6643530Z         torch.manual_seed(123)
2025-04-11T04:23:18.6643619Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6643713Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6643717Z 
2025-04-11T04:23:18.6643871Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6643980Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6643984Z 
2025-04-11T04:23:18.6644065Z device = None
2025-04-11T04:23:18.6644069Z 
2025-04-11T04:23:18.6644185Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6644336Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6644414Z     
2025-04-11T04:23:18.6644491Z         Args:
2025-04-11T04:23:18.6644657Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6644819Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6644930Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6645004Z         """
2025-04-11T04:23:18.6645082Z         _lazy_init()
2025-04-11T04:23:18.6645178Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6645282Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6645382Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6645663Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6645798Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6645956Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6645960Z 
2025-04-11T04:23:18.6646209Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6646380Z ____________ test_flash_decoding[True-False-5-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.6646384Z 
2025-04-11T04:23:18.6646538Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6646701Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6646784Z use_new_kcache_layout = True
2025-04-11T04:23:18.6646790Z 
2025-04-11T04:23:18.6646987Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6647088Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6647206Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6647341Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6647544Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6647658Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6647794Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6647900Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6648032Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6648182Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6648264Z     def test_flash_decoding(
2025-04-11T04:23:18.6648343Z         bsz: int,
2025-04-11T04:23:18.6648422Z         block_size: int,
2025-04-11T04:23:18.6648512Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6648597Z         num_attn_heads: int,
2025-04-11T04:23:18.6648681Z         kv_group_num: int,
2025-04-11T04:23:18.6648769Z         same_context_len: bool,
2025-04-11T04:23:18.6648843Z         q_len: int,
2025-04-11T04:23:18.6648927Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6649023Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6649095Z     ):
2025-04-11T04:23:18.6649208Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6649400Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6649667Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6649837Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6649999Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6650158Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6650229Z     
2025-04-11T04:23:18.6650317Z         torch.manual_seed(123)
2025-04-11T04:23:18.6650406Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6650499Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6650502Z 
2025-04-11T04:23:18.6650659Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6650768Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6650780Z 
2025-04-11T04:23:18.6650858Z device = None
2025-04-11T04:23:18.6650862Z 
2025-04-11T04:23:18.6650977Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6651129Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6651198Z     
2025-04-11T04:23:18.6651274Z         Args:
2025-04-11T04:23:18.6651439Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6651603Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6651710Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6651782Z         """
2025-04-11T04:23:18.6651860Z         _lazy_init()
2025-04-11T04:23:18.6651960Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6652063Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6652166Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6652452Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6652589Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6652746Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6652750Z 
2025-04-11T04:23:18.6652995Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6653161Z _____________ test_flash_decoding[True-False-5-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.6653165Z 
2025-04-11T04:23:18.6653318Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6653564Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6653655Z use_new_kcache_layout = True
2025-04-11T04:23:18.6653659Z 
2025-04-11T04:23:18.6653857Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6653964Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6654082Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6654219Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6654335Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6654444Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6654577Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6654679Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6654812Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6654963Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6655051Z     def test_flash_decoding(
2025-04-11T04:23:18.6655129Z         bsz: int,
2025-04-11T04:23:18.6655208Z         block_size: int,
2025-04-11T04:23:18.6655296Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6655485Z         num_attn_heads: int,
2025-04-11T04:23:18.6655569Z         kv_group_num: int,
2025-04-11T04:23:18.6655656Z         same_context_len: bool,
2025-04-11T04:23:18.6655731Z         q_len: int,
2025-04-11T04:23:18.6655817Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6655904Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6655978Z     ):
2025-04-11T04:23:18.6656089Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6656279Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6656459Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6656634Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6656798Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6656953Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6657028Z     
2025-04-11T04:23:18.6657117Z         torch.manual_seed(123)
2025-04-11T04:23:18.6657203Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6657295Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6657299Z 
2025-04-11T04:23:18.6657451Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6657560Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6657567Z 
2025-04-11T04:23:18.6657643Z device = None
2025-04-11T04:23:18.6657647Z 
2025-04-11T04:23:18.6657760Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6657912Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6657986Z     
2025-04-11T04:23:18.6658062Z         Args:
2025-04-11T04:23:18.6658227Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6658392Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6658499Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6658571Z         """
2025-04-11T04:23:18.6658649Z         _lazy_init()
2025-04-11T04:23:18.6658742Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6658844Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6658948Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6659231Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6659371Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6659608Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6659613Z 
2025-04-11T04:23:18.6659858Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6660033Z ____________ test_flash_decoding[True-False-5-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.6660037Z 
2025-04-11T04:23:18.6660191Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6660354Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.6660443Z use_new_kcache_layout = True
2025-04-11T04:23:18.6660447Z 
2025-04-11T04:23:18.6660644Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6660745Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6660863Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6660998Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6661118Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6661228Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6661364Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6661552Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6661686Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6661842Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6661928Z     def test_flash_decoding(
2025-04-11T04:23:18.6662008Z         bsz: int,
2025-04-11T04:23:18.6662089Z         block_size: int,
2025-04-11T04:23:18.6662179Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6662264Z         num_attn_heads: int,
2025-04-11T04:23:18.6662346Z         kv_group_num: int,
2025-04-11T04:23:18.6662434Z         same_context_len: bool,
2025-04-11T04:23:18.6662508Z         q_len: int,
2025-04-11T04:23:18.6662594Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6662684Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6662756Z     ):
2025-04-11T04:23:18.6662869Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6663062Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6663249Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6663421Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6663584Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6663739Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6663811Z     
2025-04-11T04:23:18.6663899Z         torch.manual_seed(123)
2025-04-11T04:23:18.6663985Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6664085Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6664089Z 
2025-04-11T04:23:18.6664248Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6664361Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6664368Z 
2025-04-11T04:23:18.6664444Z device = None
2025-04-11T04:23:18.6664448Z 
2025-04-11T04:23:18.6664561Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6664712Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6664781Z     
2025-04-11T04:23:18.6664856Z         Args:
2025-04-11T04:23:18.6665020Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6665187Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6665290Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6665362Z         """
2025-04-11T04:23:18.6665443Z         _lazy_init()
2025-04-11T04:23:18.6665624Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6665731Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6665834Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6666117Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6666257Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6666413Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6666416Z 
2025-04-11T04:23:18.6666655Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6666820Z ______________ test_flash_decoding[False-True-1-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6666824Z 
2025-04-11T04:23:18.6666977Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6667140Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6667235Z use_new_kcache_layout = False
2025-04-11T04:23:18.6667239Z 
2025-04-11T04:23:18.6667437Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6667633Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6667749Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6667889Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6668006Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6668117Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6668254Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6668355Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6668521Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6668674Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6668761Z     def test_flash_decoding(
2025-04-11T04:23:18.6668839Z         bsz: int,
2025-04-11T04:23:18.6668919Z         block_size: int,
2025-04-11T04:23:18.6669009Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6669094Z         num_attn_heads: int,
2025-04-11T04:23:18.6669174Z         kv_group_num: int,
2025-04-11T04:23:18.6669261Z         same_context_len: bool,
2025-04-11T04:23:18.6669335Z         q_len: int,
2025-04-11T04:23:18.6669421Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6669508Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6669581Z     ):
2025-04-11T04:23:18.6669692Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6669885Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6670067Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6670241Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6670404Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6670559Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6670636Z     
2025-04-11T04:23:18.6670725Z         torch.manual_seed(123)
2025-04-11T04:23:18.6670813Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6670910Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6670914Z 
2025-04-11T04:23:18.6671070Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6671182Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6671185Z 
2025-04-11T04:23:18.6671262Z device = None
2025-04-11T04:23:18.6671266Z 
2025-04-11T04:23:18.6671378Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6671528Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6671695Z     
2025-04-11T04:23:18.6671773Z         Args:
2025-04-11T04:23:18.6671938Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6672102Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6672210Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6672282Z         """
2025-04-11T04:23:18.6672362Z         _lazy_init()
2025-04-11T04:23:18.6672456Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6672559Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6672661Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6672943Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6673078Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6673236Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6673240Z 
2025-04-11T04:23:18.6673483Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6673748Z _____________ test_flash_decoding[False-True-1-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6673751Z 
2025-04-11T04:23:18.6673909Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6674070Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6674160Z use_new_kcache_layout = False
2025-04-11T04:23:18.6674164Z 
2025-04-11T04:23:18.6674364Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6674470Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6674586Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6674721Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6674842Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6674951Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6675087Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6675195Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6675327Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6675481Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6675565Z     def test_flash_decoding(
2025-04-11T04:23:18.6675646Z         bsz: int,
2025-04-11T04:23:18.6675726Z         block_size: int,
2025-04-11T04:23:18.6675818Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6675900Z         num_attn_heads: int,
2025-04-11T04:23:18.6675981Z         kv_group_num: int,
2025-04-11T04:23:18.6676066Z         same_context_len: bool,
2025-04-11T04:23:18.6676141Z         q_len: int,
2025-04-11T04:23:18.6676229Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6676317Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6676388Z     ):
2025-04-11T04:23:18.6676501Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6676692Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6676877Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6677046Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6677215Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6677372Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6677442Z     
2025-04-11T04:23:18.6677530Z         torch.manual_seed(123)
2025-04-11T04:23:18.6677615Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6677707Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6677711Z 
2025-04-11T04:23:18.6677961Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6678079Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6678087Z 
2025-04-11T04:23:18.6678165Z device = None
2025-04-11T04:23:18.6678169Z 
2025-04-11T04:23:18.6678286Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6678435Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6678506Z     
2025-04-11T04:23:18.6678580Z         Args:
2025-04-11T04:23:18.6678744Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6678912Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6679015Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6679088Z         """
2025-04-11T04:23:18.6679169Z         _lazy_init()
2025-04-11T04:23:18.6679264Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6679368Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6679470Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6679756Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6679988Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6680143Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6680147Z 
2025-04-11T04:23:18.6680389Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6680553Z ______________ test_flash_decoding[False-True-1-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6680556Z 
2025-04-11T04:23:18.6680708Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6680870Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6680960Z use_new_kcache_layout = False
2025-04-11T04:23:18.6680964Z 
2025-04-11T04:23:18.6681163Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6681272Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6681386Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6681523Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6681639Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6681750Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6681887Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6681989Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6682127Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6682275Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6682362Z     def test_flash_decoding(
2025-04-11T04:23:18.6682441Z         bsz: int,
2025-04-11T04:23:18.6682521Z         block_size: int,
2025-04-11T04:23:18.6682613Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6682697Z         num_attn_heads: int,
2025-04-11T04:23:18.6682778Z         kv_group_num: int,
2025-04-11T04:23:18.6682866Z         same_context_len: bool,
2025-04-11T04:23:18.6682940Z         q_len: int,
2025-04-11T04:23:18.6683026Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6683113Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6683186Z     ):
2025-04-11T04:23:18.6683298Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6683489Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6683672Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6683929Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6684100Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6684258Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6684335Z     
2025-04-11T04:23:18.6684425Z         torch.manual_seed(123)
2025-04-11T04:23:18.6684511Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6684605Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6684609Z 
2025-04-11T04:23:18.6684765Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6684878Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6684882Z 
2025-04-11T04:23:18.6684958Z device = None
2025-04-11T04:23:18.6684962Z 
2025-04-11T04:23:18.6685080Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6685229Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6685300Z     
2025-04-11T04:23:18.6685382Z         Args:
2025-04-11T04:23:18.6685550Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6685719Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6685907Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6685983Z         """
2025-04-11T04:23:18.6686067Z         _lazy_init()
2025-04-11T04:23:18.6686166Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6686273Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6686382Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6686675Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6686816Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6686979Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6686987Z 
2025-04-11T04:23:18.6687232Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6687403Z _____________ test_flash_decoding[False-True-1-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6687410Z 
2025-04-11T04:23:18.6687571Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6687736Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6687831Z use_new_kcache_layout = False
2025-04-11T04:23:18.6687835Z 
2025-04-11T04:23:18.6688039Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6688149Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6688266Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6688406Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6688532Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6688649Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6688791Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6688899Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6689041Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6689193Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6689280Z     def test_flash_decoding(
2025-04-11T04:23:18.6689364Z         bsz: int,
2025-04-11T04:23:18.6689447Z         block_size: int,
2025-04-11T04:23:18.6689544Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6689629Z         num_attn_heads: int,
2025-04-11T04:23:18.6689716Z         kv_group_num: int,
2025-04-11T04:23:18.6689807Z         same_context_len: bool,
2025-04-11T04:23:18.6689885Z         q_len: int,
2025-04-11T04:23:18.6689978Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6690152Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6690229Z     ):
2025-04-11T04:23:18.6690342Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6690534Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6690721Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6690893Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6691058Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6691215Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6691289Z     
2025-04-11T04:23:18.6691375Z         torch.manual_seed(123)
2025-04-11T04:23:18.6691461Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6691555Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6691559Z 
2025-04-11T04:23:18.6691716Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6691830Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6691916Z 
2025-04-11T04:23:18.6691996Z device = None
2025-04-11T04:23:18.6692001Z 
2025-04-11T04:23:18.6692119Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6692268Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6692339Z     
2025-04-11T04:23:18.6692416Z         Args:
2025-04-11T04:23:18.6692578Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6692744Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6692845Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6692920Z         """
2025-04-11T04:23:18.6692995Z         _lazy_init()
2025-04-11T04:23:18.6693090Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6693193Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6693295Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6693577Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6693714Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6693867Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6693874Z 
2025-04-11T04:23:18.6694108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6694272Z _____________ test_flash_decoding[False-True-1-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6694276Z 
2025-04-11T04:23:18.6694432Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6694594Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6694686Z use_new_kcache_layout = False
2025-04-11T04:23:18.6694690Z 
2025-04-11T04:23:18.6694887Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6694995Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6695109Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6695243Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6695361Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6695472Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6695608Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6695708Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6695843Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6695990Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6696158Z     def test_flash_decoding(
2025-04-11T04:23:18.6696242Z         bsz: int,
2025-04-11T04:23:18.6696322Z         block_size: int,
2025-04-11T04:23:18.6696414Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6696499Z         num_attn_heads: int,
2025-04-11T04:23:18.6696583Z         kv_group_num: int,
2025-04-11T04:23:18.6696666Z         same_context_len: bool,
2025-04-11T04:23:18.6696740Z         q_len: int,
2025-04-11T04:23:18.6696827Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6696912Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6696987Z     ):
2025-04-11T04:23:18.6697095Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6697288Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6697472Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6697646Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6697812Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6697968Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6698130Z     
2025-04-11T04:23:18.6698219Z         torch.manual_seed(123)
2025-04-11T04:23:18.6698307Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6698402Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6698406Z 
2025-04-11T04:23:18.6698559Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6698671Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6698675Z 
2025-04-11T04:23:18.6698753Z device = None
2025-04-11T04:23:18.6698757Z 
2025-04-11T04:23:18.6698874Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6699021Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6699092Z     
2025-04-11T04:23:18.6699173Z         Args:
2025-04-11T04:23:18.6699337Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6699501Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6699607Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6699682Z         """
2025-04-11T04:23:18.6699757Z         _lazy_init()
2025-04-11T04:23:18.6699850Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6699954Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6700057Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6700337Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6700472Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6700633Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6700637Z 
2025-04-11T04:23:18.6700872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6701038Z _____________ test_flash_decoding[False-True-1-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6701048Z 
2025-04-11T04:23:18.6701200Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6701360Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6701452Z use_new_kcache_layout = False
2025-04-11T04:23:18.6701456Z 
2025-04-11T04:23:18.6701655Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6701758Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6701873Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6702010Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6702223Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6702336Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6702472Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6702575Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6702710Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6702857Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6702943Z     def test_flash_decoding(
2025-04-11T04:23:18.6703022Z         bsz: int,
2025-04-11T04:23:18.6703102Z         block_size: int,
2025-04-11T04:23:18.6703194Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6703273Z         num_attn_heads: int,
2025-04-11T04:23:18.6703358Z         kv_group_num: int,
2025-04-11T04:23:18.6703442Z         same_context_len: bool,
2025-04-11T04:23:18.6703517Z         q_len: int,
2025-04-11T04:23:18.6703604Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6703694Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6703767Z     ):
2025-04-11T04:23:18.6703876Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6704064Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6704342Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6704508Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6704672Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6704827Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6704901Z     
2025-04-11T04:23:18.6704985Z         torch.manual_seed(123)
2025-04-11T04:23:18.6705071Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6705165Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6705169Z 
2025-04-11T04:23:18.6705325Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6705437Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6705441Z 
2025-04-11T04:23:18.6705521Z device = None
2025-04-11T04:23:18.6705525Z 
2025-04-11T04:23:18.6705641Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6705787Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6705857Z     
2025-04-11T04:23:18.6705932Z         Args:
2025-04-11T04:23:18.6706095Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6706260Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6706362Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6706438Z         """
2025-04-11T04:23:18.6706515Z         _lazy_init()
2025-04-11T04:23:18.6706607Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6706712Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6706816Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6707099Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6707236Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6707394Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6707398Z 
2025-04-11T04:23:18.6707636Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6707800Z _____________ test_flash_decoding[False-True-1-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6707807Z 
2025-04-11T04:23:18.6707958Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6708117Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6708295Z use_new_kcache_layout = False
2025-04-11T04:23:18.6708300Z 
2025-04-11T04:23:18.6708532Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6708643Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6708757Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6708899Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6709011Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6709124Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6709261Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6709362Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6709499Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6709652Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6709743Z     def test_flash_decoding(
2025-04-11T04:23:18.6709818Z         bsz: int,
2025-04-11T04:23:18.6709898Z         block_size: int,
2025-04-11T04:23:18.6709989Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6710162Z         num_attn_heads: int,
2025-04-11T04:23:18.6710246Z         kv_group_num: int,
2025-04-11T04:23:18.6710329Z         same_context_len: bool,
2025-04-11T04:23:18.6710404Z         q_len: int,
2025-04-11T04:23:18.6710491Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6710579Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6710653Z     ):
2025-04-11T04:23:18.6710760Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6710951Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6711132Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6711300Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6711468Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6711623Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6711700Z     
2025-04-11T04:23:18.6711784Z         torch.manual_seed(123)
2025-04-11T04:23:18.6711870Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6711964Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6711968Z 
2025-04-11T04:23:18.6712120Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6712232Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6712236Z 
2025-04-11T04:23:18.6712312Z device = None
2025-04-11T04:23:18.6712315Z 
2025-04-11T04:23:18.6712431Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6712577Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6712650Z     
2025-04-11T04:23:18.6712726Z         Args:
2025-04-11T04:23:18.6712889Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6713054Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6713161Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6713235Z         """
2025-04-11T04:23:18.6713311Z         _lazy_init()
2025-04-11T04:23:18.6713402Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6713505Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6713605Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6713887Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6714020Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6714266Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6714271Z 
2025-04-11T04:23:18.6714510Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6714678Z _____________ test_flash_decoding[False-True-1-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6714685Z 
2025-04-11T04:23:18.6714837Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6714996Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6715086Z use_new_kcache_layout = False
2025-04-11T04:23:18.6715090Z 
2025-04-11T04:23:18.6715287Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6715392Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6715504Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6715642Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6715757Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6715867Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6716005Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6716189Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6716324Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6716473Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6716560Z     def test_flash_decoding(
2025-04-11T04:23:18.6716634Z         bsz: int,
2025-04-11T04:23:18.6716714Z         block_size: int,
2025-04-11T04:23:18.6716804Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6716884Z         num_attn_heads: int,
2025-04-11T04:23:18.6716968Z         kv_group_num: int,
2025-04-11T04:23:18.6717051Z         same_context_len: bool,
2025-04-11T04:23:18.6717124Z         q_len: int,
2025-04-11T04:23:18.6717212Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6717301Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6717376Z     ):
2025-04-11T04:23:18.6717484Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6717679Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6717860Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6718028Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6718193Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6718347Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6718421Z     
2025-04-11T04:23:18.6718507Z         torch.manual_seed(123)
2025-04-11T04:23:18.6718596Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6718686Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6718690Z 
2025-04-11T04:23:18.6718844Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6718955Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6718959Z 
2025-04-11T04:23:18.6719039Z device = None
2025-04-11T04:23:18.6719043Z 
2025-04-11T04:23:18.6719160Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6719306Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6719378Z     
2025-04-11T04:23:18.6719451Z         Args:
2025-04-11T04:23:18.6719613Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6719780Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6719881Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6719957Z         """
2025-04-11T04:23:18.6720032Z         _lazy_init()
2025-04-11T04:23:18.6720126Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6720320Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6720425Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6720713Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6720855Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6721018Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6721022Z 
2025-04-11T04:23:18.6721261Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6721429Z ______________ test_flash_decoding[False-True-1-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6721433Z 
2025-04-11T04:23:18.6721583Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6721745Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6721840Z use_new_kcache_layout = False
2025-04-11T04:23:18.6721844Z 
2025-04-11T04:23:18.6722044Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6722247Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6722361Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6722501Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6722613Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6722722Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6722859Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6722958Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6723096Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6723244Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6723336Z     def test_flash_decoding(
2025-04-11T04:23:18.6723412Z         bsz: int,
2025-04-11T04:23:18.6723493Z         block_size: int,
2025-04-11T04:23:18.6723586Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6723667Z         num_attn_heads: int,
2025-04-11T04:23:18.6723758Z         kv_group_num: int,
2025-04-11T04:23:18.6723842Z         same_context_len: bool,
2025-04-11T04:23:18.6723917Z         q_len: int,
2025-04-11T04:23:18.6724005Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6724092Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6724167Z     ):
2025-04-11T04:23:18.6724275Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6724468Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6724647Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6724814Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6724983Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6725137Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6725215Z     
2025-04-11T04:23:18.6725301Z         torch.manual_seed(123)
2025-04-11T04:23:18.6725391Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6725481Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6725485Z 
2025-04-11T04:23:18.6725636Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6725749Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6725753Z 
2025-04-11T04:23:18.6725831Z device = None
2025-04-11T04:23:18.6725835Z 
2025-04-11T04:23:18.6725952Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6726097Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6726170Z     
2025-04-11T04:23:18.6726329Z         Args:
2025-04-11T04:23:18.6726494Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6726661Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6726770Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6726844Z         """
2025-04-11T04:23:18.6726921Z         _lazy_init()
2025-04-11T04:23:18.6727017Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6727116Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6727219Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6727501Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6727635Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6727792Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6727800Z 
2025-04-11T04:23:18.6728038Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6728205Z _____________ test_flash_decoding[False-True-1-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6728294Z 
2025-04-11T04:23:18.6728447Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6728609Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6728696Z use_new_kcache_layout = False
2025-04-11T04:23:18.6728700Z 
2025-04-11T04:23:18.6728903Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6729009Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6729124Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6729265Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6729382Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6729497Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6729630Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6729735Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6729875Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6730024Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6730110Z     def test_flash_decoding(
2025-04-11T04:23:18.6730186Z         bsz: int,
2025-04-11T04:23:18.6730266Z         block_size: int,
2025-04-11T04:23:18.6730358Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6730439Z         num_attn_heads: int,
2025-04-11T04:23:18.6730523Z         kv_group_num: int,
2025-04-11T04:23:18.6730608Z         same_context_len: bool,
2025-04-11T04:23:18.6730686Z         q_len: int,
2025-04-11T04:23:18.6730770Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6730862Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6730937Z     ):
2025-04-11T04:23:18.6731047Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6731243Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6731425Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6731597Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6731765Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6731921Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6731997Z     
2025-04-11T04:23:18.6732083Z         torch.manual_seed(123)
2025-04-11T04:23:18.6732173Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6732263Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6732267Z 
2025-04-11T04:23:18.6732500Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6732618Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6732622Z 
2025-04-11T04:23:18.6732703Z device = None
2025-04-11T04:23:18.6732707Z 
2025-04-11T04:23:18.6732829Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6732978Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6733051Z     
2025-04-11T04:23:18.6733123Z         Args:
2025-04-11T04:23:18.6733291Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6733458Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6733562Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6733636Z         """
2025-04-11T04:23:18.6733710Z         _lazy_init()
2025-04-11T04:23:18.6733807Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6733910Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6734012Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6734297Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6734516Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6734675Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6734679Z 
2025-04-11T04:23:18.6734915Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6735082Z ______________ test_flash_decoding[False-True-1-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6735085Z 
2025-04-11T04:23:18.6735233Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6735396Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6735487Z use_new_kcache_layout = False
2025-04-11T04:23:18.6735491Z 
2025-04-11T04:23:18.6735687Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6735796Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6735909Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6736046Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6736158Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6736272Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6736406Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6736505Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6736640Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6736786Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6736872Z     def test_flash_decoding(
2025-04-11T04:23:18.6736950Z         bsz: int,
2025-04-11T04:23:18.6737031Z         block_size: int,
2025-04-11T04:23:18.6737121Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6737203Z         num_attn_heads: int,
2025-04-11T04:23:18.6737294Z         kv_group_num: int,
2025-04-11T04:23:18.6737377Z         same_context_len: bool,
2025-04-11T04:23:18.6737454Z         q_len: int,
2025-04-11T04:23:18.6737536Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6737621Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6737697Z     ):
2025-04-11T04:23:18.6737805Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6737998Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6738178Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6738348Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6738657Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6738817Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6738896Z     
2025-04-11T04:23:18.6738981Z         torch.manual_seed(123)
2025-04-11T04:23:18.6739072Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6739162Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6739166Z 
2025-04-11T04:23:18.6739318Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6739430Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6739434Z 
2025-04-11T04:23:18.6739511Z device = None
2025-04-11T04:23:18.6739514Z 
2025-04-11T04:23:18.6739633Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6739780Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6739854Z     
2025-04-11T04:23:18.6739929Z         Args:
2025-04-11T04:23:18.6740096Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6740259Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6740470Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6740548Z         """
2025-04-11T04:23:18.6740625Z         _lazy_init()
2025-04-11T04:23:18.6740723Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6740822Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6740926Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6741213Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6741349Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6741509Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6741516Z 
2025-04-11T04:23:18.6741754Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6741923Z _____________ test_flash_decoding[False-True-1-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6741930Z 
2025-04-11T04:23:18.6742081Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6742243Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6742330Z use_new_kcache_layout = False
2025-04-11T04:23:18.6742334Z 
2025-04-11T04:23:18.6742533Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6742642Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6742756Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6742898Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6743015Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6743130Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6743263Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6743367Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6743506Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6743655Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6743740Z     def test_flash_decoding(
2025-04-11T04:23:18.6743815Z         bsz: int,
2025-04-11T04:23:18.6743898Z         block_size: int,
2025-04-11T04:23:18.6743985Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6744064Z         num_attn_heads: int,
2025-04-11T04:23:18.6744152Z         kv_group_num: int,
2025-04-11T04:23:18.6744235Z         same_context_len: bool,
2025-04-11T04:23:18.6744312Z         q_len: int,
2025-04-11T04:23:18.6744395Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6744481Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6744645Z     ):
2025-04-11T04:23:18.6744757Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6744953Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6745138Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6745310Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6745471Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6745626Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6745701Z     
2025-04-11T04:23:18.6745786Z         torch.manual_seed(123)
2025-04-11T04:23:18.6745875Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6745966Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6745969Z 
2025-04-11T04:23:18.6746127Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6746236Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6746240Z 
2025-04-11T04:23:18.6746316Z device = None
2025-04-11T04:23:18.6746408Z 
2025-04-11T04:23:18.6746528Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6746679Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6746753Z     
2025-04-11T04:23:18.6746825Z         Args:
2025-04-11T04:23:18.6746992Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6747155Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6747259Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6747335Z         """
2025-04-11T04:23:18.6747412Z         _lazy_init()
2025-04-11T04:23:18.6747509Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6747611Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6747714Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6747995Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6748133Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6748292Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6748297Z 
2025-04-11T04:23:18.6748583Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6748753Z _____________ test_flash_decoding[False-True-1-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6748757Z 
2025-04-11T04:23:18.6748907Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6749066Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6749156Z use_new_kcache_layout = False
2025-04-11T04:23:18.6749160Z 
2025-04-11T04:23:18.6749361Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6749467Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6749580Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6749721Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6749834Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6749947Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6750081Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6750180Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6750315Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6750463Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6750550Z     def test_flash_decoding(
2025-04-11T04:23:18.6750720Z         bsz: int,
2025-04-11T04:23:18.6750805Z         block_size: int,
2025-04-11T04:23:18.6750895Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6750977Z         num_attn_heads: int,
2025-04-11T04:23:18.6751067Z         kv_group_num: int,
2025-04-11T04:23:18.6751152Z         same_context_len: bool,
2025-04-11T04:23:18.6751230Z         q_len: int,
2025-04-11T04:23:18.6751312Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6751397Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6751472Z     ):
2025-04-11T04:23:18.6751579Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6751774Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6751955Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6752128Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6752294Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6752450Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6752619Z     
2025-04-11T04:23:18.6752708Z         torch.manual_seed(123)
2025-04-11T04:23:18.6752801Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6752892Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6752896Z 
2025-04-11T04:23:18.6753050Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6753159Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6753163Z 
2025-04-11T04:23:18.6753240Z device = None
2025-04-11T04:23:18.6753244Z 
2025-04-11T04:23:18.6753363Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6753509Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6753583Z     
2025-04-11T04:23:18.6753655Z         Args:
2025-04-11T04:23:18.6753826Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6753989Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6754097Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6754174Z         """
2025-04-11T04:23:18.6754250Z         _lazy_init()
2025-04-11T04:23:18.6754352Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6754452Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6754559Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6754837Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6754971Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6755129Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6755136Z 
2025-04-11T04:23:18.6755372Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6755541Z _____________ test_flash_decoding[False-True-1-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6755548Z 
2025-04-11T04:23:18.6755701Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6755861Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6755947Z use_new_kcache_layout = False
2025-04-11T04:23:18.6755951Z 
2025-04-11T04:23:18.6756151Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6756253Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6756367Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6756504Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6756618Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6756814Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6756951Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6757056Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6757194Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6757345Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6757433Z     def test_flash_decoding(
2025-04-11T04:23:18.6757508Z         bsz: int,
2025-04-11T04:23:18.6757590Z         block_size: int,
2025-04-11T04:23:18.6757676Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6757758Z         num_attn_heads: int,
2025-04-11T04:23:18.6757843Z         kv_group_num: int,
2025-04-11T04:23:18.6757927Z         same_context_len: bool,
2025-04-11T04:23:18.6758003Z         q_len: int,
2025-04-11T04:23:18.6758086Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6758172Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6758250Z     ):
2025-04-11T04:23:18.6758358Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6758554Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6758822Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6758997Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6759162Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6759319Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6759394Z     
2025-04-11T04:23:18.6759484Z         torch.manual_seed(123)
2025-04-11T04:23:18.6759578Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6759673Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6759677Z 
2025-04-11T04:23:18.6759840Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6759955Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6759959Z 
2025-04-11T04:23:18.6760038Z device = None
2025-04-11T04:23:18.6760048Z 
2025-04-11T04:23:18.6760167Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6760319Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6760397Z     
2025-04-11T04:23:18.6760472Z         Args:
2025-04-11T04:23:18.6760644Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6760811Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6760917Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6760998Z         """
2025-04-11T04:23:18.6761078Z         _lazy_init()
2025-04-11T04:23:18.6761179Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6761285Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6761396Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6761684Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6761827Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6761991Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6761995Z 
2025-04-11T04:23:18.6762244Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6762418Z _____________ test_flash_decoding[False-True-1-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6762422Z 
2025-04-11T04:23:18.6762577Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6762745Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6762922Z use_new_kcache_layout = False
2025-04-11T04:23:18.6762927Z 
2025-04-11T04:23:18.6763130Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6763235Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6763349Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6763490Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6763603Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6763717Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6763851Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6763954Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6764087Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6764235Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6764323Z     def test_flash_decoding(
2025-04-11T04:23:18.6764404Z         bsz: int,
2025-04-11T04:23:18.6764487Z         block_size: int,
2025-04-11T04:23:18.6764577Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6764656Z         num_attn_heads: int,
2025-04-11T04:23:18.6764829Z         kv_group_num: int,
2025-04-11T04:23:18.6764913Z         same_context_len: bool,
2025-04-11T04:23:18.6764992Z         q_len: int,
2025-04-11T04:23:18.6765075Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6765166Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6765238Z     ):
2025-04-11T04:23:18.6765348Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6765548Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6765727Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6765902Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6766069Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6766234Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6766308Z     
2025-04-11T04:23:18.6766394Z         torch.manual_seed(123)
2025-04-11T04:23:18.6766483Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6766572Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6766576Z 
2025-04-11T04:23:18.6766735Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6766844Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6766848Z 
2025-04-11T04:23:18.6766930Z device = None
2025-04-11T04:23:18.6766934Z 
2025-04-11T04:23:18.6767049Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6767195Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6767268Z     
2025-04-11T04:23:18.6767341Z         Args:
2025-04-11T04:23:18.6767515Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6767678Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6767787Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6767860Z         """
2025-04-11T04:23:18.6767937Z         _lazy_init()
2025-04-11T04:23:18.6768033Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6768132Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6768238Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6768523Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6768658Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6768816Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6768820Z 
2025-04-11T04:23:18.6769166Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6769340Z _____________ test_flash_decoding[False-True-1-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6769348Z 
2025-04-11T04:23:18.6769499Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6769662Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6769751Z use_new_kcache_layout = False
2025-04-11T04:23:18.6769755Z 
2025-04-11T04:23:18.6769955Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6770056Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6770169Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6770306Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6770422Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6770542Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6770677Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6770782Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6771001Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6771153Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6771242Z     def test_flash_decoding(
2025-04-11T04:23:18.6771317Z         bsz: int,
2025-04-11T04:23:18.6771403Z         block_size: int,
2025-04-11T04:23:18.6771491Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6771575Z         num_attn_heads: int,
2025-04-11T04:23:18.6771656Z         kv_group_num: int,
2025-04-11T04:23:18.6771740Z         same_context_len: bool,
2025-04-11T04:23:18.6771818Z         q_len: int,
2025-04-11T04:23:18.6771902Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6771993Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6772071Z     ):
2025-04-11T04:23:18.6772179Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6772372Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6772555Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6772727Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6772890Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6773049Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6773119Z     
2025-04-11T04:23:18.6773204Z         torch.manual_seed(123)
2025-04-11T04:23:18.6773295Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6773386Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6773390Z 
2025-04-11T04:23:18.6773547Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6773656Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6773660Z 
2025-04-11T04:23:18.6773741Z device = None
2025-04-11T04:23:18.6773748Z 
2025-04-11T04:23:18.6773862Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6774007Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6774081Z     
2025-04-11T04:23:18.6774153Z         Args:
2025-04-11T04:23:18.6774321Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6774486Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6774592Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6774664Z         """
2025-04-11T04:23:18.6774741Z         _lazy_init()
2025-04-11T04:23:18.6774839Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6775024Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6775135Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6775418Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6775562Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6775718Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6775722Z 
2025-04-11T04:23:18.6775960Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6776130Z _____________ test_flash_decoding[False-True-1-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6776134Z 
2025-04-11T04:23:18.6776282Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6776446Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6776537Z use_new_kcache_layout = False
2025-04-11T04:23:18.6776541Z 
2025-04-11T04:23:18.6776741Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6776843Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6777049Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6777188Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6777303Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6777419Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6777554Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6777658Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6777792Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6777941Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6778028Z     def test_flash_decoding(
2025-04-11T04:23:18.6778105Z         bsz: int,
2025-04-11T04:23:18.6778186Z         block_size: int,
2025-04-11T04:23:18.6778273Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6778355Z         num_attn_heads: int,
2025-04-11T04:23:18.6778441Z         kv_group_num: int,
2025-04-11T04:23:18.6778525Z         same_context_len: bool,
2025-04-11T04:23:18.6778602Z         q_len: int,
2025-04-11T04:23:18.6778685Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6778774Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6778845Z     ):
2025-04-11T04:23:18.6778953Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6779145Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6779326Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6779499Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6779665Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6779823Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6779896Z     
2025-04-11T04:23:18.6779982Z         torch.manual_seed(123)
2025-04-11T04:23:18.6780072Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6780161Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6780165Z 
2025-04-11T04:23:18.6780320Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6780429Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6780433Z 
2025-04-11T04:23:18.6780513Z device = None
2025-04-11T04:23:18.6780516Z 
2025-04-11T04:23:18.6780632Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6780779Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6780853Z     
2025-04-11T04:23:18.6780927Z         Args:
2025-04-11T04:23:18.6781179Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6781348Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6781460Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6781532Z         """
2025-04-11T04:23:18.6781608Z         _lazy_init()
2025-04-11T04:23:18.6781705Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6781805Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6781910Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6782190Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6782327Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6782483Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6782486Z 
2025-04-11T04:23:18.6782725Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6782895Z _____________ test_flash_decoding[False-True-1-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.6782982Z 
2025-04-11T04:23:18.6783135Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6783303Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6783390Z use_new_kcache_layout = False
2025-04-11T04:23:18.6783394Z 
2025-04-11T04:23:18.6783593Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6783695Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6783814Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6783949Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6784064Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6784182Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6784317Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6784420Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6784557Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6784709Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6784791Z     def test_flash_decoding(
2025-04-11T04:23:18.6784866Z         bsz: int,
2025-04-11T04:23:18.6784948Z         block_size: int,
2025-04-11T04:23:18.6785036Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6785119Z         num_attn_heads: int,
2025-04-11T04:23:18.6785201Z         kv_group_num: int,
2025-04-11T04:23:18.6785284Z         same_context_len: bool,
2025-04-11T04:23:18.6785361Z         q_len: int,
2025-04-11T04:23:18.6785446Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6785534Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6785608Z     ):
2025-04-11T04:23:18.6785716Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6785907Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6786090Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6786260Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6786422Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6786580Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6786651Z     
2025-04-11T04:23:18.6786735Z         torch.manual_seed(123)
2025-04-11T04:23:18.6786825Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6786915Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6786919Z 
2025-04-11T04:23:18.6787157Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6787269Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6787273Z 
2025-04-11T04:23:18.6787354Z device = None
2025-04-11T04:23:18.6787361Z 
2025-04-11T04:23:18.6787476Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6787626Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6787697Z     
2025-04-11T04:23:18.6787770Z         Args:
2025-04-11T04:23:18.6787937Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6788100Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6788207Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6788279Z         """
2025-04-11T04:23:18.6788356Z         _lazy_init()
2025-04-11T04:23:18.6788483Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6788588Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6788695Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6788976Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6789212Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6789369Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6789373Z 
2025-04-11T04:23:18.6789609Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6789776Z _____________ test_flash_decoding[False-True-1-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.6789780Z 
2025-04-11T04:23:18.6789926Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6790091Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6790180Z use_new_kcache_layout = False
2025-04-11T04:23:18.6790184Z 
2025-04-11T04:23:18.6790384Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6790487Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6790608Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6790743Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6790856Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6790970Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6791104Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6791206Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6791337Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6791487Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6791571Z     def test_flash_decoding(
2025-04-11T04:23:18.6791648Z         bsz: int,
2025-04-11T04:23:18.6791733Z         block_size: int,
2025-04-11T04:23:18.6791821Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6791904Z         num_attn_heads: int,
2025-04-11T04:23:18.6791988Z         kv_group_num: int,
2025-04-11T04:23:18.6792071Z         same_context_len: bool,
2025-04-11T04:23:18.6792149Z         q_len: int,
2025-04-11T04:23:18.6792232Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6792323Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6792394Z     ):
2025-04-11T04:23:18.6792501Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6792690Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6792867Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6793037Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6793300Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6793462Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6793537Z     
2025-04-11T04:23:18.6793622Z         torch.manual_seed(123)
2025-04-11T04:23:18.6793713Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6793803Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6793807Z 
2025-04-11T04:23:18.6793964Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6794074Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6794078Z 
2025-04-11T04:23:18.6794158Z device = None
2025-04-11T04:23:18.6794162Z 
2025-04-11T04:23:18.6794274Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6794425Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6794496Z     
2025-04-11T04:23:18.6794569Z         Args:
2025-04-11T04:23:18.6794740Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6794904Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6795092Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6795164Z         """
2025-04-11T04:23:18.6795241Z         _lazy_init()
2025-04-11T04:23:18.6795340Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6795439Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6795545Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6795828Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6795970Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6796126Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6796130Z 
2025-04-11T04:23:18.6796377Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6796543Z _____________ test_flash_decoding[False-True-1-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.6796550Z 
2025-04-11T04:23:18.6796702Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6796867Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6796954Z use_new_kcache_layout = False
2025-04-11T04:23:18.6796957Z 
2025-04-11T04:23:18.6797160Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6797263Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6797381Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6797517Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6797632Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6797750Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6797886Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6797992Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6798126Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6798278Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6798363Z     def test_flash_decoding(
2025-04-11T04:23:18.6798437Z         bsz: int,
2025-04-11T04:23:18.6798521Z         block_size: int,
2025-04-11T04:23:18.6798608Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6798691Z         num_attn_heads: int,
2025-04-11T04:23:18.6798773Z         kv_group_num: int,
2025-04-11T04:23:18.6798860Z         same_context_len: bool,
2025-04-11T04:23:18.6798938Z         q_len: int,
2025-04-11T04:23:18.6799021Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6799110Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6799279Z     ):
2025-04-11T04:23:18.6799392Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6799587Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6799770Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6799945Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6800106Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6800262Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6800334Z     
2025-04-11T04:23:18.6800422Z         torch.manual_seed(123)
2025-04-11T04:23:18.6800508Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6800599Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6800603Z 
2025-04-11T04:23:18.6800765Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6800874Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6800878Z 
2025-04-11T04:23:18.6800958Z device = None
2025-04-11T04:23:18.6801043Z 
2025-04-11T04:23:18.6801160Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6801312Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6801382Z     
2025-04-11T04:23:18.6801454Z         Args:
2025-04-11T04:23:18.6801621Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6801786Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6801892Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6801963Z         """
2025-04-11T04:23:18.6802042Z         _lazy_init()
2025-04-11T04:23:18.6802135Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6802237Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6802345Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6802632Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6802773Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6802930Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6802934Z 
2025-04-11T04:23:18.6803179Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6803345Z _____________ test_flash_decoding[False-True-1-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.6803349Z 
2025-04-11T04:23:18.6803500Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6803666Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6803756Z use_new_kcache_layout = False
2025-04-11T04:23:18.6803760Z 
2025-04-11T04:23:18.6803966Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6804068Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6804190Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6804327Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6804444Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6804554Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6804691Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6804796Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6804931Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6805086Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6805171Z     def test_flash_decoding(
2025-04-11T04:23:18.6805331Z         bsz: int,
2025-04-11T04:23:18.6805420Z         block_size: int,
2025-04-11T04:23:18.6805510Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6805595Z         num_attn_heads: int,
2025-04-11T04:23:18.6805683Z         kv_group_num: int,
2025-04-11T04:23:18.6805769Z         same_context_len: bool,
2025-04-11T04:23:18.6805846Z         q_len: int,
2025-04-11T04:23:18.6805929Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6806020Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6806093Z     ):
2025-04-11T04:23:18.6806204Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6806395Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6806573Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6806745Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6806909Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6807066Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6807139Z     
2025-04-11T04:23:18.6807317Z         torch.manual_seed(123)
2025-04-11T04:23:18.6807406Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6807497Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6807501Z 
2025-04-11T04:23:18.6807660Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6807771Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6807775Z 
2025-04-11T04:23:18.6807856Z device = None
2025-04-11T04:23:18.6807859Z 
2025-04-11T04:23:18.6807976Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6808129Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6808200Z     
2025-04-11T04:23:18.6808273Z         Args:
2025-04-11T04:23:18.6808448Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6808614Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6808724Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6808797Z         """
2025-04-11T04:23:18.6808880Z         _lazy_init()
2025-04-11T04:23:18.6808973Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6809072Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6809182Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6809470Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6809607Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6809764Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6809768Z 
2025-04-11T04:23:18.6810017Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6810186Z ____________ test_flash_decoding[False-True-1-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.6810194Z 
2025-04-11T04:23:18.6810349Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6810512Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6810599Z use_new_kcache_layout = False
2025-04-11T04:23:18.6810603Z 
2025-04-11T04:23:18.6810806Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6810908Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6811024Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6811161Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6811277Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6811471Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6811608Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6811712Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6811853Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6812007Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6812092Z     def test_flash_decoding(
2025-04-11T04:23:18.6812166Z         bsz: int,
2025-04-11T04:23:18.6812250Z         block_size: int,
2025-04-11T04:23:18.6812337Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6812419Z         num_attn_heads: int,
2025-04-11T04:23:18.6812500Z         kv_group_num: int,
2025-04-11T04:23:18.6812587Z         same_context_len: bool,
2025-04-11T04:23:18.6812662Z         q_len: int,
2025-04-11T04:23:18.6812745Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6812836Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6812915Z     ):
2025-04-11T04:23:18.6813026Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6813216Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6813495Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6813666Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6813827Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6813987Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6814058Z     
2025-04-11T04:23:18.6814148Z         torch.manual_seed(123)
2025-04-11T04:23:18.6814235Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6814324Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6814327Z 
2025-04-11T04:23:18.6814487Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6814598Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6814602Z 
2025-04-11T04:23:18.6814685Z device = None
2025-04-11T04:23:18.6814692Z 
2025-04-11T04:23:18.6814806Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6814956Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6815026Z     
2025-04-11T04:23:18.6815098Z         Args:
2025-04-11T04:23:18.6815267Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6815430Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6815536Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6815608Z         """
2025-04-11T04:23:18.6815687Z         _lazy_init()
2025-04-11T04:23:18.6815781Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6815884Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6815992Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6816275Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6816420Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6816574Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6816578Z 
2025-04-11T04:23:18.6816816Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6816981Z _____________ test_flash_decoding[False-True-1-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.6816985Z 
2025-04-11T04:23:18.6817137Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6817297Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6817383Z use_new_kcache_layout = False
2025-04-11T04:23:18.6817472Z 
2025-04-11T04:23:18.6817676Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6817779Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6817902Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6818040Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6818157Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6818268Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6818401Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6818509Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6818641Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6818792Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6818876Z     def test_flash_decoding(
2025-04-11T04:23:18.6818959Z         bsz: int,
2025-04-11T04:23:18.6819038Z         block_size: int,
2025-04-11T04:23:18.6819127Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6819213Z         num_attn_heads: int,
2025-04-11T04:23:18.6819381Z         kv_group_num: int,
2025-04-11T04:23:18.6819468Z         same_context_len: bool,
2025-04-11T04:23:18.6819543Z         q_len: int,
2025-04-11T04:23:18.6819626Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6819715Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6819787Z     ):
2025-04-11T04:23:18.6819899Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6820089Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6820273Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6820448Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6820613Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6820772Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6820844Z     
2025-04-11T04:23:18.6820937Z         torch.manual_seed(123)
2025-04-11T04:23:18.6821023Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6821113Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6821121Z 
2025-04-11T04:23:18.6821274Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6821383Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6821387Z 
2025-04-11T04:23:18.6821467Z device = None
2025-04-11T04:23:18.6821471Z 
2025-04-11T04:23:18.6821586Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6821736Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6821806Z     
2025-04-11T04:23:18.6821885Z         Args:
2025-04-11T04:23:18.6822052Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6822219Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6822329Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6822402Z         """
2025-04-11T04:23:18.6822483Z         _lazy_init()
2025-04-11T04:23:18.6822577Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6822676Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6822782Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6823064Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6823203Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6823359Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6823363Z 
2025-04-11T04:23:18.6823691Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6823864Z ____________ test_flash_decoding[False-True-1-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.6823871Z 
2025-04-11T04:23:18.6824027Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6824187Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6824273Z use_new_kcache_layout = False
2025-04-11T04:23:18.6824277Z 
2025-04-11T04:23:18.6824479Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6824582Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6824700Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6824836Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6824952Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6825065Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6825198Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6825302Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6825519Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6825674Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6825758Z     def test_flash_decoding(
2025-04-11T04:23:18.6825836Z         bsz: int,
2025-04-11T04:23:18.6825916Z         block_size: int,
2025-04-11T04:23:18.6826002Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6826088Z         num_attn_heads: int,
2025-04-11T04:23:18.6826170Z         kv_group_num: int,
2025-04-11T04:23:18.6826257Z         same_context_len: bool,
2025-04-11T04:23:18.6826332Z         q_len: int,
2025-04-11T04:23:18.6826416Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6826508Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6826580Z     ):
2025-04-11T04:23:18.6826692Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6826885Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6827068Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6827234Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6827393Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6827554Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6827625Z     
2025-04-11T04:23:18.6827714Z         torch.manual_seed(123)
2025-04-11T04:23:18.6827800Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6827888Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6827897Z 
2025-04-11T04:23:18.6828049Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6828160Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6828164Z 
2025-04-11T04:23:18.6828243Z device = None
2025-04-11T04:23:18.6828250Z 
2025-04-11T04:23:18.6828367Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6828558Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6828629Z     
2025-04-11T04:23:18.6828705Z         Args:
2025-04-11T04:23:18.6828868Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6829028Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6829134Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6829208Z         """
2025-04-11T04:23:18.6829287Z         _lazy_init()
2025-04-11T04:23:18.6829380Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6829575Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6829685Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6829970Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6830114Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6830271Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6830275Z 
2025-04-11T04:23:18.6830515Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6830683Z _____________ test_flash_decoding[False-True-1-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.6830687Z 
2025-04-11T04:23:18.6830841Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6831003Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6831092Z use_new_kcache_layout = False
2025-04-11T04:23:18.6831102Z 
2025-04-11T04:23:18.6831300Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6831402Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6831622Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6831759Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6831877Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6831988Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6832122Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6832226Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6832356Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6832508Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6832593Z     def test_flash_decoding(
2025-04-11T04:23:18.6832671Z         bsz: int,
2025-04-11T04:23:18.6832754Z         block_size: int,
2025-04-11T04:23:18.6832843Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6832928Z         num_attn_heads: int,
2025-04-11T04:23:18.6833013Z         kv_group_num: int,
2025-04-11T04:23:18.6833102Z         same_context_len: bool,
2025-04-11T04:23:18.6833177Z         q_len: int,
2025-04-11T04:23:18.6833261Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6833351Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6833426Z     ):
2025-04-11T04:23:18.6833540Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6833732Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6833919Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6834088Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6834251Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6834412Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6834485Z     
2025-04-11T04:23:18.6834578Z         torch.manual_seed(123)
2025-04-11T04:23:18.6834665Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6834760Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6834764Z 
2025-04-11T04:23:18.6834917Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6835028Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6835031Z 
2025-04-11T04:23:18.6835111Z device = None
2025-04-11T04:23:18.6835115Z 
2025-04-11T04:23:18.6835231Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6835383Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6835453Z     
2025-04-11T04:23:18.6835528Z         Args:
2025-04-11T04:23:18.6835868Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6836037Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6836149Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6836222Z         """
2025-04-11T04:23:18.6836305Z         _lazy_init()
2025-04-11T04:23:18.6836400Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6836503Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6836605Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6836892Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6837032Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6837187Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6837191Z 
2025-04-11T04:23:18.6837442Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6837607Z _____________ test_flash_decoding[False-True-1-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.6837709Z 
2025-04-11T04:23:18.6837865Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6838029Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6838118Z use_new_kcache_layout = False
2025-04-11T04:23:18.6838122Z 
2025-04-11T04:23:18.6838321Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6838425Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6838545Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6838682Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6838798Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6838910Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6839047Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6839147Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6839284Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6839437Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6839520Z     def test_flash_decoding(
2025-04-11T04:23:18.6839599Z         bsz: int,
2025-04-11T04:23:18.6839680Z         block_size: int,
2025-04-11T04:23:18.6839768Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6839855Z         num_attn_heads: int,
2025-04-11T04:23:18.6839937Z         kv_group_num: int,
2025-04-11T04:23:18.6840024Z         same_context_len: bool,
2025-04-11T04:23:18.6840099Z         q_len: int,
2025-04-11T04:23:18.6840186Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6840272Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6840344Z     ):
2025-04-11T04:23:18.6840458Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6840649Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6840833Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6840999Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6841160Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6841318Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6841388Z     
2025-04-11T04:23:18.6841475Z         torch.manual_seed(123)
2025-04-11T04:23:18.6841561Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6841653Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6841657Z 
2025-04-11T04:23:18.6841809Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6842003Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6842011Z 
2025-04-11T04:23:18.6842090Z device = None
2025-04-11T04:23:18.6842095Z 
2025-04-11T04:23:18.6842213Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6842364Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6842434Z     
2025-04-11T04:23:18.6842509Z         Args:
2025-04-11T04:23:18.6842672Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6842831Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6842939Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6843012Z         """
2025-04-11T04:23:18.6843092Z         _lazy_init()
2025-04-11T04:23:18.6843184Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6843290Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6843392Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6843670Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6843895Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6844051Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6844055Z 
2025-04-11T04:23:18.6844294Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6844459Z _____________ test_flash_decoding[False-True-1-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.6844463Z 
2025-04-11T04:23:18.6844613Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6844774Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6844864Z use_new_kcache_layout = False
2025-04-11T04:23:18.6844872Z 
2025-04-11T04:23:18.6845070Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6845171Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6845301Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6845440Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6845557Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6845668Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6845803Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6845902Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6846035Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6846187Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6846272Z     def test_flash_decoding(
2025-04-11T04:23:18.6846351Z         bsz: int,
2025-04-11T04:23:18.6846437Z         block_size: int,
2025-04-11T04:23:18.6846526Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6846612Z         num_attn_heads: int,
2025-04-11T04:23:18.6846694Z         kv_group_num: int,
2025-04-11T04:23:18.6846787Z         same_context_len: bool,
2025-04-11T04:23:18.6846860Z         q_len: int,
2025-04-11T04:23:18.6846950Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6847038Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6847111Z     ):
2025-04-11T04:23:18.6847227Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6847422Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6847606Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6847778Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6848026Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6848191Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6848265Z     
2025-04-11T04:23:18.6848362Z         torch.manual_seed(123)
2025-04-11T04:23:18.6848451Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6848545Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6848549Z 
2025-04-11T04:23:18.6848704Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6848818Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6848822Z 
2025-04-11T04:23:18.6848898Z device = None
2025-04-11T04:23:18.6848902Z 
2025-04-11T04:23:18.6849019Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6849168Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6849239Z     
2025-04-11T04:23:18.6849315Z         Args:
2025-04-11T04:23:18.6849483Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6849647Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6849840Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6849918Z         """
2025-04-11T04:23:18.6850004Z         _lazy_init()
2025-04-11T04:23:18.6850104Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6850207Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6850308Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6850595Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6850735Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6850891Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6850895Z 
2025-04-11T04:23:18.6851145Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6851311Z _____________ test_flash_decoding[False-True-1-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.6851318Z 
2025-04-11T04:23:18.6851472Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6851637Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6851727Z use_new_kcache_layout = False
2025-04-11T04:23:18.6851731Z 
2025-04-11T04:23:18.6851931Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6852037Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6852158Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6852296Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6852415Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6852529Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6852670Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6852774Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6852913Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6853067Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6853153Z     def test_flash_decoding(
2025-04-11T04:23:18.6853234Z         bsz: int,
2025-04-11T04:23:18.6853317Z         block_size: int,
2025-04-11T04:23:18.6853411Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6853495Z         num_attn_heads: int,
2025-04-11T04:23:18.6853576Z         kv_group_num: int,
2025-04-11T04:23:18.6853665Z         same_context_len: bool,
2025-04-11T04:23:18.6853740Z         q_len: int,
2025-04-11T04:23:18.6853827Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6853915Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6853986Z     ):
2025-04-11T04:23:18.6854184Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6854379Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6854567Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6854734Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6854899Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6855055Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6855128Z     
2025-04-11T04:23:18.6855220Z         torch.manual_seed(123)
2025-04-11T04:23:18.6855307Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6855401Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6855404Z 
2025-04-11T04:23:18.6855562Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6855678Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6855682Z 
2025-04-11T04:23:18.6855759Z device = None
2025-04-11T04:23:18.6855763Z 
2025-04-11T04:23:18.6855965Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6856119Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6856190Z     
2025-04-11T04:23:18.6856266Z         Args:
2025-04-11T04:23:18.6856433Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6856604Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6856708Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6856779Z         """
2025-04-11T04:23:18.6856860Z         _lazy_init()
2025-04-11T04:23:18.6856953Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6857055Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6857160Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6857446Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6857586Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6857744Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6857748Z 
2025-04-11T04:23:18.6857991Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6858159Z _____________ test_flash_decoding[False-True-1-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.6858163Z 
2025-04-11T04:23:18.6858318Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6858480Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6858569Z use_new_kcache_layout = False
2025-04-11T04:23:18.6858576Z 
2025-04-11T04:23:18.6858776Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6858885Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6859002Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6859141Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6859259Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6859372Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6859510Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6859614Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6859747Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6859901Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6859986Z     def test_flash_decoding(
2025-04-11T04:23:18.6860065Z         bsz: int,
2025-04-11T04:23:18.6860261Z         block_size: int,
2025-04-11T04:23:18.6860357Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6860438Z         num_attn_heads: int,
2025-04-11T04:23:18.6860518Z         kv_group_num: int,
2025-04-11T04:23:18.6860611Z         same_context_len: bool,
2025-04-11T04:23:18.6860687Z         q_len: int,
2025-04-11T04:23:18.6860776Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6860862Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6860934Z     ):
2025-04-11T04:23:18.6861046Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6861239Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6861424Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6861593Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6861763Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6861925Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6861998Z     
2025-04-11T04:23:18.6862184Z         torch.manual_seed(123)
2025-04-11T04:23:18.6862278Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6862372Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6862376Z 
2025-04-11T04:23:18.6862529Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6862645Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6862649Z 
2025-04-11T04:23:18.6862725Z device = None
2025-04-11T04:23:18.6862729Z 
2025-04-11T04:23:18.6862843Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6862993Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6863064Z     
2025-04-11T04:23:18.6863143Z         Args:
2025-04-11T04:23:18.6863311Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6863481Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6863588Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6863660Z         """
2025-04-11T04:23:18.6863740Z         _lazy_init()
2025-04-11T04:23:18.6863834Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6863942Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6864047Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6864335Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6864474Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6864629Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6864633Z 
2025-04-11T04:23:18.6864878Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6865048Z ____________ test_flash_decoding[False-True-1-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.6865054Z 
2025-04-11T04:23:18.6865211Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6865372Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6865463Z use_new_kcache_layout = False
2025-04-11T04:23:18.6865467Z 
2025-04-11T04:23:18.6865663Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6865768Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6865885Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6866022Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6866139Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6866335Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6866475Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6866575Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6866716Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6866866Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6866949Z     def test_flash_decoding(
2025-04-11T04:23:18.6867029Z         bsz: int,
2025-04-11T04:23:18.6867110Z         block_size: int,
2025-04-11T04:23:18.6867201Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6867281Z         num_attn_heads: int,
2025-04-11T04:23:18.6867362Z         kv_group_num: int,
2025-04-11T04:23:18.6867449Z         same_context_len: bool,
2025-04-11T04:23:18.6867522Z         q_len: int,
2025-04-11T04:23:18.6867609Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6867695Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6867766Z     ):
2025-04-11T04:23:18.6867888Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6868082Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6868352Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6868549Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6868718Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6868873Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6868944Z     
2025-04-11T04:23:18.6869032Z         torch.manual_seed(123)
2025-04-11T04:23:18.6869118Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6869216Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6869220Z 
2025-04-11T04:23:18.6869372Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6869492Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6869496Z 
2025-04-11T04:23:18.6869572Z device = None
2025-04-11T04:23:18.6869575Z 
2025-04-11T04:23:18.6869695Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6869843Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6869914Z     
2025-04-11T04:23:18.6869990Z         Args:
2025-04-11T04:23:18.6870153Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6870321Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6870423Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6870494Z         """
2025-04-11T04:23:18.6870572Z         _lazy_init()
2025-04-11T04:23:18.6870668Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6870772Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6870877Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6871166Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6871303Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6871459Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6871466Z 
2025-04-11T04:23:18.6871707Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6871872Z _____________ test_flash_decoding[False-True-1-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.6871876Z 
2025-04-11T04:23:18.6872030Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6872191Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6872281Z use_new_kcache_layout = False
2025-04-11T04:23:18.6872385Z 
2025-04-11T04:23:18.6872586Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6872691Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6872809Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6872946Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6873065Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6873175Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6873313Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6873413Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6873547Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6873696Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6873780Z     def test_flash_decoding(
2025-04-11T04:23:18.6873858Z         bsz: int,
2025-04-11T04:23:18.6873942Z         block_size: int,
2025-04-11T04:23:18.6874034Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6874113Z         num_attn_heads: int,
2025-04-11T04:23:18.6874194Z         kv_group_num: int,
2025-04-11T04:23:18.6874375Z         same_context_len: bool,
2025-04-11T04:23:18.6874450Z         q_len: int,
2025-04-11T04:23:18.6874536Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6874623Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6874702Z     ):
2025-04-11T04:23:18.6874812Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6875003Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6875187Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6875357Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6875524Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6875682Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6875755Z     
2025-04-11T04:23:18.6875846Z         torch.manual_seed(123)
2025-04-11T04:23:18.6875934Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6876025Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6876029Z 
2025-04-11T04:23:18.6876183Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6876297Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6876301Z 
2025-04-11T04:23:18.6876377Z device = None
2025-04-11T04:23:18.6876381Z 
2025-04-11T04:23:18.6876499Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6876647Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6876717Z     
2025-04-11T04:23:18.6876796Z         Args:
2025-04-11T04:23:18.6876963Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6877131Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6877235Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6877314Z         """
2025-04-11T04:23:18.6877391Z         _lazy_init()
2025-04-11T04:23:18.6877483Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6877586Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6877687Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6877975Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6878107Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6878262Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6878270Z 
2025-04-11T04:23:18.6878595Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6878770Z ____________ test_flash_decoding[False-True-1-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.6878777Z 
2025-04-11T04:23:18.6878937Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6879098Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T04:23:18.6879188Z use_new_kcache_layout = False
2025-04-11T04:23:18.6879192Z 
2025-04-11T04:23:18.6879389Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6879498Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6879612Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6879746Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6879864Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6879976Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6880115Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6880216Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6880437Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6880592Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6880675Z     def test_flash_decoding(
2025-04-11T04:23:18.6880757Z         bsz: int,
2025-04-11T04:23:18.6880836Z         block_size: int,
2025-04-11T04:23:18.6880930Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6881012Z         num_attn_heads: int,
2025-04-11T04:23:18.6881092Z         kv_group_num: int,
2025-04-11T04:23:18.6881178Z         same_context_len: bool,
2025-04-11T04:23:18.6881251Z         q_len: int,
2025-04-11T04:23:18.6881338Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6881427Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6881503Z     ):
2025-04-11T04:23:18.6881614Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6881807Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6881993Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6882160Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6882326Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6882480Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6882554Z     
2025-04-11T04:23:18.6882639Z         torch.manual_seed(123)
2025-04-11T04:23:18.6882726Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6882821Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6882825Z 
2025-04-11T04:23:18.6882976Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6883092Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6883096Z 
2025-04-11T04:23:18.6883172Z device = None
2025-04-11T04:23:18.6883177Z 
2025-04-11T04:23:18.6883296Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6883444Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6883514Z     
2025-04-11T04:23:18.6883589Z         Args:
2025-04-11T04:23:18.6883754Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6883919Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6884021Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6884096Z         """
2025-04-11T04:23:18.6884171Z         _lazy_init()
2025-04-11T04:23:18.6884263Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6884365Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6884568Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6884858Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6884997Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6885160Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6885164Z 
2025-04-11T04:23:18.6885402Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6885569Z ______________ test_flash_decoding[False-True-5-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6885579Z 
2025-04-11T04:23:18.6885729Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6885888Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6885980Z use_new_kcache_layout = False
2025-04-11T04:23:18.6885984Z 
2025-04-11T04:23:18.6886190Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6886293Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6886495Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6886638Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6886754Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6886867Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6887006Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6887107Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6887244Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6887394Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6887478Z     def test_flash_decoding(
2025-04-11T04:23:18.6887556Z         bsz: int,
2025-04-11T04:23:18.6887643Z         block_size: int,
2025-04-11T04:23:18.6887737Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6887817Z         num_attn_heads: int,
2025-04-11T04:23:18.6887903Z         kv_group_num: int,
2025-04-11T04:23:18.6887993Z         same_context_len: bool,
2025-04-11T04:23:18.6888069Z         q_len: int,
2025-04-11T04:23:18.6888157Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6888244Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6888320Z     ):
2025-04-11T04:23:18.6888431Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6888620Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6888802Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6888968Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6889136Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6889291Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6889367Z     
2025-04-11T04:23:18.6889454Z         torch.manual_seed(123)
2025-04-11T04:23:18.6889544Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6889640Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6889644Z 
2025-04-11T04:23:18.6889796Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6889908Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6889911Z 
2025-04-11T04:23:18.6889988Z device = None
2025-04-11T04:23:18.6889992Z 
2025-04-11T04:23:18.6890114Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6890269Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6890341Z     
2025-04-11T04:23:18.6890423Z         Args:
2025-04-11T04:23:18.6890674Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6890851Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6890960Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6891040Z         """
2025-04-11T04:23:18.6891117Z         _lazy_init()
2025-04-11T04:23:18.6891210Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6891314Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6891417Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6891705Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6891842Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6892004Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6892007Z 
2025-04-11T04:23:18.6892250Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6892417Z _____________ test_flash_decoding[False-True-5-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6892506Z 
2025-04-11T04:23:18.6892658Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6892818Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6892909Z use_new_kcache_layout = False
2025-04-11T04:23:18.6892912Z 
2025-04-11T04:23:18.6893109Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6893217Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6893331Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6893470Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6893583Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6893696Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6893833Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6893933Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6894074Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6894222Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6894307Z     def test_flash_decoding(
2025-04-11T04:23:18.6894384Z         bsz: int,
2025-04-11T04:23:18.6894466Z         block_size: int,
2025-04-11T04:23:18.6894559Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6894639Z         num_attn_heads: int,
2025-04-11T04:23:18.6894723Z         kv_group_num: int,
2025-04-11T04:23:18.6894808Z         same_context_len: bool,
2025-04-11T04:23:18.6894885Z         q_len: int,
2025-04-11T04:23:18.6894979Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6895071Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6895150Z     ):
2025-04-11T04:23:18.6895264Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6895454Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6895635Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6895807Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6895974Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6896132Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6896206Z     
2025-04-11T04:23:18.6896292Z         torch.manual_seed(123)
2025-04-11T04:23:18.6896383Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6896482Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6896486Z 
2025-04-11T04:23:18.6896638Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6896844Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6896849Z 
2025-04-11T04:23:18.6896933Z device = None
2025-04-11T04:23:18.6896936Z 
2025-04-11T04:23:18.6897063Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6897217Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6897292Z     
2025-04-11T04:23:18.6897364Z         Args:
2025-04-11T04:23:18.6897530Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6897695Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6897800Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6897874Z         """
2025-04-11T04:23:18.6897950Z         _lazy_init()
2025-04-11T04:23:18.6898041Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6898145Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6898250Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6898538Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6898766Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6898927Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6898931Z 
2025-04-11T04:23:18.6899171Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6899340Z ______________ test_flash_decoding[False-True-5-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6899343Z 
2025-04-11T04:23:18.6899492Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6899653Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6899748Z use_new_kcache_layout = False
2025-04-11T04:23:18.6899751Z 
2025-04-11T04:23:18.6899953Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6900059Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6900175Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6900315Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6900429Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6900540Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6900676Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6900775Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6900911Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6901060Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6901147Z     def test_flash_decoding(
2025-04-11T04:23:18.6901222Z         bsz: int,
2025-04-11T04:23:18.6901306Z         block_size: int,
2025-04-11T04:23:18.6901401Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6901481Z         num_attn_heads: int,
2025-04-11T04:23:18.6901565Z         kv_group_num: int,
2025-04-11T04:23:18.6901652Z         same_context_len: bool,
2025-04-11T04:23:18.6901726Z         q_len: int,
2025-04-11T04:23:18.6901813Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6901898Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6901972Z     ):
2025-04-11T04:23:18.6902082Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6902275Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6902462Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6902631Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6902885Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6903049Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6903125Z     
2025-04-11T04:23:18.6903209Z         torch.manual_seed(123)
2025-04-11T04:23:18.6903304Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6903403Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6903407Z 
2025-04-11T04:23:18.6903565Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6903682Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6903686Z 
2025-04-11T04:23:18.6903766Z device = None
2025-04-11T04:23:18.6903769Z 
2025-04-11T04:23:18.6903891Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6904038Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6904113Z     
2025-04-11T04:23:18.6904185Z         Args:
2025-04-11T04:23:18.6904351Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6904521Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6904625Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6904801Z         """
2025-04-11T04:23:18.6904882Z         _lazy_init()
2025-04-11T04:23:18.6904975Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6905079Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6905181Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6905465Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6905598Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6905756Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6905760Z 
2025-04-11T04:23:18.6906000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6906170Z _____________ test_flash_decoding[False-True-5-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6906174Z 
2025-04-11T04:23:18.6906327Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6906487Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6906578Z use_new_kcache_layout = False
2025-04-11T04:23:18.6906582Z 
2025-04-11T04:23:18.6906780Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6906889Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6907006Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6907151Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6907265Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6907382Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6907523Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6907625Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6907766Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6907916Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6908002Z     def test_flash_decoding(
2025-04-11T04:23:18.6908077Z         bsz: int,
2025-04-11T04:23:18.6908158Z         block_size: int,
2025-04-11T04:23:18.6908249Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6908329Z         num_attn_heads: int,
2025-04-11T04:23:18.6908448Z         kv_group_num: int,
2025-04-11T04:23:18.6908537Z         same_context_len: bool,
2025-04-11T04:23:18.6908610Z         q_len: int,
2025-04-11T04:23:18.6908699Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6908786Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6908862Z     ):
2025-04-11T04:23:18.6909065Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6909264Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6909443Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6909615Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6909784Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6909942Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6910020Z     
2025-04-11T04:23:18.6910110Z         torch.manual_seed(123)
2025-04-11T04:23:18.6910205Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6910294Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6910298Z 
2025-04-11T04:23:18.6910455Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6910573Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6910577Z 
2025-04-11T04:23:18.6910653Z device = None
2025-04-11T04:23:18.6910657Z 
2025-04-11T04:23:18.6910776Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6911016Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6911093Z     
2025-04-11T04:23:18.6911167Z         Args:
2025-04-11T04:23:18.6911328Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6911495Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6911603Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6911678Z         """
2025-04-11T04:23:18.6911755Z         _lazy_init()
2025-04-11T04:23:18.6911853Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6911953Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6912058Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6912349Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6912487Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6912644Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6912649Z 
2025-04-11T04:23:18.6912886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6913052Z _____________ test_flash_decoding[False-True-5-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6913056Z 
2025-04-11T04:23:18.6913204Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6913364Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6913453Z use_new_kcache_layout = False
2025-04-11T04:23:18.6913457Z 
2025-04-11T04:23:18.6913657Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6913765Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6913882Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6914021Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6914136Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6914251Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6914384Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6914484Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6914623Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6914771Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6914858Z     def test_flash_decoding(
2025-04-11T04:23:18.6914934Z         bsz: int,
2025-04-11T04:23:18.6915098Z         block_size: int,
2025-04-11T04:23:18.6915198Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6915280Z         num_attn_heads: int,
2025-04-11T04:23:18.6915370Z         kv_group_num: int,
2025-04-11T04:23:18.6915462Z         same_context_len: bool,
2025-04-11T04:23:18.6915536Z         q_len: int,
2025-04-11T04:23:18.6915626Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6915715Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6915791Z     ):
2025-04-11T04:23:18.6915899Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6916098Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6916278Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6916445Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6916618Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6916774Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6916848Z     
2025-04-11T04:23:18.6916932Z         torch.manual_seed(123)
2025-04-11T04:23:18.6917122Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6917214Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6917217Z 
2025-04-11T04:23:18.6917374Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6917492Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6917496Z 
2025-04-11T04:23:18.6917572Z device = None
2025-04-11T04:23:18.6917576Z 
2025-04-11T04:23:18.6917695Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6917849Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6917928Z     
2025-04-11T04:23:18.6918003Z         Args:
2025-04-11T04:23:18.6918176Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6918344Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6918449Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6918529Z         """
2025-04-11T04:23:18.6918605Z         _lazy_init()
2025-04-11T04:23:18.6918704Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6918803Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6918907Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6919190Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6919324Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6919485Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6919488Z 
2025-04-11T04:23:18.6919730Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6919897Z _____________ test_flash_decoding[False-True-5-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6919901Z 
2025-04-11T04:23:18.6920054Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6920216Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6920304Z use_new_kcache_layout = False
2025-04-11T04:23:18.6920307Z 
2025-04-11T04:23:18.6920503Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6920610Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6920723Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6920861Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6920976Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6921177Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6921316Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6921415Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6921553Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6921708Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6921794Z     def test_flash_decoding(
2025-04-11T04:23:18.6921869Z         bsz: int,
2025-04-11T04:23:18.6921949Z         block_size: int,
2025-04-11T04:23:18.6922046Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6922127Z         num_attn_heads: int,
2025-04-11T04:23:18.6922211Z         kv_group_num: int,
2025-04-11T04:23:18.6922296Z         same_context_len: bool,
2025-04-11T04:23:18.6922373Z         q_len: int,
2025-04-11T04:23:18.6922457Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6922545Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6922623Z     ):
2025-04-11T04:23:18.6922738Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6922934Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6923113Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6923372Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6923539Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6923695Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6923772Z     
2025-04-11T04:23:18.6923858Z         torch.manual_seed(123)
2025-04-11T04:23:18.6923946Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6924035Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6924039Z 
2025-04-11T04:23:18.6924192Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6924309Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6924313Z 
2025-04-11T04:23:18.6924389Z device = None
2025-04-11T04:23:18.6924393Z 
2025-04-11T04:23:18.6924512Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6924664Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6924737Z     
2025-04-11T04:23:18.6924809Z         Args:
2025-04-11T04:23:18.6924973Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6925144Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6925251Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6925330Z         """
2025-04-11T04:23:18.6925406Z         _lazy_init()
2025-04-11T04:23:18.6925504Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6925603Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6925709Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6925992Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6926135Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6926295Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6926299Z 
2025-04-11T04:23:18.6926538Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6926706Z _____________ test_flash_decoding[False-True-5-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6926710Z 
2025-04-11T04:23:18.6926859Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6927022Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6927109Z use_new_kcache_layout = False
2025-04-11T04:23:18.6927113Z 
2025-04-11T04:23:18.6927397Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6927506Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6927620Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6927765Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6927876Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6927990Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6928127Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6928228Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6928370Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6928517Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6928611Z     def test_flash_decoding(
2025-04-11T04:23:18.6928686Z         bsz: int,
2025-04-11T04:23:18.6928774Z         block_size: int,
2025-04-11T04:23:18.6928864Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6928947Z         num_attn_heads: int,
2025-04-11T04:23:18.6929035Z         kv_group_num: int,
2025-04-11T04:23:18.6929314Z         same_context_len: bool,
2025-04-11T04:23:18.6929392Z         q_len: int,
2025-04-11T04:23:18.6929474Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6929560Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6929636Z     ):
2025-04-11T04:23:18.6929746Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6929945Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6930126Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6930300Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6930471Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6930629Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6930706Z     
2025-04-11T04:23:18.6930793Z         torch.manual_seed(123)
2025-04-11T04:23:18.6930893Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6930987Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6930992Z 
2025-04-11T04:23:18.6931151Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6931261Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6931265Z 
2025-04-11T04:23:18.6931341Z device = None
2025-04-11T04:23:18.6931346Z 
2025-04-11T04:23:18.6931468Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6931616Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6931689Z     
2025-04-11T04:23:18.6931761Z         Args:
2025-04-11T04:23:18.6931935Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6932101Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6932204Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6932285Z         """
2025-04-11T04:23:18.6932362Z         _lazy_init()
2025-04-11T04:23:18.6932459Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6932561Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6932667Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6932959Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6933093Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6933257Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6933261Z 
2025-04-11T04:23:18.6933583Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6933758Z _____________ test_flash_decoding[False-True-5-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6933762Z 
2025-04-11T04:23:18.6933917Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6934080Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6934169Z use_new_kcache_layout = False
2025-04-11T04:23:18.6934173Z 
2025-04-11T04:23:18.6934372Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6934477Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6934591Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6934731Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6934844Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6934961Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6935098Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6935198Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6935335Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6935572Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6935660Z     def test_flash_decoding(
2025-04-11T04:23:18.6935736Z         bsz: int,
2025-04-11T04:23:18.6935820Z         block_size: int,
2025-04-11T04:23:18.6935908Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6935992Z         num_attn_heads: int,
2025-04-11T04:23:18.6936083Z         kv_group_num: int,
2025-04-11T04:23:18.6936166Z         same_context_len: bool,
2025-04-11T04:23:18.6936244Z         q_len: int,
2025-04-11T04:23:18.6936327Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6936413Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6936491Z     ):
2025-04-11T04:23:18.6936601Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6936795Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6936974Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6937149Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6937312Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6937466Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6937541Z     
2025-04-11T04:23:18.6937626Z         torch.manual_seed(123)
2025-04-11T04:23:18.6937716Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6937806Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6937810Z 
2025-04-11T04:23:18.6937963Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6938075Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6938078Z 
2025-04-11T04:23:18.6938155Z device = None
2025-04-11T04:23:18.6938159Z 
2025-04-11T04:23:18.6938280Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6938429Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6938508Z     
2025-04-11T04:23:18.6938582Z         Args:
2025-04-11T04:23:18.6938748Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6938913Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6939015Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6939096Z         """
2025-04-11T04:23:18.6939172Z         _lazy_init()
2025-04-11T04:23:18.6939271Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6939370Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6939566Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6939856Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6939994Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6940152Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6940156Z 
2025-04-11T04:23:18.6940399Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6940567Z ______________ test_flash_decoding[False-True-5-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6940571Z 
2025-04-11T04:23:18.6940723Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6940886Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6940971Z use_new_kcache_layout = False
2025-04-11T04:23:18.6940975Z 
2025-04-11T04:23:18.6941178Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6941281Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6941395Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6941619Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6941732Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6941845Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6941978Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6942080Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6942211Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6942358Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6942446Z     def test_flash_decoding(
2025-04-11T04:23:18.6942522Z         bsz: int,
2025-04-11T04:23:18.6942607Z         block_size: int,
2025-04-11T04:23:18.6942697Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6942775Z         num_attn_heads: int,
2025-04-11T04:23:18.6942862Z         kv_group_num: int,
2025-04-11T04:23:18.6942946Z         same_context_len: bool,
2025-04-11T04:23:18.6943029Z         q_len: int,
2025-04-11T04:23:18.6943115Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6943203Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6943280Z     ):
2025-04-11T04:23:18.6943387Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6943581Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6943759Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6943935Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6944095Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6944251Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6944327Z     
2025-04-11T04:23:18.6944413Z         torch.manual_seed(123)
2025-04-11T04:23:18.6944507Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6944597Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6944601Z 
2025-04-11T04:23:18.6944756Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6944867Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6944871Z 
2025-04-11T04:23:18.6944947Z device = None
2025-04-11T04:23:18.6944954Z 
2025-04-11T04:23:18.6945068Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6945215Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6945290Z     
2025-04-11T04:23:18.6945363Z         Args:
2025-04-11T04:23:18.6945613Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6945781Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6945885Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6945966Z         """
2025-04-11T04:23:18.6946043Z         _lazy_init()
2025-04-11T04:23:18.6946140Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6946239Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6946345Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6946626Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6946761Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6946919Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6946923Z 
2025-04-11T04:23:18.6947163Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6947331Z _____________ test_flash_decoding[False-True-5-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6947335Z 
2025-04-11T04:23:18.6947486Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6947735Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6947820Z use_new_kcache_layout = False
2025-04-11T04:23:18.6947824Z 
2025-04-11T04:23:18.6948024Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6948127Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6948242Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6948381Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6948526Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6948641Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6948780Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6948885Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6949021Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6949175Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6949264Z     def test_flash_decoding(
2025-04-11T04:23:18.6949339Z         bsz: int,
2025-04-11T04:23:18.6949423Z         block_size: int,
2025-04-11T04:23:18.6949511Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6949591Z         num_attn_heads: int,
2025-04-11T04:23:18.6949680Z         kv_group_num: int,
2025-04-11T04:23:18.6949763Z         same_context_len: bool,
2025-04-11T04:23:18.6949838Z         q_len: int,
2025-04-11T04:23:18.6949921Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6950010Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6950081Z     ):
2025-04-11T04:23:18.6950193Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6950390Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6950569Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6950745Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6950908Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6951069Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6951140Z     
2025-04-11T04:23:18.6951226Z         torch.manual_seed(123)
2025-04-11T04:23:18.6951314Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6951404Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6951408Z 
2025-04-11T04:23:18.6951563Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6951785Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6951790Z 
2025-04-11T04:23:18.6951872Z device = None
2025-04-11T04:23:18.6951876Z 
2025-04-11T04:23:18.6951991Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6952142Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6952218Z     
2025-04-11T04:23:18.6952291Z         Args:
2025-04-11T04:23:18.6952458Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6952624Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6952731Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6952803Z         """
2025-04-11T04:23:18.6952878Z         _lazy_init()
2025-04-11T04:23:18.6952978Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6953077Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6953187Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6953466Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6953696Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6953856Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6953861Z 
2025-04-11T04:23:18.6954098Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6954264Z ______________ test_flash_decoding[False-True-5-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6954268Z 
2025-04-11T04:23:18.6954416Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6954582Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6954668Z use_new_kcache_layout = False
2025-04-11T04:23:18.6954672Z 
2025-04-11T04:23:18.6954877Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6954982Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6955093Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6955236Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6955348Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6955463Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6955596Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6955700Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6955833Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6955980Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6956072Z     def test_flash_decoding(
2025-04-11T04:23:18.6956147Z         bsz: int,
2025-04-11T04:23:18.6956234Z         block_size: int,
2025-04-11T04:23:18.6956324Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6956404Z         num_attn_heads: int,
2025-04-11T04:23:18.6956490Z         kv_group_num: int,
2025-04-11T04:23:18.6956572Z         same_context_len: bool,
2025-04-11T04:23:18.6956652Z         q_len: int,
2025-04-11T04:23:18.6956736Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6956826Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6956897Z     ):
2025-04-11T04:23:18.6957005Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6957200Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6957380Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6957555Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6957719Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6957963Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6958038Z     
2025-04-11T04:23:18.6958125Z         torch.manual_seed(123)
2025-04-11T04:23:18.6958223Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6958314Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6958318Z 
2025-04-11T04:23:18.6958477Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6958588Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6958592Z 
2025-04-11T04:23:18.6958672Z device = None
2025-04-11T04:23:18.6958676Z 
2025-04-11T04:23:18.6958791Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6958937Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6959013Z     
2025-04-11T04:23:18.6959084Z         Args:
2025-04-11T04:23:18.6959256Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6959421Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6959527Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6959685Z         """
2025-04-11T04:23:18.6959762Z         _lazy_init()
2025-04-11T04:23:18.6959862Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6959960Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6960066Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6960347Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6960481Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6960641Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6960645Z 
2025-04-11T04:23:18.6960886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6961056Z _____________ test_flash_decoding[False-True-5-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6961060Z 
2025-04-11T04:23:18.6961211Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6961381Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6961469Z use_new_kcache_layout = False
2025-04-11T04:23:18.6961472Z 
2025-04-11T04:23:18.6961674Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6961776Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6961889Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6962029Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6962144Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6962259Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6962396Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6962502Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6962635Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6962789Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6962875Z     def test_flash_decoding(
2025-04-11T04:23:18.6962949Z         bsz: int,
2025-04-11T04:23:18.6963034Z         block_size: int,
2025-04-11T04:23:18.6963122Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6963207Z         num_attn_heads: int,
2025-04-11T04:23:18.6963288Z         kv_group_num: int,
2025-04-11T04:23:18.6963371Z         same_context_len: bool,
2025-04-11T04:23:18.6963447Z         q_len: int,
2025-04-11T04:23:18.6963531Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6963621Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6963694Z     ):
2025-04-11T04:23:18.6963891Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6964095Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6964276Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6964455Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6964619Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6964779Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6964850Z     
2025-04-11T04:23:18.6964934Z         torch.manual_seed(123)
2025-04-11T04:23:18.6965031Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6965122Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6965126Z 
2025-04-11T04:23:18.6965281Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6965394Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6965397Z 
2025-04-11T04:23:18.6965478Z device = None
2025-04-11T04:23:18.6965481Z 
2025-04-11T04:23:18.6965596Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6965829Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6965904Z     
2025-04-11T04:23:18.6965978Z         Args:
2025-04-11T04:23:18.6966145Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6966308Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6966415Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6966489Z         """
2025-04-11T04:23:18.6966564Z         _lazy_init()
2025-04-11T04:23:18.6966661Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6966760Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6966869Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6967149Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6967292Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6967448Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6967452Z 
2025-04-11T04:23:18.6967690Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6967858Z _____________ test_flash_decoding[False-True-5-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6967861Z 
2025-04-11T04:23:18.6968011Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6968175Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6968261Z use_new_kcache_layout = False
2025-04-11T04:23:18.6968265Z 
2025-04-11T04:23:18.6968468Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6968573Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6968691Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6968831Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6968946Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6969060Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6969195Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6969299Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6969434Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6969585Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6969672Z     def test_flash_decoding(
2025-04-11T04:23:18.6969747Z         bsz: int,
2025-04-11T04:23:18.6969830Z         block_size: int,
2025-04-11T04:23:18.6970003Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6970092Z         num_attn_heads: int,
2025-04-11T04:23:18.6970173Z         kv_group_num: int,
2025-04-11T04:23:18.6970257Z         same_context_len: bool,
2025-04-11T04:23:18.6970342Z         q_len: int,
2025-04-11T04:23:18.6970428Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6970520Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6970592Z     ):
2025-04-11T04:23:18.6970704Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6970902Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6971085Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6971264Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6971431Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6971595Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6971668Z     
2025-04-11T04:23:18.6971753Z         torch.manual_seed(123)
2025-04-11T04:23:18.6971930Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6972020Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6972024Z 
2025-04-11T04:23:18.6972182Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6972294Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6972297Z 
2025-04-11T04:23:18.6972377Z device = None
2025-04-11T04:23:18.6972380Z 
2025-04-11T04:23:18.6972495Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6972645Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6972717Z     
2025-04-11T04:23:18.6972790Z         Args:
2025-04-11T04:23:18.6972962Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6973124Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6973232Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6973307Z         """
2025-04-11T04:23:18.6973383Z         _lazy_init()
2025-04-11T04:23:18.6973480Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6973579Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6973683Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6973966Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6974103Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6974259Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6974263Z 
2025-04-11T04:23:18.6974506Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6974680Z _____________ test_flash_decoding[False-True-5-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6974684Z 
2025-04-11T04:23:18.6974837Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6975009Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6975097Z use_new_kcache_layout = False
2025-04-11T04:23:18.6975102Z 
2025-04-11T04:23:18.6975304Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6975406Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6975521Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6975657Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6975770Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6975889Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6976122Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6976230Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6976364Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6976521Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6976603Z     def test_flash_decoding(
2025-04-11T04:23:18.6976679Z         bsz: int,
2025-04-11T04:23:18.6976761Z         block_size: int,
2025-04-11T04:23:18.6976851Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6976934Z         num_attn_heads: int,
2025-04-11T04:23:18.6977014Z         kv_group_num: int,
2025-04-11T04:23:18.6977097Z         same_context_len: bool,
2025-04-11T04:23:18.6977179Z         q_len: int,
2025-04-11T04:23:18.6977262Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6977353Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6977424Z     ):
2025-04-11T04:23:18.6977533Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6977730Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6977910Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6978191Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6978354Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6978515Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6978587Z     
2025-04-11T04:23:18.6978672Z         torch.manual_seed(123)
2025-04-11T04:23:18.6978763Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6978854Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6978858Z 
2025-04-11T04:23:18.6979016Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6979130Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6979134Z 
2025-04-11T04:23:18.6979214Z device = None
2025-04-11T04:23:18.6979218Z 
2025-04-11T04:23:18.6979333Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6979487Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6979557Z     
2025-04-11T04:23:18.6979629Z         Args:
2025-04-11T04:23:18.6979797Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6979962Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6980069Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6980140Z         """
2025-04-11T04:23:18.6980216Z         _lazy_init()
2025-04-11T04:23:18.6980314Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6980413Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6980522Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6980803Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6980939Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6981104Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6981108Z 
2025-04-11T04:23:18.6981347Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6981512Z _____________ test_flash_decoding[False-True-5-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6981516Z 
2025-04-11T04:23:18.6981666Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6981832Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6981918Z use_new_kcache_layout = False
2025-04-11T04:23:18.6981922Z 
2025-04-11T04:23:18.6982205Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6982310Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6982428Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6982566Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6982680Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6982794Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6982929Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6983032Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6983166Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6983318Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6983403Z     def test_flash_decoding(
2025-04-11T04:23:18.6983480Z         bsz: int,
2025-04-11T04:23:18.6983570Z         block_size: int,
2025-04-11T04:23:18.6983662Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6983747Z         num_attn_heads: int,
2025-04-11T04:23:18.6983827Z         kv_group_num: int,
2025-04-11T04:23:18.6983911Z         same_context_len: bool,
2025-04-11T04:23:18.6984075Z         q_len: int,
2025-04-11T04:23:18.6984160Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6984250Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6984322Z     ):
2025-04-11T04:23:18.6984432Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6984620Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6984800Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6984971Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6985133Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6985295Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6985366Z     
2025-04-11T04:23:18.6985454Z         torch.manual_seed(123)
2025-04-11T04:23:18.6985543Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6985637Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6985641Z 
2025-04-11T04:23:18.6985797Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6985906Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6985910Z 
2025-04-11T04:23:18.6985989Z device = None
2025-04-11T04:23:18.6985992Z 
2025-04-11T04:23:18.6986106Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6986257Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6986327Z     
2025-04-11T04:23:18.6986400Z         Args:
2025-04-11T04:23:18.6986578Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6986744Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6986856Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6986951Z         """
2025-04-11T04:23:18.6987034Z         _lazy_init()
2025-04-11T04:23:18.6987128Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6987230Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6987339Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6987623Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6987764Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6987922Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6987926Z 
2025-04-11T04:23:18.6988252Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6988459Z _____________ test_flash_decoding[False-True-5-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6988463Z 
2025-04-11T04:23:18.6988615Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.6988784Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6988872Z use_new_kcache_layout = False
2025-04-11T04:23:18.6988877Z 
2025-04-11T04:23:18.6989080Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6989184Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6989306Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6989445Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6989561Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6989677Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6989815Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6989922Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6990056Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6990303Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6990390Z     def test_flash_decoding(
2025-04-11T04:23:18.6990469Z         bsz: int,
2025-04-11T04:23:18.6990562Z         block_size: int,
2025-04-11T04:23:18.6990655Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6990748Z         num_attn_heads: int,
2025-04-11T04:23:18.6990836Z         kv_group_num: int,
2025-04-11T04:23:18.6990927Z         same_context_len: bool,
2025-04-11T04:23:18.6991010Z         q_len: int,
2025-04-11T04:23:18.6991099Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6991195Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6991274Z     ):
2025-04-11T04:23:18.6991391Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6991590Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6991775Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6991957Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6992124Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6992288Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6992364Z     
2025-04-11T04:23:18.6992460Z         torch.manual_seed(123)
2025-04-11T04:23:18.6992554Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6992649Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6992653Z 
2025-04-11T04:23:18.6992817Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.6992936Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.6992940Z 
2025-04-11T04:23:18.6993027Z device = None
2025-04-11T04:23:18.6993031Z 
2025-04-11T04:23:18.6993151Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.6993310Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6993385Z     
2025-04-11T04:23:18.6993461Z         Args:
2025-04-11T04:23:18.6993634Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.6993802Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.6993914Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.6993990Z         """
2025-04-11T04:23:18.6994076Z         _lazy_init()
2025-04-11T04:23:18.6994177Z         with torch.cuda.device(device):
2025-04-11T04:23:18.6994281Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.6994487Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.6994774Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.6994914Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.6995077Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6995081Z 
2025-04-11T04:23:18.6995326Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.6995492Z _____________ test_flash_decoding[False-True-5-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6995496Z 
2025-04-11T04:23:18.6995650Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.6995815Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.6995903Z use_new_kcache_layout = False
2025-04-11T04:23:18.6995907Z 
2025-04-11T04:23:18.6996115Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.6996220Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.6996340Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.6996576Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.6996696Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.6996811Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.6996947Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.6997056Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.6997191Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.6997346Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.6997430Z     def test_flash_decoding(
2025-04-11T04:23:18.6997507Z         bsz: int,
2025-04-11T04:23:18.6997592Z         block_size: int,
2025-04-11T04:23:18.6997684Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.6997772Z         num_attn_heads: int,
2025-04-11T04:23:18.6997855Z         kv_group_num: int,
2025-04-11T04:23:18.6997947Z         same_context_len: bool,
2025-04-11T04:23:18.6998028Z         q_len: int,
2025-04-11T04:23:18.6998114Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.6998211Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.6998285Z     ):
2025-04-11T04:23:18.6998403Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.6998598Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.6998782Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.6998956Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.6999121Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.6999291Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6999364Z     
2025-04-11T04:23:18.6999456Z         torch.manual_seed(123)
2025-04-11T04:23:18.6999546Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.6999641Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.6999645Z 
2025-04-11T04:23:18.6999807Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7046267Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7046285Z 
2025-04-11T04:23:18.7046418Z device = None
2025-04-11T04:23:18.7046423Z 
2025-04-11T04:23:18.7046585Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7046764Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7046838Z     
2025-04-11T04:23:18.7046913Z         Args:
2025-04-11T04:23:18.7047107Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7047494Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7047611Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7047692Z         """
2025-04-11T04:23:18.7047776Z         _lazy_init()
2025-04-11T04:23:18.7047875Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7047982Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7048091Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7048391Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7048540Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7048699Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7048705Z 
2025-04-11T04:23:18.7048968Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7049142Z _____________ test_flash_decoding[False-True-5-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.7049146Z 
2025-04-11T04:23:18.7049306Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7049590Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7049687Z use_new_kcache_layout = False
2025-04-11T04:23:18.7049692Z 
2025-04-11T04:23:18.7049900Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7050008Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7050135Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7050276Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7050395Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7050508Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7050653Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7050756Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7050894Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7051057Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7051147Z     def test_flash_decoding(
2025-04-11T04:23:18.7051226Z         bsz: int,
2025-04-11T04:23:18.7051308Z         block_size: int,
2025-04-11T04:23:18.7051400Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7051487Z         num_attn_heads: int,
2025-04-11T04:23:18.7051570Z         kv_group_num: int,
2025-04-11T04:23:18.7051663Z         same_context_len: bool,
2025-04-11T04:23:18.7051739Z         q_len: int,
2025-04-11T04:23:18.7051827Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7051915Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7051985Z     ):
2025-04-11T04:23:18.7052104Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7052311Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7052498Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7052676Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7052842Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7053000Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7053071Z     
2025-04-11T04:23:18.7053162Z         torch.manual_seed(123)
2025-04-11T04:23:18.7053251Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7053344Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7053348Z 
2025-04-11T04:23:18.7053508Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7053714Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7053719Z 
2025-04-11T04:23:18.7053799Z device = None
2025-04-11T04:23:18.7053804Z 
2025-04-11T04:23:18.7053928Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7054089Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7054157Z     
2025-04-11T04:23:18.7054234Z         Args:
2025-04-11T04:23:18.7054407Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7054578Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7054684Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7054757Z         """
2025-04-11T04:23:18.7054838Z         _lazy_init()
2025-04-11T04:23:18.7054933Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7055038Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7055143Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7055437Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7055578Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7055823Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7055827Z 
2025-04-11T04:23:18.7056077Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7056245Z _____________ test_flash_decoding[False-True-5-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.7056249Z 
2025-04-11T04:23:18.7056405Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7056570Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7056662Z use_new_kcache_layout = False
2025-04-11T04:23:18.7056666Z 
2025-04-11T04:23:18.7056869Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7056975Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7057097Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7057237Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7057356Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7057469Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7057607Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7057709Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7057844Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7058001Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7058088Z     def test_flash_decoding(
2025-04-11T04:23:18.7058166Z         bsz: int,
2025-04-11T04:23:18.7058247Z         block_size: int,
2025-04-11T04:23:18.7058345Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7058427Z         num_attn_heads: int,
2025-04-11T04:23:18.7058509Z         kv_group_num: int,
2025-04-11T04:23:18.7058598Z         same_context_len: bool,
2025-04-11T04:23:18.7058676Z         q_len: int,
2025-04-11T04:23:18.7058764Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7058851Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7058921Z     ):
2025-04-11T04:23:18.7059036Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7059233Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7059418Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7059589Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7059755Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7060086Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7060162Z     
2025-04-11T04:23:18.7060249Z         torch.manual_seed(123)
2025-04-11T04:23:18.7060338Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7060440Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7060443Z 
2025-04-11T04:23:18.7060601Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7060715Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7060719Z 
2025-04-11T04:23:18.7060795Z device = None
2025-04-11T04:23:18.7060799Z 
2025-04-11T04:23:18.7060921Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7061070Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7061139Z     
2025-04-11T04:23:18.7061213Z         Args:
2025-04-11T04:23:18.7061382Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7061556Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7061664Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7061829Z         """
2025-04-11T04:23:18.7061906Z         _lazy_init()
2025-04-11T04:23:18.7062000Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7062105Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7062209Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7062506Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7062643Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7062801Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7062805Z 
2025-04-11T04:23:18.7063045Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7063214Z _____________ test_flash_decoding[False-True-5-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.7063222Z 
2025-04-11T04:23:18.7063371Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7063536Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7063627Z use_new_kcache_layout = False
2025-04-11T04:23:18.7063631Z 
2025-04-11T04:23:18.7063830Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7063937Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7064052Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7064192Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7064306Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7064418Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7064561Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7064663Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7064801Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7064957Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7065043Z     def test_flash_decoding(
2025-04-11T04:23:18.7065120Z         bsz: int,
2025-04-11T04:23:18.7065200Z         block_size: int,
2025-04-11T04:23:18.7065290Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7065370Z         num_attn_heads: int,
2025-04-11T04:23:18.7065456Z         kv_group_num: int,
2025-04-11T04:23:18.7065540Z         same_context_len: bool,
2025-04-11T04:23:18.7065615Z         q_len: int,
2025-04-11T04:23:18.7065701Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7065788Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7065860Z     ):
2025-04-11T04:23:18.7065969Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7066248Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7066437Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7066612Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7066778Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7066933Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7067004Z     
2025-04-11T04:23:18.7067090Z         torch.manual_seed(123)
2025-04-11T04:23:18.7067177Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7067270Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7067274Z 
2025-04-11T04:23:18.7067429Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7067545Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7067549Z 
2025-04-11T04:23:18.7067624Z device = None
2025-04-11T04:23:18.7067628Z 
2025-04-11T04:23:18.7067748Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7068004Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7068072Z     
2025-04-11T04:23:18.7068146Z         Args:
2025-04-11T04:23:18.7068314Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7068531Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7068639Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7068713Z         """
2025-04-11T04:23:18.7068789Z         _lazy_init()
2025-04-11T04:23:18.7068882Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7068987Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7069091Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7069380Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7069515Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7069675Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7069679Z 
2025-04-11T04:23:18.7069915Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7070083Z _____________ test_flash_decoding[False-True-5-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.7070091Z 
2025-04-11T04:23:18.7070243Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7070407Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7070497Z use_new_kcache_layout = False
2025-04-11T04:23:18.7070501Z 
2025-04-11T04:23:18.7070704Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7070811Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7070927Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7071070Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7071185Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7071298Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7071437Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7071538Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7071673Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7071825Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7071913Z     def test_flash_decoding(
2025-04-11T04:23:18.7071986Z         bsz: int,
2025-04-11T04:23:18.7072065Z         block_size: int,
2025-04-11T04:23:18.7072249Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7072336Z         num_attn_heads: int,
2025-04-11T04:23:18.7072423Z         kv_group_num: int,
2025-04-11T04:23:18.7072509Z         same_context_len: bool,
2025-04-11T04:23:18.7072590Z         q_len: int,
2025-04-11T04:23:18.7072681Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7072768Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7072842Z     ):
2025-04-11T04:23:18.7072952Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7073142Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7073331Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7073503Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7073669Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7073829Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7073902Z     
2025-04-11T04:23:18.7073989Z         torch.manual_seed(123)
2025-04-11T04:23:18.7074076Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7074264Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7074267Z 
2025-04-11T04:23:18.7074423Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7074536Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7074540Z 
2025-04-11T04:23:18.7074615Z device = None
2025-04-11T04:23:18.7074619Z 
2025-04-11T04:23:18.7074742Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7074890Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7074961Z     
2025-04-11T04:23:18.7075032Z         Args:
2025-04-11T04:23:18.7075198Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7075368Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7075473Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7075550Z         """
2025-04-11T04:23:18.7075625Z         _lazy_init()
2025-04-11T04:23:18.7075719Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7075823Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7075927Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7076210Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7076345Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7076500Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7076504Z 
2025-04-11T04:23:18.7076741Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7076914Z ____________ test_flash_decoding[False-True-5-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.7076917Z 
2025-04-11T04:23:18.7077069Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7077233Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7077326Z use_new_kcache_layout = False
2025-04-11T04:23:18.7077329Z 
2025-04-11T04:23:18.7077527Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7077632Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7077747Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7077884Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7077997Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7078108Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7078332Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7078435Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7078574Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7078728Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7078814Z     def test_flash_decoding(
2025-04-11T04:23:18.7078886Z         bsz: int,
2025-04-11T04:23:18.7078965Z         block_size: int,
2025-04-11T04:23:18.7079056Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7079137Z         num_attn_heads: int,
2025-04-11T04:23:18.7079222Z         kv_group_num: int,
2025-04-11T04:23:18.7079305Z         same_context_len: bool,
2025-04-11T04:23:18.7079380Z         q_len: int,
2025-04-11T04:23:18.7079466Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7079553Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7079625Z     ):
2025-04-11T04:23:18.7079734Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7079930Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7080110Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7080366Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7080532Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7080689Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7080764Z     
2025-04-11T04:23:18.7080853Z         torch.manual_seed(123)
2025-04-11T04:23:18.7080945Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7081041Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7081045Z 
2025-04-11T04:23:18.7081202Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7081321Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7081328Z 
2025-04-11T04:23:18.7081406Z device = None
2025-04-11T04:23:18.7081410Z 
2025-04-11T04:23:18.7081533Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7081689Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7081763Z     
2025-04-11T04:23:18.7081836Z         Args:
2025-04-11T04:23:18.7082006Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7082177Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7082286Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7082363Z         """
2025-04-11T04:23:18.7082443Z         _lazy_init()
2025-04-11T04:23:18.7082539Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7082647Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7082756Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7083049Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7083188Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7083353Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7083357Z 
2025-04-11T04:23:18.7083599Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7083772Z _____________ test_flash_decoding[False-True-5-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.7083776Z 
2025-04-11T04:23:18.7083930Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7084096Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7084191Z use_new_kcache_layout = False
2025-04-11T04:23:18.7084195Z 
2025-04-11T04:23:18.7084499Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7084609Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7084726Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7084870Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7084987Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7085099Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7085237Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7085338Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7085476Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7085625Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7085714Z     def test_flash_decoding(
2025-04-11T04:23:18.7085788Z         bsz: int,
2025-04-11T04:23:18.7085867Z         block_size: int,
2025-04-11T04:23:18.7085961Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7086042Z         num_attn_heads: int,
2025-04-11T04:23:18.7086128Z         kv_group_num: int,
2025-04-11T04:23:18.7086213Z         same_context_len: bool,
2025-04-11T04:23:18.7086372Z         q_len: int,
2025-04-11T04:23:18.7086461Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7086548Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7086620Z     ):
2025-04-11T04:23:18.7086731Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7086930Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7087113Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7087284Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7087450Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7087612Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7087684Z     
2025-04-11T04:23:18.7087768Z         torch.manual_seed(123)
2025-04-11T04:23:18.7087855Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7087947Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7087951Z 
2025-04-11T04:23:18.7088106Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7088220Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7088223Z 
2025-04-11T04:23:18.7088298Z device = None
2025-04-11T04:23:18.7088302Z 
2025-04-11T04:23:18.7088425Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7088574Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7088646Z     
2025-04-11T04:23:18.7088716Z         Args:
2025-04-11T04:23:18.7088883Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7089053Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7089158Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7089233Z         """
2025-04-11T04:23:18.7089315Z         _lazy_init()
2025-04-11T04:23:18.7089413Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7089516Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7089619Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7089906Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7090042Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7090200Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7090204Z 
2025-04-11T04:23:18.7090446Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7090718Z ____________ test_flash_decoding[False-True-5-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.7090722Z 
2025-04-11T04:23:18.7090876Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7091044Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7091131Z use_new_kcache_layout = False
2025-04-11T04:23:18.7091135Z 
2025-04-11T04:23:18.7091331Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7091438Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7091552Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7091693Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7091807Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7091921Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7092060Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7092163Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7092302Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7092536Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7092630Z     def test_flash_decoding(
2025-04-11T04:23:18.7092704Z         bsz: int,
2025-04-11T04:23:18.7092789Z         block_size: int,
2025-04-11T04:23:18.7092877Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7092959Z         num_attn_heads: int,
2025-04-11T04:23:18.7093045Z         kv_group_num: int,
2025-04-11T04:23:18.7093130Z         same_context_len: bool,
2025-04-11T04:23:18.7093209Z         q_len: int,
2025-04-11T04:23:18.7093292Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7093379Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7093452Z     ):
2025-04-11T04:23:18.7093562Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7093757Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7093935Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7094110Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7094270Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7094423Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7094495Z     
2025-04-11T04:23:18.7094580Z         torch.manual_seed(123)
2025-04-11T04:23:18.7094671Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7094761Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7094765Z 
2025-04-11T04:23:18.7094921Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7095033Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7095040Z 
2025-04-11T04:23:18.7095116Z device = None
2025-04-11T04:23:18.7095124Z 
2025-04-11T04:23:18.7095240Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7095390Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7095462Z     
2025-04-11T04:23:18.7095536Z         Args:
2025-04-11T04:23:18.7095703Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7095865Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7095968Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7096043Z         """
2025-04-11T04:23:18.7096120Z         _lazy_init()
2025-04-11T04:23:18.7096215Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7096315Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7096422Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7096787Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7096927Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7097088Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7097092Z 
2025-04-11T04:23:18.7097329Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7097499Z _____________ test_flash_decoding[False-True-5-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.7097503Z 
2025-04-11T04:23:18.7097653Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7097817Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7097904Z use_new_kcache_layout = False
2025-04-11T04:23:18.7097908Z 
2025-04-11T04:23:18.7098110Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7098212Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7098328Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7098550Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7098665Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7098779Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7098913Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7099015Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7099149Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7099296Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7099384Z     def test_flash_decoding(
2025-04-11T04:23:18.7099457Z         bsz: int,
2025-04-11T04:23:18.7099541Z         block_size: int,
2025-04-11T04:23:18.7099632Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7099714Z         num_attn_heads: int,
2025-04-11T04:23:18.7099799Z         kv_group_num: int,
2025-04-11T04:23:18.7099882Z         same_context_len: bool,
2025-04-11T04:23:18.7099963Z         q_len: int,
2025-04-11T04:23:18.7100045Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7100134Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7100203Z     ):
2025-04-11T04:23:18.7100309Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7100501Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7100678Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7100849Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7101010Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7101170Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7101239Z     
2025-04-11T04:23:18.7101323Z         torch.manual_seed(123)
2025-04-11T04:23:18.7101412Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7101502Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7101506Z 
2025-04-11T04:23:18.7101663Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7101773Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7101777Z 
2025-04-11T04:23:18.7101856Z device = None
2025-04-11T04:23:18.7101859Z 
2025-04-11T04:23:18.7101974Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7102121Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7102192Z     
2025-04-11T04:23:18.7102263Z         Args:
2025-04-11T04:23:18.7102429Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7102679Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7102791Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7102865Z         """
2025-04-11T04:23:18.7102944Z         _lazy_init()
2025-04-11T04:23:18.7103041Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7103142Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7103247Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7103528Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7103664Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7103820Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7103824Z 
2025-04-11T04:23:18.7104061Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7104234Z _____________ test_flash_decoding[False-True-5-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.7104237Z 
2025-04-11T04:23:18.7104388Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7104638Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7104725Z use_new_kcache_layout = False
2025-04-11T04:23:18.7104729Z 
2025-04-11T04:23:18.7104929Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7105030Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7105145Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7105285Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7105398Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7105512Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7105650Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7105754Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7105887Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7106038Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7106127Z     def test_flash_decoding(
2025-04-11T04:23:18.7106201Z         bsz: int,
2025-04-11T04:23:18.7106283Z         block_size: int,
2025-04-11T04:23:18.7106369Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7106450Z         num_attn_heads: int,
2025-04-11T04:23:18.7106534Z         kv_group_num: int,
2025-04-11T04:23:18.7106618Z         same_context_len: bool,
2025-04-11T04:23:18.7106695Z         q_len: int,
2025-04-11T04:23:18.7106778Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7106866Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7106934Z     ):
2025-04-11T04:23:18.7107042Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7107238Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7107415Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7107589Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7107750Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7107906Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7107974Z     
2025-04-11T04:23:18.7108056Z         torch.manual_seed(123)
2025-04-11T04:23:18.7108143Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7108229Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7108233Z 
2025-04-11T04:23:18.7108386Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7108543Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7108644Z 
2025-04-11T04:23:18.7108727Z device = None
2025-04-11T04:23:18.7108731Z 
2025-04-11T04:23:18.7108851Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7108999Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7109075Z     
2025-04-11T04:23:18.7109145Z         Args:
2025-04-11T04:23:18.7109318Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7109483Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7109590Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7109662Z         """
2025-04-11T04:23:18.7109735Z         _lazy_init()
2025-04-11T04:23:18.7109830Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7109931Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7110036Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7110323Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7110460Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7110710Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7110714Z 
2025-04-11T04:23:18.7110951Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7111119Z _____________ test_flash_decoding[False-True-5-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.7111123Z 
2025-04-11T04:23:18.7111272Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7111438Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7111524Z use_new_kcache_layout = False
2025-04-11T04:23:18.7111528Z 
2025-04-11T04:23:18.7111733Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7111835Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7111955Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7112096Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7112214Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7112332Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7112469Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7112575Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7112713Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7112862Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7112950Z     def test_flash_decoding(
2025-04-11T04:23:18.7113023Z         bsz: int,
2025-04-11T04:23:18.7113106Z         block_size: int,
2025-04-11T04:23:18.7113194Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7113283Z         num_attn_heads: int,
2025-04-11T04:23:18.7113364Z         kv_group_num: int,
2025-04-11T04:23:18.7113447Z         same_context_len: bool,
2025-04-11T04:23:18.7113525Z         q_len: int,
2025-04-11T04:23:18.7113614Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7113703Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7113773Z     ):
2025-04-11T04:23:18.7113882Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7114080Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7114259Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7114432Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7114594Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7114860Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7114933Z     
2025-04-11T04:23:18.7115018Z         torch.manual_seed(123)
2025-04-11T04:23:18.7115111Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7115206Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7115209Z 
2025-04-11T04:23:18.7115367Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7115477Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7115481Z 
2025-04-11T04:23:18.7115558Z device = None
2025-04-11T04:23:18.7115562Z 
2025-04-11T04:23:18.7115679Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7115825Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7115896Z     
2025-04-11T04:23:18.7115968Z         Args:
2025-04-11T04:23:18.7116137Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7116302Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7116411Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7116482Z         """
2025-04-11T04:23:18.7116650Z         _lazy_init()
2025-04-11T04:23:18.7116746Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7116846Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7116951Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7117231Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7117369Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7117524Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7117528Z 
2025-04-11T04:23:18.7117762Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7117932Z _____________ test_flash_decoding[False-True-5-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.7117936Z 
2025-04-11T04:23:18.7118084Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7118252Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7118340Z use_new_kcache_layout = False
2025-04-11T04:23:18.7118344Z 
2025-04-11T04:23:18.7118541Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7118643Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7118760Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7118897Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7119011Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7119127Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7119265Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7119367Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7119501Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7119657Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7119741Z     def test_flash_decoding(
2025-04-11T04:23:18.7119815Z         bsz: int,
2025-04-11T04:23:18.7119898Z         block_size: int,
2025-04-11T04:23:18.7119985Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7120072Z         num_attn_heads: int,
2025-04-11T04:23:18.7120153Z         kv_group_num: int,
2025-04-11T04:23:18.7120235Z         same_context_len: bool,
2025-04-11T04:23:18.7120314Z         q_len: int,
2025-04-11T04:23:18.7120399Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7120490Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7120559Z     ):
2025-04-11T04:23:18.7120667Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7120946Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7121129Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7121306Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7121467Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7121626Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7121694Z     
2025-04-11T04:23:18.7121776Z         torch.manual_seed(123)
2025-04-11T04:23:18.7121865Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7121952Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7121956Z 
2025-04-11T04:23:18.7122113Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7122223Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7122231Z 
2025-04-11T04:23:18.7122309Z device = None
2025-04-11T04:23:18.7122313Z 
2025-04-11T04:23:18.7122427Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7122577Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7122731Z     
2025-04-11T04:23:18.7122802Z         Args:
2025-04-11T04:23:18.7122973Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7123136Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7123244Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7123315Z         """
2025-04-11T04:23:18.7123390Z         _lazy_init()
2025-04-11T04:23:18.7123485Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7123585Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7123691Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7123983Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7124120Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7124279Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7124283Z 
2025-04-11T04:23:18.7124517Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7124687Z _____________ test_flash_decoding[False-True-5-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.7124691Z 
2025-04-11T04:23:18.7124839Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7125007Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7125095Z use_new_kcache_layout = False
2025-04-11T04:23:18.7125099Z 
2025-04-11T04:23:18.7125304Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7125407Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7125527Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7125667Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7125782Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7125898Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7126032Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7126137Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7126270Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7126423Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7126507Z     def test_flash_decoding(
2025-04-11T04:23:18.7126579Z         bsz: int,
2025-04-11T04:23:18.7126662Z         block_size: int,
2025-04-11T04:23:18.7126750Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7126922Z         num_attn_heads: int,
2025-04-11T04:23:18.7127008Z         kv_group_num: int,
2025-04-11T04:23:18.7127093Z         same_context_len: bool,
2025-04-11T04:23:18.7127172Z         q_len: int,
2025-04-11T04:23:18.7127258Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7127348Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7127418Z     ):
2025-04-11T04:23:18.7127525Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7127720Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7127901Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7128076Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7128239Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7128401Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7128468Z     
2025-04-11T04:23:18.7128552Z         torch.manual_seed(123)
2025-04-11T04:23:18.7128642Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7128812Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7128816Z 
2025-04-11T04:23:18.7128974Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7129084Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7129088Z 
2025-04-11T04:23:18.7129165Z device = None
2025-04-11T04:23:18.7129168Z 
2025-04-11T04:23:18.7129284Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7129435Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7129503Z     
2025-04-11T04:23:18.7129573Z         Args:
2025-04-11T04:23:18.7129741Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7129908Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7130017Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7130088Z         """
2025-04-11T04:23:18.7130166Z         _lazy_init()
2025-04-11T04:23:18.7130261Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7130361Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7130469Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7130754Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7130892Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7131047Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7131051Z 
2025-04-11T04:23:18.7131289Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7131462Z ____________ test_flash_decoding[False-True-5-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.7131466Z 
2025-04-11T04:23:18.7131616Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7131785Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7131873Z use_new_kcache_layout = False
2025-04-11T04:23:18.7131877Z 
2025-04-11T04:23:18.7132078Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7132181Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7132300Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7132435Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7132548Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7132665Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7132884Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7132992Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7133127Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7133282Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7133366Z     def test_flash_decoding(
2025-04-11T04:23:18.7133438Z         bsz: int,
2025-04-11T04:23:18.7133523Z         block_size: int,
2025-04-11T04:23:18.7133610Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7133695Z         num_attn_heads: int,
2025-04-11T04:23:18.7133776Z         kv_group_num: int,
2025-04-11T04:23:18.7133860Z         same_context_len: bool,
2025-04-11T04:23:18.7133940Z         q_len: int,
2025-04-11T04:23:18.7134022Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7134111Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7134180Z     ):
2025-04-11T04:23:18.7134291Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7134483Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7134664Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7134940Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7135100Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7135257Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7135326Z     
2025-04-11T04:23:18.7135412Z         torch.manual_seed(123)
2025-04-11T04:23:18.7135497Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7135584Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7135587Z 
2025-04-11T04:23:18.7135744Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7135852Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7135859Z 
2025-04-11T04:23:18.7135937Z device = None
2025-04-11T04:23:18.7135940Z 
2025-04-11T04:23:18.7136055Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7136205Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7136275Z     
2025-04-11T04:23:18.7136345Z         Args:
2025-04-11T04:23:18.7136511Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7136675Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7136781Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7136852Z         """
2025-04-11T04:23:18.7136930Z         _lazy_init()
2025-04-11T04:23:18.7137022Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7137128Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7137236Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7137523Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7137661Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7137817Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7137821Z 
2025-04-11T04:23:18.7138060Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7138225Z _____________ test_flash_decoding[False-True-5-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.7138229Z 
2025-04-11T04:23:18.7138378Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7138544Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7138629Z use_new_kcache_layout = False
2025-04-11T04:23:18.7138633Z 
2025-04-11T04:23:18.7138915Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7139020Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7139140Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7139278Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7139399Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7139511Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7139646Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7139751Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7139885Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7140041Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7140128Z     def test_flash_decoding(
2025-04-11T04:23:18.7140202Z         bsz: int,
2025-04-11T04:23:18.7140288Z         block_size: int,
2025-04-11T04:23:18.7140375Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7140461Z         num_attn_heads: int,
2025-04-11T04:23:18.7140542Z         kv_group_num: int,
2025-04-11T04:23:18.7140625Z         same_context_len: bool,
2025-04-11T04:23:18.7140704Z         q_len: int,
2025-04-11T04:23:18.7140870Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7140961Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7141031Z     ):
2025-04-11T04:23:18.7141145Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7141338Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7141515Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7141689Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7141852Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7142015Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7142084Z     
2025-04-11T04:23:18.7142171Z         torch.manual_seed(123)
2025-04-11T04:23:18.7142256Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7142347Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7142351Z 
2025-04-11T04:23:18.7142508Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7142619Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7142623Z 
2025-04-11T04:23:18.7142703Z device = None
2025-04-11T04:23:18.7142706Z 
2025-04-11T04:23:18.7142823Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7142975Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7143044Z     
2025-04-11T04:23:18.7143115Z         Args:
2025-04-11T04:23:18.7143285Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7143450Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7143557Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7143629Z         """
2025-04-11T04:23:18.7143711Z         _lazy_init()
2025-04-11T04:23:18.7143804Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7143904Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7144011Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7144289Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7144430Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7144584Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7144588Z 
2025-04-11T04:23:18.7144827Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7145104Z ____________ test_flash_decoding[False-True-5-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.7145109Z 
2025-04-11T04:23:18.7145263Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7145428Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T04:23:18.7145516Z use_new_kcache_layout = False
2025-04-11T04:23:18.7145520Z 
2025-04-11T04:23:18.7145721Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7145824Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7145943Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7146081Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7146198Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7146313Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7146451Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7146556Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7146688Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7146840Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7147008Z     def test_flash_decoding(
2025-04-11T04:23:18.7147081Z         bsz: int,
2025-04-11T04:23:18.7147165Z         block_size: int,
2025-04-11T04:23:18.7147253Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7147337Z         num_attn_heads: int,
2025-04-11T04:23:18.7147416Z         kv_group_num: int,
2025-04-11T04:23:18.7147503Z         same_context_len: bool,
2025-04-11T04:23:18.7147577Z         q_len: int,
2025-04-11T04:23:18.7147659Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7147749Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7147817Z     ):
2025-04-11T04:23:18.7147930Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7148123Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7148300Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7148519Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7148683Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7148842Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7148911Z     
2025-04-11T04:23:18.7148996Z         torch.manual_seed(123)
2025-04-11T04:23:18.7149081Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7149169Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7149173Z 
2025-04-11T04:23:18.7149329Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7149440Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7149444Z 
2025-04-11T04:23:18.7149527Z device = None
2025-04-11T04:23:18.7149530Z 
2025-04-11T04:23:18.7149646Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7149799Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7149870Z     
2025-04-11T04:23:18.7149941Z         Args:
2025-04-11T04:23:18.7150111Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7150276Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7150383Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7150456Z         """
2025-04-11T04:23:18.7150536Z         _lazy_init()
2025-04-11T04:23:18.7150626Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7150725Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7150832Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7151210Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7151351Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7151510Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7151514Z 
2025-04-11T04:23:18.7151751Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7151918Z _____________ test_flash_decoding[False-False-1-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.7151922Z 
2025-04-11T04:23:18.7152075Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7152238Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7152325Z use_new_kcache_layout = False
2025-04-11T04:23:18.7152329Z 
2025-04-11T04:23:18.7152534Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7152635Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7152753Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7152887Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7153097Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7153210Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7153346Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7153450Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7153588Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7153740Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7153824Z     def test_flash_decoding(
2025-04-11T04:23:18.7153904Z         bsz: int,
2025-04-11T04:23:18.7153984Z         block_size: int,
2025-04-11T04:23:18.7154072Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7154161Z         num_attn_heads: int,
2025-04-11T04:23:18.7154242Z         kv_group_num: int,
2025-04-11T04:23:18.7154329Z         same_context_len: bool,
2025-04-11T04:23:18.7154403Z         q_len: int,
2025-04-11T04:23:18.7154492Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7154583Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7154653Z     ):
2025-04-11T04:23:18.7154764Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7154958Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7155139Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7155315Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7155478Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7155639Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7155707Z     
2025-04-11T04:23:18.7155793Z         torch.manual_seed(123)
2025-04-11T04:23:18.7155878Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7155973Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7155981Z 
2025-04-11T04:23:18.7156135Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7156243Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7156247Z 
2025-04-11T04:23:18.7156327Z device = None
2025-04-11T04:23:18.7156331Z 
2025-04-11T04:23:18.7156447Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7156597Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7156665Z     
2025-04-11T04:23:18.7156739Z         Args:
2025-04-11T04:23:18.7156908Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7157300Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7157413Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7157485Z         """
2025-04-11T04:23:18.7157569Z         _lazy_init()
2025-04-11T04:23:18.7157660Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7157761Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7157868Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7158152Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7158288Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7158441Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7158445Z 
2025-04-11T04:23:18.7158688Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7158857Z _____________ test_flash_decoding[False-False-1-True-1-16-8-16-16] _____________
2025-04-11T04:23:18.7158861Z 
2025-04-11T04:23:18.7159014Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7159279Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7159367Z use_new_kcache_layout = False
2025-04-11T04:23:18.7159371Z 
2025-04-11T04:23:18.7159574Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7159678Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7159797Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7159934Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7160052Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7160165Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7160299Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7160408Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7160545Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7160698Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7160785Z     def test_flash_decoding(
2025-04-11T04:23:18.7160862Z         bsz: int,
2025-04-11T04:23:18.7160942Z         block_size: int,
2025-04-11T04:23:18.7161030Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7161117Z         num_attn_heads: int,
2025-04-11T04:23:18.7161197Z         kv_group_num: int,
2025-04-11T04:23:18.7161286Z         same_context_len: bool,
2025-04-11T04:23:18.7161361Z         q_len: int,
2025-04-11T04:23:18.7161445Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7161535Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7161603Z     ):
2025-04-11T04:23:18.7161713Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7161906Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7162088Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7162263Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7162425Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7162585Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7162652Z     
2025-04-11T04:23:18.7162742Z         torch.manual_seed(123)
2025-04-11T04:23:18.7162831Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7162919Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7162928Z 
2025-04-11T04:23:18.7163080Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7163190Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7163194Z 
2025-04-11T04:23:18.7163357Z device = None
2025-04-11T04:23:18.7163362Z 
2025-04-11T04:23:18.7163479Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7163632Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7163704Z     
2025-04-11T04:23:18.7163778Z         Args:
2025-04-11T04:23:18.7163945Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7164111Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7164221Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7164293Z         """
2025-04-11T04:23:18.7164369Z         _lazy_init()
2025-04-11T04:23:18.7164462Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7164563Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7164671Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7164958Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7165097Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7165336Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7165341Z 
2025-04-11T04:23:18.7165583Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7165753Z _____________ test_flash_decoding[False-False-1-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.7165757Z 
2025-04-11T04:23:18.7165910Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7166072Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7166161Z use_new_kcache_layout = False
2025-04-11T04:23:18.7166170Z 
2025-04-11T04:23:18.7166371Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7166472Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7166593Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7166730Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7166853Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7166965Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7167099Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7167203Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7167338Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7167491Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7167579Z     def test_flash_decoding(
2025-04-11T04:23:18.7167655Z         bsz: int,
2025-04-11T04:23:18.7167734Z         block_size: int,
2025-04-11T04:23:18.7167823Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7167912Z         num_attn_heads: int,
2025-04-11T04:23:18.7167994Z         kv_group_num: int,
2025-04-11T04:23:18.7168082Z         same_context_len: bool,
2025-04-11T04:23:18.7168157Z         q_len: int,
2025-04-11T04:23:18.7168246Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7168337Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7168406Z     ):
2025-04-11T04:23:18.7168519Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7168710Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7168893Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7169065Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7169228Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7169472Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7169544Z     
2025-04-11T04:23:18.7169636Z         torch.manual_seed(123)
2025-04-11T04:23:18.7169722Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7169817Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7169824Z 
2025-04-11T04:23:18.7169978Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7170088Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7170092Z 
2025-04-11T04:23:18.7170175Z device = None
2025-04-11T04:23:18.7170179Z 
2025-04-11T04:23:18.7170295Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7170448Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7170516Z     
2025-04-11T04:23:18.7170590Z         Args:
2025-04-11T04:23:18.7170755Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7170924Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7171033Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7171104Z         """
2025-04-11T04:23:18.7171272Z         _lazy_init()
2025-04-11T04:23:18.7171365Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7171469Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7171571Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7171853Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7171992Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7172145Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7172149Z 
2025-04-11T04:23:18.7172386Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7172554Z _____________ test_flash_decoding[False-False-1-True-1-16-8-32-16] _____________
2025-04-11T04:23:18.7172558Z 
2025-04-11T04:23:18.7172710Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7172875Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7172965Z use_new_kcache_layout = False
2025-04-11T04:23:18.7172969Z 
2025-04-11T04:23:18.7173166Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7173268Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7173389Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7173525Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7173644Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7173758Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7173897Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7174000Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7174133Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7174289Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7174378Z     def test_flash_decoding(
2025-04-11T04:23:18.7174455Z         bsz: int,
2025-04-11T04:23:18.7174534Z         block_size: int,
2025-04-11T04:23:18.7174620Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7174705Z         num_attn_heads: int,
2025-04-11T04:23:18.7174786Z         kv_group_num: int,
2025-04-11T04:23:18.7174875Z         same_context_len: bool,
2025-04-11T04:23:18.7174950Z         q_len: int,
2025-04-11T04:23:18.7175032Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7175129Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7175197Z     ):
2025-04-11T04:23:18.7175311Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7175579Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7175767Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7175942Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7176108Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7176266Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7176334Z     
2025-04-11T04:23:18.7176421Z         torch.manual_seed(123)
2025-04-11T04:23:18.7176506Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7176597Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7176600Z 
2025-04-11T04:23:18.7176754Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7176865Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7176873Z 
2025-04-11T04:23:18.7176951Z device = None
2025-04-11T04:23:18.7176955Z 
2025-04-11T04:23:18.7177073Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7177229Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7177385Z     
2025-04-11T04:23:18.7177461Z         Args:
2025-04-11T04:23:18.7177627Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7177792Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7177900Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7177970Z         """
2025-04-11T04:23:18.7178049Z         _lazy_init()
2025-04-11T04:23:18.7178142Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7178245Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7178346Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7178632Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7178769Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7178929Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7178933Z 
2025-04-11T04:23:18.7179173Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7179339Z _____________ test_flash_decoding[False-False-1-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.7179343Z 
2025-04-11T04:23:18.7179498Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7179661Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7179754Z use_new_kcache_layout = False
2025-04-11T04:23:18.7179758Z 
2025-04-11T04:23:18.7179958Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7180063Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7180186Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7180323Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7180447Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7180561Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7180699Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7180799Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7180934Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7181090Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7181175Z     def test_flash_decoding(
2025-04-11T04:23:18.7181254Z         bsz: int,
2025-04-11T04:23:18.7181334Z         block_size: int,
2025-04-11T04:23:18.7181422Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7181599Z         num_attn_heads: int,
2025-04-11T04:23:18.7181686Z         kv_group_num: int,
2025-04-11T04:23:18.7181773Z         same_context_len: bool,
2025-04-11T04:23:18.7181848Z         q_len: int,
2025-04-11T04:23:18.7181939Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7182025Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7182094Z     ):
2025-04-11T04:23:18.7182206Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7182394Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7182581Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7182753Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7182917Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7183077Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7183146Z     
2025-04-11T04:23:18.7183234Z         torch.manual_seed(123)
2025-04-11T04:23:18.7183321Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7183413Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7183500Z 
2025-04-11T04:23:18.7183657Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7183771Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7183775Z 
2025-04-11T04:23:18.7183850Z device = None
2025-04-11T04:23:18.7183854Z 
2025-04-11T04:23:18.7183969Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7184124Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7184192Z     
2025-04-11T04:23:18.7184265Z         Args:
2025-04-11T04:23:18.7184430Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7184600Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7184706Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7184778Z         """
2025-04-11T04:23:18.7184857Z         _lazy_init()
2025-04-11T04:23:18.7184954Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7185059Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7185160Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7185443Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7185580Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7185734Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7185738Z 
2025-04-11T04:23:18.7185977Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7186149Z ____________ test_flash_decoding[False-False-1-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.7186153Z 
2025-04-11T04:23:18.7186308Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7186474Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7186565Z use_new_kcache_layout = False
2025-04-11T04:23:18.7186569Z 
2025-04-11T04:23:18.7186767Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7186869Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7186989Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7187127Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7187244Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7187354Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7187491Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7187678Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7187818Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7187972Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7188059Z     def test_flash_decoding(
2025-04-11T04:23:18.7188134Z         bsz: int,
2025-04-11T04:23:18.7188214Z         block_size: int,
2025-04-11T04:23:18.7188302Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7188383Z         num_attn_heads: int,
2025-04-11T04:23:18.7188497Z         kv_group_num: int,
2025-04-11T04:23:18.7188584Z         same_context_len: bool,
2025-04-11T04:23:18.7188659Z         q_len: int,
2025-04-11T04:23:18.7188743Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7188828Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7188897Z     ):
2025-04-11T04:23:18.7189008Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7189200Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7189381Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7189646Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7189811Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7189967Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7190035Z     
2025-04-11T04:23:18.7190124Z         torch.manual_seed(123)
2025-04-11T04:23:18.7190211Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7190302Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7190306Z 
2025-04-11T04:23:18.7190461Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7190574Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7190578Z 
2025-04-11T04:23:18.7190657Z device = None
2025-04-11T04:23:18.7190661Z 
2025-04-11T04:23:18.7190778Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7190930Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7191002Z     
2025-04-11T04:23:18.7191075Z         Args:
2025-04-11T04:23:18.7191243Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7191410Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7191514Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7191586Z         """
2025-04-11T04:23:18.7191667Z         _lazy_init()
2025-04-11T04:23:18.7191758Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7191862Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7191965Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7192251Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7192387Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7192544Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7192548Z 
2025-04-11T04:23:18.7192791Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7192956Z _____________ test_flash_decoding[False-False-1-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.7192960Z 
2025-04-11T04:23:18.7193113Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7193275Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7193368Z use_new_kcache_layout = False
2025-04-11T04:23:18.7193372Z 
2025-04-11T04:23:18.7193569Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7193761Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7193880Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7194016Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7194139Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7194252Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7194389Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7194492Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7194630Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7194785Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7194868Z     def test_flash_decoding(
2025-04-11T04:23:18.7194945Z         bsz: int,
2025-04-11T04:23:18.7195025Z         block_size: int,
2025-04-11T04:23:18.7195117Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7195203Z         num_attn_heads: int,
2025-04-11T04:23:18.7195285Z         kv_group_num: int,
2025-04-11T04:23:18.7195374Z         same_context_len: bool,
2025-04-11T04:23:18.7195448Z         q_len: int,
2025-04-11T04:23:18.7195620Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7195708Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7195776Z     ):
2025-04-11T04:23:18.7195890Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7196080Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7196263Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7196435Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7196604Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7196763Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7196830Z     
2025-04-11T04:23:18.7196918Z         torch.manual_seed(123)
2025-04-11T04:23:18.7197004Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7197093Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7197102Z 
2025-04-11T04:23:18.7197257Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7197370Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7197374Z 
2025-04-11T04:23:18.7197450Z device = None
2025-04-11T04:23:18.7197454Z 
2025-04-11T04:23:18.7197570Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7197723Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7197790Z     
2025-04-11T04:23:18.7197868Z         Args:
2025-04-11T04:23:18.7198035Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7198206Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7198311Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7198380Z         """
2025-04-11T04:23:18.7198460Z         _lazy_init()
2025-04-11T04:23:18.7198556Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7198658Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7198761Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7199050Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7199186Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7199339Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7199343Z 
2025-04-11T04:23:18.7199588Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7199841Z ____________ test_flash_decoding[False-False-1-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.7199845Z 
2025-04-11T04:23:18.7200006Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7200173Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7200265Z use_new_kcache_layout = False
2025-04-11T04:23:18.7200269Z 
2025-04-11T04:23:18.7200467Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7200574Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7200690Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7200827Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7200946Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7201058Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7201202Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7201307Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7201444Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7201595Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7201781Z     def test_flash_decoding(
2025-04-11T04:23:18.7201862Z         bsz: int,
2025-04-11T04:23:18.7201944Z         block_size: int,
2025-04-11T04:23:18.7202036Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7202119Z         num_attn_heads: int,
2025-04-11T04:23:18.7202203Z         kv_group_num: int,
2025-04-11T04:23:18.7202291Z         same_context_len: bool,
2025-04-11T04:23:18.7202369Z         q_len: int,
2025-04-11T04:23:18.7202456Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7202543Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7202611Z     ):
2025-04-11T04:23:18.7202725Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7202919Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7203105Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7203275Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7203445Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7203600Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7203668Z     
2025-04-11T04:23:18.7203756Z         torch.manual_seed(123)
2025-04-11T04:23:18.7203841Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7203933Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7203937Z 
2025-04-11T04:23:18.7204089Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7204201Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7204204Z 
2025-04-11T04:23:18.7204283Z device = None
2025-04-11T04:23:18.7204287Z 
2025-04-11T04:23:18.7204406Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7204556Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7204628Z     
2025-04-11T04:23:18.7204701Z         Args:
2025-04-11T04:23:18.7204865Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7205032Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7205139Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7205208Z         """
2025-04-11T04:23:18.7205288Z         _lazy_init()
2025-04-11T04:23:18.7205382Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7205487Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7205589Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7205989Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7206127Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7206285Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7206293Z 
2025-04-11T04:23:18.7206533Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7206700Z _____________ test_flash_decoding[False-False-1-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.7206705Z 
2025-04-11T04:23:18.7206856Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7207019Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7207110Z use_new_kcache_layout = False
2025-04-11T04:23:18.7207114Z 
2025-04-11T04:23:18.7207312Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7207419Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7207534Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7207673Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7207878Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7207989Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7208129Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7208229Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7208366Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7208515Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7208598Z     def test_flash_decoding(
2025-04-11T04:23:18.7208674Z         bsz: int,
2025-04-11T04:23:18.7208754Z         block_size: int,
2025-04-11T04:23:18.7208843Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7208928Z         num_attn_heads: int,
2025-04-11T04:23:18.7209009Z         kv_group_num: int,
2025-04-11T04:23:18.7209095Z         same_context_len: bool,
2025-04-11T04:23:18.7209170Z         q_len: int,
2025-04-11T04:23:18.7209259Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7209348Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7209421Z     ):
2025-04-11T04:23:18.7209528Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7209719Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7209903Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7210072Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7210237Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7210397Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7210470Z     
2025-04-11T04:23:18.7210553Z         torch.manual_seed(123)
2025-04-11T04:23:18.7210637Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7210729Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7210737Z 
2025-04-11T04:23:18.7210887Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7210998Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7211002Z 
2025-04-11T04:23:18.7211076Z device = None
2025-04-11T04:23:18.7211080Z 
2025-04-11T04:23:18.7211200Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7211347Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7211415Z     
2025-04-11T04:23:18.7211489Z         Args:
2025-04-11T04:23:18.7211654Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7211906Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7212014Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7212091Z         """
2025-04-11T04:23:18.7212169Z         _lazy_init()
2025-04-11T04:23:18.7212265Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7212369Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7212474Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7212760Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7212897Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7213049Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7213057Z 
2025-04-11T04:23:18.7213292Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7213461Z _____________ test_flash_decoding[False-False-1-True-4-16-8-16-16] _____________
2025-04-11T04:23:18.7213465Z 
2025-04-11T04:23:18.7213618Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7213868Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7213961Z use_new_kcache_layout = False
2025-04-11T04:23:18.7213965Z 
2025-04-11T04:23:18.7214165Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7214271Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7214388Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7214524Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7214643Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7214759Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7214896Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7215002Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7215140Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7215292Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7215379Z     def test_flash_decoding(
2025-04-11T04:23:18.7215456Z         bsz: int,
2025-04-11T04:23:18.7215536Z         block_size: int,
2025-04-11T04:23:18.7215627Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7215710Z         num_attn_heads: int,
2025-04-11T04:23:18.7215791Z         kv_group_num: int,
2025-04-11T04:23:18.7215878Z         same_context_len: bool,
2025-04-11T04:23:18.7215953Z         q_len: int,
2025-04-11T04:23:18.7216038Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7216127Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7216199Z     ):
2025-04-11T04:23:18.7216309Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7216502Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7216690Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7216858Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7217025Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7217180Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7217252Z     
2025-04-11T04:23:18.7217337Z         torch.manual_seed(123)
2025-04-11T04:23:18.7217423Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7217516Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7217519Z 
2025-04-11T04:23:18.7217673Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7217783Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7217787Z 
2025-04-11T04:23:18.7217949Z device = None
2025-04-11T04:23:18.7217953Z 
2025-04-11T04:23:18.7218073Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7218223Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7218296Z     
2025-04-11T04:23:18.7218371Z         Args:
2025-04-11T04:23:18.7218539Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7218708Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7218814Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7218888Z         """
2025-04-11T04:23:18.7218965Z         _lazy_init()
2025-04-11T04:23:18.7219057Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7219160Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7219262Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7219555Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7219692Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7219941Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7219945Z 
2025-04-11T04:23:18.7220178Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7220343Z _____________ test_flash_decoding[False-False-1-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.7220347Z 
2025-04-11T04:23:18.7220500Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7220661Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7220751Z use_new_kcache_layout = False
2025-04-11T04:23:18.7220755Z 
2025-04-11T04:23:18.7220953Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7221059Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7221176Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7221317Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7221434Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7221545Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7221682Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7221781Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7221917Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7222064Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7222148Z     def test_flash_decoding(
2025-04-11T04:23:18.7222228Z         bsz: int,
2025-04-11T04:23:18.7222305Z         block_size: int,
2025-04-11T04:23:18.7222394Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7222477Z         num_attn_heads: int,
2025-04-11T04:23:18.7222563Z         kv_group_num: int,
2025-04-11T04:23:18.7222647Z         same_context_len: bool,
2025-04-11T04:23:18.7222723Z         q_len: int,
2025-04-11T04:23:18.7222809Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7222898Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7222970Z     ):
2025-04-11T04:23:18.7223078Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7223269Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7223451Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7223625Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7223794Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7223948Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7224102Z     
2025-04-11T04:23:18.7224189Z         torch.manual_seed(123)
2025-04-11T04:23:18.7224276Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7224368Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7224375Z 
2025-04-11T04:23:18.7224529Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7224641Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7224644Z 
2025-04-11T04:23:18.7224719Z device = None
2025-04-11T04:23:18.7224723Z 
2025-04-11T04:23:18.7224841Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7224990Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7225058Z     
2025-04-11T04:23:18.7225132Z         Args:
2025-04-11T04:23:18.7225296Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7225464Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7225570Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7225643Z         """
2025-04-11T04:23:18.7225718Z         _lazy_init()
2025-04-11T04:23:18.7225917Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7226020Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7226122Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7226406Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7226540Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7226696Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7226700Z 
2025-04-11T04:23:18.7226934Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7227101Z _____________ test_flash_decoding[False-False-1-True-4-16-8-32-16] _____________
2025-04-11T04:23:18.7227108Z 
2025-04-11T04:23:18.7227257Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7227418Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7227513Z use_new_kcache_layout = False
2025-04-11T04:23:18.7227516Z 
2025-04-11T04:23:18.7227714Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7227824Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7227941Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7228084Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7228197Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7228310Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7228483Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7228589Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7228728Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7228875Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7228966Z     def test_flash_decoding(
2025-04-11T04:23:18.7229039Z         bsz: int,
2025-04-11T04:23:18.7229117Z         block_size: int,
2025-04-11T04:23:18.7229208Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7229288Z         num_attn_heads: int,
2025-04-11T04:23:18.7229371Z         kv_group_num: int,
2025-04-11T04:23:18.7229454Z         same_context_len: bool,
2025-04-11T04:23:18.7229528Z         q_len: int,
2025-04-11T04:23:18.7229614Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7229700Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7229771Z     ):
2025-04-11T04:23:18.7229878Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7230156Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7230344Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7230515Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7230682Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7230836Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7230906Z     
2025-04-11T04:23:18.7230991Z         torch.manual_seed(123)
2025-04-11T04:23:18.7231076Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7231167Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7231170Z 
2025-04-11T04:23:18.7231322Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7231435Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7231438Z 
2025-04-11T04:23:18.7231516Z device = None
2025-04-11T04:23:18.7231520Z 
2025-04-11T04:23:18.7231637Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7231784Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7231945Z     
2025-04-11T04:23:18.7232019Z         Args:
2025-04-11T04:23:18.7232185Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7232353Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7232458Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7232532Z         """
2025-04-11T04:23:18.7232607Z         _lazy_init()
2025-04-11T04:23:18.7232699Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7232802Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7232903Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7233190Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7233323Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7233483Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7233491Z 
2025-04-11T04:23:18.7233725Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7233893Z _____________ test_flash_decoding[False-False-1-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.7233897Z 
2025-04-11T04:23:18.7234046Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7234209Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7234300Z use_new_kcache_layout = False
2025-04-11T04:23:18.7234304Z 
2025-04-11T04:23:18.7234503Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7234611Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7234728Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7234869Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7234988Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7235101Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7235241Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7235342Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7235479Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7235630Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7235717Z     def test_flash_decoding(
2025-04-11T04:23:18.7235791Z         bsz: int,
2025-04-11T04:23:18.7235870Z         block_size: int,
2025-04-11T04:23:18.7235960Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7236125Z         num_attn_heads: int,
2025-04-11T04:23:18.7236212Z         kv_group_num: int,
2025-04-11T04:23:18.7236297Z         same_context_len: bool,
2025-04-11T04:23:18.7236372Z         q_len: int,
2025-04-11T04:23:18.7236459Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7236549Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7236622Z     ):
2025-04-11T04:23:18.7236731Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7236925Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7237111Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7237281Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7237448Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7237606Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7237681Z     
2025-04-11T04:23:18.7237765Z         torch.manual_seed(123)
2025-04-11T04:23:18.7237851Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7237944Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7238034Z 
2025-04-11T04:23:18.7238188Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7238301Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7238305Z 
2025-04-11T04:23:18.7238380Z device = None
2025-04-11T04:23:18.7238383Z 
2025-04-11T04:23:18.7238503Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7238651Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7238721Z     
2025-04-11T04:23:18.7238792Z         Args:
2025-04-11T04:23:18.7238956Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7239127Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7239232Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7239304Z         """
2025-04-11T04:23:18.7239380Z         _lazy_init()
2025-04-11T04:23:18.7239476Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7239579Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7239681Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7239969Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7240103Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7240261Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7240264Z 
2025-04-11T04:23:18.7240502Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7240674Z ____________ test_flash_decoding[False-False-1-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.7240678Z 
2025-04-11T04:23:18.7240827Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7240989Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7241083Z use_new_kcache_layout = False
2025-04-11T04:23:18.7241087Z 
2025-04-11T04:23:18.7241285Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7241390Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7241505Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7241643Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7241757Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7241869Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7242004Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7242188Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7242333Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7242485Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7242575Z     def test_flash_decoding(
2025-04-11T04:23:18.7242648Z         bsz: int,
2025-04-11T04:23:18.7242728Z         block_size: int,
2025-04-11T04:23:18.7242819Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7242901Z         num_attn_heads: int,
2025-04-11T04:23:18.7242984Z         kv_group_num: int,
2025-04-11T04:23:18.7243066Z         same_context_len: bool,
2025-04-11T04:23:18.7243139Z         q_len: int,
2025-04-11T04:23:18.7243225Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7243310Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7243381Z     ):
2025-04-11T04:23:18.7243490Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7243686Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7243865Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7244034Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7244284Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7244440Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7244509Z     
2025-04-11T04:23:18.7244594Z         torch.manual_seed(123)
2025-04-11T04:23:18.7244684Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7244772Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7244776Z 
2025-04-11T04:23:18.7244928Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7245042Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7245046Z 
2025-04-11T04:23:18.7245121Z device = None
2025-04-11T04:23:18.7245127Z 
2025-04-11T04:23:18.7245245Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7245393Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7245467Z     
2025-04-11T04:23:18.7245537Z         Args:
2025-04-11T04:23:18.7245701Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7245868Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7245971Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7246045Z         """
2025-04-11T04:23:18.7246121Z         _lazy_init()
2025-04-11T04:23:18.7246215Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7246315Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7246419Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7246707Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7246841Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7246997Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7247004Z 
2025-04-11T04:23:18.7247238Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7247406Z _____________ test_flash_decoding[False-False-1-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.7247410Z 
2025-04-11T04:23:18.7247560Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7247721Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7247809Z use_new_kcache_layout = False
2025-04-11T04:23:18.7247813Z 
2025-04-11T04:23:18.7248011Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7248198Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7248316Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7248456Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7248578Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7248700Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7248839Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7248945Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7249086Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7249241Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7249333Z     def test_flash_decoding(
2025-04-11T04:23:18.7249409Z         bsz: int,
2025-04-11T04:23:18.7249493Z         block_size: int,
2025-04-11T04:23:18.7249588Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7249672Z         num_attn_heads: int,
2025-04-11T04:23:18.7249764Z         kv_group_num: int,
2025-04-11T04:23:18.7249850Z         same_context_len: bool,
2025-04-11T04:23:18.7249927Z         q_len: int,
2025-04-11T04:23:18.7250018Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7250294Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7250369Z     ):
2025-04-11T04:23:18.7250478Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7250673Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7250855Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7251024Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7251191Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7251348Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7251425Z     
2025-04-11T04:23:18.7251509Z         torch.manual_seed(123)
2025-04-11T04:23:18.7251598Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7251686Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7251694Z 
2025-04-11T04:23:18.7251848Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7251962Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7251966Z 
2025-04-11T04:23:18.7252041Z device = None
2025-04-11T04:23:18.7252044Z 
2025-04-11T04:23:18.7252165Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7252315Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7252388Z     
2025-04-11T04:23:18.7252460Z         Args:
2025-04-11T04:23:18.7252624Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7252792Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7252897Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7252971Z         """
2025-04-11T04:23:18.7253047Z         _lazy_init()
2025-04-11T04:23:18.7253146Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7253245Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7253349Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7253633Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7253765Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7253920Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7253924Z 
2025-04-11T04:23:18.7254158Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7254414Z ____________ test_flash_decoding[False-False-1-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.7254419Z 
2025-04-11T04:23:18.7254572Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7254739Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7254829Z use_new_kcache_layout = False
2025-04-11T04:23:18.7254833Z 
2025-04-11T04:23:18.7255033Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7255138Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7255252Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7255392Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7255505Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7255620Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7255755Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7255862Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7256001Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7256152Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7256327Z     def test_flash_decoding(
2025-04-11T04:23:18.7256401Z         bsz: int,
2025-04-11T04:23:18.7256481Z         block_size: int,
2025-04-11T04:23:18.7256572Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7256654Z         num_attn_heads: int,
2025-04-11T04:23:18.7256740Z         kv_group_num: int,
2025-04-11T04:23:18.7256822Z         same_context_len: bool,
2025-04-11T04:23:18.7256900Z         q_len: int,
2025-04-11T04:23:18.7256983Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7257070Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7257143Z     ):
2025-04-11T04:23:18.7257253Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7257448Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7257626Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7257796Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7257965Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7258120Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7258191Z     
2025-04-11T04:23:18.7258276Z         torch.manual_seed(123)
2025-04-11T04:23:18.7258365Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7258453Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7258457Z 
2025-04-11T04:23:18.7258610Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7258724Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7258728Z 
2025-04-11T04:23:18.7258804Z device = None
2025-04-11T04:23:18.7258813Z 
2025-04-11T04:23:18.7258934Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7259079Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7259156Z     
2025-04-11T04:23:18.7259227Z         Args:
2025-04-11T04:23:18.7259392Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7259559Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7259663Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7259737Z         """
2025-04-11T04:23:18.7259813Z         _lazy_init()
2025-04-11T04:23:18.7259907Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7260007Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7260109Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7260479Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7260614Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7260771Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7260778Z 
2025-04-11T04:23:18.7261013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7261181Z _____________ test_flash_decoding[False-False-1-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.7261186Z 
2025-04-11T04:23:18.7261336Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7261502Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7261588Z use_new_kcache_layout = False
2025-04-11T04:23:18.7261592Z 
2025-04-11T04:23:18.7261788Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7261900Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7262015Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7262156Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7262375Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7262488Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7262623Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7262723Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7262862Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7263010Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7263099Z     def test_flash_decoding(
2025-04-11T04:23:18.7263171Z         bsz: int,
2025-04-11T04:23:18.7263254Z         block_size: int,
2025-04-11T04:23:18.7263341Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7263421Z         num_attn_heads: int,
2025-04-11T04:23:18.7263510Z         kv_group_num: int,
2025-04-11T04:23:18.7263594Z         same_context_len: bool,
2025-04-11T04:23:18.7263671Z         q_len: int,
2025-04-11T04:23:18.7263754Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7263843Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7263918Z     ):
2025-04-11T04:23:18.7264025Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7264221Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7264402Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7264573Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7264734Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7264887Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7264962Z     
2025-04-11T04:23:18.7265047Z         torch.manual_seed(123)
2025-04-11T04:23:18.7265135Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7265222Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7265229Z 
2025-04-11T04:23:18.7265385Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7265492Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7265496Z 
2025-04-11T04:23:18.7265570Z device = None
2025-04-11T04:23:18.7265574Z 
2025-04-11T04:23:18.7265692Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7265839Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7265910Z     
2025-04-11T04:23:18.7265980Z         Args:
2025-04-11T04:23:18.7266145Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7266391Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7266500Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7266574Z         """
2025-04-11T04:23:18.7266651Z         _lazy_init()
2025-04-11T04:23:18.7266750Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7266851Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7266953Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7267239Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7267372Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7267532Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7267536Z 
2025-04-11T04:23:18.7267771Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7267946Z ____________ test_flash_decoding[False-False-1-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.7267950Z 
2025-04-11T04:23:18.7268098Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7268264Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7268496Z use_new_kcache_layout = False
2025-04-11T04:23:18.7268500Z 
2025-04-11T04:23:18.7268702Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7268809Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7268930Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7269069Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7269186Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7269303Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7269439Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7269542Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7269681Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7269831Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7269924Z     def test_flash_decoding(
2025-04-11T04:23:18.7270000Z         bsz: int,
2025-04-11T04:23:18.7270083Z         block_size: int,
2025-04-11T04:23:18.7270170Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7270252Z         num_attn_heads: int,
2025-04-11T04:23:18.7270339Z         kv_group_num: int,
2025-04-11T04:23:18.7270423Z         same_context_len: bool,
2025-04-11T04:23:18.7270501Z         q_len: int,
2025-04-11T04:23:18.7270585Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7270672Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7270746Z     ):
2025-04-11T04:23:18.7270854Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7271052Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7271232Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7271405Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7271572Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7271728Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7271801Z     
2025-04-11T04:23:18.7271896Z         torch.manual_seed(123)
2025-04-11T04:23:18.7271991Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7272081Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7272085Z 
2025-04-11T04:23:18.7272246Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7272360Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7272363Z 
2025-04-11T04:23:18.7272440Z device = None
2025-04-11T04:23:18.7272551Z 
2025-04-11T04:23:18.7272679Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7272830Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7272912Z     
2025-04-11T04:23:18.7272985Z         Args:
2025-04-11T04:23:18.7273153Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7273319Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7273426Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7273501Z         """
2025-04-11T04:23:18.7273579Z         _lazy_init()
2025-04-11T04:23:18.7273677Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7273779Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7273885Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7274176Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7274311Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7274472Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7274569Z 
2025-04-11T04:23:18.7274813Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7274984Z _____________ test_flash_decoding[False-False-1-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.7274989Z 
2025-04-11T04:23:18.7275138Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7275306Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7275394Z use_new_kcache_layout = False
2025-04-11T04:23:18.7275398Z 
2025-04-11T04:23:18.7275600Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7275709Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7275826Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7275967Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7276086Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7276199Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7276333Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7276439Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7276571Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7276720Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7276813Z     def test_flash_decoding(
2025-04-11T04:23:18.7276887Z         bsz: int,
2025-04-11T04:23:18.7276971Z         block_size: int,
2025-04-11T04:23:18.7277060Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7277142Z         num_attn_heads: int,
2025-04-11T04:23:18.7277232Z         kv_group_num: int,
2025-04-11T04:23:18.7277317Z         same_context_len: bool,
2025-04-11T04:23:18.7277394Z         q_len: int,
2025-04-11T04:23:18.7277477Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7277567Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7277640Z     ):
2025-04-11T04:23:18.7277748Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7277942Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7278121Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7278292Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7278452Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7278607Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7278765Z     
2025-04-11T04:23:18.7278855Z         torch.manual_seed(123)
2025-04-11T04:23:18.7278945Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7279035Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7279042Z 
2025-04-11T04:23:18.7279202Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7279311Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7279315Z 
2025-04-11T04:23:18.7279390Z device = None
2025-04-11T04:23:18.7279398Z 
2025-04-11T04:23:18.7279517Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7279664Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7279738Z     
2025-04-11T04:23:18.7279810Z         Args:
2025-04-11T04:23:18.7279978Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7280144Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7280250Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7280325Z         """
2025-04-11T04:23:18.7280402Z         _lazy_init()
2025-04-11T04:23:18.7280590Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7280691Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7280798Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7281081Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7281215Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7281373Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7281377Z 
2025-04-11T04:23:18.7281618Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7281793Z ____________ test_flash_decoding[False-False-1-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.7281797Z 
2025-04-11T04:23:18.7281948Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7282115Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7282206Z use_new_kcache_layout = False
2025-04-11T04:23:18.7282210Z 
2025-04-11T04:23:18.7282415Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7282520Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7282637Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7282777Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7282890Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7283005Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7283138Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7283247Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7283381Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7283531Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7283625Z     def test_flash_decoding(
2025-04-11T04:23:18.7283698Z         bsz: int,
2025-04-11T04:23:18.7283782Z         block_size: int,
2025-04-11T04:23:18.7283870Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7283950Z         num_attn_heads: int,
2025-04-11T04:23:18.7284036Z         kv_group_num: int,
2025-04-11T04:23:18.7284120Z         same_context_len: bool,
2025-04-11T04:23:18.7284197Z         q_len: int,
2025-04-11T04:23:18.7284281Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7284367Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7284441Z     ):
2025-04-11T04:23:18.7284551Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7284832Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7285014Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7285185Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7285348Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7285505Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7285576Z     
2025-04-11T04:23:18.7285662Z         torch.manual_seed(123)
2025-04-11T04:23:18.7285751Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7285840Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7285844Z 
2025-04-11T04:23:18.7285999Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7286108Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7286112Z 
2025-04-11T04:23:18.7286189Z device = None
2025-04-11T04:23:18.7286196Z 
2025-04-11T04:23:18.7286311Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7286457Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7286613Z     
2025-04-11T04:23:18.7286685Z         Args:
2025-04-11T04:23:18.7286855Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7287019Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7287128Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7287199Z         """
2025-04-11T04:23:18.7287275Z         _lazy_init()
2025-04-11T04:23:18.7287375Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7287476Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7287583Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7287868Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7288005Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7288165Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7288172Z 
2025-04-11T04:23:18.7288411Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7288583Z ____________ test_flash_decoding[False-False-1-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.7288587Z 
2025-04-11T04:23:18.7288740Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7288907Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7288996Z use_new_kcache_layout = False
2025-04-11T04:23:18.7289000Z 
2025-04-11T04:23:18.7289203Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7289311Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7289427Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7289566Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7289684Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7289801Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7289935Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7290041Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7290174Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7290322Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7290412Z     def test_flash_decoding(
2025-04-11T04:23:18.7290485Z         bsz: int,
2025-04-11T04:23:18.7290568Z         block_size: int,
2025-04-11T04:23:18.7290657Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7290738Z         num_attn_heads: int,
2025-04-11T04:23:18.7290910Z         kv_group_num: int,
2025-04-11T04:23:18.7290999Z         same_context_len: bool,
2025-04-11T04:23:18.7291076Z         q_len: int,
2025-04-11T04:23:18.7291161Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7291255Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7291323Z     ):
2025-04-11T04:23:18.7291433Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7291628Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7291810Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7291983Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7292145Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7292303Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7292378Z     
2025-04-11T04:23:18.7292464Z         torch.manual_seed(123)
2025-04-11T04:23:18.7292554Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7292645Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7292731Z 
2025-04-11T04:23:18.7292892Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7293003Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7293006Z 
2025-04-11T04:23:18.7293087Z device = None
2025-04-11T04:23:18.7293090Z 
2025-04-11T04:23:18.7293206Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7293355Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7293430Z     
2025-04-11T04:23:18.7293500Z         Args:
2025-04-11T04:23:18.7293668Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7293836Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7293946Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7294017Z         """
2025-04-11T04:23:18.7294093Z         _lazy_init()
2025-04-11T04:23:18.7294194Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7294294Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7294400Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7294681Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7294815Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7294974Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7294978Z 
2025-04-11T04:23:18.7295211Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7295385Z ____________ test_flash_decoding[False-False-1-False-1-16-16-16-16] ____________
2025-04-11T04:23:18.7295389Z 
2025-04-11T04:23:18.7295538Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7295704Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7295794Z use_new_kcache_layout = False
2025-04-11T04:23:18.7295798Z 
2025-04-11T04:23:18.7296000Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7296102Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7296219Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7296359Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7296473Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7296587Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7296721Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7296925Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7297064Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7297213Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7297305Z     def test_flash_decoding(
2025-04-11T04:23:18.7297378Z         bsz: int,
2025-04-11T04:23:18.7297461Z         block_size: int,
2025-04-11T04:23:18.7297549Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7297633Z         num_attn_heads: int,
2025-04-11T04:23:18.7297713Z         kv_group_num: int,
2025-04-11T04:23:18.7297796Z         same_context_len: bool,
2025-04-11T04:23:18.7297874Z         q_len: int,
2025-04-11T04:23:18.7297956Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7298046Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7298115Z     ):
2025-04-11T04:23:18.7298227Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7298424Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7298603Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7298773Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7299025Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7299184Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7299257Z     
2025-04-11T04:23:18.7299343Z         torch.manual_seed(123)
2025-04-11T04:23:18.7299433Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7299524Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7299528Z 
2025-04-11T04:23:18.7299687Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7299800Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7299803Z 
2025-04-11T04:23:18.7299881Z device = None
2025-04-11T04:23:18.7299890Z 
2025-04-11T04:23:18.7300009Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7300156Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7300230Z     
2025-04-11T04:23:18.7300301Z         Args:
2025-04-11T04:23:18.7300469Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7300632Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7300739Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7300811Z         """
2025-04-11T04:23:18.7300888Z         _lazy_init()
2025-04-11T04:23:18.7300984Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7301083Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7301189Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7301474Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7301610Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7301766Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7301772Z 
2025-04-11T04:23:18.7302008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7302179Z ____________ test_flash_decoding[False-False-1-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.7302183Z 
2025-04-11T04:23:18.7302331Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7302497Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7302584Z use_new_kcache_layout = False
2025-04-11T04:23:18.7302588Z 
2025-04-11T04:23:18.7302789Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7302979Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7303100Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7303238Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7303356Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7303471Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7303606Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7303711Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7303846Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7303995Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7304084Z     def test_flash_decoding(
2025-04-11T04:23:18.7304157Z         bsz: int,
2025-04-11T04:23:18.7304241Z         block_size: int,
2025-04-11T04:23:18.7304327Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7304412Z         num_attn_heads: int,
2025-04-11T04:23:18.7304497Z         kv_group_num: int,
2025-04-11T04:23:18.7304580Z         same_context_len: bool,
2025-04-11T04:23:18.7304658Z         q_len: int,
2025-04-11T04:23:18.7304741Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7304914Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7304984Z     ):
2025-04-11T04:23:18.7305092Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7305284Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7305462Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7305633Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7305794Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7305950Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7306022Z     
2025-04-11T04:23:18.7306107Z         torch.manual_seed(123)
2025-04-11T04:23:18.7306196Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7306285Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7306292Z 
2025-04-11T04:23:18.7306449Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7306558Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7306562Z 
2025-04-11T04:23:18.7306639Z device = None
2025-04-11T04:23:18.7306642Z 
2025-04-11T04:23:18.7306758Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7306911Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7306981Z     
2025-04-11T04:23:18.7307053Z         Args:
2025-04-11T04:23:18.7307221Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7307384Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7307494Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7307564Z         """
2025-04-11T04:23:18.7307640Z         _lazy_init()
2025-04-11T04:23:18.7307736Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7307839Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7307945Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7308224Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7308361Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7308554Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7308559Z 
2025-04-11T04:23:18.7308795Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7309064Z ____________ test_flash_decoding[False-False-1-False-1-16-16-32-16] ____________
2025-04-11T04:23:18.7309068Z 
2025-04-11T04:23:18.7309221Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7309389Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7309480Z use_new_kcache_layout = False
2025-04-11T04:23:18.7309484Z 
2025-04-11T04:23:18.7309684Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7309787Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7309904Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7310043Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7310157Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7310271Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7310407Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7310515Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7310652Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7310804Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7310985Z     def test_flash_decoding(
2025-04-11T04:23:18.7311059Z         bsz: int,
2025-04-11T04:23:18.7311144Z         block_size: int,
2025-04-11T04:23:18.7311234Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7311320Z         num_attn_heads: int,
2025-04-11T04:23:18.7311401Z         kv_group_num: int,
2025-04-11T04:23:18.7311484Z         same_context_len: bool,
2025-04-11T04:23:18.7311562Z         q_len: int,
2025-04-11T04:23:18.7311644Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7311734Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7311802Z     ):
2025-04-11T04:23:18.7311912Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7312107Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7312284Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7312456Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7312620Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7312777Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7312846Z     
2025-04-11T04:23:18.7312931Z         torch.manual_seed(123)
2025-04-11T04:23:18.7313019Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7313107Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7313111Z 
2025-04-11T04:23:18.7313267Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7313375Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7313378Z 
2025-04-11T04:23:18.7313455Z device = None
2025-04-11T04:23:18.7313461Z 
2025-04-11T04:23:18.7313575Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7313726Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7313798Z     
2025-04-11T04:23:18.7313868Z         Args:
2025-04-11T04:23:18.7314039Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7314202Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7314310Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7314381Z         """
2025-04-11T04:23:18.7314457Z         _lazy_init()
2025-04-11T04:23:18.7314553Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7314653Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7314759Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7315125Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7315266Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7315420Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7315427Z 
2025-04-11T04:23:18.7315667Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7315832Z _____________ test_flash_decoding[False-False-1-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.7315835Z 
2025-04-11T04:23:18.7315985Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7316150Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7316238Z use_new_kcache_layout = False
2025-04-11T04:23:18.7316242Z 
2025-04-11T04:23:18.7316443Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7316549Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7316666Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7316802Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7317023Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7317139Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7317274Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7317379Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7317514Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7317668Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7317752Z     def test_flash_decoding(
2025-04-11T04:23:18.7317825Z         bsz: int,
2025-04-11T04:23:18.7317911Z         block_size: int,
2025-04-11T04:23:18.7317999Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7318083Z         num_attn_heads: int,
2025-04-11T04:23:18.7318166Z         kv_group_num: int,
2025-04-11T04:23:18.7318254Z         same_context_len: bool,
2025-04-11T04:23:18.7318332Z         q_len: int,
2025-04-11T04:23:18.7318415Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7318508Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7318578Z     ):
2025-04-11T04:23:18.7318684Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7318877Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7319056Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7319229Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7319389Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7319550Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7319619Z     
2025-04-11T04:23:18.7319709Z         torch.manual_seed(123)
2025-04-11T04:23:18.7319795Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7319884Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7319891Z 
2025-04-11T04:23:18.7320050Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7320157Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7320161Z 
2025-04-11T04:23:18.7320240Z device = None
2025-04-11T04:23:18.7320243Z 
2025-04-11T04:23:18.7320357Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7320507Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7320574Z     
2025-04-11T04:23:18.7320644Z         Args:
2025-04-11T04:23:18.7320811Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7320973Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7321179Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7321252Z         """
2025-04-11T04:23:18.7321329Z         _lazy_init()
2025-04-11T04:23:18.7321426Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7321530Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7321636Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7321921Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7322061Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7322217Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7322222Z 
2025-04-11T04:23:18.7322462Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7322633Z ____________ test_flash_decoding[False-False-1-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.7322637Z 
2025-04-11T04:23:18.7322789Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7322957Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7323127Z use_new_kcache_layout = False
2025-04-11T04:23:18.7323131Z 
2025-04-11T04:23:18.7323334Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7323437Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7323555Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7323694Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7323809Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7323925Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7324060Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7324168Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7324300Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7324452Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7324540Z     def test_flash_decoding(
2025-04-11T04:23:18.7324613Z         bsz: int,
2025-04-11T04:23:18.7324697Z         block_size: int,
2025-04-11T04:23:18.7324783Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7324866Z         num_attn_heads: int,
2025-04-11T04:23:18.7324948Z         kv_group_num: int,
2025-04-11T04:23:18.7325032Z         same_context_len: bool,
2025-04-11T04:23:18.7325110Z         q_len: int,
2025-04-11T04:23:18.7325192Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7325281Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7325349Z     ):
2025-04-11T04:23:18.7325459Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7325655Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7325835Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7326010Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7326178Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7326336Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7326406Z     
2025-04-11T04:23:18.7326494Z         torch.manual_seed(123)
2025-04-11T04:23:18.7326583Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7326674Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7326678Z 
2025-04-11T04:23:18.7326836Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7326946Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7326949Z 
2025-04-11T04:23:18.7327028Z device = None
2025-04-11T04:23:18.7327116Z 
2025-04-11T04:23:18.7327237Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7327392Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7327467Z     
2025-04-11T04:23:18.7327544Z         Args:
2025-04-11T04:23:18.7327714Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7327879Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7327990Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7328065Z         """
2025-04-11T04:23:18.7328147Z         _lazy_init()
2025-04-11T04:23:18.7328244Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7328348Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7328457Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7328744Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7328886Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7329043Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7329128Z 
2025-04-11T04:23:18.7329377Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7329544Z _____________ test_flash_decoding[False-False-1-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.7329548Z 
2025-04-11T04:23:18.7329698Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7329866Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7329954Z use_new_kcache_layout = False
2025-04-11T04:23:18.7329957Z 
2025-04-11T04:23:18.7330162Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7330269Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7330388Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7330525Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7330648Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7330759Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7330895Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7331000Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7331133Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7331284Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7331367Z     def test_flash_decoding(
2025-04-11T04:23:18.7331440Z         bsz: int,
2025-04-11T04:23:18.7331522Z         block_size: int,
2025-04-11T04:23:18.7331610Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7331692Z         num_attn_heads: int,
2025-04-11T04:23:18.7331776Z         kv_group_num: int,
2025-04-11T04:23:18.7331863Z         same_context_len: bool,
2025-04-11T04:23:18.7331937Z         q_len: int,
2025-04-11T04:23:18.7332022Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7332118Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7332187Z     ):
2025-04-11T04:23:18.7332298Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7332490Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7332668Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7332842Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7333005Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7333163Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7333232Z     
2025-04-11T04:23:18.7333402Z         torch.manual_seed(123)
2025-04-11T04:23:18.7333491Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7333580Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7333589Z 
2025-04-11T04:23:18.7333749Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7333859Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7333863Z 
2025-04-11T04:23:18.7333941Z device = None
2025-04-11T04:23:18.7333944Z 
2025-04-11T04:23:18.7334060Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7334210Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7334279Z     
2025-04-11T04:23:18.7334351Z         Args:
2025-04-11T04:23:18.7334517Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7334682Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7334795Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7334865Z         """
2025-04-11T04:23:18.7334942Z         _lazy_init()
2025-04-11T04:23:18.7335035Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7335224Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7335336Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7335620Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7335764Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7335924Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7335928Z 
2025-04-11T04:23:18.7336172Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7336343Z ____________ test_flash_decoding[False-False-1-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.7336350Z 
2025-04-11T04:23:18.7336509Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7336676Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7336771Z use_new_kcache_layout = False
2025-04-11T04:23:18.7336774Z 
2025-04-11T04:23:18.7336983Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7337090Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7337212Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7337355Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7337477Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7337592Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7337731Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7337848Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7337988Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7338146Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7338237Z     def test_flash_decoding(
2025-04-11T04:23:18.7338318Z         bsz: int,
2025-04-11T04:23:18.7338403Z         block_size: int,
2025-04-11T04:23:18.7338496Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7338585Z         num_attn_heads: int,
2025-04-11T04:23:18.7338672Z         kv_group_num: int,
2025-04-11T04:23:18.7338764Z         same_context_len: bool,
2025-04-11T04:23:18.7338841Z         q_len: int,
2025-04-11T04:23:18.7338929Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7339024Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7339096Z     ):
2025-04-11T04:23:18.7339213Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7339409Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7339676Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7339852Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7340016Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7340176Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7340244Z     
2025-04-11T04:23:18.7340333Z         torch.manual_seed(123)
2025-04-11T04:23:18.7340421Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7340509Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7340513Z 
2025-04-11T04:23:18.7340672Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7340784Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7340788Z 
2025-04-11T04:23:18.7340866Z device = None
2025-04-11T04:23:18.7340869Z 
2025-04-11T04:23:18.7340990Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7341141Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7341310Z     
2025-04-11T04:23:18.7341386Z         Args:
2025-04-11T04:23:18.7341553Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7341719Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7341829Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7341901Z         """
2025-04-11T04:23:18.7341983Z         _lazy_init()
2025-04-11T04:23:18.7342075Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7342177Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7342288Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7342576Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7342715Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7342872Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7342879Z 
2025-04-11T04:23:18.7343121Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7343287Z ____________ test_flash_decoding[False-False-1-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.7343291Z 
2025-04-11T04:23:18.7343447Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7343611Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7343699Z use_new_kcache_layout = False
2025-04-11T04:23:18.7343703Z 
2025-04-11T04:23:18.7343906Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7344013Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7344133Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7344272Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7344394Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7344508Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7344642Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7344747Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7344880Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7345033Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7345117Z     def test_flash_decoding(
2025-04-11T04:23:18.7345192Z         bsz: int,
2025-04-11T04:23:18.7345273Z         block_size: int,
2025-04-11T04:23:18.7345360Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7345444Z         num_attn_heads: int,
2025-04-11T04:23:18.7345617Z         kv_group_num: int,
2025-04-11T04:23:18.7345707Z         same_context_len: bool,
2025-04-11T04:23:18.7345782Z         q_len: int,
2025-04-11T04:23:18.7345864Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7345959Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7346028Z     ):
2025-04-11T04:23:18.7346139Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7346330Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7346510Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7346678Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7346840Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7346997Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7347067Z     
2025-04-11T04:23:18.7347156Z         torch.manual_seed(123)
2025-04-11T04:23:18.7347243Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7347330Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7347337Z 
2025-04-11T04:23:18.7347666Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7347776Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7347780Z 
2025-04-11T04:23:18.7347859Z device = None
2025-04-11T04:23:18.7347862Z 
2025-04-11T04:23:18.7347979Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7348130Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7348198Z     
2025-04-11T04:23:18.7348272Z         Args:
2025-04-11T04:23:18.7348477Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7348644Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7348757Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7348830Z         """
2025-04-11T04:23:18.7348910Z         _lazy_init()
2025-04-11T04:23:18.7349002Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7349107Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7349214Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7349495Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7349632Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7349789Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7349793Z 
2025-04-11T04:23:18.7350035Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7350205Z ____________ test_flash_decoding[False-False-1-False-4-16-16-16-16] ____________
2025-04-11T04:23:18.7350212Z 
2025-04-11T04:23:18.7350367Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7350532Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7350621Z use_new_kcache_layout = False
2025-04-11T04:23:18.7350629Z 
2025-04-11T04:23:18.7350828Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7350931Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7351048Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7351184Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7351301Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7351413Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7351546Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7351743Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7351883Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7352036Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7352123Z     def test_flash_decoding(
2025-04-11T04:23:18.7352199Z         bsz: int,
2025-04-11T04:23:18.7352279Z         block_size: int,
2025-04-11T04:23:18.7352366Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7352452Z         num_attn_heads: int,
2025-04-11T04:23:18.7352534Z         kv_group_num: int,
2025-04-11T04:23:18.7352619Z         same_context_len: bool,
2025-04-11T04:23:18.7352693Z         q_len: int,
2025-04-11T04:23:18.7352776Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7352867Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7352936Z     ):
2025-04-11T04:23:18.7353050Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7353242Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7353428Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7353599Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7353851Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7354012Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7354082Z     
2025-04-11T04:23:18.7354170Z         torch.manual_seed(123)
2025-04-11T04:23:18.7354256Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7354348Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7354351Z 
2025-04-11T04:23:18.7354503Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7354612Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7354616Z 
2025-04-11T04:23:18.7354692Z device = None
2025-04-11T04:23:18.7354696Z 
2025-04-11T04:23:18.7354814Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7354968Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7355039Z     
2025-04-11T04:23:18.7355113Z         Args:
2025-04-11T04:23:18.7355274Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7355436Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7355541Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7355612Z         """
2025-04-11T04:23:18.7355691Z         _lazy_init()
2025-04-11T04:23:18.7355781Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7355883Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7355985Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7356270Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7356408Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7356564Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7356571Z 
2025-04-11T04:23:18.7356809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7356974Z ____________ test_flash_decoding[False-False-1-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.7356978Z 
2025-04-11T04:23:18.7357127Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7357288Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7357379Z use_new_kcache_layout = False
2025-04-11T04:23:18.7357383Z 
2025-04-11T04:23:18.7357579Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7357765Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7357889Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7358027Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7358148Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7358262Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7358399Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7358502Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7358635Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7358789Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7358874Z     def test_flash_decoding(
2025-04-11T04:23:18.7358951Z         bsz: int,
2025-04-11T04:23:18.7359031Z         block_size: int,
2025-04-11T04:23:18.7359119Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7359203Z         num_attn_heads: int,
2025-04-11T04:23:18.7359288Z         kv_group_num: int,
2025-04-11T04:23:18.7359374Z         same_context_len: bool,
2025-04-11T04:23:18.7359448Z         q_len: int,
2025-04-11T04:23:18.7359532Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7359713Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7359784Z     ):
2025-04-11T04:23:18.7359900Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7360095Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7360279Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7360448Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7360612Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7360770Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7360839Z     
2025-04-11T04:23:18.7360930Z         torch.manual_seed(123)
2025-04-11T04:23:18.7361017Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7361110Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7361113Z 
2025-04-11T04:23:18.7361272Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7361383Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7361391Z 
2025-04-11T04:23:18.7361466Z device = None
2025-04-11T04:23:18.7361470Z 
2025-04-11T04:23:18.7361585Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7361742Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7361810Z     
2025-04-11T04:23:18.7361883Z         Args:
2025-04-11T04:23:18.7362050Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7362215Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7362325Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7362397Z         """
2025-04-11T04:23:18.7362475Z         _lazy_init()
2025-04-11T04:23:18.7362566Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7362671Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7362776Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7363057Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7363194Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7363349Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7363353Z 
2025-04-11T04:23:18.7363601Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7363770Z ____________ test_flash_decoding[False-False-1-False-4-16-16-32-16] ____________
2025-04-11T04:23:18.7363875Z 
2025-04-11T04:23:18.7364030Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7364193Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T04:23:18.7364287Z use_new_kcache_layout = False
2025-04-11T04:23:18.7364291Z 
2025-04-11T04:23:18.7364491Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7364593Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7364714Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7364849Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7364964Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7365075Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7365212Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7365317Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7365451Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7365600Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7365770Z     def test_flash_decoding(
2025-04-11T04:23:18.7365848Z         bsz: int,
2025-04-11T04:23:18.7365928Z         block_size: int,
2025-04-11T04:23:18.7366015Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7366098Z         num_attn_heads: int,
2025-04-11T04:23:18.7366179Z         kv_group_num: int,
2025-04-11T04:23:18.7366264Z         same_context_len: bool,
2025-04-11T04:23:18.7366338Z         q_len: int,
2025-04-11T04:23:18.7366425Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7366512Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7366582Z     ):
2025-04-11T04:23:18.7366696Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7366892Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7367080Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7367252Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7367422Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7367578Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7367647Z     
2025-04-11T04:23:18.7367737Z         torch.manual_seed(123)
2025-04-11T04:23:18.7367823Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7367912Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7367916Z 
2025-04-11T04:23:18.7368070Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7368179Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7368186Z 
2025-04-11T04:23:18.7368261Z device = None
2025-04-11T04:23:18.7368264Z 
2025-04-11T04:23:18.7368383Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7368533Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7368606Z     
2025-04-11T04:23:18.7368681Z         Args:
2025-04-11T04:23:18.7368843Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7369011Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7369114Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7369185Z         """
2025-04-11T04:23:18.7369264Z         _lazy_init()
2025-04-11T04:23:18.7369357Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7369461Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7369563Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7369931Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7370073Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7370228Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7370234Z 
2025-04-11T04:23:18.7370477Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7370645Z _____________ test_flash_decoding[False-False-5-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.7370649Z 
2025-04-11T04:23:18.7370801Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7370965Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7371056Z use_new_kcache_layout = False
2025-04-11T04:23:18.7371060Z 
2025-04-11T04:23:18.7371260Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7371366Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7371484Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7371620Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7371822Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7371933Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7372072Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7372175Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7372308Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7372462Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7372547Z     def test_flash_decoding(
2025-04-11T04:23:18.7372624Z         bsz: int,
2025-04-11T04:23:18.7372705Z         block_size: int,
2025-04-11T04:23:18.7372795Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7372877Z         num_attn_heads: int,
2025-04-11T04:23:18.7372963Z         kv_group_num: int,
2025-04-11T04:23:18.7373052Z         same_context_len: bool,
2025-04-11T04:23:18.7373126Z         q_len: int,
2025-04-11T04:23:18.7373213Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7373304Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7373374Z     ):
2025-04-11T04:23:18.7373488Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7373679Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7373863Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7374031Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7374197Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7374352Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7374422Z     
2025-04-11T04:23:18.7374516Z         torch.manual_seed(123)
2025-04-11T04:23:18.7374602Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7374694Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7374698Z 
2025-04-11T04:23:18.7374858Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7374970Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7374974Z 
2025-04-11T04:23:18.7375048Z device = None
2025-04-11T04:23:18.7375052Z 
2025-04-11T04:23:18.7375167Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7375317Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7375386Z     
2025-04-11T04:23:18.7375461Z         Args:
2025-04-11T04:23:18.7375625Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7375789Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7375984Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7376059Z         """
2025-04-11T04:23:18.7376142Z         _lazy_init()
2025-04-11T04:23:18.7376236Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7376343Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7376447Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7376729Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7376870Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7377027Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7377031Z 
2025-04-11T04:23:18.7377279Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7377445Z _____________ test_flash_decoding[False-False-5-True-1-16-8-16-16] _____________
2025-04-11T04:23:18.7377452Z 
2025-04-11T04:23:18.7377604Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7377767Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7377948Z use_new_kcache_layout = False
2025-04-11T04:23:18.7377952Z 
2025-04-11T04:23:18.7378154Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7378259Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7378373Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7378511Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7378628Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7378739Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7378875Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7378976Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7379113Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7379265Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7379354Z     def test_flash_decoding(
2025-04-11T04:23:18.7379432Z         bsz: int,
2025-04-11T04:23:18.7379512Z         block_size: int,
2025-04-11T04:23:18.7379603Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7379682Z         num_attn_heads: int,
2025-04-11T04:23:18.7379762Z         kv_group_num: int,
2025-04-11T04:23:18.7379847Z         same_context_len: bool,
2025-04-11T04:23:18.7379921Z         q_len: int,
2025-04-11T04:23:18.7380007Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7380092Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7380161Z     ):
2025-04-11T04:23:18.7380274Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7380465Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7380651Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7380819Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7380985Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7381139Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7381208Z     
2025-04-11T04:23:18.7381295Z         torch.manual_seed(123)
2025-04-11T04:23:18.7381382Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7381472Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7381476Z 
2025-04-11T04:23:18.7381630Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7381743Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7381747Z 
2025-04-11T04:23:18.7381821Z device = None
2025-04-11T04:23:18.7381825Z 
2025-04-11T04:23:18.7382040Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7382197Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7382270Z     
2025-04-11T04:23:18.7382346Z         Args:
2025-04-11T04:23:18.7382512Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7382682Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7382787Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7382859Z         """
2025-04-11T04:23:18.7382940Z         _lazy_init()
2025-04-11T04:23:18.7383035Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7383138Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7383242Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7383529Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7383663Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7383818Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7383904Z 
2025-04-11T04:23:18.7384147Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7384312Z _____________ test_flash_decoding[False-False-5-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.7384316Z 
2025-04-11T04:23:18.7384469Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7384630Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7384721Z use_new_kcache_layout = False
2025-04-11T04:23:18.7384725Z 
2025-04-11T04:23:18.7384923Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7385026Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7385144Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7385283Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7385403Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7385519Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7385657Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7385759Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7385895Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7386044Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7386128Z     def test_flash_decoding(
2025-04-11T04:23:18.7386207Z         bsz: int,
2025-04-11T04:23:18.7386287Z         block_size: int,
2025-04-11T04:23:18.7386380Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7386463Z         num_attn_heads: int,
2025-04-11T04:23:18.7386548Z         kv_group_num: int,
2025-04-11T04:23:18.7386639Z         same_context_len: bool,
2025-04-11T04:23:18.7386713Z         q_len: int,
2025-04-11T04:23:18.7386802Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7386892Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7386961Z     ):
2025-04-11T04:23:18.7387074Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7387265Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7387448Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7387615Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7387781Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7387936Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7388005Z     
2025-04-11T04:23:18.7388196Z         torch.manual_seed(123)
2025-04-11T04:23:18.7388287Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7388378Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7388382Z 
2025-04-11T04:23:18.7388597Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7388710Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7388714Z 
2025-04-11T04:23:18.7388787Z device = None
2025-04-11T04:23:18.7388791Z 
2025-04-11T04:23:18.7388911Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7389061Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7389129Z     
2025-04-11T04:23:18.7389204Z         Args:
2025-04-11T04:23:18.7389368Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7389535Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7389642Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7389713Z         """
2025-04-11T04:23:18.7389791Z         _lazy_init()
2025-04-11T04:23:18.7389884Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7390081Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7390183Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7390467Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7390603Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7390760Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7390763Z 
2025-04-11T04:23:18.7391012Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7391178Z _____________ test_flash_decoding[False-False-5-True-1-16-8-32-16] _____________
2025-04-11T04:23:18.7391186Z 
2025-04-11T04:23:18.7391340Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7391505Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7391598Z use_new_kcache_layout = False
2025-04-11T04:23:18.7391601Z 
2025-04-11T04:23:18.7391803Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7391909Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7392024Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7392160Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7392277Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7392389Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7392528Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7392629Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7392771Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7392921Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7393010Z     def test_flash_decoding(
2025-04-11T04:23:18.7393088Z         bsz: int,
2025-04-11T04:23:18.7393169Z         block_size: int,
2025-04-11T04:23:18.7393260Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7393341Z         num_attn_heads: int,
2025-04-11T04:23:18.7393422Z         kv_group_num: int,
2025-04-11T04:23:18.7393510Z         same_context_len: bool,
2025-04-11T04:23:18.7393583Z         q_len: int,
2025-04-11T04:23:18.7393669Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7393755Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7393823Z     ):
2025-04-11T04:23:18.7393935Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7394128Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7394406Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7394577Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7394748Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7394904Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7394976Z     
2025-04-11T04:23:18.7395061Z         torch.manual_seed(123)
2025-04-11T04:23:18.7395148Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7395240Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7395244Z 
2025-04-11T04:23:18.7395397Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7395511Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7395515Z 
2025-04-11T04:23:18.7395589Z device = None
2025-04-11T04:23:18.7395593Z 
2025-04-11T04:23:18.7395715Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7395863Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7395931Z     
2025-04-11T04:23:18.7396091Z         Args:
2025-04-11T04:23:18.7396256Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7396424Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7396528Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7396599Z         """
2025-04-11T04:23:18.7396679Z         _lazy_init()
2025-04-11T04:23:18.7396773Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7396877Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7396980Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7397270Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7397404Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7397559Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7397570Z 
2025-04-11T04:23:18.7397808Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7397973Z _____________ test_flash_decoding[False-False-5-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.7397977Z 
2025-04-11T04:23:18.7398129Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7398290Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7398383Z use_new_kcache_layout = False
2025-04-11T04:23:18.7398387Z 
2025-04-11T04:23:18.7398587Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7398691Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7398808Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7398944Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7399061Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7399175Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7399312Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7399414Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7399550Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7399699Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7399786Z     def test_flash_decoding(
2025-04-11T04:23:18.7399862Z         bsz: int,
2025-04-11T04:23:18.7399943Z         block_size: int,
2025-04-11T04:23:18.7400033Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7400114Z         num_attn_heads: int,
2025-04-11T04:23:18.7400288Z         kv_group_num: int,
2025-04-11T04:23:18.7400380Z         same_context_len: bool,
2025-04-11T04:23:18.7400455Z         q_len: int,
2025-04-11T04:23:18.7400543Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7400629Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7400704Z     ):
2025-04-11T04:23:18.7400815Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7401006Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7401189Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7401357Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7401522Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7401675Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7401747Z     
2025-04-11T04:23:18.7401836Z         torch.manual_seed(123)
2025-04-11T04:23:18.7401924Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7402018Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7402021Z 
2025-04-11T04:23:18.7402264Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7402377Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7402381Z 
2025-04-11T04:23:18.7402455Z device = None
2025-04-11T04:23:18.7402459Z 
2025-04-11T04:23:18.7402579Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7402730Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7402798Z     
2025-04-11T04:23:18.7402874Z         Args:
2025-04-11T04:23:18.7403041Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7403207Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7403315Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7403388Z         """
2025-04-11T04:23:18.7403465Z         _lazy_init()
2025-04-11T04:23:18.7403558Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7403665Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7403768Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7404054Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7404190Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7404351Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7404355Z 
2025-04-11T04:23:18.7404594Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7404761Z ____________ test_flash_decoding[False-False-5-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.7404768Z 
2025-04-11T04:23:18.7404924Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7405087Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7405183Z use_new_kcache_layout = False
2025-04-11T04:23:18.7405187Z 
2025-04-11T04:23:18.7405388Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7405496Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7405613Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7405751Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7405868Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7405979Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7406117Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7406220Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7406441Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7406593Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7406678Z     def test_flash_decoding(
2025-04-11T04:23:18.7406760Z         bsz: int,
2025-04-11T04:23:18.7406840Z         block_size: int,
2025-04-11T04:23:18.7406931Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7407012Z         num_attn_heads: int,
2025-04-11T04:23:18.7407098Z         kv_group_num: int,
2025-04-11T04:23:18.7407181Z         same_context_len: bool,
2025-04-11T04:23:18.7407254Z         q_len: int,
2025-04-11T04:23:18.7407342Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7407428Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7407500Z     ):
2025-04-11T04:23:18.7407608Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7407801Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7407988Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7408157Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7408421Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7408576Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7408648Z     
2025-04-11T04:23:18.7408734Z         torch.manual_seed(123)
2025-04-11T04:23:18.7408826Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7408917Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7408921Z 
2025-04-11T04:23:18.7409075Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7409190Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7409194Z 
2025-04-11T04:23:18.7409270Z device = None
2025-04-11T04:23:18.7409274Z 
2025-04-11T04:23:18.7409400Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7409552Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7409620Z     
2025-04-11T04:23:18.7409700Z         Args:
2025-04-11T04:23:18.7409863Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7410029Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7410134Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7410206Z         """
2025-04-11T04:23:18.7410282Z         _lazy_init()
2025-04-11T04:23:18.7410374Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7410477Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7410580Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7410869Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7411004Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7411162Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7411169Z 
2025-04-11T04:23:18.7411406Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7411572Z _____________ test_flash_decoding[False-False-5-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.7411579Z 
2025-04-11T04:23:18.7411728Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7411889Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7411980Z use_new_kcache_layout = False
2025-04-11T04:23:18.7411984Z 
2025-04-11T04:23:18.7412182Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7412287Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7412490Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7412634Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7412749Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7412862Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7413000Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7413102Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7413237Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7413386Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7413474Z     def test_flash_decoding(
2025-04-11T04:23:18.7413547Z         bsz: int,
2025-04-11T04:23:18.7413628Z         block_size: int,
2025-04-11T04:23:18.7413720Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7413801Z         num_attn_heads: int,
2025-04-11T04:23:18.7413886Z         kv_group_num: int,
2025-04-11T04:23:18.7413974Z         same_context_len: bool,
2025-04-11T04:23:18.7414049Z         q_len: int,
2025-04-11T04:23:18.7414136Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7414221Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7414380Z     ):
2025-04-11T04:23:18.7414496Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7414689Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7414875Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7415045Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7415212Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7415368Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7415439Z     
2025-04-11T04:23:18.7415528Z         torch.manual_seed(123)
2025-04-11T04:23:18.7415615Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7415709Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7415713Z 
2025-04-11T04:23:18.7415875Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7415990Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7415994Z 
2025-04-11T04:23:18.7416069Z device = None
2025-04-11T04:23:18.7416073Z 
2025-04-11T04:23:18.7416196Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7416346Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7416418Z     
2025-04-11T04:23:18.7416490Z         Args:
2025-04-11T04:23:18.7416659Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7416827Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7416935Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7417009Z         """
2025-04-11T04:23:18.7417085Z         _lazy_init()
2025-04-11T04:23:18.7417179Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7417288Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7417394Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7417680Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7417815Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7417976Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7417980Z 
2025-04-11T04:23:18.7418224Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7418392Z ____________ test_flash_decoding[False-False-5-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.7418400Z 
2025-04-11T04:23:18.7418633Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7418802Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7418895Z use_new_kcache_layout = False
2025-04-11T04:23:18.7418899Z 
2025-04-11T04:23:18.7419098Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7419206Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7419321Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7419460Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7419573Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7419686Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7419824Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7419926Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7420067Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7420216Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7420307Z     def test_flash_decoding(
2025-04-11T04:23:18.7420472Z         bsz: int,
2025-04-11T04:23:18.7420553Z         block_size: int,
2025-04-11T04:23:18.7420651Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7420734Z         num_attn_heads: int,
2025-04-11T04:23:18.7420823Z         kv_group_num: int,
2025-04-11T04:23:18.7420906Z         same_context_len: bool,
2025-04-11T04:23:18.7420981Z         q_len: int,
2025-04-11T04:23:18.7421071Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7421158Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7421231Z     ):
2025-04-11T04:23:18.7421342Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7421531Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7421718Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7421888Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7422057Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7422212Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7422284Z     
2025-04-11T04:23:18.7422369Z         torch.manual_seed(123)
2025-04-11T04:23:18.7422456Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7422547Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7422551Z 
2025-04-11T04:23:18.7422705Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7422817Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7422821Z 
2025-04-11T04:23:18.7422895Z device = None
2025-04-11T04:23:18.7422899Z 
2025-04-11T04:23:18.7423020Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7423170Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7423242Z     
2025-04-11T04:23:18.7423318Z         Args:
2025-04-11T04:23:18.7423484Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7423650Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7423753Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7423826Z         """
2025-04-11T04:23:18.7423901Z         _lazy_init()
2025-04-11T04:23:18.7423993Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7424096Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7424197Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7424568Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7424704Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7424862Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7424869Z 
2025-04-11T04:23:18.7425110Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7425279Z _____________ test_flash_decoding[False-False-5-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.7425283Z 
2025-04-11T04:23:18.7425431Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7425591Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7425683Z use_new_kcache_layout = False
2025-04-11T04:23:18.7425687Z 
2025-04-11T04:23:18.7425886Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7425992Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7426111Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7426252Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7426366Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7426568Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7426707Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7426808Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7426944Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7427090Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7427177Z     def test_flash_decoding(
2025-04-11T04:23:18.7427249Z         bsz: int,
2025-04-11T04:23:18.7427329Z         block_size: int,
2025-04-11T04:23:18.7427422Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7427501Z         num_attn_heads: int,
2025-04-11T04:23:18.7427585Z         kv_group_num: int,
2025-04-11T04:23:18.7427671Z         same_context_len: bool,
2025-04-11T04:23:18.7427744Z         q_len: int,
2025-04-11T04:23:18.7427832Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7427919Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7427995Z     ):
2025-04-11T04:23:18.7428103Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7428301Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7428512Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7428686Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7428853Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7429009Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7429082Z     
2025-04-11T04:23:18.7429171Z         torch.manual_seed(123)
2025-04-11T04:23:18.7429260Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7429347Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7429351Z 
2025-04-11T04:23:18.7429504Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7429623Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7429627Z 
2025-04-11T04:23:18.7429702Z device = None
2025-04-11T04:23:18.7429705Z 
2025-04-11T04:23:18.7429826Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7429976Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7430047Z     
2025-04-11T04:23:18.7430118Z         Args:
2025-04-11T04:23:18.7430282Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7430450Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7430646Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7430722Z         """
2025-04-11T04:23:18.7430799Z         _lazy_init()
2025-04-11T04:23:18.7430894Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7431002Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7431106Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7431391Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7431524Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7431684Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7431688Z 
2025-04-11T04:23:18.7431925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7432092Z _____________ test_flash_decoding[False-False-5-True-4-16-8-16-16] _____________
2025-04-11T04:23:18.7432095Z 
2025-04-11T04:23:18.7432247Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7432408Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7432603Z use_new_kcache_layout = False
2025-04-11T04:23:18.7432607Z 
2025-04-11T04:23:18.7432807Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7432915Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7433028Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7433169Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7434971Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7435085Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7435227Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7435332Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7435477Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7435628Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7435713Z     def test_flash_decoding(
2025-04-11T04:23:18.7435794Z         bsz: int,
2025-04-11T04:23:18.7435876Z         block_size: int,
2025-04-11T04:23:18.7435969Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7436050Z         num_attn_heads: int,
2025-04-11T04:23:18.7436137Z         kv_group_num: int,
2025-04-11T04:23:18.7436222Z         same_context_len: bool,
2025-04-11T04:23:18.7436296Z         q_len: int,
2025-04-11T04:23:18.7436383Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7436500Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7436570Z     ):
2025-04-11T04:23:18.7436685Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7436876Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7437061Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7437228Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7437393Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7437549Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7437617Z     
2025-04-11T04:23:18.7437707Z         torch.manual_seed(123)
2025-04-11T04:23:18.7437794Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7437888Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7437892Z 
2025-04-11T04:23:18.7438048Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7438161Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7438164Z 
2025-04-11T04:23:18.7438240Z device = None
2025-04-11T04:23:18.7438244Z 
2025-04-11T04:23:18.7438446Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7438604Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7438672Z     
2025-04-11T04:23:18.7438749Z         Args:
2025-04-11T04:23:18.7438914Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7439082Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7439187Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7439258Z         """
2025-04-11T04:23:18.7439336Z         _lazy_init()
2025-04-11T04:23:18.7439429Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7439533Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7439637Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7439931Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7440065Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7440223Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7440294Z 
2025-04-11T04:23:18.7440536Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7440702Z _____________ test_flash_decoding[False-False-5-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.7440706Z 
2025-04-11T04:23:18.7440858Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7441020Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7441194Z use_new_kcache_layout = False
2025-04-11T04:23:18.7441198Z 
2025-04-11T04:23:18.7441398Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7441503Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7441621Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7441759Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7441877Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7441992Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7442132Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7442234Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7442371Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7442520Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7442606Z     def test_flash_decoding(
2025-04-11T04:23:18.7442684Z         bsz: int,
2025-04-11T04:23:18.7442763Z         block_size: int,
2025-04-11T04:23:18.7442853Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7442935Z         num_attn_heads: int,
2025-04-11T04:23:18.7443016Z         kv_group_num: int,
2025-04-11T04:23:18.7443107Z         same_context_len: bool,
2025-04-11T04:23:18.7443180Z         q_len: int,
2025-04-11T04:23:18.7443265Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7443351Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7443423Z     ):
2025-04-11T04:23:18.7443537Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7443729Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7443914Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7444084Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7444250Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7444408Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7444476Z     
2025-04-11T04:23:18.7444740Z         torch.manual_seed(123)
2025-04-11T04:23:18.7444829Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7444924Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7444928Z 
2025-04-11T04:23:18.7445084Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7445204Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7445208Z 
2025-04-11T04:23:18.7445286Z device = None
2025-04-11T04:23:18.7445290Z 
2025-04-11T04:23:18.7445414Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7445564Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7445632Z     
2025-04-11T04:23:18.7445710Z         Args:
2025-04-11T04:23:18.7445876Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7446044Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7446150Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7446221Z         """
2025-04-11T04:23:18.7446301Z         _lazy_init()
2025-04-11T04:23:18.7446394Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7446548Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7446652Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7446939Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7447074Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7447229Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7447295Z 
2025-04-11T04:23:18.7447535Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7447703Z _____________ test_flash_decoding[False-False-5-True-4-16-8-32-16] _____________
2025-04-11T04:23:18.7447707Z 
2025-04-11T04:23:18.7447865Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7448028Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7448122Z use_new_kcache_layout = False
2025-04-11T04:23:18.7448126Z 
2025-04-11T04:23:18.7448325Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7448429Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7448544Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7448681Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7448798Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7448911Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7449049Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7449153Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7449295Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7449445Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7449530Z     def test_flash_decoding(
2025-04-11T04:23:18.7449611Z         bsz: int,
2025-04-11T04:23:18.7449689Z         block_size: int,
2025-04-11T04:23:18.7449781Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7449863Z         num_attn_heads: int,
2025-04-11T04:23:18.7449945Z         kv_group_num: int,
2025-04-11T04:23:18.7450033Z         same_context_len: bool,
2025-04-11T04:23:18.7450107Z         q_len: int,
2025-04-11T04:23:18.7450194Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7450282Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7450355Z     ):
2025-04-11T04:23:18.7450463Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7450653Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7450919Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7451092Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7451259Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7451415Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7451490Z     
2025-04-11T04:23:18.7451575Z         torch.manual_seed(123)
2025-04-11T04:23:18.7451663Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7451758Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7451762Z 
2025-04-11T04:23:18.7451918Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7452032Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7452036Z 
2025-04-11T04:23:18.7452110Z device = None
2025-04-11T04:23:18.7452114Z 
2025-04-11T04:23:18.7452235Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7452382Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7452451Z     
2025-04-11T04:23:18.7452579Z         Args:
2025-04-11T04:23:18.7452745Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7452914Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7453020Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7453097Z         """
2025-04-11T04:23:18.7453172Z         _lazy_init()
2025-04-11T04:23:18.7453265Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7453420Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7453523Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7453815Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7453952Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7454108Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7454117Z 
2025-04-11T04:23:18.7454355Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7454519Z _____________ test_flash_decoding[False-False-5-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.7454523Z 
2025-04-11T04:23:18.7454677Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7454840Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7454932Z use_new_kcache_layout = False
2025-04-11T04:23:18.7454935Z 
2025-04-11T04:23:18.7455134Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7455240Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7455358Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7455496Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7455614Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7455731Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7455875Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7455977Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7456116Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7456264Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7456351Z     def test_flash_decoding(
2025-04-11T04:23:18.7456428Z         bsz: int,
2025-04-11T04:23:18.7456509Z         block_size: int,
2025-04-11T04:23:18.7456600Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7456681Z         num_attn_heads: int,
2025-04-11T04:23:18.7456765Z         kv_group_num: int,
2025-04-11T04:23:18.7456950Z         same_context_len: bool,
2025-04-11T04:23:18.7457029Z         q_len: int,
2025-04-11T04:23:18.7457117Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7457203Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7457277Z     ):
2025-04-11T04:23:18.7457388Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7457578Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7457759Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7457925Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7458090Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7458246Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7458319Z     
2025-04-11T04:23:18.7458408Z         torch.manual_seed(123)
2025-04-11T04:23:18.7458495Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7458589Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7458592Z 
2025-04-11T04:23:18.7458745Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7458909Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7458913Z 
2025-04-11T04:23:18.7458989Z device = None
2025-04-11T04:23:18.7458993Z 
2025-04-11T04:23:18.7459116Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7459262Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7459331Z     
2025-04-11T04:23:18.7459445Z         Args:
2025-04-11T04:23:18.7459610Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7459775Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7459881Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7459954Z         """
2025-04-11T04:23:18.7460030Z         _lazy_init()
2025-04-11T04:23:18.7460122Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7460228Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7460332Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7460614Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7460747Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7460904Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7460910Z 
2025-04-11T04:23:18.7461148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7461317Z ____________ test_flash_decoding[False-False-5-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.7461324Z 
2025-04-11T04:23:18.7461478Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7461640Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7461734Z use_new_kcache_layout = False
2025-04-11T04:23:18.7461737Z 
2025-04-11T04:23:18.7461935Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7462041Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7462159Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7462298Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7462410Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7462523Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7462660Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7462761Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7462982Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7463136Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7463222Z     def test_flash_decoding(
2025-04-11T04:23:18.7463304Z         bsz: int,
2025-04-11T04:23:18.7463384Z         block_size: int,
2025-04-11T04:23:18.7463475Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7463556Z         num_attn_heads: int,
2025-04-11T04:23:18.7463642Z         kv_group_num: int,
2025-04-11T04:23:18.7463725Z         same_context_len: bool,
2025-04-11T04:23:18.7463800Z         q_len: int,
2025-04-11T04:23:18.7463886Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7463972Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7464047Z     ):
2025-04-11T04:23:18.7464156Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7464346Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7464535Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7464704Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7464921Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7465076Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7465150Z     
2025-04-11T04:23:18.7465236Z         torch.manual_seed(123)
2025-04-11T04:23:18.7465324Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7465415Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7465419Z 
2025-04-11T04:23:18.7465626Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7465739Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7465743Z 
2025-04-11T04:23:18.7465819Z device = None
2025-04-11T04:23:18.7465822Z 
2025-04-11T04:23:18.7465945Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7466094Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7466162Z     
2025-04-11T04:23:18.7466239Z         Args:
2025-04-11T04:23:18.7466403Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7466568Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7466674Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7466750Z         """
2025-04-11T04:23:18.7466825Z         _lazy_init()
2025-04-11T04:23:18.7466918Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7467024Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7467126Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7467411Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7467546Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7467704Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7467710Z 
2025-04-11T04:23:18.7467946Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7468111Z _____________ test_flash_decoding[False-False-5-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.7468118Z 
2025-04-11T04:23:18.7468268Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7468461Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7468557Z use_new_kcache_layout = False
2025-04-11T04:23:18.7468561Z 
2025-04-11T04:23:18.7468760Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7468867Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7469078Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7469226Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7469342Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7469455Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7469596Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7469699Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7469838Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7469988Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7470076Z     def test_flash_decoding(
2025-04-11T04:23:18.7470154Z         bsz: int,
2025-04-11T04:23:18.7470235Z         block_size: int,
2025-04-11T04:23:18.7470325Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7470407Z         num_attn_heads: int,
2025-04-11T04:23:18.7470491Z         kv_group_num: int,
2025-04-11T04:23:18.7470578Z         same_context_len: bool,
2025-04-11T04:23:18.7470652Z         q_len: int,
2025-04-11T04:23:18.7470740Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7470826Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7470952Z     ):
2025-04-11T04:23:18.7471062Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7471256Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7471443Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7471616Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7471837Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7471993Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7472065Z     
2025-04-11T04:23:18.7472152Z         torch.manual_seed(123)
2025-04-11T04:23:18.7472243Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7472337Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7472341Z 
2025-04-11T04:23:18.7472495Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7472613Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7472617Z 
2025-04-11T04:23:18.7472692Z device = None
2025-04-11T04:23:18.7472695Z 
2025-04-11T04:23:18.7472814Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7472964Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7473034Z     
2025-04-11T04:23:18.7473106Z         Args:
2025-04-11T04:23:18.7473271Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7473440Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7473546Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7473620Z         """
2025-04-11T04:23:18.7473695Z         _lazy_init()
2025-04-11T04:23:18.7473788Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7473893Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7473998Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7474281Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7474415Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7474572Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7474578Z 
2025-04-11T04:23:18.7474815Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7474984Z ____________ test_flash_decoding[False-False-5-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.7474988Z 
2025-04-11T04:23:18.7475224Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7475388Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7475480Z use_new_kcache_layout = False
2025-04-11T04:23:18.7475484Z 
2025-04-11T04:23:18.7475683Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7475790Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7475905Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7476047Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7476161Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7476275Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7476416Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7476518Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7476657Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7476805Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7476892Z     def test_flash_decoding(
2025-04-11T04:23:18.7477017Z         bsz: int,
2025-04-11T04:23:18.7477098Z         block_size: int,
2025-04-11T04:23:18.7477190Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7477272Z         num_attn_heads: int,
2025-04-11T04:23:18.7477357Z         kv_group_num: int,
2025-04-11T04:23:18.7477439Z         same_context_len: bool,
2025-04-11T04:23:18.7477513Z         q_len: int,
2025-04-11T04:23:18.7477599Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7477685Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7477830Z     ):
2025-04-11T04:23:18.7477941Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7478143Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7478328Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7478498Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7478670Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7478826Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7478900Z     
2025-04-11T04:23:18.7478984Z         torch.manual_seed(123)
2025-04-11T04:23:18.7479071Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7479161Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7479165Z 
2025-04-11T04:23:18.7479317Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7479433Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7479437Z 
2025-04-11T04:23:18.7479512Z device = None
2025-04-11T04:23:18.7479516Z 
2025-04-11T04:23:18.7479638Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7479785Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7479855Z     
2025-04-11T04:23:18.7479926Z         Args:
2025-04-11T04:23:18.7480091Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7480259Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7480362Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7480438Z         """
2025-04-11T04:23:18.7480512Z         _lazy_init()
2025-04-11T04:23:18.7480608Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7480715Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7480820Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7481111Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7481327Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7481491Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7481497Z 
2025-04-11T04:23:18.7481731Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7481900Z _____________ test_flash_decoding[False-False-5-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.7481903Z 
2025-04-11T04:23:18.7482052Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7482212Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7482305Z use_new_kcache_layout = False
2025-04-11T04:23:18.7482309Z 
2025-04-11T04:23:18.7482510Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7482616Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7482736Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7482874Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7482988Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7483152Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7483291Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7483391Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7483529Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7483677Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7483763Z     def test_flash_decoding(
2025-04-11T04:23:18.7483890Z         bsz: int,
2025-04-11T04:23:18.7483974Z         block_size: int,
2025-04-11T04:23:18.7484067Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7484148Z         num_attn_heads: int,
2025-04-11T04:23:18.7484233Z         kv_group_num: int,
2025-04-11T04:23:18.7484321Z         same_context_len: bool,
2025-04-11T04:23:18.7484395Z         q_len: int,
2025-04-11T04:23:18.7484485Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7484570Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7484644Z     ):
2025-04-11T04:23:18.7484755Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7484948Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7485131Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7485301Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7485470Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7485627Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7485702Z     
2025-04-11T04:23:18.7485789Z         torch.manual_seed(123)
2025-04-11T04:23:18.7485885Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7485976Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7485979Z 
2025-04-11T04:23:18.7486133Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7486253Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7486257Z 
2025-04-11T04:23:18.7486331Z device = None
2025-04-11T04:23:18.7486335Z 
2025-04-11T04:23:18.7486455Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7486604Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7486677Z     
2025-04-11T04:23:18.7486750Z         Args:
2025-04-11T04:23:18.7486915Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7487084Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7487272Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7487350Z         """
2025-04-11T04:23:18.7487429Z         _lazy_init()
2025-04-11T04:23:18.7487526Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7487632Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7487735Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7488021Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7488158Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7488320Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7488325Z 
2025-04-11T04:23:18.7488565Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7488735Z ____________ test_flash_decoding[False-False-5-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.7488739Z 
2025-04-11T04:23:18.7488892Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7489060Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7489202Z use_new_kcache_layout = False
2025-04-11T04:23:18.7489206Z 
2025-04-11T04:23:18.7489404Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7489511Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7489627Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7489770Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7489886Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7490057Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7490194Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7490295Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7490435Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7490582Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7490670Z     def test_flash_decoding(
2025-04-11T04:23:18.7490745Z         bsz: int,
2025-04-11T04:23:18.7490826Z         block_size: int,
2025-04-11T04:23:18.7490916Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7490996Z         num_attn_heads: int,
2025-04-11T04:23:18.7491080Z         kv_group_num: int,
2025-04-11T04:23:18.7491163Z         same_context_len: bool,
2025-04-11T04:23:18.7491242Z         q_len: int,
2025-04-11T04:23:18.7491327Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7491412Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7491487Z     ):
2025-04-11T04:23:18.7491594Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7491787Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7491972Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7492139Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7492304Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7492460Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7492533Z     
2025-04-11T04:23:18.7492617Z         torch.manual_seed(123)
2025-04-11T04:23:18.7492707Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7492794Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7492797Z 
2025-04-11T04:23:18.7492950Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7493066Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7493069Z 
2025-04-11T04:23:18.7493144Z device = None
2025-04-11T04:23:18.7493148Z 
2025-04-11T04:23:18.7493352Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7493506Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7493578Z     
2025-04-11T04:23:18.7493650Z         Args:
2025-04-11T04:23:18.7493816Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7493983Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7494088Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7494163Z         """
2025-04-11T04:23:18.7494239Z         _lazy_init()
2025-04-11T04:23:18.7494334Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7494437Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7494540Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7494830Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7494967Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7495125Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7495182Z 
2025-04-11T04:23:18.7495425Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7495593Z _____________ test_flash_decoding[False-False-5-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.7495597Z 
2025-04-11T04:23:18.7495746Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7495910Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7496048Z use_new_kcache_layout = False
2025-04-11T04:23:18.7496052Z 
2025-04-11T04:23:18.7496249Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7496354Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7496471Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7496611Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7496725Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7496841Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7496977Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7497077Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7497216Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7497364Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7497452Z     def test_flash_decoding(
2025-04-11T04:23:18.7497528Z         bsz: int,
2025-04-11T04:23:18.7497608Z         block_size: int,
2025-04-11T04:23:18.7497699Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7497781Z         num_attn_heads: int,
2025-04-11T04:23:18.7497864Z         kv_group_num: int,
2025-04-11T04:23:18.7497954Z         same_context_len: bool,
2025-04-11T04:23:18.7498032Z         q_len: int,
2025-04-11T04:23:18.7498115Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7498202Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7498278Z     ):
2025-04-11T04:23:18.7498389Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7498582Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7498763Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7498932Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7499096Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7499251Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7499324Z     
2025-04-11T04:23:18.7499408Z         torch.manual_seed(123)
2025-04-11T04:23:18.7499599Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7499692Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7499696Z 
2025-04-11T04:23:18.7499851Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7499968Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7499972Z 
2025-04-11T04:23:18.7500047Z device = None
2025-04-11T04:23:18.7500051Z 
2025-04-11T04:23:18.7500175Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7500322Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7500393Z     
2025-04-11T04:23:18.7500470Z         Args:
2025-04-11T04:23:18.7500639Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7500809Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7500921Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7500997Z         """
2025-04-11T04:23:18.7501074Z         _lazy_init()
2025-04-11T04:23:18.7501172Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7501271Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7501447Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7501737Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7501871Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7502030Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7502083Z 
2025-04-11T04:23:18.7502326Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7502496Z ____________ test_flash_decoding[False-False-5-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.7502500Z 
2025-04-11T04:23:18.7502651Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7502819Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7502909Z use_new_kcache_layout = False
2025-04-11T04:23:18.7502913Z 
2025-04-11T04:23:18.7503111Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7503217Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7503333Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7503476Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7503591Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7503706Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7503841Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7503942Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7504083Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7504232Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7504323Z     def test_flash_decoding(
2025-04-11T04:23:18.7504400Z         bsz: int,
2025-04-11T04:23:18.7504484Z         block_size: int,
2025-04-11T04:23:18.7504571Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7504652Z         num_attn_heads: int,
2025-04-11T04:23:18.7504738Z         kv_group_num: int,
2025-04-11T04:23:18.7504821Z         same_context_len: bool,
2025-04-11T04:23:18.7504896Z         q_len: int,
2025-04-11T04:23:18.7504980Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7505066Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7505140Z     ):
2025-04-11T04:23:18.7505249Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7505443Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7505704Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7505881Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7506046Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7506199Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7506274Z     
2025-04-11T04:23:18.7506358Z         torch.manual_seed(123)
2025-04-11T04:23:18.7506449Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7506539Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7506542Z 
2025-04-11T04:23:18.7506699Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7506812Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7506816Z 
2025-04-11T04:23:18.7506890Z device = None
2025-04-11T04:23:18.7506893Z 
2025-04-11T04:23:18.7507016Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7507165Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7507235Z     
2025-04-11T04:23:18.7507307Z         Args:
2025-04-11T04:23:18.7507529Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7507694Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7507798Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7507875Z         """
2025-04-11T04:23:18.7507950Z         _lazy_init()
2025-04-11T04:23:18.7508047Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7508202Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7508306Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7508635Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7508774Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7508934Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7508940Z 
2025-04-11T04:23:18.7509180Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7509352Z ____________ test_flash_decoding[False-False-5-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.7509355Z 
2025-04-11T04:23:18.7509506Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7509670Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7509759Z use_new_kcache_layout = False
2025-04-11T04:23:18.7509763Z 
2025-04-11T04:23:18.7509966Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7510069Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7510188Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7510328Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7510443Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7510559Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7510695Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7510795Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7510932Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7511080Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7511170Z     def test_flash_decoding(
2025-04-11T04:23:18.7511244Z         bsz: int,
2025-04-11T04:23:18.7511327Z         block_size: int,
2025-04-11T04:23:18.7511415Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7511496Z         num_attn_heads: int,
2025-04-11T04:23:18.7511582Z         kv_group_num: int,
2025-04-11T04:23:18.7511753Z         same_context_len: bool,
2025-04-11T04:23:18.7511835Z         q_len: int,
2025-04-11T04:23:18.7511919Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7512005Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7512080Z     ):
2025-04-11T04:23:18.7512189Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7512381Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7512562Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7512738Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7512906Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7513062Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7513136Z     
2025-04-11T04:23:18.7513219Z         torch.manual_seed(123)
2025-04-11T04:23:18.7513313Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7513402Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7513405Z 
2025-04-11T04:23:18.7513561Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7513726Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7513730Z 
2025-04-11T04:23:18.7513805Z device = None
2025-04-11T04:23:18.7513809Z 
2025-04-11T04:23:18.7513930Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7514080Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7514152Z     
2025-04-11T04:23:18.7514263Z         Args:
2025-04-11T04:23:18.7514432Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7514596Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7514704Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7514781Z         """
2025-04-11T04:23:18.7514856Z         _lazy_init()
2025-04-11T04:23:18.7514952Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7515051Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7515159Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7515440Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7515575Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7515734Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7515740Z 
2025-04-11T04:23:18.7515980Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7516152Z ____________ test_flash_decoding[False-False-5-False-1-16-16-16-16] ____________
2025-04-11T04:23:18.7516155Z 
2025-04-11T04:23:18.7516308Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7516472Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7516557Z use_new_kcache_layout = False
2025-04-11T04:23:18.7516563Z 
2025-04-11T04:23:18.7516765Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7516867Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7516985Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7517125Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7517238Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7517354Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7517489Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7517594Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7517811Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7517964Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7518053Z     def test_flash_decoding(
2025-04-11T04:23:18.7518130Z         bsz: int,
2025-04-11T04:23:18.7518214Z         block_size: int,
2025-04-11T04:23:18.7518300Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7518380Z         num_attn_heads: int,
2025-04-11T04:23:18.7518466Z         kv_group_num: int,
2025-04-11T04:23:18.7518549Z         same_context_len: bool,
2025-04-11T04:23:18.7518627Z         q_len: int,
2025-04-11T04:23:18.7518709Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7518794Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7518868Z     ):
2025-04-11T04:23:18.7518977Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7519170Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7519354Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7519526Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7519688Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7519897Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7519969Z     
2025-04-11T04:23:18.7520053Z         torch.manual_seed(123)
2025-04-11T04:23:18.7520144Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7520234Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7520238Z 
2025-04-11T04:23:18.7520396Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7520555Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7520559Z 
2025-04-11T04:23:18.7520634Z device = None
2025-04-11T04:23:18.7520645Z 
2025-04-11T04:23:18.7520765Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7520915Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7520987Z     
2025-04-11T04:23:18.7521059Z         Args:
2025-04-11T04:23:18.7521229Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7521395Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7521499Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7521573Z         """
2025-04-11T04:23:18.7521648Z         _lazy_init()
2025-04-11T04:23:18.7521743Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7521844Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7521948Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7522231Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7522367Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7522529Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7522535Z 
2025-04-11T04:23:18.7522776Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7522949Z ____________ test_flash_decoding[False-False-5-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.7522953Z 
2025-04-11T04:23:18.7523105Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7523272Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7523362Z use_new_kcache_layout = False
2025-04-11T04:23:18.7523366Z 
2025-04-11T04:23:18.7523573Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7523676Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7523889Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7524036Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7524151Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7524266Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7524399Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7524502Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7524635Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7524783Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7524873Z     def test_flash_decoding(
2025-04-11T04:23:18.7524949Z         bsz: int,
2025-04-11T04:23:18.7525033Z         block_size: int,
2025-04-11T04:23:18.7525122Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7525203Z         num_attn_heads: int,
2025-04-11T04:23:18.7525291Z         kv_group_num: int,
2025-04-11T04:23:18.7525379Z         same_context_len: bool,
2025-04-11T04:23:18.7525456Z         q_len: int,
2025-04-11T04:23:18.7525540Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7525630Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7525750Z     ):
2025-04-11T04:23:18.7525859Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7526053Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7526233Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7526406Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7526621Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7526781Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7526852Z     
2025-04-11T04:23:18.7526937Z         torch.manual_seed(123)
2025-04-11T04:23:18.7527030Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7527119Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7527122Z 
2025-04-11T04:23:18.7527278Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7527390Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7527394Z 
2025-04-11T04:23:18.7527471Z device = None
2025-04-11T04:23:18.7527475Z 
2025-04-11T04:23:18.7527591Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7527741Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7527812Z     
2025-04-11T04:23:18.7527886Z         Args:
2025-04-11T04:23:18.7528054Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7528218Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7528328Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7528399Z         """
2025-04-11T04:23:18.7528474Z         _lazy_init()
2025-04-11T04:23:18.7528570Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7528670Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7528778Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7529064Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7529199Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7529355Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7529361Z 
2025-04-11T04:23:18.7529598Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7529773Z ____________ test_flash_decoding[False-False-5-False-1-16-16-32-16] ____________
2025-04-11T04:23:18.7529777Z 
2025-04-11T04:23:18.7530014Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7530184Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7530271Z use_new_kcache_layout = False
2025-04-11T04:23:18.7530277Z 
2025-04-11T04:23:18.7530479Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7530582Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7530697Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7530840Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7530954Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7531072Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7531206Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7531311Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7531448Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7531598Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7531687Z     def test_flash_decoding(
2025-04-11T04:23:18.7531813Z         bsz: int,
2025-04-11T04:23:18.7531898Z         block_size: int,
2025-04-11T04:23:18.7531987Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7532066Z         num_attn_heads: int,
2025-04-11T04:23:18.7532154Z         kv_group_num: int,
2025-04-11T04:23:18.7532237Z         same_context_len: bool,
2025-04-11T04:23:18.7532318Z         q_len: int,
2025-04-11T04:23:18.7532401Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7532491Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7532613Z     ):
2025-04-11T04:23:18.7532725Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7532920Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7533106Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7533281Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7533443Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7533605Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7533674Z     
2025-04-11T04:23:18.7533761Z         torch.manual_seed(123)
2025-04-11T04:23:18.7533853Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7533944Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7533947Z 
2025-04-11T04:23:18.7534108Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7534223Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7534227Z 
2025-04-11T04:23:18.7534305Z device = None
2025-04-11T04:23:18.7534309Z 
2025-04-11T04:23:18.7534428Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7534577Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7534650Z     
2025-04-11T04:23:18.7534722Z         Args:
2025-04-11T04:23:18.7534892Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7535057Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7535164Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7535235Z         """
2025-04-11T04:23:18.7535309Z         _lazy_init()
2025-04-11T04:23:18.7535406Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7535507Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7535614Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7535895Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7536113Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7536276Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7536280Z 
2025-04-11T04:23:18.7536520Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7536692Z _____________ test_flash_decoding[False-False-5-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.7536696Z 
2025-04-11T04:23:18.7536847Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7537013Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7537103Z use_new_kcache_layout = False
2025-04-11T04:23:18.7537106Z 
2025-04-11T04:23:18.7537308Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7537413Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7537534Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7537670Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7537782Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7537952Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7538088Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7538192Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7538328Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7538478Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7538565Z     def test_flash_decoding(
2025-04-11T04:23:18.7538779Z         bsz: int,
2025-04-11T04:23:18.7538864Z         block_size: int,
2025-04-11T04:23:18.7538951Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7539036Z         num_attn_heads: int,
2025-04-11T04:23:18.7539118Z         kv_group_num: int,
2025-04-11T04:23:18.7539204Z         same_context_len: bool,
2025-04-11T04:23:18.7539284Z         q_len: int,
2025-04-11T04:23:18.7539367Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7539455Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7539527Z     ):
2025-04-11T04:23:18.7539638Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7539829Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7540008Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7540180Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7540342Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7540501Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7540570Z     
2025-04-11T04:23:18.7540653Z         torch.manual_seed(123)
2025-04-11T04:23:18.7540748Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7540839Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7540843Z 
2025-04-11T04:23:18.7541000Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7541112Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7541116Z 
2025-04-11T04:23:18.7541194Z device = None
2025-04-11T04:23:18.7541198Z 
2025-04-11T04:23:18.7541311Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7541460Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7541533Z     
2025-04-11T04:23:18.7541604Z         Args:
2025-04-11T04:23:18.7541774Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7541935Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7542043Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7542197Z         """
2025-04-11T04:23:18.7542278Z         _lazy_init()
2025-04-11T04:23:18.7542377Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7542478Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7542587Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7542870Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7543009Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7543166Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7543171Z 
2025-04-11T04:23:18.7543411Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7543583Z ____________ test_flash_decoding[False-False-5-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.7543587Z 
2025-04-11T04:23:18.7543740Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7543908Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7543996Z use_new_kcache_layout = False
2025-04-11T04:23:18.7544046Z 
2025-04-11T04:23:18.7544249Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7544353Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7544472Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7544612Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7544726Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7544894Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7545031Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7545142Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7545282Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7545439Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7545529Z     def test_flash_decoding(
2025-04-11T04:23:18.7545606Z         bsz: int,
2025-04-11T04:23:18.7545692Z         block_size: int,
2025-04-11T04:23:18.7545779Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7545863Z         num_attn_heads: int,
2025-04-11T04:23:18.7545943Z         kv_group_num: int,
2025-04-11T04:23:18.7546026Z         same_context_len: bool,
2025-04-11T04:23:18.7546105Z         q_len: int,
2025-04-11T04:23:18.7546188Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7546283Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7546355Z     ):
2025-04-11T04:23:18.7546462Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7546656Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7546839Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7547014Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7547176Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7547338Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7547407Z     
2025-04-11T04:23:18.7547493Z         torch.manual_seed(123)
2025-04-11T04:23:18.7547585Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7547675Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7547678Z 
2025-04-11T04:23:18.7547837Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7547949Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7547952Z 
2025-04-11T04:23:18.7548029Z device = None
2025-04-11T04:23:18.7548032Z 
2025-04-11T04:23:18.7548240Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7548395Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7548498Z     
2025-04-11T04:23:18.7548571Z         Args:
2025-04-11T04:23:18.7548743Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7548906Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7549016Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7549086Z         """
2025-04-11T04:23:18.7549162Z         _lazy_init()
2025-04-11T04:23:18.7549259Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7549361Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7549474Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7549762Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7549909Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7550067Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7550071Z 
2025-04-11T04:23:18.7550372Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7550547Z _____________ test_flash_decoding[False-False-5-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.7550550Z 
2025-04-11T04:23:18.7550703Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7550871Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7551015Z use_new_kcache_layout = False
2025-04-11T04:23:18.7551019Z 
2025-04-11T04:23:18.7551218Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7551325Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7551446Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7551582Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7551696Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7551812Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7551945Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7552050Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7552182Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7552333Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7552417Z     def test_flash_decoding(
2025-04-11T04:23:18.7552493Z         bsz: int,
2025-04-11T04:23:18.7552576Z         block_size: int,
2025-04-11T04:23:18.7552662Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7552746Z         num_attn_heads: int,
2025-04-11T04:23:18.7552825Z         kv_group_num: int,
2025-04-11T04:23:18.7552911Z         same_context_len: bool,
2025-04-11T04:23:18.7552989Z         q_len: int,
2025-04-11T04:23:18.7553073Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7553162Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7553234Z     ):
2025-04-11T04:23:18.7553342Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7553534Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7553713Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7553886Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7554051Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7554210Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7554279Z     
2025-04-11T04:23:18.7554366Z         torch.manual_seed(123)
2025-04-11T04:23:18.7554550Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7554644Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7554648Z 
2025-04-11T04:23:18.7554809Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7554922Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7554926Z 
2025-04-11T04:23:18.7555005Z device = None
2025-04-11T04:23:18.7555009Z 
2025-04-11T04:23:18.7555125Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7555280Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7555349Z     
2025-04-11T04:23:18.7555422Z         Args:
2025-04-11T04:23:18.7555594Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7555757Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7555866Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7555941Z         """
2025-04-11T04:23:18.7556018Z         _lazy_init()
2025-04-11T04:23:18.7556116Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7556215Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7556377Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7556659Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7556796Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7556950Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7557004Z 
2025-04-11T04:23:18.7557247Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7557415Z ____________ test_flash_decoding[False-False-5-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.7557419Z 
2025-04-11T04:23:18.7557570Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T04:23:18.7557736Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7557823Z use_new_kcache_layout = False
2025-04-11T04:23:18.7557829Z 
2025-04-11T04:23:18.7558035Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7558139Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7558258Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7558399Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7558517Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7558636Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7558771Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7558876Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7559017Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7559171Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7559255Z     def test_flash_decoding(
2025-04-11T04:23:18.7559331Z         bsz: int,
2025-04-11T04:23:18.7559416Z         block_size: int,
2025-04-11T04:23:18.7559503Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7559588Z         num_attn_heads: int,
2025-04-11T04:23:18.7559669Z         kv_group_num: int,
2025-04-11T04:23:18.7559754Z         same_context_len: bool,
2025-04-11T04:23:18.7559833Z         q_len: int,
2025-04-11T04:23:18.7559918Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7560006Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7560080Z     ):
2025-04-11T04:23:18.7560192Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7560385Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7560663Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7560842Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7561006Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7561166Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7561236Z     
2025-04-11T04:23:18.7561326Z         torch.manual_seed(123)
2025-04-11T04:23:18.7561413Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7561504Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7561508Z 
2025-04-11T04:23:18.7561671Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7561782Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7561785Z 
2025-04-11T04:23:18.7561863Z device = None
2025-04-11T04:23:18.7561866Z 
2025-04-11T04:23:18.7561986Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7562141Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7562210Z     
2025-04-11T04:23:18.7562281Z         Args:
2025-04-11T04:23:18.7562503Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7562669Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7562777Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7562850Z         """
2025-04-11T04:23:18.7562931Z         _lazy_init()
2025-04-11T04:23:18.7563025Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7563175Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7563286Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7563566Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7563707Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7563862Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7563866Z 
2025-04-11T04:23:18.7564109Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7564277Z ____________ test_flash_decoding[False-False-5-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.7564281Z 
2025-04-11T04:23:18.7564433Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7564604Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7564695Z use_new_kcache_layout = False
2025-04-11T04:23:18.7564699Z 
2025-04-11T04:23:18.7564907Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7565011Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7565134Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7565273Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7565391Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7565506Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7565639Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7565745Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7565881Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7566034Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7566120Z     def test_flash_decoding(
2025-04-11T04:23:18.7566195Z         bsz: int,
2025-04-11T04:23:18.7566278Z         block_size: int,
2025-04-11T04:23:18.7566366Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7566449Z         num_attn_heads: int,
2025-04-11T04:23:18.7566530Z         kv_group_num: int,
2025-04-11T04:23:18.7566699Z         same_context_len: bool,
2025-04-11T04:23:18.7566779Z         q_len: int,
2025-04-11T04:23:18.7566862Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7566952Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7567025Z     ):
2025-04-11T04:23:18.7567136Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7567326Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7567505Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7567677Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7567841Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7568002Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7568070Z     
2025-04-11T04:23:18.7568159Z         torch.manual_seed(123)
2025-04-11T04:23:18.7568249Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7568338Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7568342Z 
2025-04-11T04:23:18.7568498Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7568659Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7568664Z 
2025-04-11T04:23:18.7568743Z device = None
2025-04-11T04:23:18.7568747Z 
2025-04-11T04:23:18.7568862Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7569016Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7569085Z     
2025-04-11T04:23:18.7569158Z         Args:
2025-04-11T04:23:18.7569394Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7569559Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7569668Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7569742Z         """
2025-04-11T04:23:18.7569822Z         _lazy_init()
2025-04-11T04:23:18.7569915Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7570014Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7570123Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7570403Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7570540Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7570697Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7570703Z 
2025-04-11T04:23:18.7570944Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7571114Z ____________ test_flash_decoding[False-False-5-False-4-16-16-16-16] ____________
2025-04-11T04:23:18.7571117Z 
2025-04-11T04:23:18.7571274Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7571435Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7571521Z use_new_kcache_layout = False
2025-04-11T04:23:18.7571527Z 
2025-04-11T04:23:18.7571729Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7571831Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7571949Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7572086Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7572203Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7572315Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7572451Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7572556Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7572770Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7572926Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7573012Z     def test_flash_decoding(
2025-04-11T04:23:18.7573086Z         bsz: int,
2025-04-11T04:23:18.7573172Z         block_size: int,
2025-04-11T04:23:18.7573260Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7573348Z         num_attn_heads: int,
2025-04-11T04:23:18.7573429Z         kv_group_num: int,
2025-04-11T04:23:18.7573516Z         same_context_len: bool,
2025-04-11T04:23:18.7573590Z         q_len: int,
2025-04-11T04:23:18.7573673Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7573764Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7573836Z     ):
2025-04-11T04:23:18.7573948Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7574137Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7574317Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7574491Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7574653Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7574864Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7574933Z     
2025-04-11T04:23:18.7575020Z         torch.manual_seed(123)
2025-04-11T04:23:18.7575107Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7575195Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7575199Z 
2025-04-11T04:23:18.7575357Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7575519Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7575522Z 
2025-04-11T04:23:18.7575601Z device = None
2025-04-11T04:23:18.7575605Z 
2025-04-11T04:23:18.7575720Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7575876Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7575946Z     
2025-04-11T04:23:18.7576018Z         Args:
2025-04-11T04:23:18.7576185Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7576346Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7576455Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7576526Z         """
2025-04-11T04:23:18.7576605Z         _lazy_init()
2025-04-11T04:23:18.7576697Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7576796Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7576905Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7577184Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7577326Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7577482Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7577485Z 
2025-04-11T04:23:18.7577729Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7577894Z ____________ test_flash_decoding[False-False-5-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.7577898Z 
2025-04-11T04:23:18.7578050Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7578212Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7578304Z use_new_kcache_layout = False
2025-04-11T04:23:18.7578308Z 
2025-04-11T04:23:18.7578509Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7578611Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7578813Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7578954Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7579072Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7579184Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7579319Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7579425Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7579558Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7579708Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7579794Z     def test_flash_decoding(
2025-04-11T04:23:18.7579873Z         bsz: int,
2025-04-11T04:23:18.7579953Z         block_size: int,
2025-04-11T04:23:18.7580040Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7580125Z         num_attn_heads: int,
2025-04-11T04:23:18.7580205Z         kv_group_num: int,
2025-04-11T04:23:18.7580295Z         same_context_len: bool,
2025-04-11T04:23:18.7580370Z         q_len: int,
2025-04-11T04:23:18.7580455Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7580546Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7580666Z     ):
2025-04-11T04:23:18.7580780Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7580967Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7581150Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7581322Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7581540Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7581701Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7581771Z     
2025-04-11T04:23:18.7581859Z         torch.manual_seed(123)
2025-04-11T04:23:18.7581950Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7582040Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7582049Z 
2025-04-11T04:23:18.7582203Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7582315Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7582319Z 
2025-04-11T04:23:18.7582398Z device = None
2025-04-11T04:23:18.7582402Z 
2025-04-11T04:23:18.7582518Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7582669Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7582738Z     
2025-04-11T04:23:18.7582813Z         Args:
2025-04-11T04:23:18.7582980Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7583142Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7583250Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7583325Z         """
2025-04-11T04:23:18.7583404Z         _lazy_init()
2025-04-11T04:23:18.7583497Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7583596Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7583705Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7583988Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7584127Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7584281Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7584287Z 
2025-04-11T04:23:18.7584528Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7584696Z ____________ test_flash_decoding[False-False-5-False-4-16-16-32-16] ____________
2025-04-11T04:23:18.7584700Z 
2025-04-11T04:23:18.7584936Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T04:23:18.7585101Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T04:23:18.7585190Z use_new_kcache_layout = False
2025-04-11T04:23:18.7585196Z 
2025-04-11T04:23:18.7585400Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7585502Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T04:23:18.7585621Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T04:23:18.7585756Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T04:23:18.7585873Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T04:23:18.7585986Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T04:23:18.7586118Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7586223Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T04:23:18.7586360Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T04:23:18.7586513Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7586599Z     def test_flash_decoding(
2025-04-11T04:23:18.7586677Z         bsz: int,
2025-04-11T04:23:18.7586814Z         block_size: int,
2025-04-11T04:23:18.7586903Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7586989Z         num_attn_heads: int,
2025-04-11T04:23:18.7587070Z         kv_group_num: int,
2025-04-11T04:23:18.7587156Z         same_context_len: bool,
2025-04-11T04:23:18.7587230Z         q_len: int,
2025-04-11T04:23:18.7587313Z         use_alibi_slopes: bool,
2025-04-11T04:23:18.7587402Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7587523Z     ):
2025-04-11T04:23:18.7587635Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T04:23:18.7587825Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T04:23:18.7588010Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T04:23:18.7588183Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T04:23:18.7588346Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T04:23:18.7588536Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7588607Z     
2025-04-11T04:23:18.7588696Z         torch.manual_seed(123)
2025-04-11T04:23:18.7588784Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7588873Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7588882Z 
2025-04-11T04:23:18.7589037Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T04:23:18.7589148Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7589152Z 
2025-04-11T04:23:18.7589231Z device = None
2025-04-11T04:23:18.7589235Z 
2025-04-11T04:23:18.7589351Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7589506Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7589576Z     
2025-04-11T04:23:18.7589650Z         Args:
2025-04-11T04:23:18.7589817Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7589981Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7590088Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7590160Z         """
2025-04-11T04:23:18.7590239Z         _lazy_init()
2025-04-11T04:23:18.7590332Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7590432Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7590540Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7590822Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7591058Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7591216Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7591220Z 
2025-04-11T04:23:18.7591464Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7591619Z ________________ test_copy_kv_to_caches[True-1-True-16-16-16-7] ________________
2025-04-11T04:23:18.7591623Z 
2025-04-11T04:23:18.7591775Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7591932Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7591936Z 
2025-04-11T04:23:18.7592153Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7592256Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7592378Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7592521Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7592635Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7592778Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7592965Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7593121Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7593209Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7593281Z         bsz: int,
2025-04-11T04:23:18.7593365Z         block_size: int,
2025-04-11T04:23:18.7593453Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7593537Z         num_kv_heads: int,
2025-04-11T04:23:18.7593620Z         same_context_len: bool,
2025-04-11T04:23:18.7593755Z         n_tokens: int,
2025-04-11T04:23:18.7593848Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7593918Z     ):
2025-04-11T04:23:18.7594007Z         torch.manual_seed(123)
2025-04-11T04:23:18.7594095Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7594185Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7594188Z 
2025-04-11T04:23:18.7594343Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7594455Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7594461Z 
2025-04-11T04:23:18.7594540Z device = None
2025-04-11T04:23:18.7594543Z 
2025-04-11T04:23:18.7594658Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7594811Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7594879Z     
2025-04-11T04:23:18.7594950Z         Args:
2025-04-11T04:23:18.7595120Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7595286Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7595393Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7595463Z         """
2025-04-11T04:23:18.7595546Z         _lazy_init()
2025-04-11T04:23:18.7595638Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7595736Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7595846Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7596128Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7596263Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7596418Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7596422Z 
2025-04-11T04:23:18.7596661Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7596818Z _______________ test_copy_kv_to_caches[True-1-True-16-16-16-32] ________________
2025-04-11T04:23:18.7596821Z 
2025-04-11T04:23:18.7596974Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7597210Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7597215Z 
2025-04-11T04:23:18.7597417Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7597528Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7597651Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7597789Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7597901Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7598043Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7598150Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7598301Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7598392Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7598465Z         bsz: int,
2025-04-11T04:23:18.7598549Z         block_size: int,
2025-04-11T04:23:18.7598642Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7598725Z         num_kv_heads: int,
2025-04-11T04:23:18.7598812Z         same_context_len: bool,
2025-04-11T04:23:18.7598889Z         n_tokens: int,
2025-04-11T04:23:18.7599039Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7599112Z     ):
2025-04-11T04:23:18.7599200Z         torch.manual_seed(123)
2025-04-11T04:23:18.7599287Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7599375Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7599378Z 
2025-04-11T04:23:18.7599536Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7599646Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7599702Z 
2025-04-11T04:23:18.7599783Z device = None
2025-04-11T04:23:18.7599787Z 
2025-04-11T04:23:18.7599904Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7600056Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7600132Z     
2025-04-11T04:23:18.7600204Z         Args:
2025-04-11T04:23:18.7600375Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7600542Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7600649Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7600721Z         """
2025-04-11T04:23:18.7600801Z         _lazy_init()
2025-04-11T04:23:18.7600893Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7600993Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7601099Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7601381Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7601517Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7601676Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7601680Z 
2025-04-11T04:23:18.7601919Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7602072Z ________________ test_copy_kv_to_caches[True-1-True-16-16-32-7] ________________
2025-04-11T04:23:18.7602075Z 
2025-04-11T04:23:18.7602222Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7602377Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7602381Z 
2025-04-11T04:23:18.7602578Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7602688Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7602810Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7602945Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7603141Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7603288Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7603395Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7603546Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7603639Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7603713Z         bsz: int,
2025-04-11T04:23:18.7603798Z         block_size: int,
2025-04-11T04:23:18.7603887Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7603969Z         num_kv_heads: int,
2025-04-11T04:23:18.7604057Z         same_context_len: bool,
2025-04-11T04:23:18.7604135Z         n_tokens: int,
2025-04-11T04:23:18.7604230Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7604300Z     ):
2025-04-11T04:23:18.7604385Z         torch.manual_seed(123)
2025-04-11T04:23:18.7604477Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7604567Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7604571Z 
2025-04-11T04:23:18.7604730Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7604839Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7604843Z 
2025-04-11T04:23:18.7604974Z device = None
2025-04-11T04:23:18.7604978Z 
2025-04-11T04:23:18.7605096Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7605252Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7605322Z     
2025-04-11T04:23:18.7605397Z         Args:
2025-04-11T04:23:18.7605570Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7605736Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7605892Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7605963Z         """
2025-04-11T04:23:18.7606039Z         _lazy_init()
2025-04-11T04:23:18.7606139Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7606240Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7606348Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7606628Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7606769Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7606926Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7606930Z 
2025-04-11T04:23:18.7607168Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7607327Z _______________ test_copy_kv_to_caches[True-1-True-16-16-32-32] ________________
2025-04-11T04:23:18.7607332Z 
2025-04-11T04:23:18.7607482Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7607635Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7607642Z 
2025-04-11T04:23:18.7607843Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7607950Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7608076Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7608214Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7608326Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7608464Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7608575Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7608723Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7608816Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7608889Z         bsz: int,
2025-04-11T04:23:18.7608972Z         block_size: int,
2025-04-11T04:23:18.7609058Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7609140Z         num_kv_heads: int,
2025-04-11T04:23:18.7609308Z         same_context_len: bool,
2025-04-11T04:23:18.7609390Z         n_tokens: int,
2025-04-11T04:23:18.7609482Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7609552Z     ):
2025-04-11T04:23:18.7609640Z         torch.manual_seed(123)
2025-04-11T04:23:18.7609730Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7609819Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7609823Z 
2025-04-11T04:23:18.7609978Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7610088Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7610092Z 
2025-04-11T04:23:18.7610169Z device = None
2025-04-11T04:23:18.7610175Z 
2025-04-11T04:23:18.7610290Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7610440Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7610513Z     
2025-04-11T04:23:18.7610585Z         Args:
2025-04-11T04:23:18.7610758Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7610923Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7611082Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7611154Z         """
2025-04-11T04:23:18.7611231Z         _lazy_init()
2025-04-11T04:23:18.7611328Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7611428Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7611534Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7611813Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7612005Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7612160Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7612164Z 
2025-04-11T04:23:18.7612401Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7612556Z ________________ test_copy_kv_to_caches[True-1-True-16-16-64-7] ________________
2025-04-11T04:23:18.7612562Z 
2025-04-11T04:23:18.7612712Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7612865Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7612869Z 
2025-04-11T04:23:18.7613068Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7613174Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7613295Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7613436Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7613548Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7613684Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7613797Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7613944Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7614031Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7614107Z         bsz: int,
2025-04-11T04:23:18.7614187Z         block_size: int,
2025-04-11T04:23:18.7614280Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7614361Z         num_kv_heads: int,
2025-04-11T04:23:18.7614445Z         same_context_len: bool,
2025-04-11T04:23:18.7614523Z         n_tokens: int,
2025-04-11T04:23:18.7614612Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7614682Z     ):
2025-04-11T04:23:18.7614765Z         torch.manual_seed(123)
2025-04-11T04:23:18.7614858Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7614946Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7614950Z 
2025-04-11T04:23:18.7615101Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7615312Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7615316Z 
2025-04-11T04:23:18.7615395Z device = None
2025-04-11T04:23:18.7615402Z 
2025-04-11T04:23:18.7615516Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7615665Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7615738Z     
2025-04-11T04:23:18.7615809Z         Args:
2025-04-11T04:23:18.7615978Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7616142Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7616245Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7616320Z         """
2025-04-11T04:23:18.7616396Z         _lazy_init()
2025-04-11T04:23:18.7616489Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7616588Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7616696Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7616979Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7617166Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7617324Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7617328Z 
2025-04-11T04:23:18.7617567Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7617723Z _______________ test_copy_kv_to_caches[True-1-True-16-16-64-32] ________________
2025-04-11T04:23:18.7617790Z 
2025-04-11T04:23:18.7617940Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7618094Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7618098Z 
2025-04-11T04:23:18.7618300Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7618403Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7618524Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7618660Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7618775Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7618910Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7619019Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7619167Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7619255Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7619331Z         bsz: int,
2025-04-11T04:23:18.7619410Z         block_size: int,
2025-04-11T04:23:18.7619501Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7619581Z         num_kv_heads: int,
2025-04-11T04:23:18.7619666Z         same_context_len: bool,
2025-04-11T04:23:18.7619742Z         n_tokens: int,
2025-04-11T04:23:18.7619831Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7619904Z     ):
2025-04-11T04:23:18.7619988Z         torch.manual_seed(123)
2025-04-11T04:23:18.7620079Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7620169Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7620173Z 
2025-04-11T04:23:18.7620325Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7620434Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7620438Z 
2025-04-11T04:23:18.7620511Z device = None
2025-04-11T04:23:18.7620515Z 
2025-04-11T04:23:18.7620635Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7620783Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7620856Z     
2025-04-11T04:23:18.7620927Z         Args:
2025-04-11T04:23:18.7621094Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7621342Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7621447Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7621526Z         """
2025-04-11T04:23:18.7621603Z         _lazy_init()
2025-04-11T04:23:18.7621697Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7621795Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7621898Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7622182Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7622314Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7622473Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7622477Z 
2025-04-11T04:23:18.7622711Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7622870Z _______________ test_copy_kv_to_caches[True-1-False-16-16-16-7] ________________
2025-04-11T04:23:18.7622874Z 
2025-04-11T04:23:18.7623020Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7623228Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7623232Z 
2025-04-11T04:23:18.7623427Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7623532Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7623652Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7623784Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7623947Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7624082Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7624190Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7624344Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7624431Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7624508Z         bsz: int,
2025-04-11T04:23:18.7624586Z         block_size: int,
2025-04-11T04:23:18.7624680Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7624761Z         num_kv_heads: int,
2025-04-11T04:23:18.7624846Z         same_context_len: bool,
2025-04-11T04:23:18.7624922Z         n_tokens: int,
2025-04-11T04:23:18.7625009Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7625083Z     ):
2025-04-11T04:23:18.7625166Z         torch.manual_seed(123)
2025-04-11T04:23:18.7625254Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7625343Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7625347Z 
2025-04-11T04:23:18.7625497Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7625609Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7625613Z 
2025-04-11T04:23:18.7625690Z device = None
2025-04-11T04:23:18.7625694Z 
2025-04-11T04:23:18.7625813Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7625961Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7626035Z     
2025-04-11T04:23:18.7626108Z         Args:
2025-04-11T04:23:18.7626274Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7626442Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7626544Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7626619Z         """
2025-04-11T04:23:18.7626696Z         _lazy_init()
2025-04-11T04:23:18.7626790Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7626888Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7626988Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7627360Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7627497Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7627658Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7627663Z 
2025-04-11T04:23:18.7627898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7628054Z _______________ test_copy_kv_to_caches[True-1-False-16-16-16-32] _______________
2025-04-11T04:23:18.7628058Z 
2025-04-11T04:23:18.7628207Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7628363Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7628367Z 
2025-04-11T04:23:18.7628599Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7628701Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7628829Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7628965Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7629077Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7629278Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7629387Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7629534Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7629622Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7629699Z         bsz: int,
2025-04-11T04:23:18.7629778Z         block_size: int,
2025-04-11T04:23:18.7629927Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7630007Z         num_kv_heads: int,
2025-04-11T04:23:18.7630091Z         same_context_len: bool,
2025-04-11T04:23:18.7630172Z         n_tokens: int,
2025-04-11T04:23:18.7630257Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7630328Z     ):
2025-04-11T04:23:18.7630415Z         torch.manual_seed(123)
2025-04-11T04:23:18.7630506Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7630594Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7630599Z 
2025-04-11T04:23:18.7630748Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7630860Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7630864Z 
2025-04-11T04:23:18.7630939Z device = None
2025-04-11T04:23:18.7630942Z 
2025-04-11T04:23:18.7631060Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7631208Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7631281Z     
2025-04-11T04:23:18.7631352Z         Args:
2025-04-11T04:23:18.7631517Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7631687Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7631792Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7631868Z         """
2025-04-11T04:23:18.7631943Z         _lazy_init()
2025-04-11T04:23:18.7632038Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7632140Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7632241Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7632525Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7632656Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7632813Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7632820Z 
2025-04-11T04:23:18.7633055Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7633209Z _______________ test_copy_kv_to_caches[True-1-False-16-16-32-7] ________________
2025-04-11T04:23:18.7633304Z 
2025-04-11T04:23:18.7633455Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7633608Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7633617Z 
2025-04-11T04:23:18.7633814Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7633918Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7634045Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7634179Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7634295Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7634432Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7634542Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7634689Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7634780Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7634859Z         bsz: int,
2025-04-11T04:23:18.7634940Z         block_size: int,
2025-04-11T04:23:18.7635030Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7635258Z         num_kv_heads: int,
2025-04-11T04:23:18.7635341Z         same_context_len: bool,
2025-04-11T04:23:18.7635423Z         n_tokens: int,
2025-04-11T04:23:18.7635512Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7635586Z     ):
2025-04-11T04:23:18.7635669Z         torch.manual_seed(123)
2025-04-11T04:23:18.7635754Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7635847Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7635851Z 
2025-04-11T04:23:18.7636051Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7636171Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7636175Z 
2025-04-11T04:23:18.7636249Z device = None
2025-04-11T04:23:18.7636253Z 
2025-04-11T04:23:18.7636376Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7636525Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7636597Z     
2025-04-11T04:23:18.7636667Z         Args:
2025-04-11T04:23:18.7636837Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7637007Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7637110Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7637184Z         """
2025-04-11T04:23:18.7637259Z         _lazy_init()
2025-04-11T04:23:18.7637350Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7637455Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7637559Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7637862Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7637996Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7638156Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7638162Z 
2025-04-11T04:23:18.7638399Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7638558Z _______________ test_copy_kv_to_caches[True-1-False-16-16-32-32] _______________
2025-04-11T04:23:18.7638562Z 
2025-04-11T04:23:18.7638709Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7638860Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7638866Z 
2025-04-11T04:23:18.7639067Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7639169Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7639293Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7639523Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7639640Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7639778Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7639885Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7640036Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7640122Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7640199Z         bsz: int,
2025-04-11T04:23:18.7640277Z         block_size: int,
2025-04-11T04:23:18.7640369Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7640449Z         num_kv_heads: int,
2025-04-11T04:23:18.7640534Z         same_context_len: bool,
2025-04-11T04:23:18.7640614Z         n_tokens: int,
2025-04-11T04:23:18.7640701Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7640772Z     ):
2025-04-11T04:23:18.7640855Z         torch.manual_seed(123)
2025-04-11T04:23:18.7640947Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7641038Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7641042Z 
2025-04-11T04:23:18.7641192Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7641355Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7641359Z 
2025-04-11T04:23:18.7641434Z device = None
2025-04-11T04:23:18.7641438Z 
2025-04-11T04:23:18.7641558Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7641704Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7649766Z     
2025-04-11T04:23:18.7649880Z         Args:
2025-04-11T04:23:18.7650182Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7650358Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7650466Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7650557Z         """
2025-04-11T04:23:18.7650635Z         _lazy_init()
2025-04-11T04:23:18.7650734Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7650838Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7650947Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7651244Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7651381Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7651541Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7651546Z 
2025-04-11T04:23:18.7651788Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7651947Z _______________ test_copy_kv_to_caches[True-1-False-16-16-64-7] ________________
2025-04-11T04:23:18.7651951Z 
2025-04-11T04:23:18.7652105Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7652258Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7652265Z 
2025-04-11T04:23:18.7652464Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7652571Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7652698Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7652834Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7652950Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7653089Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7653199Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7653353Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7653440Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7653518Z         bsz: int,
2025-04-11T04:23:18.7653695Z         block_size: int,
2025-04-11T04:23:18.7653790Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7653871Z         num_kv_heads: int,
2025-04-11T04:23:18.7653956Z         same_context_len: bool,
2025-04-11T04:23:18.7654040Z         n_tokens: int,
2025-04-11T04:23:18.7654128Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7654202Z     ):
2025-04-11T04:23:18.7654287Z         torch.manual_seed(123)
2025-04-11T04:23:18.7654373Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7654466Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7654470Z 
2025-04-11T04:23:18.7654622Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7654740Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7654746Z 
2025-04-11T04:23:18.7654822Z device = None
2025-04-11T04:23:18.7654826Z 
2025-04-11T04:23:18.7654946Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7655101Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7655170Z     
2025-04-11T04:23:18.7655247Z         Args:
2025-04-11T04:23:18.7655414Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7655647Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7655751Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7655827Z         """
2025-04-11T04:23:18.7655904Z         _lazy_init()
2025-04-11T04:23:18.7655997Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7656104Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7656258Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7656546Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7656679Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7656842Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7656846Z 
2025-04-11T04:23:18.7657084Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7657241Z _______________ test_copy_kv_to_caches[True-1-False-16-16-64-32] _______________
2025-04-11T04:23:18.7657248Z 
2025-04-11T04:23:18.7657398Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7657548Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7657552Z 
2025-04-11T04:23:18.7657753Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7657857Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7657983Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7658116Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7658234Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7658373Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7658482Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7658635Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7658721Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7658798Z         bsz: int,
2025-04-11T04:23:18.7658878Z         block_size: int,
2025-04-11T04:23:18.7658966Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7659052Z         num_kv_heads: int,
2025-04-11T04:23:18.7659135Z         same_context_len: bool,
2025-04-11T04:23:18.7659214Z         n_tokens: int,
2025-04-11T04:23:18.7659303Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7659376Z     ):
2025-04-11T04:23:18.7659462Z         torch.manual_seed(123)
2025-04-11T04:23:18.7659550Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7659641Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7659645Z 
2025-04-11T04:23:18.7659875Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7659993Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7659999Z 
2025-04-11T04:23:18.7660075Z device = None
2025-04-11T04:23:18.7660078Z 
2025-04-11T04:23:18.7660198Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7660349Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7660418Z     
2025-04-11T04:23:18.7660493Z         Args:
2025-04-11T04:23:18.7660661Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7660832Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7660936Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7661010Z         """
2025-04-11T04:23:18.7661088Z         _lazy_init()
2025-04-11T04:23:18.7661184Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7661289Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7661392Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7661729Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7661862Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7662017Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7662024Z 
2025-04-11T04:23:18.7662258Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7662464Z ________________ test_copy_kv_to_caches[True-5-True-16-16-16-7] ________________
2025-04-11T04:23:18.7662468Z 
2025-04-11T04:23:18.7662615Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7662772Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7662779Z 
2025-04-11T04:23:18.7662976Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7663078Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7663203Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7663334Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7663448Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7663583Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7663689Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7663841Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7663927Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7664004Z         bsz: int,
2025-04-11T04:23:18.7664083Z         block_size: int,
2025-04-11T04:23:18.7664174Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7664258Z         num_kv_heads: int,
2025-04-11T04:23:18.7664342Z         same_context_len: bool,
2025-04-11T04:23:18.7664421Z         n_tokens: int,
2025-04-11T04:23:18.7664507Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7664581Z     ):
2025-04-11T04:23:18.7664664Z         torch.manual_seed(123)
2025-04-11T04:23:18.7664749Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7664840Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7664843Z 
2025-04-11T04:23:18.7664991Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7665104Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7665108Z 
2025-04-11T04:23:18.7665184Z device = None
2025-04-11T04:23:18.7665188Z 
2025-04-11T04:23:18.7665306Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7665454Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7665526Z     
2025-04-11T04:23:18.7665687Z         Args:
2025-04-11T04:23:18.7665857Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7666026Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7666130Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7666205Z         """
2025-04-11T04:23:18.7666280Z         _lazy_init()
2025-04-11T04:23:18.7666373Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7666476Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7666580Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7666863Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7666999Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7667158Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7667166Z 
2025-04-11T04:23:18.7667399Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7667549Z _______________ test_copy_kv_to_caches[True-5-True-16-16-16-32] ________________
2025-04-11T04:23:18.7667607Z 
2025-04-11T04:23:18.7667755Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7667906Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7667910Z 
2025-04-11T04:23:18.7668108Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7668212Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7668440Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7668574Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7668688Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7668829Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7668935Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7669087Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7669175Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7669252Z         bsz: int,
2025-04-11T04:23:18.7669331Z         block_size: int,
2025-04-11T04:23:18.7669419Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7669504Z         num_kv_heads: int,
2025-04-11T04:23:18.7669588Z         same_context_len: bool,
2025-04-11T04:23:18.7669670Z         n_tokens: int,
2025-04-11T04:23:18.7669757Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7669831Z     ):
2025-04-11T04:23:18.7669915Z         torch.manual_seed(123)
2025-04-11T04:23:18.7670001Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7670093Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7670097Z 
2025-04-11T04:23:18.7670245Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7670362Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7670366Z 
2025-04-11T04:23:18.7670442Z device = None
2025-04-11T04:23:18.7670445Z 
2025-04-11T04:23:18.7670565Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7670712Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7670781Z     
2025-04-11T04:23:18.7670856Z         Args:
2025-04-11T04:23:18.7671022Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7671190Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7671294Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7671368Z         """
2025-04-11T04:23:18.7671443Z         _lazy_init()
2025-04-11T04:23:18.7671535Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7671637Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7671832Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7672119Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7672254Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7672413Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7672417Z 
2025-04-11T04:23:18.7672660Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7672811Z ________________ test_copy_kv_to_caches[True-5-True-16-16-32-7] ________________
2025-04-11T04:23:18.7672817Z 
2025-04-11T04:23:18.7672968Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7673118Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7673122Z 
2025-04-11T04:23:18.7673329Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7673434Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7673560Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7673749Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7673863Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7674001Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7674109Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7674261Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7674348Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7674479Z         bsz: int,
2025-04-11T04:23:18.7674558Z         block_size: int,
2025-04-11T04:23:18.7674644Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7674729Z         num_kv_heads: int,
2025-04-11T04:23:18.7674811Z         same_context_len: bool,
2025-04-11T04:23:18.7674894Z         n_tokens: int,
2025-04-11T04:23:18.7674981Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7675052Z     ):
2025-04-11T04:23:18.7675140Z         torch.manual_seed(123)
2025-04-11T04:23:18.7675229Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7675319Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7675323Z 
2025-04-11T04:23:18.7675471Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7675583Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7675587Z 
2025-04-11T04:23:18.7675662Z device = None
2025-04-11T04:23:18.7675666Z 
2025-04-11T04:23:18.7675784Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7675936Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7676003Z     
2025-04-11T04:23:18.7676079Z         Args:
2025-04-11T04:23:18.7676248Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7676418Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7676521Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7676594Z         """
2025-04-11T04:23:18.7676673Z         _lazy_init()
2025-04-11T04:23:18.7676766Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7676868Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7676970Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7677254Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7677390Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7677545Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7677549Z 
2025-04-11T04:23:18.7677867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7678023Z _______________ test_copy_kv_to_caches[True-5-True-16-16-32-32] ________________
2025-04-11T04:23:18.7678027Z 
2025-04-11T04:23:18.7678178Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7678331Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7678335Z 
2025-04-11T04:23:18.7678535Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7678638Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7678759Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7678894Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7679006Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7679150Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7679256Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7679412Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7679499Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7679575Z         bsz: int,
2025-04-11T04:23:18.7679710Z         block_size: int,
2025-04-11T04:23:18.7679798Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7679883Z         num_kv_heads: int,
2025-04-11T04:23:18.7679966Z         same_context_len: bool,
2025-04-11T04:23:18.7680046Z         n_tokens: int,
2025-04-11T04:23:18.7680134Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7680203Z     ):
2025-04-11T04:23:18.7680291Z         torch.manual_seed(123)
2025-04-11T04:23:18.7680378Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7680521Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7680525Z 
2025-04-11T04:23:18.7680677Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7680790Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7680797Z 
2025-04-11T04:23:18.7680872Z device = None
2025-04-11T04:23:18.7680875Z 
2025-04-11T04:23:18.7680989Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7681142Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7681213Z     
2025-04-11T04:23:18.7681288Z         Args:
2025-04-11T04:23:18.7681456Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7681624Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7681726Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7681798Z         """
2025-04-11T04:23:18.7681877Z         _lazy_init()
2025-04-11T04:23:18.7681969Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7682070Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7682172Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7682459Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7682594Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7682751Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7682755Z 
2025-04-11T04:23:18.7682997Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7683155Z ________________ test_copy_kv_to_caches[True-5-True-16-16-64-7] ________________
2025-04-11T04:23:18.7683159Z 
2025-04-11T04:23:18.7683308Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7683461Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7683465Z 
2025-04-11T04:23:18.7683665Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7683846Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7683972Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7684105Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7684218Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7684359Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7684465Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7684616Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7684702Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7684775Z         bsz: int,
2025-04-11T04:23:18.7684859Z         block_size: int,
2025-04-11T04:23:18.7684947Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7685031Z         num_kv_heads: int,
2025-04-11T04:23:18.7685115Z         same_context_len: bool,
2025-04-11T04:23:18.7685195Z         n_tokens: int,
2025-04-11T04:23:18.7685281Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7685355Z     ):
2025-04-11T04:23:18.7685446Z         torch.manual_seed(123)
2025-04-11T04:23:18.7685533Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7685623Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7685697Z 
2025-04-11T04:23:18.7685849Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7685958Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7685965Z 
2025-04-11T04:23:18.7686040Z device = None
2025-04-11T04:23:18.7686044Z 
2025-04-11T04:23:18.7686161Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7686315Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7686436Z     
2025-04-11T04:23:18.7686511Z         Args:
2025-04-11T04:23:18.7686678Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7686846Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7686951Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7687022Z         """
2025-04-11T04:23:18.7687102Z         _lazy_init()
2025-04-11T04:23:18.7687197Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7687300Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7687402Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7687685Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7687821Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7687978Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7687982Z 
2025-04-11T04:23:18.7688221Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7688376Z _______________ test_copy_kv_to_caches[True-5-True-16-16-64-32] ________________
2025-04-11T04:23:18.7688379Z 
2025-04-11T04:23:18.7688527Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7688676Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7688682Z 
2025-04-11T04:23:18.7688883Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7688986Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7689105Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7689242Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7689351Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7689493Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7689599Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7689749Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7689920Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7689996Z         bsz: int,
2025-04-11T04:23:18.7690079Z         block_size: int,
2025-04-11T04:23:18.7690166Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7690252Z         num_kv_heads: int,
2025-04-11T04:23:18.7690333Z         same_context_len: bool,
2025-04-11T04:23:18.7690409Z         n_tokens: int,
2025-04-11T04:23:18.7690499Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7690567Z     ):
2025-04-11T04:23:18.7690654Z         torch.manual_seed(123)
2025-04-11T04:23:18.7690740Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7690826Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7690836Z 
2025-04-11T04:23:18.7690984Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7691094Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7691099Z 
2025-04-11T04:23:18.7691176Z device = None
2025-04-11T04:23:18.7691180Z 
2025-04-11T04:23:18.7691298Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7691450Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7691518Z     
2025-04-11T04:23:18.7691659Z         Args:
2025-04-11T04:23:18.7691824Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7691990Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7692097Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7692168Z         """
2025-04-11T04:23:18.7692245Z         _lazy_init()
2025-04-11T04:23:18.7692336Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7692486Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7692591Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7692873Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7693011Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7693165Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7693171Z 
2025-04-11T04:23:18.7693407Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7693561Z _______________ test_copy_kv_to_caches[True-5-False-16-16-16-7] ________________
2025-04-11T04:23:18.7693565Z 
2025-04-11T04:23:18.7693713Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7693864Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7693870Z 
2025-04-11T04:23:18.7694071Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7694172Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7694305Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7694438Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7694547Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7694688Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7694793Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7694941Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7695027Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7695099Z         bsz: int,
2025-04-11T04:23:18.7695183Z         block_size: int,
2025-04-11T04:23:18.7695269Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7695353Z         num_kv_heads: int,
2025-04-11T04:23:18.7695436Z         same_context_len: bool,
2025-04-11T04:23:18.7695516Z         n_tokens: int,
2025-04-11T04:23:18.7695602Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7695672Z     ):
2025-04-11T04:23:18.7695761Z         torch.manual_seed(123)
2025-04-11T04:23:18.7695934Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7696031Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7696035Z 
2025-04-11T04:23:18.7696183Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7696294Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7696301Z 
2025-04-11T04:23:18.7696375Z device = None
2025-04-11T04:23:18.7696379Z 
2025-04-11T04:23:18.7696494Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7696645Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7696714Z     
2025-04-11T04:23:18.7696790Z         Args:
2025-04-11T04:23:18.7696956Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7697119Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7697230Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7697303Z         """
2025-04-11T04:23:18.7697381Z         _lazy_init()
2025-04-11T04:23:18.7697472Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7697575Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7697730Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7698013Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7698151Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7698308Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7698361Z 
2025-04-11T04:23:18.7698600Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7698751Z _______________ test_copy_kv_to_caches[True-5-False-16-16-16-32] _______________
2025-04-11T04:23:18.7698755Z 
2025-04-11T04:23:18.7698910Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7699060Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7699064Z 
2025-04-11T04:23:18.7699265Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7699369Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7699490Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7699628Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7699737Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7699878Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7699986Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7700136Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7700221Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7700295Z         bsz: int,
2025-04-11T04:23:18.7700381Z         block_size: int,
2025-04-11T04:23:18.7700467Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7700551Z         num_kv_heads: int,
2025-04-11T04:23:18.7700634Z         same_context_len: bool,
2025-04-11T04:23:18.7700712Z         n_tokens: int,
2025-04-11T04:23:18.7700802Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7700870Z     ):
2025-04-11T04:23:18.7700957Z         torch.manual_seed(123)
2025-04-11T04:23:18.7701043Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7701131Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7701138Z 
2025-04-11T04:23:18.7701287Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7701398Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7701402Z 
2025-04-11T04:23:18.7701481Z device = None
2025-04-11T04:23:18.7701485Z 
2025-04-11T04:23:18.7701599Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7701839Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7701910Z     
2025-04-11T04:23:18.7701986Z         Args:
2025-04-11T04:23:18.7702153Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7702321Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7702429Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7702500Z         """
2025-04-11T04:23:18.7702579Z         _lazy_init()
2025-04-11T04:23:18.7702671Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7702771Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7702878Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7703158Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7703297Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7703453Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7703457Z 
2025-04-11T04:23:18.7703693Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7703897Z _______________ test_copy_kv_to_caches[True-5-False-16-16-32-7] ________________
2025-04-11T04:23:18.7703901Z 
2025-04-11T04:23:18.7704048Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7704199Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7704203Z 
2025-04-11T04:23:18.7704404Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7704557Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7704677Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7704813Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7704927Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7705067Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7705172Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7705323Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7705412Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7705485Z         bsz: int,
2025-04-11T04:23:18.7705568Z         block_size: int,
2025-04-11T04:23:18.7705654Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7705739Z         num_kv_heads: int,
2025-04-11T04:23:18.7705821Z         same_context_len: bool,
2025-04-11T04:23:18.7705900Z         n_tokens: int,
2025-04-11T04:23:18.7705992Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7706061Z     ):
2025-04-11T04:23:18.7706149Z         torch.manual_seed(123)
2025-04-11T04:23:18.7706234Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7706320Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7706327Z 
2025-04-11T04:23:18.7706480Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7706590Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7706596Z 
2025-04-11T04:23:18.7706673Z device = None
2025-04-11T04:23:18.7706677Z 
2025-04-11T04:23:18.7706791Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7706940Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7707008Z     
2025-04-11T04:23:18.7707080Z         Args:
2025-04-11T04:23:18.7707249Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7707419Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7707530Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7707602Z         """
2025-04-11T04:23:18.7707680Z         _lazy_init()
2025-04-11T04:23:18.7707856Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7707959Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7708064Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7708352Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7708533Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7708688Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7708692Z 
2025-04-11T04:23:18.7708930Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7709088Z _______________ test_copy_kv_to_caches[True-5-False-16-16-32-32] _______________
2025-04-11T04:23:18.7709092Z 
2025-04-11T04:23:18.7709242Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7709399Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7709403Z 
2025-04-11T04:23:18.7709599Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7709765Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7709885Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7710022Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7710133Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7710274Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7710379Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7710578Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7710670Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7710742Z         bsz: int,
2025-04-11T04:23:18.7710823Z         block_size: int,
2025-04-11T04:23:18.7710913Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7710995Z         num_kv_heads: int,
2025-04-11T04:23:18.7711081Z         same_context_len: bool,
2025-04-11T04:23:18.7711158Z         n_tokens: int,
2025-04-11T04:23:18.7711246Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7711317Z     ):
2025-04-11T04:23:18.7711405Z         torch.manual_seed(123)
2025-04-11T04:23:18.7711491Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7711579Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7711583Z 
2025-04-11T04:23:18.7711735Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7711843Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7711849Z 
2025-04-11T04:23:18.7711927Z device = None
2025-04-11T04:23:18.7711931Z 
2025-04-11T04:23:18.7712045Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7712195Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7712263Z     
2025-04-11T04:23:18.7712338Z         Args:
2025-04-11T04:23:18.7712507Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7712674Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7712781Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7712853Z         """
2025-04-11T04:23:18.7712931Z         _lazy_init()
2025-04-11T04:23:18.7713022Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7713122Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7713227Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7713510Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7713645Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7713912Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7713917Z 
2025-04-11T04:23:18.7714160Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7714314Z _______________ test_copy_kv_to_caches[True-5-False-16-16-64-7] ________________
2025-04-11T04:23:18.7714319Z 
2025-04-11T04:23:18.7714464Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7714618Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7714622Z 
2025-04-11T04:23:18.7714818Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7714924Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7715047Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7715180Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7715290Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7715433Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7715538Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7715684Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7715829Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7715902Z         bsz: int,
2025-04-11T04:23:18.7715985Z         block_size: int,
2025-04-11T04:23:18.7716072Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7716152Z         num_kv_heads: int,
2025-04-11T04:23:18.7716241Z         same_context_len: bool,
2025-04-11T04:23:18.7716318Z         n_tokens: int,
2025-04-11T04:23:18.7716409Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7716529Z     ):
2025-04-11T04:23:18.7716616Z         torch.manual_seed(123)
2025-04-11T04:23:18.7716707Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7716796Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7716800Z 
2025-04-11T04:23:18.7716956Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7717067Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7717071Z 
2025-04-11T04:23:18.7717148Z device = None
2025-04-11T04:23:18.7717154Z 
2025-04-11T04:23:18.7717271Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7717420Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7717488Z     
2025-04-11T04:23:18.7717558Z         Args:
2025-04-11T04:23:18.7717729Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7717893Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7718001Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7718072Z         """
2025-04-11T04:23:18.7718148Z         _lazy_init()
2025-04-11T04:23:18.7718245Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7718348Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7718455Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7718736Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7718876Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7719032Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7719036Z 
2025-04-11T04:23:18.7719271Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7719426Z _______________ test_copy_kv_to_caches[True-5-False-16-16-64-32] _______________
2025-04-11T04:23:18.7719432Z 
2025-04-11T04:23:18.7719578Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7719735Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7719739Z 
2025-04-11T04:23:18.7720015Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7720125Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7720245Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7720383Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7720493Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7720632Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7720743Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7720891Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7720981Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7721054Z         bsz: int,
2025-04-11T04:23:18.7721137Z         block_size: int,
2025-04-11T04:23:18.7721224Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7721304Z         num_kv_heads: int,
2025-04-11T04:23:18.7721398Z         same_context_len: bool,
2025-04-11T04:23:18.7721475Z         n_tokens: int,
2025-04-11T04:23:18.7721565Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7721635Z     ):
2025-04-11T04:23:18.7721719Z         torch.manual_seed(123)
2025-04-11T04:23:18.7721863Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7721953Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7721958Z 
2025-04-11T04:23:18.7722109Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7722219Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7722223Z 
2025-04-11T04:23:18.7722300Z device = None
2025-04-11T04:23:18.7722304Z 
2025-04-11T04:23:18.7722457Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7722604Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7722677Z     
2025-04-11T04:23:18.7722748Z         Args:
2025-04-11T04:23:18.7722920Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7723085Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7723189Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7723262Z         """
2025-04-11T04:23:18.7723337Z         _lazy_init()
2025-04-11T04:23:18.7723432Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7723532Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7723637Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7723917Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7724055Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7724209Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7724213Z 
2025-04-11T04:23:18.7724448Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7724603Z _______________ test_copy_kv_to_caches[False-1-True-16-16-16-7] ________________
2025-04-11T04:23:18.7724606Z 
2025-04-11T04:23:18.7724753Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7724908Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7724911Z 
2025-04-11T04:23:18.7725108Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7725211Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7725331Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7725468Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7725578Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7725716Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7725909Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7726060Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7726149Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7726227Z         bsz: int,
2025-04-11T04:23:18.7726307Z         block_size: int,
2025-04-11T04:23:18.7726400Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7726480Z         num_kv_heads: int,
2025-04-11T04:23:18.7726567Z         same_context_len: bool,
2025-04-11T04:23:18.7726645Z         n_tokens: int,
2025-04-11T04:23:18.7726735Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7726804Z     ):
2025-04-11T04:23:18.7726889Z         torch.manual_seed(123)
2025-04-11T04:23:18.7726981Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7727070Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7727074Z 
2025-04-11T04:23:18.7727225Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7727337Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7727341Z 
2025-04-11T04:23:18.7727416Z device = None
2025-04-11T04:23:18.7727423Z 
2025-04-11T04:23:18.7727538Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7727735Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7727807Z     
2025-04-11T04:23:18.7727877Z         Args:
2025-04-11T04:23:18.7728046Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7728210Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7728313Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7728437Z         """
2025-04-11T04:23:18.7728513Z         _lazy_init()
2025-04-11T04:23:18.7728610Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7728710Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7728821Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7729106Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7729239Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7729401Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7729405Z 
2025-04-11T04:23:18.7729640Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7729795Z _______________ test_copy_kv_to_caches[False-1-True-16-16-16-32] _______________
2025-04-11T04:23:18.7729799Z 
2025-04-11T04:23:18.7729945Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7730101Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7730105Z 
2025-04-11T04:23:18.7730300Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7730407Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7730530Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7730661Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7730778Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7730914Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7731026Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7731173Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7731261Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7731334Z         bsz: int,
2025-04-11T04:23:18.7731415Z         block_size: int,
2025-04-11T04:23:18.7731508Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7731590Z         num_kv_heads: int,
2025-04-11T04:23:18.7731674Z         same_context_len: bool,
2025-04-11T04:23:18.7731751Z         n_tokens: int,
2025-04-11T04:23:18.7731918Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7731993Z     ):
2025-04-11T04:23:18.7732078Z         torch.manual_seed(123)
2025-04-11T04:23:18.7732167Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7732256Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7732260Z 
2025-04-11T04:23:18.7732410Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7732523Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7732526Z 
2025-04-11T04:23:18.7732599Z device = None
2025-04-11T04:23:18.7732603Z 
2025-04-11T04:23:18.7732720Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7732869Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7732942Z     
2025-04-11T04:23:18.7733013Z         Args:
2025-04-11T04:23:18.7733179Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7733344Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7733445Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7733520Z         """
2025-04-11T04:23:18.7733648Z         _lazy_init()
2025-04-11T04:23:18.7733744Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7733844Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7733945Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7734230Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7734363Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7734577Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7734581Z 
2025-04-11T04:23:18.7734817Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7734974Z _______________ test_copy_kv_to_caches[False-1-True-16-16-32-7] ________________
2025-04-11T04:23:18.7734978Z 
2025-04-11T04:23:18.7735124Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7735281Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7735284Z 
2025-04-11T04:23:18.7735482Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7735587Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7735708Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7735839Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7735958Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7736094Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7736204Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7736354Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7736441Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7736518Z         bsz: int,
2025-04-11T04:23:18.7736597Z         block_size: int,
2025-04-11T04:23:18.7736689Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7736769Z         num_kv_heads: int,
2025-04-11T04:23:18.7736854Z         same_context_len: bool,
2025-04-11T04:23:18.7736930Z         n_tokens: int,
2025-04-11T04:23:18.7737016Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7737087Z     ):
2025-04-11T04:23:18.7737171Z         torch.manual_seed(123)
2025-04-11T04:23:18.7737261Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7737348Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7737354Z 
2025-04-11T04:23:18.7737502Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7737615Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7737619Z 
2025-04-11T04:23:18.7737696Z device = None
2025-04-11T04:23:18.7737891Z 
2025-04-11T04:23:18.7738014Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7738165Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7738240Z     
2025-04-11T04:23:18.7738311Z         Args:
2025-04-11T04:23:18.7738480Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7738652Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7738759Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7738836Z         """
2025-04-11T04:23:18.7738913Z         _lazy_init()
2025-04-11T04:23:18.7739015Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7739115Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7739218Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7739512Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7739644Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7739803Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7739857Z 
2025-04-11T04:23:18.7740093Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7740249Z _______________ test_copy_kv_to_caches[False-1-True-16-16-32-32] _______________
2025-04-11T04:23:18.7740253Z 
2025-04-11T04:23:18.7740398Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7740551Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7740602Z 
2025-04-11T04:23:18.7740801Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7740903Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7741030Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7741163Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7741277Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7741414Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7741524Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7741669Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7741756Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7741836Z         bsz: int,
2025-04-11T04:23:18.7741918Z         block_size: int,
2025-04-11T04:23:18.7742010Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7742095Z         num_kv_heads: int,
2025-04-11T04:23:18.7742178Z         same_context_len: bool,
2025-04-11T04:23:18.7742258Z         n_tokens: int,
2025-04-11T04:23:18.7742345Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7742415Z     ):
2025-04-11T04:23:18.7742503Z         torch.manual_seed(123)
2025-04-11T04:23:18.7742596Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7742683Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7742687Z 
2025-04-11T04:23:18.7742834Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7742949Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7742953Z 
2025-04-11T04:23:18.7743028Z device = None
2025-04-11T04:23:18.7743031Z 
2025-04-11T04:23:18.7743150Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7743298Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7743371Z     
2025-04-11T04:23:18.7743443Z         Args:
2025-04-11T04:23:18.7743607Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7743775Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7743971Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7744047Z         """
2025-04-11T04:23:18.7744122Z         _lazy_init()
2025-04-11T04:23:18.7744217Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7744320Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7744423Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7744709Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7744842Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7745003Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7745009Z 
2025-04-11T04:23:18.7745244Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7745397Z _______________ test_copy_kv_to_caches[False-1-True-16-16-64-7] ________________
2025-04-11T04:23:18.7745401Z 
2025-04-11T04:23:18.7745551Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7745701Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7745759Z 
2025-04-11T04:23:18.7745961Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7746062Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7746187Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7746319Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7746432Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7746619Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7746728Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7746877Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7746964Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7747045Z         bsz: int,
2025-04-11T04:23:18.7747125Z         block_size: int,
2025-04-11T04:23:18.7747215Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7747297Z         num_kv_heads: int,
2025-04-11T04:23:18.7747381Z         same_context_len: bool,
2025-04-11T04:23:18.7747461Z         n_tokens: int,
2025-04-11T04:23:18.7747547Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7747619Z     ):
2025-04-11T04:23:18.7747702Z         torch.manual_seed(123)
2025-04-11T04:23:18.7747785Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7747876Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7747879Z 
2025-04-11T04:23:18.7748027Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7748143Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7748147Z 
2025-04-11T04:23:18.7748221Z device = None
2025-04-11T04:23:18.7748224Z 
2025-04-11T04:23:18.7748342Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7748544Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7748618Z     
2025-04-11T04:23:18.7748690Z         Args:
2025-04-11T04:23:18.7748859Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7749030Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7749133Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7749207Z         """
2025-04-11T04:23:18.7749283Z         _lazy_init()
2025-04-11T04:23:18.7749375Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7749481Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7749588Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7749880Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7750110Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7750272Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7750276Z 
2025-04-11T04:23:18.7750511Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7750666Z _______________ test_copy_kv_to_caches[False-1-True-16-16-64-32] _______________
2025-04-11T04:23:18.7750670Z 
2025-04-11T04:23:18.7750816Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7750964Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7750968Z 
2025-04-11T04:23:18.7751169Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7751271Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7751395Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7751529Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7751645Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7751780Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7751942Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7752094Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7752180Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7752256Z         bsz: int,
2025-04-11T04:23:18.7752335Z         block_size: int,
2025-04-11T04:23:18.7752427Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7752508Z         num_kv_heads: int,
2025-04-11T04:23:18.7752647Z         same_context_len: bool,
2025-04-11T04:23:18.7752728Z         n_tokens: int,
2025-04-11T04:23:18.7752815Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7752887Z     ):
2025-04-11T04:23:18.7752973Z         torch.manual_seed(123)
2025-04-11T04:23:18.7753060Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7753153Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7753156Z 
2025-04-11T04:23:18.7753305Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7753418Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7753423Z 
2025-04-11T04:23:18.7753497Z device = None
2025-04-11T04:23:18.7753501Z 
2025-04-11T04:23:18.7753617Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7753764Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7753831Z     
2025-04-11T04:23:18.7753906Z         Args:
2025-04-11T04:23:18.7754071Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7754241Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7754344Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7754417Z         """
2025-04-11T04:23:18.7754496Z         _lazy_init()
2025-04-11T04:23:18.7754588Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7754690Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7754791Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7755074Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7755206Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7755364Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7755368Z 
2025-04-11T04:23:18.7755602Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7755758Z _______________ test_copy_kv_to_caches[False-1-False-16-16-16-7] _______________
2025-04-11T04:23:18.7755766Z 
2025-04-11T04:23:18.7755913Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7756147Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7756152Z 
2025-04-11T04:23:18.7756357Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7756462Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7756588Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7756719Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7756834Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7756970Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7757079Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7757230Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7757315Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7757391Z         bsz: int,
2025-04-11T04:23:18.7757469Z         block_size: int,
2025-04-11T04:23:18.7757557Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7757641Z         num_kv_heads: int,
2025-04-11T04:23:18.7757724Z         same_context_len: bool,
2025-04-11T04:23:18.7757803Z         n_tokens: int,
2025-04-11T04:23:18.7757943Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7758011Z     ):
2025-04-11T04:23:18.7758098Z         torch.manual_seed(123)
2025-04-11T04:23:18.7758185Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7758278Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7758281Z 
2025-04-11T04:23:18.7758427Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7758540Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7758605Z 
2025-04-11T04:23:18.7758682Z device = None
2025-04-11T04:23:18.7758685Z 
2025-04-11T04:23:18.7758803Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7758955Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7759025Z     
2025-04-11T04:23:18.7759098Z         Args:
2025-04-11T04:23:18.7759262Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7759431Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7759532Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7759603Z         """
2025-04-11T04:23:18.7759681Z         _lazy_init()
2025-04-11T04:23:18.7759772Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7759877Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7759980Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7760270Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7760403Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7760559Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7760567Z 
2025-04-11T04:23:18.7760797Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7760954Z ______________ test_copy_kv_to_caches[False-1-False-16-16-16-32] _______________
2025-04-11T04:23:18.7760958Z 
2025-04-11T04:23:18.7761109Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7761264Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7761268Z 
2025-04-11T04:23:18.7761469Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7761574Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7761699Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7761831Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7762056Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7762201Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7762308Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7762463Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7762550Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7762625Z         bsz: int,
2025-04-11T04:23:18.7762705Z         block_size: int,
2025-04-11T04:23:18.7762792Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7762875Z         num_kv_heads: int,
2025-04-11T04:23:18.7762957Z         same_context_len: bool,
2025-04-11T04:23:18.7763038Z         n_tokens: int,
2025-04-11T04:23:18.7763129Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7763197Z     ):
2025-04-11T04:23:18.7763286Z         torch.manual_seed(123)
2025-04-11T04:23:18.7763372Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7763461Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7763465Z 
2025-04-11T04:23:18.7763617Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7763732Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7763736Z 
2025-04-11T04:23:18.7763863Z device = None
2025-04-11T04:23:18.7763867Z 
2025-04-11T04:23:18.7763984Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7764141Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7764211Z     
2025-04-11T04:23:18.7764286Z         Args:
2025-04-11T04:23:18.7764451Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7764617Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7764770Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7764842Z         """
2025-04-11T04:23:18.7764920Z         _lazy_init()
2025-04-11T04:23:18.7765012Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7765118Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7765223Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7765505Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7765640Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7765797Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7765801Z 
2025-04-11T04:23:18.7766041Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7766196Z _______________ test_copy_kv_to_caches[False-1-False-16-16-32-7] _______________
2025-04-11T04:23:18.7766200Z 
2025-04-11T04:23:18.7766348Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7766501Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7766508Z 
2025-04-11T04:23:18.7766711Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7766814Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7766940Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7767072Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7767183Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7767327Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7767433Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7767586Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7767672Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7767744Z         bsz: int,
2025-04-11T04:23:18.7767827Z         block_size: int,
2025-04-11T04:23:18.7767914Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7767997Z         num_kv_heads: int,
2025-04-11T04:23:18.7768163Z         same_context_len: bool,
2025-04-11T04:23:18.7768246Z         n_tokens: int,
2025-04-11T04:23:18.7768335Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7768407Z     ):
2025-04-11T04:23:18.7768494Z         torch.manual_seed(123)
2025-04-11T04:23:18.7768581Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7768673Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7768677Z 
2025-04-11T04:23:18.7768827Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7768936Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7768943Z 
2025-04-11T04:23:18.7769016Z device = None
2025-04-11T04:23:18.7769024Z 
2025-04-11T04:23:18.7769142Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7769293Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7769361Z     
2025-04-11T04:23:18.7769435Z         Args:
2025-04-11T04:23:18.7769605Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7769770Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7769925Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7770000Z         """
2025-04-11T04:23:18.7770083Z         _lazy_init()
2025-04-11T04:23:18.7770180Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7770288Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7770397Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7770684Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7770873Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7771029Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7771033Z 
2025-04-11T04:23:18.7771276Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7771430Z ______________ test_copy_kv_to_caches[False-1-False-16-16-32-32] _______________
2025-04-11T04:23:18.7771436Z 
2025-04-11T04:23:18.7771587Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7771741Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7771745Z 
2025-04-11T04:23:18.7771944Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7772048Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7772171Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7772305Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7772416Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7772558Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7772666Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7772819Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7772902Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7772978Z         bsz: int,
2025-04-11T04:23:18.7773063Z         block_size: int,
2025-04-11T04:23:18.7773150Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7773234Z         num_kv_heads: int,
2025-04-11T04:23:18.7773316Z         same_context_len: bool,
2025-04-11T04:23:18.7773391Z         n_tokens: int,
2025-04-11T04:23:18.7773482Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7773550Z     ):
2025-04-11T04:23:18.7773637Z         torch.manual_seed(123)
2025-04-11T04:23:18.7773723Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7773812Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7773816Z 
2025-04-11T04:23:18.7773961Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7774159Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7774164Z 
2025-04-11T04:23:18.7774247Z device = None
2025-04-11T04:23:18.7774251Z 
2025-04-11T04:23:18.7774369Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7774523Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7774592Z     
2025-04-11T04:23:18.7774665Z         Args:
2025-04-11T04:23:18.7774830Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7774994Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7775105Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7775176Z         """
2025-04-11T04:23:18.7775254Z         _lazy_init()
2025-04-11T04:23:18.7775346Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7775449Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7775554Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7775839Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7776032Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7776187Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7776191Z 
2025-04-11T04:23:18.7776428Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7776579Z _______________ test_copy_kv_to_caches[False-1-False-16-16-64-7] _______________
2025-04-11T04:23:18.7776632Z 
2025-04-11T04:23:18.7776784Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7776936Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7776940Z 
2025-04-11T04:23:18.7777142Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7777244Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7777365Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7777503Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7777615Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7777755Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7777860Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7778011Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7778098Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7778174Z         bsz: int,
2025-04-11T04:23:18.7778256Z         block_size: int,
2025-04-11T04:23:18.7778342Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7778426Z         num_kv_heads: int,
2025-04-11T04:23:18.7778509Z         same_context_len: bool,
2025-04-11T04:23:18.7778589Z         n_tokens: int,
2025-04-11T04:23:18.7778680Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7778750Z     ):
2025-04-11T04:23:18.7778837Z         torch.manual_seed(123)
2025-04-11T04:23:18.7778923Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7779012Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7779016Z 
2025-04-11T04:23:18.7779167Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7779276Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7779279Z 
2025-04-11T04:23:18.7779359Z device = None
2025-04-11T04:23:18.7779362Z 
2025-04-11T04:23:18.7779476Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7779629Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7779698Z     
2025-04-11T04:23:18.7779769Z         Args:
2025-04-11T04:23:18.7779937Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7780188Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7780298Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7780373Z         """
2025-04-11T04:23:18.7780453Z         _lazy_init()
2025-04-11T04:23:18.7780546Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7780645Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7780753Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7781036Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7781176Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7781332Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7781336Z 
2025-04-11T04:23:18.7781580Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7781734Z ______________ test_copy_kv_to_caches[False-1-False-16-16-64-32] _______________
2025-04-11T04:23:18.7781738Z 
2025-04-11T04:23:18.7781888Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7782113Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7782117Z 
2025-04-11T04:23:18.7782316Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7782422Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7782541Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7782683Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7782843Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7782984Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7783089Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7783239Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7783331Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7783404Z         bsz: int,
2025-04-11T04:23:18.7783490Z         block_size: int,
2025-04-11T04:23:18.7783576Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7783660Z         num_kv_heads: int,
2025-04-11T04:23:18.7783743Z         same_context_len: bool,
2025-04-11T04:23:18.7783818Z         n_tokens: int,
2025-04-11T04:23:18.7783908Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7783976Z     ):
2025-04-11T04:23:18.7784061Z         torch.manual_seed(123)
2025-04-11T04:23:18.7784145Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7784234Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7784237Z 
2025-04-11T04:23:18.7784388Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7784498Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7784502Z 
2025-04-11T04:23:18.7784582Z device = None
2025-04-11T04:23:18.7784585Z 
2025-04-11T04:23:18.7784700Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7784849Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7784918Z     
2025-04-11T04:23:18.7784988Z         Args:
2025-04-11T04:23:18.7785157Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7785324Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7785429Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7785500Z         """
2025-04-11T04:23:18.7785579Z         _lazy_init()
2025-04-11T04:23:18.7785671Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7785770Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7785875Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7786244Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7786384Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7786542Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7786546Z 
2025-04-11T04:23:18.7786784Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7786935Z _______________ test_copy_kv_to_caches[False-5-True-16-16-16-7] ________________
2025-04-11T04:23:18.7786938Z 
2025-04-11T04:23:18.7787084Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7787241Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7787244Z 
2025-04-11T04:23:18.7787443Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7787552Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7787673Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7787808Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7787974Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7788113Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7788218Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7788364Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7788489Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7788563Z         bsz: int,
2025-04-11T04:23:18.7788646Z         block_size: int,
2025-04-11T04:23:18.7788794Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7788875Z         num_kv_heads: int,
2025-04-11T04:23:18.7788961Z         same_context_len: bool,
2025-04-11T04:23:18.7789037Z         n_tokens: int,
2025-04-11T04:23:18.7789126Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7789199Z     ):
2025-04-11T04:23:18.7789282Z         torch.manual_seed(123)
2025-04-11T04:23:18.7789373Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7789460Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7789466Z 
2025-04-11T04:23:18.7789619Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7789728Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7789732Z 
2025-04-11T04:23:18.7789809Z device = None
2025-04-11T04:23:18.7789812Z 
2025-04-11T04:23:18.7789928Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7790078Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7790148Z     
2025-04-11T04:23:18.7790219Z         Args:
2025-04-11T04:23:18.7790388Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7790552Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7790660Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7790732Z         """
2025-04-11T04:23:18.7790807Z         _lazy_init()
2025-04-11T04:23:18.7790905Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7791005Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7791111Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7791396Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7791533Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7791686Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7791691Z 
2025-04-11T04:23:18.7791930Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7792171Z _______________ test_copy_kv_to_caches[False-5-True-16-16-16-32] _______________
2025-04-11T04:23:18.7792176Z 
2025-04-11T04:23:18.7792325Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7792480Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7792486Z 
2025-04-11T04:23:18.7792683Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7792790Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7792909Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7793042Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7793153Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7793291Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7793400Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7793544Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7793637Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7793711Z         bsz: int,
2025-04-11T04:23:18.7793791Z         block_size: int,
2025-04-11T04:23:18.7793878Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7794019Z         num_kv_heads: int,
2025-04-11T04:23:18.7794105Z         same_context_len: bool,
2025-04-11T04:23:18.7794181Z         n_tokens: int,
2025-04-11T04:23:18.7794268Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7794339Z     ):
2025-04-11T04:23:18.7794422Z         torch.manual_seed(123)
2025-04-11T04:23:18.7794511Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7794597Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7794601Z 
2025-04-11T04:23:18.7794803Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7794914Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7794917Z 
2025-04-11T04:23:18.7794993Z device = None
2025-04-11T04:23:18.7794997Z 
2025-04-11T04:23:18.7795114Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7795261Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7795333Z     
2025-04-11T04:23:18.7795405Z         Args:
2025-04-11T04:23:18.7795572Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7795735Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7795840Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7795910Z         """
2025-04-11T04:23:18.7795986Z         _lazy_init()
2025-04-11T04:23:18.7796083Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7796184Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7796290Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7796577Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7796713Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7796868Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7796874Z 
2025-04-11T04:23:18.7797109Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7797263Z _______________ test_copy_kv_to_caches[False-5-True-16-16-32-7] ________________
2025-04-11T04:23:18.7797267Z 
2025-04-11T04:23:18.7797409Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7797561Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7797566Z 
2025-04-11T04:23:18.7797762Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7797868Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7797987Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7798206Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7798319Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7798453Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7798563Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7798708Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7798794Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7798868Z         bsz: int,
2025-04-11T04:23:18.7798945Z         block_size: int,
2025-04-11T04:23:18.7799036Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7799117Z         num_kv_heads: int,
2025-04-11T04:23:18.7799204Z         same_context_len: bool,
2025-04-11T04:23:18.7799281Z         n_tokens: int,
2025-04-11T04:23:18.7799370Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7799440Z     ):
2025-04-11T04:23:18.7799523Z         torch.manual_seed(123)
2025-04-11T04:23:18.7799615Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7799702Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7799705Z 
2025-04-11T04:23:18.7799855Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7800019Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7800023Z 
2025-04-11T04:23:18.7800102Z device = None
2025-04-11T04:23:18.7800105Z 
2025-04-11T04:23:18.7800222Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7800368Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7800440Z     
2025-04-11T04:23:18.7800510Z         Args:
2025-04-11T04:23:18.7800713Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7800878Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7800986Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7801062Z         """
2025-04-11T04:23:18.7801138Z         _lazy_init()
2025-04-11T04:23:18.7801235Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7801339Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7801450Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7801733Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7801865Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7802026Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7802031Z 
2025-04-11T04:23:18.7802270Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7802428Z _______________ test_copy_kv_to_caches[False-5-True-16-16-32-32] _______________
2025-04-11T04:23:18.7802432Z 
2025-04-11T04:23:18.7802581Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7802735Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7802739Z 
2025-04-11T04:23:18.7802935Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7803042Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7803161Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7803293Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7803407Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7803546Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7803657Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7803806Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7803896Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7803969Z         bsz: int,
2025-04-11T04:23:18.7804165Z         block_size: int,
2025-04-11T04:23:18.7804259Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7804342Z         num_kv_heads: int,
2025-04-11T04:23:18.7804428Z         same_context_len: bool,
2025-04-11T04:23:18.7804510Z         n_tokens: int,
2025-04-11T04:23:18.7804601Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7804676Z     ):
2025-04-11T04:23:18.7804761Z         torch.manual_seed(123)
2025-04-11T04:23:18.7804851Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7804941Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7804945Z 
2025-04-11T04:23:18.7805104Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7805215Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7805219Z 
2025-04-11T04:23:18.7805294Z device = None
2025-04-11T04:23:18.7805298Z 
2025-04-11T04:23:18.7805421Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7805571Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7805644Z     
2025-04-11T04:23:18.7805715Z         Args:
2025-04-11T04:23:18.7805888Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7806103Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7806206Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7806280Z         """
2025-04-11T04:23:18.7806356Z         _lazy_init()
2025-04-11T04:23:18.7806452Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7806552Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7806709Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7806991Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7807125Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7807286Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7807290Z 
2025-04-11T04:23:18.7807527Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7808934Z _______________ test_copy_kv_to_caches[False-5-True-16-16-64-7] ________________
2025-04-11T04:23:18.7808941Z 
2025-04-11T04:23:18.7809094Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7809245Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7809252Z 
2025-04-11T04:23:18.7809452Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7809558Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7809682Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7809813Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7809929Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7810068Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7810176Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7810327Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7810436Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7810511Z         bsz: int,
2025-04-11T04:23:18.7810594Z         block_size: int,
2025-04-11T04:23:18.7810680Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7810764Z         num_kv_heads: int,
2025-04-11T04:23:18.7810846Z         same_context_len: bool,
2025-04-11T04:23:18.7810924Z         n_tokens: int,
2025-04-11T04:23:18.7811012Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7811082Z     ):
2025-04-11T04:23:18.7811170Z         torch.manual_seed(123)
2025-04-11T04:23:18.7811256Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7811343Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7811402Z 
2025-04-11T04:23:18.7811560Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7811671Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7811677Z 
2025-04-11T04:23:18.7811755Z device = None
2025-04-11T04:23:18.7811759Z 
2025-04-11T04:23:18.7811878Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7812029Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7812098Z     
2025-04-11T04:23:18.7812170Z         Args:
2025-04-11T04:23:18.7812336Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7812504Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7812611Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7812682Z         """
2025-04-11T04:23:18.7812760Z         _lazy_init()
2025-04-11T04:23:18.7812852Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7812951Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7813058Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7813395Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7813533Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7813689Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7813693Z 
2025-04-11T04:23:18.7813934Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7814137Z _______________ test_copy_kv_to_caches[False-5-True-16-16-64-32] _______________
2025-04-11T04:23:18.7814141Z 
2025-04-11T04:23:18.7814287Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7814441Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7814445Z 
2025-04-11T04:23:18.7814641Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7814751Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7814872Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7815075Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7815187Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7815330Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7815435Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7815582Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7815673Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7815746Z         bsz: int,
2025-04-11T04:23:18.7815830Z         block_size: int,
2025-04-11T04:23:18.7815916Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7816000Z         num_kv_heads: int,
2025-04-11T04:23:18.7816086Z         same_context_len: bool,
2025-04-11T04:23:18.7816163Z         n_tokens: int,
2025-04-11T04:23:18.7816252Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7816324Z     ):
2025-04-11T04:23:18.7816407Z         torch.manual_seed(123)
2025-04-11T04:23:18.7816499Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7816587Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7816591Z 
2025-04-11T04:23:18.7816743Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7816853Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7816859Z 
2025-04-11T04:23:18.7816936Z device = None
2025-04-11T04:23:18.7816939Z 
2025-04-11T04:23:18.7817055Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7817205Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7817274Z     
2025-04-11T04:23:18.7817396Z         Args:
2025-04-11T04:23:18.7817567Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7817729Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7817839Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7817912Z         """
2025-04-11T04:23:18.7817987Z         _lazy_init()
2025-04-11T04:23:18.7818082Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7818183Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7818290Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7818571Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7818711Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7818867Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7818873Z 
2025-04-11T04:23:18.7819114Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7819268Z _______________ test_copy_kv_to_caches[False-5-False-16-16-16-7] _______________
2025-04-11T04:23:18.7819322Z 
2025-04-11T04:23:18.7819471Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7819630Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7819634Z 
2025-04-11T04:23:18.7819832Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7819941Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7820113Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7820248Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7820358Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7820499Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7820608Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7820754Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7820845Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7820919Z         bsz: int,
2025-04-11T04:23:18.7821052Z         block_size: int,
2025-04-11T04:23:18.7821141Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7821222Z         num_kv_heads: int,
2025-04-11T04:23:18.7821308Z         same_context_len: bool,
2025-04-11T04:23:18.7821385Z         n_tokens: int,
2025-04-11T04:23:18.7821473Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7821544Z     ):
2025-04-11T04:23:18.7821628Z         torch.manual_seed(123)
2025-04-11T04:23:18.7821718Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7821805Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7821809Z 
2025-04-11T04:23:18.7821962Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7822075Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7822079Z 
2025-04-11T04:23:18.7822158Z device = None
2025-04-11T04:23:18.7822164Z 
2025-04-11T04:23:18.7822281Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7822431Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7822503Z     
2025-04-11T04:23:18.7822573Z         Args:
2025-04-11T04:23:18.7822741Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7822905Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7823015Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7823085Z         """
2025-04-11T04:23:18.7823160Z         _lazy_init()
2025-04-11T04:23:18.7823255Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7823405Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7823518Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7823797Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7823937Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7824096Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7824100Z 
2025-04-11T04:23:18.7824335Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7824491Z ______________ test_copy_kv_to_caches[False-5-False-16-16-16-32] _______________
2025-04-11T04:23:18.7824497Z 
2025-04-11T04:23:18.7824643Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7824801Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7824805Z 
2025-04-11T04:23:18.7825004Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7825111Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7825231Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7825418Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7825531Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7825671Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7825779Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7825926Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7826066Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7826140Z         bsz: int,
2025-04-11T04:23:18.7826221Z         block_size: int,
2025-04-11T04:23:18.7826313Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7826393Z         num_kv_heads: int,
2025-04-11T04:23:18.7826484Z         same_context_len: bool,
2025-04-11T04:23:18.7826566Z         n_tokens: int,
2025-04-11T04:23:18.7826658Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7826727Z     ):
2025-04-11T04:23:18.7826812Z         torch.manual_seed(123)
2025-04-11T04:23:18.7826904Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7826993Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7827065Z 
2025-04-11T04:23:18.7827222Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7827333Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7827337Z 
2025-04-11T04:23:18.7827413Z device = None
2025-04-11T04:23:18.7827417Z 
2025-04-11T04:23:18.7827542Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7827693Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7827762Z     
2025-04-11T04:23:18.7827836Z         Args:
2025-04-11T04:23:18.7828003Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7828165Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7828270Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7828345Z         """
2025-04-11T04:23:18.7828455Z         _lazy_init()
2025-04-11T04:23:18.7828552Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7828653Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7828756Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7829037Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7829176Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7829333Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7829337Z 
2025-04-11T04:23:18.7829633Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7829788Z _______________ test_copy_kv_to_caches[False-5-False-16-16-32-7] _______________
2025-04-11T04:23:18.7829792Z 
2025-04-11T04:23:18.7829942Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7830095Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7830099Z 
2025-04-11T04:23:18.7830300Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7830404Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7830523Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7830660Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7830770Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7830909Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7831016Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7831167Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7831254Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7831327Z         bsz: int,
2025-04-11T04:23:18.7831468Z         block_size: int,
2025-04-11T04:23:18.7831555Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7831639Z         num_kv_heads: int,
2025-04-11T04:23:18.7831723Z         same_context_len: bool,
2025-04-11T04:23:18.7831799Z         n_tokens: int,
2025-04-11T04:23:18.7831888Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7831957Z     ):
2025-04-11T04:23:18.7832043Z         torch.manual_seed(123)
2025-04-11T04:23:18.7832184Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7832274Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7832281Z 
2025-04-11T04:23:18.7832427Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7832537Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7832542Z 
2025-04-11T04:23:18.7832621Z device = None
2025-04-11T04:23:18.7832625Z 
2025-04-11T04:23:18.7832741Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7832898Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7832966Z     
2025-04-11T04:23:18.7833183Z         Args:
2025-04-11T04:23:18.7833351Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7833517Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7833623Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7833697Z         """
2025-04-11T04:23:18.7833777Z         _lazy_init()
2025-04-11T04:23:18.7833871Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7833970Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7834078Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7834359Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7834496Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7834655Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7834661Z 
2025-04-11T04:23:18.7834900Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7835054Z ______________ test_copy_kv_to_caches[False-5-False-16-16-32-32] _______________
2025-04-11T04:23:18.7835058Z 
2025-04-11T04:23:18.7835209Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7835364Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7835368Z 
2025-04-11T04:23:18.7835568Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7835722Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7835846Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7835983Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7836096Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7836238Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7836344Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7836492Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7836582Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7836654Z         bsz: int,
2025-04-11T04:23:18.7836741Z         block_size: int,
2025-04-11T04:23:18.7836827Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7836912Z         num_kv_heads: int,
2025-04-11T04:23:18.7836993Z         same_context_len: bool,
2025-04-11T04:23:18.7837070Z         n_tokens: int,
2025-04-11T04:23:18.7837162Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7837230Z     ):
2025-04-11T04:23:18.7837319Z         torch.manual_seed(123)
2025-04-11T04:23:18.7837405Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7837494Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7837551Z 
2025-04-11T04:23:18.7837706Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7837818Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7837822Z 
2025-04-11T04:23:18.7837902Z device = None
2025-04-11T04:23:18.7837906Z 
2025-04-11T04:23:18.7838021Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7838173Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7838298Z     
2025-04-11T04:23:18.7838371Z         Args:
2025-04-11T04:23:18.7838539Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7838707Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7838813Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7838883Z         """
2025-04-11T04:23:18.7838960Z         _lazy_init()
2025-04-11T04:23:18.7839056Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7839203Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7839313Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7839596Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7839732Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7839892Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7839896Z 
2025-04-11T04:23:18.7840136Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7840290Z _______________ test_copy_kv_to_caches[False-5-False-16-16-64-7] _______________
2025-04-11T04:23:18.7840293Z 
2025-04-11T04:23:18.7840443Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7840599Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7840603Z 
2025-04-11T04:23:18.7840804Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7840915Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7841034Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7841170Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7841283Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7841425Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7841532Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7841731Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7841823Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7841897Z         bsz: int,
2025-04-11T04:23:18.7841983Z         block_size: int,
2025-04-11T04:23:18.7842070Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7842153Z         num_kv_heads: int,
2025-04-11T04:23:18.7842242Z         same_context_len: bool,
2025-04-11T04:23:18.7842321Z         n_tokens: int,
2025-04-11T04:23:18.7842409Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7842478Z     ):
2025-04-11T04:23:18.7842564Z         torch.manual_seed(123)
2025-04-11T04:23:18.7842650Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7842738Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7842745Z 
2025-04-11T04:23:18.7842900Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7843007Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7843011Z 
2025-04-11T04:23:18.7843088Z device = None
2025-04-11T04:23:18.7843095Z 
2025-04-11T04:23:18.7843214Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7843364Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7843484Z     
2025-04-11T04:23:18.7843556Z         Args:
2025-04-11T04:23:18.7843732Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7843895Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7844001Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7844074Z         """
2025-04-11T04:23:18.7844148Z         _lazy_init()
2025-04-11T04:23:18.7844297Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7844399Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7844508Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7844793Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7844930Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7845091Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7845096Z 
2025-04-11T04:23:18.7845383Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7845539Z ______________ test_copy_kv_to_caches[False-5-False-16-16-64-32] _______________
2025-04-11T04:23:18.7845543Z 
2025-04-11T04:23:18.7845690Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T04:23:18.7845846Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7845851Z 
2025-04-11T04:23:18.7846048Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T04:23:18.7846156Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T04:23:18.7846277Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T04:23:18.7846412Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T04:23:18.7846524Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T04:23:18.7846666Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T04:23:18.7846779Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T04:23:18.7846927Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7847017Z     def test_copy_kv_to_caches(
2025-04-11T04:23:18.7847091Z         bsz: int,
2025-04-11T04:23:18.7847173Z         block_size: int,
2025-04-11T04:23:18.7847261Z         max_num_blocks_per_seq: int,
2025-04-11T04:23:18.7847344Z         num_kv_heads: int,
2025-04-11T04:23:18.7847433Z         same_context_len: bool,
2025-04-11T04:23:18.7847510Z         n_tokens: int,
2025-04-11T04:23:18.7847601Z         use_new_kcache_layout: bool,
2025-04-11T04:23:18.7847669Z     ):
2025-04-11T04:23:18.7847807Z         torch.manual_seed(123)
2025-04-11T04:23:18.7847903Z         torch.cuda.empty_cache()
2025-04-11T04:23:18.7847993Z >       torch.cuda.synchronize()
2025-04-11T04:23:18.7847997Z 
2025-04-11T04:23:18.7848150Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T04:23:18.7848262Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7848268Z 
2025-04-11T04:23:18.7848345Z device = None
2025-04-11T04:23:18.7848349Z 
2025-04-11T04:23:18.7848468Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.7848619Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7848688Z     
2025-04-11T04:23:18.7848762Z         Args:
2025-04-11T04:23:18.7848933Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.7849097Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.7849206Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.7849277Z         """
2025-04-11T04:23:18.7849355Z         _lazy_init()
2025-04-11T04:23:18.7849453Z         with torch.cuda.device(device):
2025-04-11T04:23:18.7849607Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7849716Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7850001Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7850141Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7850297Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7850363Z 
2025-04-11T04:23:18.7850602Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.7850737Z _______________________________ test_layer_norm ________________________________
2025-04-11T04:23:18.7850741Z 
2025-04-11T04:23:18.7850838Z kwargs = {}, val = 2, arg_map = {'M': 2}
2025-04-11T04:23:18.7851197Z partial_func = functools.partial(<function parameterize.<locals>._wrapper.<locals>._execute_function_by_param at 0x7fb5b8741750>, M=2)
2025-04-11T04:23:18.7851204Z 
2025-04-11T04:23:18.7851309Z     def _execute_function_by_param(**kwargs):
2025-04-11T04:23:18.7851440Z         for val in values:
2025-04-11T04:23:18.7851534Z             arg_map = {argument: val}
2025-04-11T04:23:18.7851642Z             partial_func = partial(func, **arg_map)
2025-04-11T04:23:18.7851731Z >           partial_func(**kwargs)
2025-04-11T04:23:18.7851735Z 
2025-04-11T04:23:18.7851827Z colossalai/testing/utils.py:64: 
2025-04-11T04:23:18.7851942Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7852100Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T04:23:18.7852187Z     partial_func(**kwargs)
2025-04-11T04:23:18.7852296Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7852300Z 
2025-04-11T04:23:18.7852373Z M = 2, N = 64
2025-04-11T04:23:18.7852377Z 
2025-04-11T04:23:18.7852466Z     @pytest.mark.skipif(
2025-04-11T04:23:18.7852715Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T04:23:18.7852792Z     )
2025-04-11T04:23:18.7852886Z     @parameterize("M", [2, 4, 8, 16])
2025-04-11T04:23:18.7852979Z     @parameterize("N", [64, 128])
2025-04-11T04:23:18.7853068Z     def test_layer_norm(M, N):
2025-04-11T04:23:18.7853158Z         dtype = torch.float16
2025-04-11T04:23:18.7853235Z         eps = 1e-5
2025-04-11T04:23:18.7853315Z         x_shape = (M, N)
2025-04-11T04:23:18.7853407Z         w_shape = (x_shape[-1],)
2025-04-11T04:23:18.7853547Z >       weight = torch.ones(w_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.7853655Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7853990Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7854128Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7854289Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7854295Z 
2025-04-11T04:23:18.7854479Z tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py:30: RuntimeError
2025-04-11T04:23:18.7854633Z ___________________ test_rotary_emb[True-dtype0-64-32-64-4] ____________________
2025-04-11T04:23:18.7854637Z 
2025-04-11T04:23:18.7854778Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
2025-04-11T04:23:18.7854872Z use_new_kcache_layout = True
2025-04-11T04:23:18.7854878Z 
2025-04-11T04:23:18.7854969Z     @pytest.mark.skipif(
2025-04-11T04:23:18.7855213Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T04:23:18.7855285Z     )
2025-04-11T04:23:18.7855398Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.7855511Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.7855609Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.7855708Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.7855886Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T04:23:18.7856044Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7856219Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
2025-04-11T04:23:18.7856315Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.7856419Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.7856600Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.7856728Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.7856821Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.7856992Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.7857084Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.7857203Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.7857309Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.7857407Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.7857542Z         cos_2 = cos[:, :32]
2025-04-11T04:23:18.7857627Z         sin_2 = sin[:, :32]
2025-04-11T04:23:18.7857747Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.7857883Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.7858097Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.7858230Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.7858298Z     
2025-04-11T04:23:18.7858378Z         # create data
2025-04-11T04:23:18.7858458Z         block_size = 32
2025-04-11T04:23:18.7858548Z         max_num_blocks_per_seq = 4
2025-04-11T04:23:18.7858643Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.7858785Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.7858899Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7859189Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7859327Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7859487Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7859490Z 
2025-04-11T04:23:18.7859693Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
2025-04-11T04:23:18.7859853Z ___________________ test_rotary_emb[False-dtype0-64-32-64-4] ___________________
2025-04-11T04:23:18.7859857Z 
2025-04-11T04:23:18.7859995Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
2025-04-11T04:23:18.7860136Z use_new_kcache_layout = False
2025-04-11T04:23:18.7860141Z 
2025-04-11T04:23:18.7860230Z     @pytest.mark.skipif(
2025-04-11T04:23:18.7860470Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T04:23:18.7860542Z     )
2025-04-11T04:23:18.7860651Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.7860759Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T04:23:18.7860854Z     @pytest.mark.parametrize("H", [32])
2025-04-11T04:23:18.7860950Z     @pytest.mark.parametrize("D", [64])
2025-04-11T04:23:18.7861073Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T04:23:18.7861230Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T04:23:18.7861408Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
2025-04-11T04:23:18.7861502Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T04:23:18.7861607Z         # our crafted op equals to Transformers
2025-04-11T04:23:18.7861737Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.7861864Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.7862011Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.7862183Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.7862275Z         cos, sin = emb(x0, position_ids)
2025-04-11T04:23:18.7862391Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T04:23:18.7862493Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.7862588Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T04:23:18.7862738Z         cos_2 = cos[:, :32]
2025-04-11T04:23:18.7862819Z         sin_2 = sin[:, :32]
2025-04-11T04:23:18.7862934Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.7863064Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T04:23:18.7863275Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T04:23:18.7863400Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.7863469Z     
2025-04-11T04:23:18.7863552Z         # create data
2025-04-11T04:23:18.7863631Z         block_size = 32
2025-04-11T04:23:18.7863718Z         max_num_blocks_per_seq = 4
2025-04-11T04:23:18.7863861Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T04:23:18.7864003Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T04:23:18.7864109Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7864393Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7864532Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7864689Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7864693Z 
2025-04-11T04:23:18.7864895Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
2025-04-11T04:23:18.7865046Z _____________________ test_get_xine_cache[dtype0-64-64-4] ______________________
2025-04-11T04:23:18.7865051Z 
2025-04-11T04:23:18.7865198Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T04:23:18.7865202Z 
2025-04-11T04:23:18.7865295Z     @pytest.mark.skipif(
2025-04-11T04:23:18.7865530Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T04:23:18.7865603Z     )
2025-04-11T04:23:18.7865709Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T04:23:18.7865823Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T04:23:18.7865930Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T04:23:18.7866055Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T04:23:18.7866207Z     def test_get_xine_cache(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T04:23:18.7866360Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T04:23:18.7866548Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T04:23:18.7866650Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7866936Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7867068Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7867224Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7867227Z 
2025-04-11T04:23:18.7867399Z tests/test_infer/test_kernels/triton/test_xine_copy.py:50: RuntimeError
2025-04-11T04:23:18.7867550Z _____________________ test_models_lazy_init[cuda-subset0] ______________________
2025-04-11T04:23:18.7867554Z 
2025-04-11T04:23:18.7867974Z subset = ['custom_hanging_param_model', 'custom_nested_model', 'custom_repeated_computed_layers', 'custom_simple_net', 'diffusers_clip_text_model', 'diffusers_auto_encoder_kl', ...]
2025-04-11T04:23:18.7868059Z default_device = 'cuda'
2025-04-11T04:23:18.7868063Z 
2025-04-11T04:23:18.7868237Z     @pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
2025-04-11T04:23:18.7868386Z     @pytest.mark.parametrize(
2025-04-11T04:23:18.7868490Z         "subset",
2025-04-11T04:23:18.7868566Z         (
2025-04-11T04:23:18.7868648Z             [COMMON_MODELS]
2025-04-11T04:23:18.7868731Z             if IS_FAST_TEST
2025-04-11T04:23:18.7868934Z             else ["torchvision", "diffusers", "timm", "transformers", "torchaudio", "deepfm", "dlrm"]
2025-04-11T04:23:18.7869009Z         ),
2025-04-11T04:23:18.7869136Z     )
2025-04-11T04:23:18.7869283Z     @pytest.mark.parametrize("default_device", ["cpu", "cuda"])
2025-04-11T04:23:18.7869407Z     def test_models_lazy_init(subset, default_device):
2025-04-11T04:23:18.7869564Z         sub_model_zoo = model_zoo.get_sub_registry(subset, allow_empty=True)
2025-04-11T04:23:18.7869676Z         for name, entry in sub_model_zoo.items():
2025-04-11T04:23:18.7869843Z             # TODO(ver217): lazy init does not support weight norm, skip these models
2025-04-11T04:23:18.7869926Z             if name in (
2025-04-11T04:23:18.7870024Z                 "torchaudio_wav2vec2_base",
2025-04-11T04:23:18.7870117Z                 "torchaudio_hubert_base",
2025-04-11T04:23:18.7870257Z                 "timm_beit",
2025-04-11T04:23:18.7870353Z                 "timm_vision_transformer",
2025-04-11T04:23:18.7870436Z                 "timm_deit",
2025-04-11T04:23:18.7870516Z                 "timm_beitv2",
2025-04-11T04:23:18.7870596Z                 "timm_deit3",
2025-04-11T04:23:18.7870679Z                 "timm_convit",
2025-04-11T04:23:18.7870769Z                 "timm_tnt_b_patch16_224",
2025-04-11T04:23:18.7870979Z             ) or name.startswith(("transformers_vit", "transformers_blip2", "transformers_whisper")):
2025-04-11T04:23:18.7871056Z                 continue
2025-04-11T04:23:18.7871222Z >           check_lazy_init(entry, verbose=True, default_device=default_device)
2025-04-11T04:23:18.7871226Z 
2025-04-11T04:23:18.7871322Z tests/test_lazy/test_models.py:33: 
2025-04-11T04:23:18.7871434Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7871580Z tests/test_lazy/lazy_init_utils.py:77: in check_lazy_init
2025-04-11T04:23:18.7871662Z     model = model_fn()
2025-04-11T04:23:18.7871827Z tests/kit/model_zoo/custom/hanging_param_model.py:17: in __init__
2025-04-11T04:23:18.7871918Z     self.proj1 = nn.Linear(4, 8)
2025-04-11T04:23:18.7872172Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:98: in __init__
2025-04-11T04:23:18.7872371Z     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
2025-04-11T04:23:18.7872479Z colossalai/lazy/lazy_init.py:506: in wrapper
2025-04-11T04:23:18.7872597Z     return self.tensor_cls(target, *args, **kwargs)
2025-04-11T04:23:18.7872762Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7872767Z 
2025-04-11T04:23:18.7872892Z cls = <class 'colossalai.lazy.lazy_init._MyTensor'>
2025-04-11T04:23:18.7873037Z func = <built-in method empty of type object at 0x7fb8e3cd9840>
2025-04-11T04:23:18.7873137Z concrete_data = None, args = ((8, 4),)
2025-04-11T04:23:18.7873237Z kwargs = {'device': 'cuda', 'dtype': None}
2025-04-11T04:23:18.7873241Z 
2025-04-11T04:23:18.7873407Z     def __new__(cls, func, *args, concrete_data=None, **kwargs) -> "_MyTensor":
2025-04-11T04:23:18.7873499Z         cls._pre_op_fn()
2025-04-11T04:23:18.7873589Z         if concrete_data is not None:
2025-04-11T04:23:18.7873683Z             # uniform api as LazyTensor
2025-04-11T04:23:18.7873771Z             data = concrete_data
2025-04-11T04:23:18.7873846Z         else:
2025-04-11T04:23:18.7873947Z             kwargs["device"] = cls.default_device
2025-04-11T04:23:18.7874035Z >           data = func(*args, **kwargs)
2025-04-11T04:23:18.7874144Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7874431Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7874648Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7874809Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7874813Z 
2025-04-11T04:23:18.7874923Z colossalai/lazy/lazy_init.py:93: RuntimeError
2025-04-11T04:23:18.7875055Z ________________________________ test_lazy_ops _________________________________
2025-04-11T04:23:18.7875059Z 
2025-04-11T04:23:18.7875226Z     @pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
2025-04-11T04:23:18.7875364Z     def test_lazy_ops():
2025-04-11T04:23:18.7875452Z         with LazyInitContext():
2025-04-11T04:23:18.7875540Z             x = torch.rand(2, 3)
2025-04-11T04:23:18.7875634Z             assert tuple(x.shape) == (2, 3)
2025-04-11T04:23:18.7875732Z             assert x.device.type == "cpu"
2025-04-11T04:23:18.7875818Z             x.requires_grad is False
2025-04-11T04:23:18.7875894Z             y = x.cuda()
2025-04-11T04:23:18.7875988Z             assert tuple(y.shape) == (2, 3)
2025-04-11T04:23:18.7876080Z             assert y.device.type == "cuda"
2025-04-11T04:23:18.7876222Z             assert y.requires_grad is False
2025-04-11T04:23:18.7876312Z             assert x.cpu() is x
2025-04-11T04:23:18.7876410Z             p = Parameter(torch.empty(2, 3))
2025-04-11T04:23:18.7876507Z             assert tuple(p.shape) == (2, 3)
2025-04-11T04:23:18.7876598Z             assert p.device.type == "cpu"
2025-04-11T04:23:18.7876692Z             assert p.requires_grad is True
2025-04-11T04:23:18.7876786Z             assert isinstance(p, Parameter)
2025-04-11T04:23:18.7876869Z         x.materialize()
2025-04-11T04:23:18.7876957Z         assert tuple(x.shape) == (2, 3)
2025-04-11T04:23:18.7877046Z         assert x.device.type == "cpu"
2025-04-11T04:23:18.7877138Z         assert x.requires_grad is False
2025-04-11T04:23:18.7877218Z >       y.materialize()
2025-04-11T04:23:18.7877222Z 
2025-04-11T04:23:18.7877315Z tests/test_lazy/test_ops.py:33: 
2025-04-11T04:23:18.7877425Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7877544Z colossalai/lazy/lazy_init.py:217: in materialize
2025-04-11T04:23:18.7877635Z     target = self._materialize_data()
2025-04-11T04:23:18.7877764Z colossalai/lazy/lazy_init.py:242: in _materialize_data
2025-04-11T04:23:18.7877846Z     init_val = func(
2025-04-11T04:23:18.7877953Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7877959Z 
2025-04-11T04:23:18.7878047Z t = tensor([[0.8823, 0.9150, 0.3829],
2025-04-11T04:23:18.7878126Z         [0.9593, 0.3904, 0.6009]])
2025-04-11T04:23:18.7878216Z kw = {'device': device(type='cuda')}
2025-04-11T04:23:18.7878224Z 
2025-04-11T04:23:18.7878319Z     def factory_fn(t: torch.Tensor, **kw):
2025-04-11T04:23:18.7878453Z >       return t.to(*args, **kwargs)
2025-04-11T04:23:18.7878563Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7878841Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7878978Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7879133Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7879137Z 
2025-04-11T04:23:18.7879247Z colossalai/lazy/lazy_init.py:380: RuntimeError
2025-04-11T04:23:18.7879380Z _____________________________ test_torch_ddp_lora ______________________________
2025-04-11T04:23:18.7879386Z 
2025-04-11T04:23:18.7879473Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.7880098Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.7880104Z 
2025-04-11T04:23:18.7880204Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.7880286Z         try_count = 0
2025-04-11T04:23:18.7880435Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.7880516Z             max_try, int
2025-04-11T04:23:18.7880664Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.7880733Z     
2025-04-11T04:23:18.7880849Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.7880923Z             try:
2025-04-11T04:23:18.7881007Z                 try_count += 1
2025-04-11T04:23:18.7881147Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.7881230Z                 return ret
2025-04-11T04:23:18.7881321Z             except exception_type as e:
2025-04-11T04:23:18.7881420Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.7881612Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.7881729Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.7881879Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.7882079Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.7882165Z                     continue
2025-04-11T04:23:18.7882240Z                 else:
2025-04-11T04:23:18.7882463Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.7882545Z >                   raise e
2025-04-11T04:23:18.7882551Z 
2025-04-11T04:23:18.7882644Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.7882758Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7882885Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.7882975Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.7883112Z tests/test_lora/test_lora.py:108: in test_torch_ddp_lora
2025-04-11T04:23:18.7883193Z     spawn(run_dist, 2)
2025-04-11T04:23:18.7883299Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.7883397Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.7883660Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.7883839Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.7884128Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.7884217Z     while not context.join():
2025-04-11T04:23:18.7884326Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7884330Z 
2025-04-11T04:23:18.7884530Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5e92b00>
2025-04-11T04:23:18.7884609Z timeout = None
2025-04-11T04:23:18.7884664Z 
2025-04-11T04:23:18.7884762Z     def join(self, timeout=None):
2025-04-11T04:23:18.7884888Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.7884963Z     
2025-04-11T04:23:18.7885117Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.7885260Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.7885424Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.7885515Z         of the first process exiting.
2025-04-11T04:23:18.7885587Z     
2025-04-11T04:23:18.7885732Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.7885873Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.7885941Z     
2025-04-11T04:23:18.7886013Z         Args:
2025-04-11T04:23:18.7886154Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.7886228Z         """
2025-04-11T04:23:18.7886368Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.7886457Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.7886534Z             return True
2025-04-11T04:23:18.7886660Z     
2025-04-11T04:23:18.7886794Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.7886917Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.7887008Z             self.sentinels.keys(),
2025-04-11T04:23:18.7887091Z             timeout=timeout,
2025-04-11T04:23:18.7887167Z         )
2025-04-11T04:23:18.7887234Z     
2025-04-11T04:23:18.7887317Z         error_index = None
2025-04-11T04:23:18.7887458Z         for sentinel in ready:
2025-04-11T04:23:18.7887567Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.7887666Z             process = self.processes[index]
2025-04-11T04:23:18.7887749Z             process.join()
2025-04-11T04:23:18.7887844Z             if process.exitcode != 0:
2025-04-11T04:23:18.7887933Z                 error_index = index
2025-04-11T04:23:18.7888011Z                 break
2025-04-11T04:23:18.7888080Z     
2025-04-11T04:23:18.7888170Z         # Return if there was no error.
2025-04-11T04:23:18.7888260Z         if error_index is None:
2025-04-11T04:23:18.7888444Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.7888544Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.7888612Z     
2025-04-11T04:23:18.7888754Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.7888852Z         for process in self.processes:
2025-04-11T04:23:18.7888938Z             if process.is_alive():
2025-04-11T04:23:18.7889033Z                 process.terminate()
2025-04-11T04:23:18.7889119Z             process.join()
2025-04-11T04:23:18.7889190Z     
2025-04-11T04:23:18.7889330Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.7889447Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.7889556Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.7889678Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.7889766Z             if exitcode < 0:
2025-04-11T04:23:18.7889873Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.7889980Z                 raise ProcessExitedException(
2025-04-11T04:23:18.7890132Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.7890226Z                     error_index=error_index,
2025-04-11T04:23:18.7890329Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.7890420Z                     exit_code=exitcode,
2025-04-11T04:23:18.7890509Z                     signal_name=name,
2025-04-11T04:23:18.7890581Z                 )
2025-04-11T04:23:18.7890654Z             else:
2025-04-11T04:23:18.7890762Z                 raise ProcessExitedException(
2025-04-11T04:23:18.7890976Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.7891073Z                     error_index=error_index,
2025-04-11T04:23:18.7891175Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.7891268Z                     exit_code=exitcode,
2025-04-11T04:23:18.7891341Z                 )
2025-04-11T04:23:18.7891413Z     
2025-04-11T04:23:18.7891550Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.7891721Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.7891809Z         msg += original_trace
2025-04-11T04:23:18.7891981Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.7892144Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.7892219Z E       
2025-04-11T04:23:18.7892347Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.7892447Z E       Traceback (most recent call last):
2025-04-11T04:23:18.7892754Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.7892836Z E           fn(i, *args)
2025-04-11T04:23:18.7893101Z E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
2025-04-11T04:23:18.7893184Z E           run_lora_test()
2025-04-11T04:23:18.7893403Z E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
2025-04-11T04:23:18.7893579Z E           check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
2025-04-11T04:23:18.7893801Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.7893951Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.7894216Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.7894317Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.7894597Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.7894702Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.7894805Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7895138Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7895272Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7895433Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7895439Z 
2025-04-11T04:23:18.7895742Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.7895900Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.7896060Z [04/11/25 04:17:31] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.7896189Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.7896298Z                              :75 launch                                         
2025-04-11T04:23:18.7896439Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.7896564Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.7896757Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.7896904Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.7898120Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.7898296Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.7899428Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.7899598Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.7900302Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.7900388Z   warnings.warn(
2025-04-11T04:23:18.7901087Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.7901223Z   warnings.warn(
2025-04-11T04:23:18.7902063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7902195Z   warnings.warn(
2025-04-11T04:23:18.7903014Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7903095Z   warnings.warn(
2025-04-11T04:23:18.7903953Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7904031Z   warnings.warn(
2025-04-11T04:23:18.7904844Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7904923Z   warnings.warn(
2025-04-11T04:23:18.7905718Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7905795Z   warnings.warn(
2025-04-11T04:23:18.7906610Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.7906688Z   warnings.warn(
2025-04-11T04:23:18.7906994Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:61905 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.7907602Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.7907683Z   warnings.warn(
2025-04-11T04:23:18.7908220Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.7908298Z   warnings.warn(
2025-04-11T04:23:18.7908867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.7908946Z   warnings.warn(
2025-04-11T04:23:18.7909471Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.7909548Z   warnings.warn(
2025-04-11T04:23:18.7909695Z _________________________ test_moe_kernel[data_type0] __________________________
2025-04-11T04:23:18.7909698Z 
2025-04-11T04:23:18.7909787Z data_type = torch.float32
2025-04-11T04:23:18.7909849Z 
2025-04-11T04:23:18.7910028Z     @pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
2025-04-11T04:23:18.7910122Z     def test_moe_kernel(data_type):
2025-04-11T04:23:18.7910213Z         torch.manual_seed(1024)
2025-04-11T04:23:18.7910294Z >       run_moe_cumsum()
2025-04-11T04:23:18.7910298Z 
2025-04-11T04:23:18.7910392Z tests/test_moe/test_kernel.py:93: 
2025-04-11T04:23:18.7910508Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7910565Z 
2025-04-11T04:23:18.7910650Z     def run_moe_cumsum():
2025-04-11T04:23:18.7910739Z         test_mask = torch.tensor(
2025-04-11T04:23:18.7910810Z             [
2025-04-11T04:23:18.7910891Z                 [0, 1, 0, 0],
2025-04-11T04:23:18.7910969Z                 [1, 0, 0, 0],
2025-04-11T04:23:18.7911042Z                 [0, 1, 0, 0],
2025-04-11T04:23:18.7911119Z                 [1, 0, 0, 0],
2025-04-11T04:23:18.7911189Z             ],
2025-04-11T04:23:18.7911281Z             dtype=torch.int32,
2025-04-11T04:23:18.7911356Z >       ).to("cuda")
2025-04-11T04:23:18.7911514Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7911809Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7911944Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7912107Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7912113Z 
2025-04-11T04:23:18.7912224Z tests/test_moe/test_kernel.py:29: RuntimeError
2025-04-11T04:23:18.7912368Z _________________________ test_moe_kernel[data_type1] __________________________
2025-04-11T04:23:18.7912372Z 
2025-04-11T04:23:18.7912457Z data_type = torch.float16
2025-04-11T04:23:18.7912463Z 
2025-04-11T04:23:18.7912633Z     @pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
2025-04-11T04:23:18.7912725Z     def test_moe_kernel(data_type):
2025-04-11T04:23:18.7912812Z         torch.manual_seed(1024)
2025-04-11T04:23:18.7912897Z >       run_moe_cumsum()
2025-04-11T04:23:18.7912901Z 
2025-04-11T04:23:18.7912996Z tests/test_moe/test_kernel.py:93: 
2025-04-11T04:23:18.7913106Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7913109Z 
2025-04-11T04:23:18.7913187Z     def run_moe_cumsum():
2025-04-11T04:23:18.7913278Z         test_mask = torch.tensor(
2025-04-11T04:23:18.7913350Z             [
2025-04-11T04:23:18.7913428Z                 [0, 1, 0, 0],
2025-04-11T04:23:18.7913507Z                 [1, 0, 0, 0],
2025-04-11T04:23:18.7913580Z                 [0, 1, 0, 0],
2025-04-11T04:23:18.7913654Z                 [1, 0, 0, 0],
2025-04-11T04:23:18.7913725Z             ],
2025-04-11T04:23:18.7913864Z             dtype=torch.int32,
2025-04-11T04:23:18.7913947Z >       ).to("cuda")
2025-04-11T04:23:18.7914051Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7914336Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7914473Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7914634Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7914638Z 
2025-04-11T04:23:18.7914748Z tests/test_moe/test_kernel.py:29: RuntimeError
2025-04-11T04:23:18.7914887Z __________________________ test_mixtral_moe_layer[4] ___________________________
2025-04-11T04:23:18.7914893Z 
2025-04-11T04:23:18.7914972Z world_size = 4
2025-04-11T04:23:18.7914976Z 
2025-04-11T04:23:18.7915084Z     @pytest.mark.parametrize("world_size", [4])
2025-04-11T04:23:18.7915196Z     def test_mixtral_moe_layer(world_size: int):
2025-04-11T04:23:18.7915288Z >       spawn(run_dist, world_size)
2025-04-11T04:23:18.7915292Z 
2025-04-11T04:23:18.7915400Z tests/test_moe/test_moe_checkpoint.py:171: 
2025-04-11T04:23:18.7915507Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7915658Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.7915762Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.7916023Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.7916203Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.7916492Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.7916633Z     while not context.join():
2025-04-11T04:23:18.7916739Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7916743Z 
2025-04-11T04:23:18.7916943Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f0aa70>
2025-04-11T04:23:18.7917020Z timeout = None
2025-04-11T04:23:18.7917024Z 
2025-04-11T04:23:18.7917113Z     def join(self, timeout=None):
2025-04-11T04:23:18.7917242Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.7917312Z     
2025-04-11T04:23:18.7917507Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.7917653Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.7917819Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.7917908Z         of the first process exiting.
2025-04-11T04:23:18.7917976Z     
2025-04-11T04:23:18.7918130Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.7918266Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.7918337Z     
2025-04-11T04:23:18.7918409Z         Args:
2025-04-11T04:23:18.7918548Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.7918624Z         """
2025-04-11T04:23:18.7918759Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.7918855Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.7918932Z             return True
2025-04-11T04:23:18.7919000Z     
2025-04-11T04:23:18.7919135Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.7919252Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.7919344Z             self.sentinels.keys(),
2025-04-11T04:23:18.7919426Z             timeout=timeout,
2025-04-11T04:23:18.7919499Z         )
2025-04-11T04:23:18.7919569Z     
2025-04-11T04:23:18.7919651Z         error_index = None
2025-04-11T04:23:18.7919740Z         for sentinel in ready:
2025-04-11T04:23:18.7919845Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.7919944Z             process = self.processes[index]
2025-04-11T04:23:18.7920079Z             process.join()
2025-04-11T04:23:18.7920172Z             if process.exitcode != 0:
2025-04-11T04:23:18.7920260Z                 error_index = index
2025-04-11T04:23:18.7920335Z                 break
2025-04-11T04:23:18.7920409Z     
2025-04-11T04:23:18.7920499Z         # Return if there was no error.
2025-04-11T04:23:18.7920583Z         if error_index is None:
2025-04-11T04:23:18.7920721Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.7920814Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.7920886Z     
2025-04-11T04:23:18.7921027Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.7921126Z         for process in self.processes:
2025-04-11T04:23:18.7921215Z             if process.is_alive():
2025-04-11T04:23:18.7921304Z                 process.terminate()
2025-04-11T04:23:18.7921392Z             process.join()
2025-04-11T04:23:18.7921459Z     
2025-04-11T04:23:18.7921605Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.7921718Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.7921826Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.7922000Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.7922083Z             if exitcode < 0:
2025-04-11T04:23:18.7922196Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.7922302Z                 raise ProcessExitedException(
2025-04-11T04:23:18.7922455Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.7922549Z                     error_index=error_index,
2025-04-11T04:23:18.7922700Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.7922791Z                     exit_code=exitcode,
2025-04-11T04:23:18.7922876Z                     signal_name=name,
2025-04-11T04:23:18.7922951Z                 )
2025-04-11T04:23:18.7923025Z             else:
2025-04-11T04:23:18.7923133Z                 raise ProcessExitedException(
2025-04-11T04:23:18.7923295Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.7923387Z                     error_index=error_index,
2025-04-11T04:23:18.7923490Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.7923643Z                     exit_code=exitcode,
2025-04-11T04:23:18.7923720Z                 )
2025-04-11T04:23:18.7923789Z     
2025-04-11T04:23:18.7923922Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.7924097Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.7924180Z         msg += original_trace
2025-04-11T04:23:18.7924360Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.7924522Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.7924596Z E       
2025-04-11T04:23:18.7924726Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.7924825Z E       Traceback (most recent call last):
2025-04-11T04:23:18.7925132Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.7925215Z E           fn(i, *args)
2025-04-11T04:23:18.7925456Z E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
2025-04-11T04:23:18.7925549Z E           check_moe_checkpoint()
2025-04-11T04:23:18.7925818Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.7925906Z E           partial_func(**kwargs)
2025-04-11T04:23:18.7926174Z E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
2025-04-11T04:23:18.7926311Z E           dist.broadcast_object_list(broadcast_objects, src=0)
2025-04-11T04:23:18.7926664Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.7926761Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.7927124Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
2025-04-11T04:23:18.7927345Z E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T04:23:18.7927673Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
2025-04-11T04:23:18.7927882Z E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T04:23:18.7928230Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
2025-04-11T04:23:18.7928367Z E           byte_tensor = torch.ByteTensor(byte_storage).to(device)
2025-04-11T04:23:18.7928476Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7928759Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7928959Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7929118Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7929122Z 
2025-04-11T04:23:18.7929438Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.7929589Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.7929797Z [04/11/25 04:17:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.7929926Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.7930031Z                              :75 launch                                         
2025-04-11T04:23:18.7930173Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.7930294Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.7930441Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.7930781Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.7931075Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.7931243Z _____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] ______________
2025-04-11T04:23:18.7931250Z 
2025-04-11T04:23:18.7931393Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7931478Z g_dtype = torch.float16
2025-04-11T04:23:18.7931482Z 
2025-04-11T04:23:18.7931606Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7931737Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7931912Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7932069Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7932161Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7932300Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7932389Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7932529Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7932617Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7932703Z >       check_adam_kernel(
2025-04-11T04:23:18.7932964Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7933035Z         )
2025-04-11T04:23:18.7933038Z 
2025-04-11T04:23:18.7933204Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7933315Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7933476Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7933634Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7933741Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7933745Z 
2025-04-11T04:23:18.7933928Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c57c10>, lr = 0.001
2025-04-11T04:23:18.7934084Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7934090Z 
2025-04-11T04:23:18.7934340Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7934485Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7934653Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7934722Z     
2025-04-11T04:23:18.7934834Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7934957Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7935259Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7935367Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7935648Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7935784Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7936085Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7936089Z 
2025-04-11T04:23:18.7936229Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7936398Z ______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] ______________
2025-04-11T04:23:18.7936401Z 
2025-04-11T04:23:18.7936531Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7936621Z g_dtype = torch.float16
2025-04-11T04:23:18.7936626Z 
2025-04-11T04:23:18.7936749Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7936926Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7937097Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7937247Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7937340Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7937469Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7937562Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7937696Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7937787Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7937870Z >       check_adam_kernel(
2025-04-11T04:23:18.7938126Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7938204Z         )
2025-04-11T04:23:18.7938209Z 
2025-04-11T04:23:18.7938321Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7938436Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7938591Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7938739Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7938848Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7938854Z 
2025-04-11T04:23:18.7939032Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5ad1e40>, lr = 0.001
2025-04-11T04:23:18.7939184Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7939188Z 
2025-04-11T04:23:18.7939479Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7939627Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7939791Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7939865Z     
2025-04-11T04:23:18.7939977Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7940098Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7940340Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7940449Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7940732Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7940865Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7941027Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7941031Z 
2025-04-11T04:23:18.7941166Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7941384Z _____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] ______________
2025-04-11T04:23:18.7941388Z 
2025-04-11T04:23:18.7941520Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.7941608Z g_dtype = torch.float16
2025-04-11T04:23:18.7941612Z 
2025-04-11T04:23:18.7941733Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7941859Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7942083Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7942231Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7942320Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7942452Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7942542Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7942674Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7942758Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7942848Z >       check_adam_kernel(
2025-04-11T04:23:18.7943157Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7943235Z         )
2025-04-11T04:23:18.7943240Z 
2025-04-11T04:23:18.7943356Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7943469Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7943628Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7943775Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7943886Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7943893Z 
2025-04-11T04:23:18.7944071Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a6fd00>, lr = 0.001
2025-04-11T04:23:18.7944223Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.7944229Z 
2025-04-11T04:23:18.7944473Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7944619Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7944780Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7944852Z     
2025-04-11T04:23:18.7944961Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7945079Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7945324Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7945482Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7945773Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7945907Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7946067Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7946073Z 
2025-04-11T04:23:18.7946207Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7946368Z ______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] ______________
2025-04-11T04:23:18.7946375Z 
2025-04-11T04:23:18.7946503Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.7946589Z g_dtype = torch.float16
2025-04-11T04:23:18.7946593Z 
2025-04-11T04:23:18.7946716Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7946840Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7947012Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7947160Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7947245Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7947427Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7947512Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7947649Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7947731Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7947816Z >       check_adam_kernel(
2025-04-11T04:23:18.7948075Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7948214Z         )
2025-04-11T04:23:18.7948218Z 
2025-04-11T04:23:18.7948338Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7948481Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7948642Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7948788Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7948897Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7948903Z 
2025-04-11T04:23:18.7949132Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c77370>, lr = 0.001
2025-04-11T04:23:18.7949286Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.7949290Z 
2025-04-11T04:23:18.7949526Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7949666Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7949830Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7949901Z     
2025-04-11T04:23:18.7950013Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7950132Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7950372Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7950477Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7950761Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7950897Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7951053Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7951057Z 
2025-04-11T04:23:18.7951197Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7951356Z _____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] ______________
2025-04-11T04:23:18.7951360Z 
2025-04-11T04:23:18.7951493Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7951630Z g_dtype = torch.float32
2025-04-11T04:23:18.7951635Z 
2025-04-11T04:23:18.7951762Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7951887Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7952057Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7952210Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7952296Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7952427Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7952513Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7952647Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7952732Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7952813Z >       check_adam_kernel(
2025-04-11T04:23:18.7953072Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7953144Z         )
2025-04-11T04:23:18.7953148Z 
2025-04-11T04:23:18.7953263Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7953370Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7953585Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7953733Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7953840Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7953844Z 
2025-04-11T04:23:18.7954023Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a84040>, lr = 0.001
2025-04-11T04:23:18.7954228Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7954232Z 
2025-04-11T04:23:18.7954473Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7954614Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7954776Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7954845Z     
2025-04-11T04:23:18.7954955Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7955071Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7955359Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7955473Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7955753Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7955892Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7956050Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7956054Z 
2025-04-11T04:23:18.7956197Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7956361Z ______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] ______________
2025-04-11T04:23:18.7956365Z 
2025-04-11T04:23:18.7956494Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7956586Z g_dtype = torch.float32
2025-04-11T04:23:18.7956590Z 
2025-04-11T04:23:18.7956714Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7956845Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7957013Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7957167Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7957263Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7957392Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7957483Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7957613Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7957754Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7957839Z >       check_adam_kernel(
2025-04-11T04:23:18.7958093Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7958166Z         )
2025-04-11T04:23:18.7958169Z 
2025-04-11T04:23:18.7958282Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7958392Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7958545Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7958692Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7958799Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7958803Z 
2025-04-11T04:23:18.7958978Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3f790>, lr = 0.001
2025-04-11T04:23:18.7959128Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7959132Z 
2025-04-11T04:23:18.7959370Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7959558Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7959721Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7959794Z     
2025-04-11T04:23:18.7959902Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7960020Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7960260Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7960414Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7960694Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7960829Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7960988Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7960992Z 
2025-04-11T04:23:18.7961127Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7961340Z _____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] ______________
2025-04-11T04:23:18.7961345Z 
2025-04-11T04:23:18.7961474Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.7961561Z g_dtype = torch.float32
2025-04-11T04:23:18.7961564Z 
2025-04-11T04:23:18.7961684Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7961812Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7961977Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7962125Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7962215Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7962341Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7962429Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7962559Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7962647Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7962731Z >       check_adam_kernel(
2025-04-11T04:23:18.7962983Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7963058Z         )
2025-04-11T04:23:18.7963062Z 
2025-04-11T04:23:18.7963174Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7963287Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7963441Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7963588Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7963741Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7963746Z 
2025-04-11T04:23:18.7963923Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a854b0>, lr = 0.001
2025-04-11T04:23:18.7964077Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.7964081Z 
2025-04-11T04:23:18.7964320Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7964462Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7964622Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7964696Z     
2025-04-11T04:23:18.7964804Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7964922Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7965165Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7965269Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7965555Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7965743Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7965904Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7965908Z 
2025-04-11T04:23:18.7966043Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7966206Z ______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] ______________
2025-04-11T04:23:18.7966245Z 
2025-04-11T04:23:18.7966374Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.7966457Z g_dtype = torch.float32
2025-04-11T04:23:18.7966465Z 
2025-04-11T04:23:18.7966582Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7966708Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7966877Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7967025Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7967116Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7967287Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7967377Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7967512Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7967597Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7967683Z >       check_adam_kernel(
2025-04-11T04:23:18.7967937Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7968011Z         )
2025-04-11T04:23:18.7968014Z 
2025-04-11T04:23:18.7968126Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7968237Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7968396Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7968539Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7968655Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7968661Z 
2025-04-11T04:23:18.7968836Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3fbb0>, lr = 0.001
2025-04-11T04:23:18.7968988Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.7968992Z 
2025-04-11T04:23:18.7969228Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7969371Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7969531Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7969600Z     
2025-04-11T04:23:18.7969763Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7969880Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7970122Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7970232Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7970516Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7970652Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7970806Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7970815Z 
2025-04-11T04:23:18.7970949Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7971109Z _____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] ______________
2025-04-11T04:23:18.7971113Z 
2025-04-11T04:23:18.7971249Z adamw = False, weight_decay = 0.0, p_dtype = torch.float16
2025-04-11T04:23:18.7971332Z g_dtype = torch.float16
2025-04-11T04:23:18.7971336Z 
2025-04-11T04:23:18.7971457Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7971648Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7971819Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7971967Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7972054Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7972184Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7972268Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7972452Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7972536Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7972620Z >       check_adam_kernel(
2025-04-11T04:23:18.7972878Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7972948Z         )
2025-04-11T04:23:18.7972951Z 
2025-04-11T04:23:18.7973065Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7973175Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7973380Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7973525Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7973635Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7973639Z 
2025-04-11T04:23:18.7973813Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a85150>, lr = 0.001
2025-04-11T04:23:18.7973965Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7973972Z 
2025-04-11T04:23:18.7974212Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7974349Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7974514Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7974584Z     
2025-04-11T04:23:18.7974696Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7974811Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7975058Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7975162Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7975447Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7975589Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7975746Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7975799Z 
2025-04-11T04:23:18.7975941Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7976100Z ______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] ______________
2025-04-11T04:23:18.7976106Z 
2025-04-11T04:23:18.7976235Z adamw = True, weight_decay = 0.0, p_dtype = torch.float16
2025-04-11T04:23:18.7976322Z g_dtype = torch.float16
2025-04-11T04:23:18.7976326Z 
2025-04-11T04:23:18.7976447Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7976571Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7976735Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7976890Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7976978Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7977109Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7977194Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7977326Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7977414Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7977497Z >       check_adam_kernel(
2025-04-11T04:23:18.7977806Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7977879Z         )
2025-04-11T04:23:18.7977883Z 
2025-04-11T04:23:18.7977998Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7978108Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7978263Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7978462Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7978570Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7978574Z 
2025-04-11T04:23:18.7978753Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5819268c0>, lr = 0.001
2025-04-11T04:23:18.7978902Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7978906Z 
2025-04-11T04:23:18.7979145Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7979331Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7979503Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7979572Z     
2025-04-11T04:23:18.7979681Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7979798Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7980039Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7980146Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7980426Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7980561Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7980716Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7980722Z 
2025-04-11T04:23:18.7980862Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7981022Z _____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] ______________
2025-04-11T04:23:18.7981026Z 
2025-04-11T04:23:18.7981154Z adamw = False, weight_decay = 0.1, p_dtype = torch.float16
2025-04-11T04:23:18.7981241Z g_dtype = torch.float16
2025-04-11T04:23:18.7981247Z 
2025-04-11T04:23:18.7981364Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7981490Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7981657Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7981854Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7981943Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7982069Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7982158Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7982286Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7982375Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7982457Z >       check_adam_kernel(
2025-04-11T04:23:18.7982715Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7982785Z         )
2025-04-11T04:23:18.7982790Z 
2025-04-11T04:23:18.7982901Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7983012Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7983166Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7983316Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7983422Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7983426Z 
2025-04-11T04:23:18.7983655Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb581d7a710>, lr = 0.001
2025-04-11T04:23:18.7983804Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.7983808Z 
2025-04-11T04:23:18.7984047Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7984183Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7984394Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7984468Z     
2025-04-11T04:23:18.7984579Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7984697Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7984935Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7985042Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7985322Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7985501Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7985662Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7985666Z 
2025-04-11T04:23:18.7985801Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7985966Z ______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] ______________
2025-04-11T04:23:18.7985970Z 
2025-04-11T04:23:18.7986096Z adamw = True, weight_decay = 0.1, p_dtype = torch.float16
2025-04-11T04:23:18.7986182Z g_dtype = torch.float16
2025-04-11T04:23:18.7986186Z 
2025-04-11T04:23:18.7986306Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7986432Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7986595Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7986744Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7986835Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7986961Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7987048Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7987175Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7987258Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7987344Z >       check_adam_kernel(
2025-04-11T04:23:18.7987594Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7987668Z         )
2025-04-11T04:23:18.7987672Z 
2025-04-11T04:23:18.7987847Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7987961Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7988115Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7988266Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7988373Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7988377Z 
2025-04-11T04:23:18.7988592Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5e929e0>, lr = 0.001
2025-04-11T04:23:18.7988746Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.7988752Z 
2025-04-11T04:23:18.7988986Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7989130Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7989292Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7989365Z     
2025-04-11T04:23:18.7989473Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7989585Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7989892Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7989997Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7990301Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7990434Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7990647Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7990651Z 
2025-04-11T04:23:18.7990785Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7990950Z _____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] ______________
2025-04-11T04:23:18.7990953Z 
2025-04-11T04:23:18.7991083Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7991169Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.7991175Z 
2025-04-11T04:23:18.7991298Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7991478Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7991655Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7991805Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7991893Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7992021Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7992106Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7992241Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7992325Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7992413Z >       check_adam_kernel(
2025-04-11T04:23:18.7992671Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7992744Z         )
2025-04-11T04:23:18.7992752Z 
2025-04-11T04:23:18.7992865Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7992977Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7993135Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7993277Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7993387Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7993392Z 
2025-04-11T04:23:18.7993566Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c54cd0>, lr = 0.001
2025-04-11T04:23:18.7993716Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7993720Z 
2025-04-11T04:23:18.7994031Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7994173Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7994335Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7994406Z     
2025-04-11T04:23:18.7994519Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7994635Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7994881Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7994987Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.7995274Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.7995406Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.7995562Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7995566Z 
2025-04-11T04:23:18.7995707Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.7995919Z ______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] ______________
2025-04-11T04:23:18.7995923Z 
2025-04-11T04:23:18.7996058Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T04:23:18.7996144Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.7996148Z 
2025-04-11T04:23:18.7996269Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.7996392Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.7996612Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.7996761Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.7996851Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.7996987Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.7997077Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.7997213Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.7997300Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.7997386Z >       check_adam_kernel(
2025-04-11T04:23:18.7997696Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.7997768Z         )
2025-04-11T04:23:18.7997772Z 
2025-04-11T04:23:18.7997886Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.7997995Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7998154Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.7998296Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.7998404Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.7998409Z 
2025-04-11T04:23:18.7998581Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c74c70>, lr = 0.001
2025-04-11T04:23:18.7998727Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7998733Z 
2025-04-11T04:23:18.7998976Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.7999113Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.7999276Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7999346Z     
2025-04-11T04:23:18.7999459Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.7999576Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.7999813Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.7999971Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8000255Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8000393Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8000551Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8000555Z 
2025-04-11T04:23:18.8000691Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8000849Z _____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] ______________
2025-04-11T04:23:18.8000852Z 
2025-04-11T04:23:18.8000985Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.8001073Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8001077Z 
2025-04-11T04:23:18.8001195Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8001324Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8001492Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8001644Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8001729Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8001910Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8001996Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8002129Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8002217Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8002299Z >       check_adam_kernel(
2025-04-11T04:23:18.8002555Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8002677Z         )
2025-04-11T04:23:18.8002681Z 
2025-04-11T04:23:18.8002798Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8002910Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8003070Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8003223Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8003332Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8003338Z 
2025-04-11T04:23:18.8003567Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a89ed0>, lr = 0.001
2025-04-11T04:23:18.8003717Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.8003721Z 
2025-04-11T04:23:18.8003962Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8004102Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8004264Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8004334Z     
2025-04-11T04:23:18.8004442Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8004564Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8004804Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8004913Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8005197Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8005336Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8005492Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8005496Z 
2025-04-11T04:23:18.8005632Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8005794Z ______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] ______________
2025-04-11T04:23:18.8005798Z 
2025-04-11T04:23:18.8005926Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T04:23:18.8006068Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8006072Z 
2025-04-11T04:23:18.8006194Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8006320Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8006488Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8006642Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8006729Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8006854Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8006941Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8007071Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8007160Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8007242Z >       check_adam_kernel(
2025-04-11T04:23:18.8007494Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8007569Z         )
2025-04-11T04:23:18.8007573Z 
2025-04-11T04:23:18.8007684Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8007796Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8008000Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8008149Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8008255Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8008259Z 
2025-04-11T04:23:18.8008436Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3c2b0>, lr = 0.001
2025-04-11T04:23:18.8008622Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.8008626Z 
2025-04-11T04:23:18.8008862Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8009004Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8009162Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8009234Z     
2025-04-11T04:23:18.8009342Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8009459Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8009744Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8009849Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8010131Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8010267Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8010423Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8010427Z 
2025-04-11T04:23:18.8010563Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8010724Z _____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] ______________
2025-04-11T04:23:18.8010728Z 
2025-04-11T04:23:18.8010863Z adamw = False, weight_decay = 0.0, p_dtype = torch.bfloat16
2025-04-11T04:23:18.8010953Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8010957Z 
2025-04-11T04:23:18.8011077Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8011202Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8011371Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8011518Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8011609Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8011736Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8011823Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8011954Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8012085Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8012176Z >       check_adam_kernel(
2025-04-11T04:23:18.8012428Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8012503Z         )
2025-04-11T04:23:18.8012507Z 
2025-04-11T04:23:18.8012619Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8012731Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8012884Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8013029Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8013140Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8013144Z 
2025-04-11T04:23:18.8013316Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a88880>, lr = 0.001
2025-04-11T04:23:18.8013469Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.8013473Z 
2025-04-11T04:23:18.8013710Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8013901Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8014063Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8014135Z     
2025-04-11T04:23:18.8014244Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8014359Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8014603Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8014758Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8015045Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8015178Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8015337Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8015343Z 
2025-04-11T04:23:18.8015477Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8015696Z ______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] ______________
2025-04-11T04:23:18.8015706Z 
2025-04-11T04:23:18.8015839Z adamw = True, weight_decay = 0.0, p_dtype = torch.bfloat16
2025-04-11T04:23:18.8015924Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8015928Z 
2025-04-11T04:23:18.8016052Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8016178Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8016349Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8016500Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8016590Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8016717Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8016802Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8016938Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8017021Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8017107Z >       check_adam_kernel(
2025-04-11T04:23:18.8017360Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8017431Z         )
2025-04-11T04:23:18.8017438Z 
2025-04-11T04:23:18.8017551Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8017661Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8017818Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8017962Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8018121Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8018126Z 
2025-04-11T04:23:18.8018303Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3fdf0>, lr = 0.001
2025-04-11T04:23:18.8018457Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.8018462Z 
2025-04-11T04:23:18.8018702Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8018841Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8019002Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8019072Z     
2025-04-11T04:23:18.8019187Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8019301Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8019545Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8019650Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8019929Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8020117Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8020275Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8020279Z 
2025-04-11T04:23:18.8020416Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8020577Z _____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] ______________
2025-04-11T04:23:18.8020614Z 
2025-04-11T04:23:18.8020753Z adamw = False, weight_decay = 0.1, p_dtype = torch.bfloat16
2025-04-11T04:23:18.8020838Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8020842Z 
2025-04-11T04:23:18.8020962Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8021088Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8021256Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8021408Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8021496Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8021671Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8021760Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8021893Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8021976Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8022058Z >       check_adam_kernel(
2025-04-11T04:23:18.8022317Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8022387Z         )
2025-04-11T04:23:18.8022390Z 
2025-04-11T04:23:18.8022505Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8022615Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8022772Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8022916Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8023025Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8023032Z 
2025-04-11T04:23:18.8023207Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a885b0>, lr = 0.001
2025-04-11T04:23:18.8023356Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.8023360Z 
2025-04-11T04:23:18.8023600Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8023738Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8023902Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8024019Z     
2025-04-11T04:23:18.8024133Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8024248Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8024489Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8024600Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8024878Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8025015Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8025169Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8025175Z 
2025-04-11T04:23:18.8025312Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8025472Z ______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] ______________
2025-04-11T04:23:18.8025475Z 
2025-04-11T04:23:18.8025611Z adamw = True, weight_decay = 0.1, p_dtype = torch.bfloat16
2025-04-11T04:23:18.8025695Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8025699Z 
2025-04-11T04:23:18.8025816Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8025996Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T04:23:18.8026163Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8026315Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T04:23:18.8026400Z         rtol, atol = 1e-5, 1e-8
2025-04-11T04:23:18.8026529Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T04:23:18.8026664Z             rtol, atol = 1e-3, 1e-3
2025-04-11T04:23:18.8026795Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T04:23:18.8026887Z             rtol, atol = 4e-3, 4e-3
2025-04-11T04:23:18.8026969Z >       check_adam_kernel(
2025-04-11T04:23:18.8027233Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T04:23:18.8027304Z         )
2025-04-11T04:23:18.8027308Z 
2025-04-11T04:23:18.8027422Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T04:23:18.8027530Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8027732Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T04:23:18.8027882Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T04:23:18.8027987Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8027991Z 
2025-04-11T04:23:18.8028170Z self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3e350>, lr = 0.001
2025-04-11T04:23:18.8028319Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.8028323Z 
2025-04-11T04:23:18.8028596Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T04:23:18.8028736Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T04:23:18.8028901Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8028974Z     
2025-04-11T04:23:18.8029085Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8029205Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8029445Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8029552Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8029836Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8029975Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8030187Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8030192Z 
2025-04-11T04:23:18.8030330Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T04:23:18.8030514Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] ______
2025-04-11T04:23:18.8030520Z 
2025-04-11T04:23:18.8030677Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8030852Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8030936Z g_dtype = torch.float32
2025-04-11T04:23:18.8030940Z 
2025-04-11T04:23:18.8031108Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8031233Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8031388Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8031482Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8031638Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8031731Z         device: torch.device,
2025-04-11T04:23:18.8031808Z         adamw: bool,
2025-04-11T04:23:18.8031900Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8031984Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8032210Z     ) -> None:
2025-04-11T04:23:18.8032477Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8032574Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8032578Z 
2025-04-11T04:23:18.8032694Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8032805Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8033124Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8033218Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8033455Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8033550Z     return self._apply(convert)
2025-04-11T04:23:18.8033792Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8033880Z     module._apply(fn)
2025-04-11T04:23:18.8034126Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8034264Z     module._apply(fn)
2025-04-11T04:23:18.8034494Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8034573Z     module._apply(fn)
2025-04-11T04:23:18.8034806Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8034898Z     param_applied = fn(param)
2025-04-11T04:23:18.8035008Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8035012Z 
2025-04-11T04:23:18.8035100Z t = Parameter containing:
2025-04-11T04:23:18.8035243Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8035339Z         [ 0.0028, -0.0014,...0,  0.0213, -0.0091],
2025-04-11T04:23:18.8035465Z         [-0.0226, -0.0230, -0.0057,  ..., -0.0094, -0.0239, -0.0399]],
2025-04-11T04:23:18.8035554Z        requires_grad=True)
2025-04-11T04:23:18.8035558Z 
2025-04-11T04:23:18.8035634Z     def convert(t):
2025-04-11T04:23:18.8035771Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8035953Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8036075Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8036282Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8036390Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8036674Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8036862Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8037025Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8037032Z 
2025-04-11T04:23:18.8037284Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8037465Z _______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] _______
2025-04-11T04:23:18.8037469Z 
2025-04-11T04:23:18.8037617Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8037788Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8037873Z g_dtype = torch.float32
2025-04-11T04:23:18.8037877Z 
2025-04-11T04:23:18.8038043Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8038163Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8038318Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8038413Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8038567Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8038706Z         device: torch.device,
2025-04-11T04:23:18.8038783Z         adamw: bool,
2025-04-11T04:23:18.8038869Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8038956Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8039031Z     ) -> None:
2025-04-11T04:23:18.8039289Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8039388Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8039454Z 
2025-04-11T04:23:18.8039568Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8039676Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8039918Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8040017Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8040242Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8040335Z     return self._apply(convert)
2025-04-11T04:23:18.8040614Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8040698Z     module._apply(fn)
2025-04-11T04:23:18.8040928Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8041006Z     module._apply(fn)
2025-04-11T04:23:18.8041237Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8041317Z     module._apply(fn)
2025-04-11T04:23:18.8041547Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8041634Z     param_applied = fn(param)
2025-04-11T04:23:18.8041748Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8041752Z 
2025-04-11T04:23:18.8041840Z t = Parameter containing:
2025-04-11T04:23:18.8041972Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8042073Z         [-0.0048,  0.0237,...2, -0.0204,  0.0268],
2025-04-11T04:23:18.8042194Z         [ 0.0211,  0.0139,  0.0082,  ...,  0.0303, -0.0201, -0.0544]],
2025-04-11T04:23:18.8042280Z        requires_grad=True)
2025-04-11T04:23:18.8042284Z 
2025-04-11T04:23:18.8042361Z     def convert(t):
2025-04-11T04:23:18.8042493Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8042671Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8042789Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8043003Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8043160Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8043450Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8043587Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8043749Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8043753Z 
2025-04-11T04:23:18.8044005Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8044190Z _____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] ______
2025-04-11T04:23:18.8044194Z 
2025-04-11T04:23:18.8044362Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8044512Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8044600Z g_dtype = torch.float32
2025-04-11T04:23:18.8044604Z 
2025-04-11T04:23:18.8044771Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8044893Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8045044Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8045195Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8045354Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8045441Z         device: torch.device,
2025-04-11T04:23:18.8045521Z         adamw: bool,
2025-04-11T04:23:18.8045606Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8045695Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8045769Z     ) -> None:
2025-04-11T04:23:18.8046022Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8046166Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8046274Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8046354Z         lr = 1e-3
2025-04-11T04:23:18.8046440Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8046521Z         eps = 1e-8
2025-04-11T04:23:18.8046624Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8046855Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8047140Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8047145Z 
2025-04-11T04:23:18.8047256Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8047373Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8047379Z 
2025-04-11T04:23:18.8047460Z self = HybridAdam (
2025-04-11T04:23:18.8047545Z Parameter Group 0
2025-04-11T04:23:18.8047623Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8047707Z     bias_correction: True
2025-04-11T04:23:18.8047788Z     eps: 1e-08
2025-04-11T04:23:18.8047861Z     lr: 0.001
2025-04-11T04:23:18.8047952Z     weig...arameter Group 1
2025-04-11T04:23:18.8048029Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8048113Z     bias_correction: True
2025-04-11T04:23:18.8048187Z     eps: 1e-08
2025-04-11T04:23:18.8048258Z     lr: 0.001
2025-04-11T04:23:18.8048345Z     weight_decay: 0.0
2025-04-11T04:23:18.8048417Z )
2025-04-11T04:23:18.8048742Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8048890Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8049044Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8049147Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8049524Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8049529Z 
2025-04-11T04:23:18.8049659Z     def __init__(
2025-04-11T04:23:18.8049733Z         self,
2025-04-11T04:23:18.8049815Z         model_params,
2025-04-11T04:23:18.8049889Z         lr=1e-3,
2025-04-11T04:23:18.8049970Z         bias_correction=True,
2025-04-11T04:23:18.8050052Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8050125Z         eps=1e-8,
2025-04-11T04:23:18.8050207Z         weight_decay=0,
2025-04-11T04:23:18.8050287Z         adamw_mode=True,
2025-04-11T04:23:18.8050383Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8050488Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8050569Z         **defaults: Any,
2025-04-11T04:23:18.8050642Z     ):
2025-04-11T04:23:18.8050721Z         super().__init__(
2025-04-11T04:23:18.8050805Z             model_params,
2025-04-11T04:23:18.8050876Z             lr,
2025-04-11T04:23:18.8050957Z             bias_correction,
2025-04-11T04:23:18.8051033Z             betas,
2025-04-11T04:23:18.8051106Z             eps,
2025-04-11T04:23:18.8051186Z             weight_decay,
2025-04-11T04:23:18.8051263Z             adamw_mode,
2025-04-11T04:23:18.8051351Z             nvme_offload_fraction,
2025-04-11T04:23:18.8051436Z             nvme_offload_dir,
2025-04-11T04:23:18.8051507Z         )
2025-04-11T04:23:18.8051657Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8051773Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8051900Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8052116Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8052225Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8052517Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8052703Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8052868Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8052872Z 
2025-04-11T04:23:18.8053010Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8053194Z _____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] ______
2025-04-11T04:23:18.8053200Z 
2025-04-11T04:23:18.8053366Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8053582Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8053675Z g_dtype = torch.float32
2025-04-11T04:23:18.8053679Z 
2025-04-11T04:23:18.8053846Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8053968Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8054125Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8054217Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8054371Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8054457Z         device: torch.device,
2025-04-11T04:23:18.8054537Z         adamw: bool,
2025-04-11T04:23:18.8054621Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8054706Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8054782Z     ) -> None:
2025-04-11T04:23:18.8055036Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8055133Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8055137Z 
2025-04-11T04:23:18.8055246Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8055360Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8055609Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8055701Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8055931Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8056072Z     return self._apply(convert)
2025-04-11T04:23:18.8056305Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8056386Z     module._apply(fn)
2025-04-11T04:23:18.8056617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8056698Z     module._apply(fn)
2025-04-11T04:23:18.8056923Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8057001Z     module._apply(fn)
2025-04-11T04:23:18.8057225Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8057314Z     param_applied = fn(param)
2025-04-11T04:23:18.8057422Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8057426Z 
2025-04-11T04:23:18.8057516Z t = Parameter containing:
2025-04-11T04:23:18.8057651Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8057750Z         [ 0.0210, -0.0131,...8, -0.0156, -0.0054],
2025-04-11T04:23:18.8057864Z         [ 0.0148,  0.0292,  0.0008,  ...,  0.0355, -0.0048, -0.0186]],
2025-04-11T04:23:18.8058002Z        requires_grad=True)
2025-04-11T04:23:18.8058006Z 
2025-04-11T04:23:18.8058083Z     def convert(t):
2025-04-11T04:23:18.8058216Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8058399Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8058517Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8058730Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8058885Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8059173Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8059310Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8059470Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8059479Z 
2025-04-11T04:23:18.8059734Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8059960Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] _______
2025-04-11T04:23:18.8059965Z 
2025-04-11T04:23:18.8060133Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8060300Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8060389Z g_dtype = torch.float32
2025-04-11T04:23:18.8060393Z 
2025-04-11T04:23:18.8060557Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8060678Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8060832Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8060922Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8061078Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8061164Z         device: torch.device,
2025-04-11T04:23:18.8061245Z         adamw: bool,
2025-04-11T04:23:18.8061328Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8061416Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8061490Z     ) -> None:
2025-04-11T04:23:18.8061737Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8061836Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8061841Z 
2025-04-11T04:23:18.8061948Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8062058Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8062300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8062443Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8062669Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8062760Z     return self._apply(convert)
2025-04-11T04:23:18.8062992Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8063071Z     module._apply(fn)
2025-04-11T04:23:18.8063300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8063379Z     module._apply(fn)
2025-04-11T04:23:18.8063604Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8063683Z     module._apply(fn)
2025-04-11T04:23:18.8063906Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8063996Z     param_applied = fn(param)
2025-04-11T04:23:18.8064105Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8064108Z 
2025-04-11T04:23:18.8064198Z t = Parameter containing:
2025-04-11T04:23:18.8064330Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8064494Z         [ 0.0168, -0.0116,...1,  0.0247, -0.0168],
2025-04-11T04:23:18.8064614Z         [ 0.0078, -0.0201,  0.0158,  ..., -0.0204,  0.0234,  0.0068]],
2025-04-11T04:23:18.8064696Z        requires_grad=True)
2025-04-11T04:23:18.8064700Z 
2025-04-11T04:23:18.8064780Z     def convert(t):
2025-04-11T04:23:18.8064909Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8065092Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8065262Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8065470Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8065578Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8065857Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8065996Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8066197Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8066201Z 
2025-04-11T04:23:18.8066458Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8066635Z _______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] ________
2025-04-11T04:23:18.8066638Z 
2025-04-11T04:23:18.8066793Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8066956Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8067041Z g_dtype = torch.float32
2025-04-11T04:23:18.8067046Z 
2025-04-11T04:23:18.8067210Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8067328Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8067486Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8067578Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8067736Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8067819Z         device: torch.device,
2025-04-11T04:23:18.8067898Z         adamw: bool,
2025-04-11T04:23:18.8067981Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8068059Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8068137Z     ) -> None:
2025-04-11T04:23:18.8068381Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8068535Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8068540Z 
2025-04-11T04:23:18.8068647Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8068815Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8069056Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8069149Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8069379Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8069468Z     return self._apply(convert)
2025-04-11T04:23:18.8069698Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8069778Z     module._apply(fn)
2025-04-11T04:23:18.8070008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8070089Z     module._apply(fn)
2025-04-11T04:23:18.8070311Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8070394Z     module._apply(fn)
2025-04-11T04:23:18.8070621Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8070711Z     param_applied = fn(param)
2025-04-11T04:23:18.8070818Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8070872Z 
2025-04-11T04:23:18.8070962Z t = Parameter containing:
2025-04-11T04:23:18.8071095Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8071190Z         [ 0.0171, -0.0037,...8, -0.0068,  0.0037],
2025-04-11T04:23:18.8071309Z         [ 0.0260, -0.0271, -0.0247,  ...,  0.0262,  0.0078,  0.0236]],
2025-04-11T04:23:18.8071393Z        requires_grad=True)
2025-04-11T04:23:18.8071435Z 
2025-04-11T04:23:18.8071516Z     def convert(t):
2025-04-11T04:23:18.8071645Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8071825Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8071942Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8072148Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8072258Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8072591Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8072730Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8072887Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8072891Z 
2025-04-11T04:23:18.8073148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8073330Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] ______
2025-04-11T04:23:18.8073333Z 
2025-04-11T04:23:18.8073501Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8073655Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8073739Z g_dtype = torch.float32
2025-04-11T04:23:18.8073743Z 
2025-04-11T04:23:18.8073911Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8074034Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8074193Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8074283Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8074439Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8074523Z         device: torch.device,
2025-04-11T04:23:18.8074599Z         adamw: bool,
2025-04-11T04:23:18.8074690Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8074771Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8074848Z     ) -> None:
2025-04-11T04:23:18.8075098Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8075243Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8075352Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8075425Z         lr = 1e-3
2025-04-11T04:23:18.8075514Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8075590Z         eps = 1e-8
2025-04-11T04:23:18.8075697Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8075925Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8076140Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8076147Z 
2025-04-11T04:23:18.8076258Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8076367Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8076371Z 
2025-04-11T04:23:18.8076453Z self = HybridAdam (
2025-04-11T04:23:18.8076533Z Parameter Group 0
2025-04-11T04:23:18.8076617Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8076699Z     bias_correction: True
2025-04-11T04:23:18.8076775Z     eps: 1e-08
2025-04-11T04:23:18.8076852Z     lr: 0.001
2025-04-11T04:23:18.8076937Z     weig...arameter Group 1
2025-04-11T04:23:18.8077068Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8077150Z     bias_correction: True
2025-04-11T04:23:18.8077229Z     eps: 1e-08
2025-04-11T04:23:18.8077305Z     lr: 0.001
2025-04-11T04:23:18.8077384Z     weight_decay: 0.0
2025-04-11T04:23:18.8077457Z )
2025-04-11T04:23:18.8077776Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8077977Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8078125Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8078220Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8078597Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8078602Z 
2025-04-11T04:23:18.8078676Z     def __init__(
2025-04-11T04:23:18.8078754Z         self,
2025-04-11T04:23:18.8078832Z         model_params,
2025-04-11T04:23:18.8078909Z         lr=1e-3,
2025-04-11T04:23:18.8079042Z         bias_correction=True,
2025-04-11T04:23:18.8079124Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8079202Z         eps=1e-8,
2025-04-11T04:23:18.8079283Z         weight_decay=0,
2025-04-11T04:23:18.8079365Z         adamw_mode=True,
2025-04-11T04:23:18.8079461Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8079567Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8079649Z         **defaults: Any,
2025-04-11T04:23:18.8079719Z     ):
2025-04-11T04:23:18.8079807Z         super().__init__(
2025-04-11T04:23:18.8079886Z             model_params,
2025-04-11T04:23:18.8079956Z             lr,
2025-04-11T04:23:18.8080045Z             bias_correction,
2025-04-11T04:23:18.8080118Z             betas,
2025-04-11T04:23:18.8080194Z             eps,
2025-04-11T04:23:18.8080271Z             weight_decay,
2025-04-11T04:23:18.8080352Z             adamw_mode,
2025-04-11T04:23:18.8080441Z             nvme_offload_fraction,
2025-04-11T04:23:18.8080523Z             nvme_offload_dir,
2025-04-11T04:23:18.8080596Z         )
2025-04-11T04:23:18.8080689Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8080810Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8080932Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8081144Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8081256Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8081598Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8081742Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8081902Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8081908Z 
2025-04-11T04:23:18.8082046Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8082229Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] ______
2025-04-11T04:23:18.8082233Z 
2025-04-11T04:23:18.8082400Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8082566Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8082651Z g_dtype = torch.float32
2025-04-11T04:23:18.8082655Z 
2025-04-11T04:23:18.8082824Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8082945Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8083105Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8083195Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8083354Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8083488Z         device: torch.device,
2025-04-11T04:23:18.8083566Z         adamw: bool,
2025-04-11T04:23:18.8083655Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8083741Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8083818Z     ) -> None:
2025-04-11T04:23:18.8084073Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8084171Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8084225Z 
2025-04-11T04:23:18.8084336Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8084447Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8084703Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8084796Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8085026Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8085118Z     return self._apply(convert)
2025-04-11T04:23:18.8085358Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8085487Z     module._apply(fn)
2025-04-11T04:23:18.8085720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8085802Z     module._apply(fn)
2025-04-11T04:23:18.8086035Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8086116Z     module._apply(fn)
2025-04-11T04:23:18.8086344Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8086435Z     param_applied = fn(param)
2025-04-11T04:23:18.8086545Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8086549Z 
2025-04-11T04:23:18.8086636Z t = Parameter containing:
2025-04-11T04:23:18.8086772Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8086869Z         [-0.0301, -0.0063,...5, -0.0105,  0.0078],
2025-04-11T04:23:18.8086991Z         [-0.0225,  0.0108,  0.0321,  ..., -0.0056, -0.0089, -0.0360]],
2025-04-11T04:23:18.8087073Z        requires_grad=True)
2025-04-11T04:23:18.8087077Z 
2025-04-11T04:23:18.8087158Z     def convert(t):
2025-04-11T04:23:18.8087289Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8087470Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8087593Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8087802Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8087980Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8088286Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8088425Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8088585Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8088589Z 
2025-04-11T04:23:18.8088844Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8089021Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] ______
2025-04-11T04:23:18.8089025Z 
2025-04-11T04:23:18.8089189Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8089362Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8089446Z g_dtype = torch.float16
2025-04-11T04:23:18.8089450Z 
2025-04-11T04:23:18.8089620Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8089740Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8089896Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8090036Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8090194Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8090283Z         device: torch.device,
2025-04-11T04:23:18.8090360Z         adamw: bool,
2025-04-11T04:23:18.8090447Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8090528Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8090603Z     ) -> None:
2025-04-11T04:23:18.8090857Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8091008Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8091012Z 
2025-04-11T04:23:18.8091125Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8091236Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8091483Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8091577Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8091859Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8091949Z     return self._apply(convert)
2025-04-11T04:23:18.8092181Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8092266Z     module._apply(fn)
2025-04-11T04:23:18.8092502Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8092588Z     module._apply(fn)
2025-04-11T04:23:18.8092821Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8092902Z     module._apply(fn)
2025-04-11T04:23:18.8093136Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8093222Z     param_applied = fn(param)
2025-04-11T04:23:18.8093333Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8093339Z 
2025-04-11T04:23:18.8093425Z t = Parameter containing:
2025-04-11T04:23:18.8093563Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8093657Z         [-0.0063,  0.0127,...8,  0.0139, -0.0372],
2025-04-11T04:23:18.8093777Z         [-0.0001,  0.0211,  0.0425,  ..., -0.0074,  0.0182,  0.0033]],
2025-04-11T04:23:18.8093861Z        requires_grad=True)
2025-04-11T04:23:18.8093864Z 
2025-04-11T04:23:18.8093943Z     def convert(t):
2025-04-11T04:23:18.8094076Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8094253Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8094424Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8094631Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8094739Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8095022Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8095156Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8095316Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8095320Z 
2025-04-11T04:23:18.8095572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8095755Z _______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] _______
2025-04-11T04:23:18.8095759Z 
2025-04-11T04:23:18.8095909Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8096081Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8096164Z g_dtype = torch.float16
2025-04-11T04:23:18.8096168Z 
2025-04-11T04:23:18.8096335Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8096507Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8096666Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8096760Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8096911Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8096996Z         device: torch.device,
2025-04-11T04:23:18.8097073Z         adamw: bool,
2025-04-11T04:23:18.8097213Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8097292Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8097366Z     ) -> None:
2025-04-11T04:23:18.8097620Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8097714Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8097718Z 
2025-04-11T04:23:18.8097831Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8097940Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8098233Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8098327Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8098550Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8098643Z     return self._apply(convert)
2025-04-11T04:23:18.8098871Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8098954Z     module._apply(fn)
2025-04-11T04:23:18.8099183Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8099265Z     module._apply(fn)
2025-04-11T04:23:18.8099494Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8099571Z     module._apply(fn)
2025-04-11T04:23:18.8099800Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8099888Z     param_applied = fn(param)
2025-04-11T04:23:18.8100001Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8100005Z 
2025-04-11T04:23:18.8100092Z t = Parameter containing:
2025-04-11T04:23:18.8100225Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8100320Z         [ 0.0058,  0.0119,...4, -0.0198,  0.0151],
2025-04-11T04:23:18.8100440Z         [-0.0479,  0.0136, -0.0425,  ..., -0.0021, -0.0081,  0.0171]],
2025-04-11T04:23:18.8100528Z        requires_grad=True)
2025-04-11T04:23:18.8100532Z 
2025-04-11T04:23:18.8100608Z     def convert(t):
2025-04-11T04:23:18.8100791Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8100968Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8101087Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8101295Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8101401Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8101688Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8101820Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8101981Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8101985Z 
2025-04-11T04:23:18.8102234Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8102422Z _____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] ______
2025-04-11T04:23:18.8102425Z 
2025-04-11T04:23:18.8102590Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8102745Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8102881Z g_dtype = torch.float16
2025-04-11T04:23:18.8102885Z 
2025-04-11T04:23:18.8103055Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8103179Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8103333Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8103426Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8103631Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8103717Z         device: torch.device,
2025-04-11T04:23:18.8103794Z         adamw: bool,
2025-04-11T04:23:18.8103877Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8103963Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8104036Z     ) -> None:
2025-04-11T04:23:18.8104287Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8104384Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8104491Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8104620Z         lr = 1e-3
2025-04-11T04:23:18.8104707Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8104789Z         eps = 1e-8
2025-04-11T04:23:18.8104893Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8105127Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8105342Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8105346Z 
2025-04-11T04:23:18.8105455Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8105570Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8105574Z 
2025-04-11T04:23:18.8105654Z self = HybridAdam (
2025-04-11T04:23:18.8105737Z Parameter Group 0
2025-04-11T04:23:18.8105816Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8105903Z     bias_correction: True
2025-04-11T04:23:18.8105978Z     eps: 1e-08
2025-04-11T04:23:18.8106051Z     lr: 0.001
2025-04-11T04:23:18.8106139Z     weig...arameter Group 1
2025-04-11T04:23:18.8106216Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8106300Z     bias_correction: True
2025-04-11T04:23:18.8106374Z     eps: 1e-08
2025-04-11T04:23:18.8106446Z     lr: 0.001
2025-04-11T04:23:18.8106528Z     weight_decay: 0.0
2025-04-11T04:23:18.8106600Z )
2025-04-11T04:23:18.8106920Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8107068Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8107275Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8107373Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8107754Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8107766Z 
2025-04-11T04:23:18.8107840Z     def __init__(
2025-04-11T04:23:18.8107913Z         self,
2025-04-11T04:23:18.8107996Z         model_params,
2025-04-11T04:23:18.8108068Z         lr=1e-3,
2025-04-11T04:23:18.8108153Z         bias_correction=True,
2025-04-11T04:23:18.8108231Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8108308Z         eps=1e-8,
2025-04-11T04:23:18.8108390Z         weight_decay=0,
2025-04-11T04:23:18.8108503Z         adamw_mode=True,
2025-04-11T04:23:18.8108602Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8108705Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8108785Z         **defaults: Any,
2025-04-11T04:23:18.8108860Z     ):
2025-04-11T04:23:18.8108940Z         super().__init__(
2025-04-11T04:23:18.8109023Z             model_params,
2025-04-11T04:23:18.8109095Z             lr,
2025-04-11T04:23:18.8109238Z             bias_correction,
2025-04-11T04:23:18.8109314Z             betas,
2025-04-11T04:23:18.8109387Z             eps,
2025-04-11T04:23:18.8109469Z             weight_decay,
2025-04-11T04:23:18.8109546Z             adamw_mode,
2025-04-11T04:23:18.8109634Z             nvme_offload_fraction,
2025-04-11T04:23:18.8109717Z             nvme_offload_dir,
2025-04-11T04:23:18.8109788Z         )
2025-04-11T04:23:18.8109884Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8110068Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8110194Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8110405Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8110515Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8110804Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8110944Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8111170Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8111174Z 
2025-04-11T04:23:18.8111310Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8111493Z _____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] ______
2025-04-11T04:23:18.8111496Z 
2025-04-11T04:23:18.8111662Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8111831Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8111914Z g_dtype = torch.float16
2025-04-11T04:23:18.8111918Z 
2025-04-11T04:23:18.8112084Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8112208Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8112359Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8112453Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8112609Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8112695Z         device: torch.device,
2025-04-11T04:23:18.8112772Z         adamw: bool,
2025-04-11T04:23:18.8112857Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8112942Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8113017Z     ) -> None:
2025-04-11T04:23:18.8113270Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8113367Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8113371Z 
2025-04-11T04:23:18.8113485Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8113650Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8113899Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8113996Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8114226Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8114319Z     return self._apply(convert)
2025-04-11T04:23:18.8114551Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8114634Z     module._apply(fn)
2025-04-11T04:23:18.8114863Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8114943Z     module._apply(fn)
2025-04-11T04:23:18.8115175Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8115252Z     module._apply(fn)
2025-04-11T04:23:18.8115482Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8115568Z     param_applied = fn(param)
2025-04-11T04:23:18.8115679Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8115732Z 
2025-04-11T04:23:18.8115821Z t = Parameter containing:
2025-04-11T04:23:18.8115954Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8116053Z         [ 0.0127, -0.0053,...6, -0.0203,  0.0294],
2025-04-11T04:23:18.8116173Z         [ 0.0315,  0.0270, -0.0379,  ...,  0.0044, -0.0077,  0.0209]],
2025-04-11T04:23:18.8116260Z        requires_grad=True)
2025-04-11T04:23:18.8116264Z 
2025-04-11T04:23:18.8116378Z     def convert(t):
2025-04-11T04:23:18.8116510Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8116686Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8116803Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8117011Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8117118Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8117455Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8117591Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8117752Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8117756Z 
2025-04-11T04:23:18.8118004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8118183Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] _______
2025-04-11T04:23:18.8118187Z 
2025-04-11T04:23:18.8118348Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8118517Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8118603Z g_dtype = torch.float16
2025-04-11T04:23:18.8118607Z 
2025-04-11T04:23:18.8118771Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8118895Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8119048Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8119142Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8119294Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8119377Z         device: torch.device,
2025-04-11T04:23:18.8119458Z         adamw: bool,
2025-04-11T04:23:18.8119545Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8119629Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8119702Z     ) -> None:
2025-04-11T04:23:18.8119950Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8120097Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8120102Z 
2025-04-11T04:23:18.8120212Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8120324Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8120572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8120667Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8120895Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8120990Z     return self._apply(convert)
2025-04-11T04:23:18.8121224Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8121310Z     module._apply(fn)
2025-04-11T04:23:18.8121548Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8121629Z     module._apply(fn)
2025-04-11T04:23:18.8121872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8121954Z     module._apply(fn)
2025-04-11T04:23:18.8122199Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8122337Z     param_applied = fn(param)
2025-04-11T04:23:18.8122450Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8122454Z 
2025-04-11T04:23:18.8122544Z t = Parameter containing:
2025-04-11T04:23:18.8122675Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8122771Z         [ 0.0126,  0.0307,...5,  0.0153,  0.0116],
2025-04-11T04:23:18.8122927Z         [-0.0007,  0.0044, -0.0020,  ..., -0.0033,  0.0164, -0.0073]],
2025-04-11T04:23:18.8123014Z        requires_grad=True)
2025-04-11T04:23:18.8123018Z 
2025-04-11T04:23:18.8123094Z     def convert(t):
2025-04-11T04:23:18.8123221Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8123399Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8123515Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8123774Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8123883Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8124169Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8124303Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8124461Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8124465Z 
2025-04-11T04:23:18.8124721Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8124900Z _______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] ________
2025-04-11T04:23:18.8124904Z 
2025-04-11T04:23:18.8125057Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8125225Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8125314Z g_dtype = torch.float16
2025-04-11T04:23:18.8125317Z 
2025-04-11T04:23:18.8125483Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8125603Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8125756Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8125846Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8126004Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8126088Z         device: torch.device,
2025-04-11T04:23:18.8126191Z         adamw: bool,
2025-04-11T04:23:18.8126276Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8126409Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8126488Z     ) -> None:
2025-04-11T04:23:18.8126742Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8126842Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8126846Z 
2025-04-11T04:23:18.8126957Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8127069Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8127310Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8127406Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8127632Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8127720Z     return self._apply(convert)
2025-04-11T04:23:18.8127961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8128045Z     module._apply(fn)
2025-04-11T04:23:18.8128283Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8128362Z     module._apply(fn)
2025-04-11T04:23:18.8128647Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8128727Z     module._apply(fn)
2025-04-11T04:23:18.8128958Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8129047Z     param_applied = fn(param)
2025-04-11T04:23:18.8129153Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8129157Z 
2025-04-11T04:23:18.8129304Z t = Parameter containing:
2025-04-11T04:23:18.8129436Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8129533Z         [ 0.0062,  0.0098,...3, -0.0036,  0.0170],
2025-04-11T04:23:18.8129647Z         [ 0.0053,  0.0281, -0.0163,  ..., -0.0098, -0.0364,  0.0040]],
2025-04-11T04:23:18.8129733Z        requires_grad=True)
2025-04-11T04:23:18.8129737Z 
2025-04-11T04:23:18.8129818Z     def convert(t):
2025-04-11T04:23:18.8129946Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8130127Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8130290Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8130501Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8130608Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8130891Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8131032Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8131189Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8131195Z 
2025-04-11T04:23:18.8131450Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8131628Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] ______
2025-04-11T04:23:18.8131634Z 
2025-04-11T04:23:18.8131804Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8131956Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8132041Z g_dtype = torch.float16
2025-04-11T04:23:18.8132045Z 
2025-04-11T04:23:18.8132208Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8132327Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8132484Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8132576Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8132732Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8132956Z         device: torch.device,
2025-04-11T04:23:18.8133042Z         adamw: bool,
2025-04-11T04:23:18.8133128Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8133211Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8133291Z     ) -> None:
2025-04-11T04:23:18.8133544Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8133641Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8133748Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8133823Z         lr = 1e-3
2025-04-11T04:23:18.8133912Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8133992Z         eps = 1e-8
2025-04-11T04:23:18.8134100Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8134330Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8134551Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8134555Z 
2025-04-11T04:23:18.8134666Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8134775Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8134832Z 
2025-04-11T04:23:18.8134914Z self = HybridAdam (
2025-04-11T04:23:18.8134998Z Parameter Group 0
2025-04-11T04:23:18.8135082Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8135165Z     bias_correction: True
2025-04-11T04:23:18.8135243Z     eps: 1e-08
2025-04-11T04:23:18.8135317Z     lr: 0.001
2025-04-11T04:23:18.8135403Z     weig...arameter Group 1
2025-04-11T04:23:18.8135485Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8135639Z     bias_correction: True
2025-04-11T04:23:18.8135717Z     eps: 1e-08
2025-04-11T04:23:18.8135790Z     lr: 0.001
2025-04-11T04:23:18.8135869Z     weight_decay: 0.0
2025-04-11T04:23:18.8135941Z )
2025-04-11T04:23:18.8136255Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8136403Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8136554Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8136650Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8137065Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8137070Z 
2025-04-11T04:23:18.8137144Z     def __init__(
2025-04-11T04:23:18.8137220Z         self,
2025-04-11T04:23:18.8137300Z         model_params,
2025-04-11T04:23:18.8137376Z         lr=1e-3,
2025-04-11T04:23:18.8137458Z         bias_correction=True,
2025-04-11T04:23:18.8137540Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8137614Z         eps=1e-8,
2025-04-11T04:23:18.8137690Z         weight_decay=0,
2025-04-11T04:23:18.8137777Z         adamw_mode=True,
2025-04-11T04:23:18.8137872Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8137978Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8138057Z         **defaults: Any,
2025-04-11T04:23:18.8138128Z     ):
2025-04-11T04:23:18.8138211Z         super().__init__(
2025-04-11T04:23:18.8138290Z             model_params,
2025-04-11T04:23:18.8138366Z             lr,
2025-04-11T04:23:18.8138447Z             bias_correction,
2025-04-11T04:23:18.8138520Z             betas,
2025-04-11T04:23:18.8138595Z             eps,
2025-04-11T04:23:18.8138671Z             weight_decay,
2025-04-11T04:23:18.8138750Z             adamw_mode,
2025-04-11T04:23:18.8138837Z             nvme_offload_fraction,
2025-04-11T04:23:18.8138919Z             nvme_offload_dir,
2025-04-11T04:23:18.8138993Z         )
2025-04-11T04:23:18.8139084Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8139201Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8139374Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8139591Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8139697Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8139984Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8140127Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8140286Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8140290Z 
2025-04-11T04:23:18.8140427Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8140606Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] ______
2025-04-11T04:23:18.8140610Z 
2025-04-11T04:23:18.8140778Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8140940Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8141028Z g_dtype = torch.float16
2025-04-11T04:23:18.8141032Z 
2025-04-11T04:23:18.8141198Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8141373Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8141533Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8141622Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8141778Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8141862Z         device: torch.device,
2025-04-11T04:23:18.8141941Z         adamw: bool,
2025-04-11T04:23:18.8142074Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8142155Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8142232Z     ) -> None:
2025-04-11T04:23:18.8142483Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8142581Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8142585Z 
2025-04-11T04:23:18.8142692Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8142801Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8143099Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8143194Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8143423Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8143511Z     return self._apply(convert)
2025-04-11T04:23:18.8143751Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8143833Z     module._apply(fn)
2025-04-11T04:23:18.8144061Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8144144Z     module._apply(fn)
2025-04-11T04:23:18.8144373Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8144454Z     module._apply(fn)
2025-04-11T04:23:18.8144680Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8144773Z     param_applied = fn(param)
2025-04-11T04:23:18.8144884Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8144888Z 
2025-04-11T04:23:18.8144978Z t = Parameter containing:
2025-04-11T04:23:18.8145110Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8145206Z         [ 0.0145, -0.0268,...4,  0.0235, -0.0067],
2025-04-11T04:23:18.8145329Z         [-0.0276, -0.0061,  0.0080,  ...,  0.0096,  0.0016, -0.0028]],
2025-04-11T04:23:18.8145411Z        requires_grad=True)
2025-04-11T04:23:18.8145415Z 
2025-04-11T04:23:18.8145494Z     def convert(t):
2025-04-11T04:23:18.8145672Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8145855Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8145973Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8146183Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8146297Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8146583Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8146721Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8146882Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8146886Z 
2025-04-11T04:23:18.8147141Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8147322Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] ______
2025-04-11T04:23:18.8147325Z 
2025-04-11T04:23:18.8147491Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8147659Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8147797Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8147801Z 
2025-04-11T04:23:18.8147974Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8148095Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8148252Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8148344Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8148607Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8148690Z         device: torch.device,
2025-04-11T04:23:18.8148768Z         adamw: bool,
2025-04-11T04:23:18.8148855Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8148939Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8149018Z     ) -> None:
2025-04-11T04:23:18.8149267Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8149363Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8149370Z 
2025-04-11T04:23:18.8149540Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8149652Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8149898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8149988Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8150217Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8150304Z     return self._apply(convert)
2025-04-11T04:23:18.8150535Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8150621Z     module._apply(fn)
2025-04-11T04:23:18.8150855Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8150938Z     module._apply(fn)
2025-04-11T04:23:18.8151170Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8151250Z     module._apply(fn)
2025-04-11T04:23:18.8151484Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8151570Z     param_applied = fn(param)
2025-04-11T04:23:18.8151680Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8151684Z 
2025-04-11T04:23:18.8151771Z t = Parameter containing:
2025-04-11T04:23:18.8151905Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8151998Z         [ 0.0360, -0.0060,...2,  0.0336, -0.0315],
2025-04-11T04:23:18.8152117Z         [ 0.0418,  0.0034,  0.0053,  ...,  0.0279, -0.0100,  0.0020]],
2025-04-11T04:23:18.8152256Z        requires_grad=True)
2025-04-11T04:23:18.8152260Z 
2025-04-11T04:23:18.8152343Z     def convert(t):
2025-04-11T04:23:18.8152473Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8152651Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8152772Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8152976Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8153086Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8153367Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8153507Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8153665Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8153671Z 
2025-04-11T04:23:18.8153920Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8154098Z _______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] _______
2025-04-11T04:23:18.8154155Z 
2025-04-11T04:23:18.8154308Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8154483Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8154569Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8154573Z 
2025-04-11T04:23:18.8154741Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8154902Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8155058Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8155150Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8155305Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8155399Z         device: torch.device,
2025-04-11T04:23:18.8155476Z         adamw: bool,
2025-04-11T04:23:18.8155562Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8155645Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8155720Z     ) -> None:
2025-04-11T04:23:18.8156023Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8156119Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8156123Z 
2025-04-11T04:23:18.8156235Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8156344Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8156587Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8156679Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8156898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8156990Z     return self._apply(convert)
2025-04-11T04:23:18.8157219Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8157302Z     module._apply(fn)
2025-04-11T04:23:18.8157531Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8157614Z     module._apply(fn)
2025-04-11T04:23:18.8157841Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8157918Z     module._apply(fn)
2025-04-11T04:23:18.8158146Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8158234Z     param_applied = fn(param)
2025-04-11T04:23:18.8158346Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8158349Z 
2025-04-11T04:23:18.8158435Z t = Parameter containing:
2025-04-11T04:23:18.8158619Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8158717Z         [-0.0029, -0.0003,...8,  0.0132,  0.0134],
2025-04-11T04:23:18.8158834Z         [-0.0017, -0.0011, -0.0088,  ...,  0.0178,  0.0258,  0.0116]],
2025-04-11T04:23:18.8158922Z        requires_grad=True)
2025-04-11T04:23:18.8158926Z 
2025-04-11T04:23:18.8159002Z     def convert(t):
2025-04-11T04:23:18.8159135Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8159308Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8159425Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8159629Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8159740Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8160021Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8160157Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8160320Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8160385Z 
2025-04-11T04:23:18.8160634Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8160825Z _____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] ______
2025-04-11T04:23:18.8160829Z 
2025-04-11T04:23:18.8160993Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8161146Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8161282Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8161285Z 
2025-04-11T04:23:18.8161457Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8161580Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8161735Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8161828Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8161982Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8162070Z         device: torch.device,
2025-04-11T04:23:18.8162148Z         adamw: bool,
2025-04-11T04:23:18.8162231Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8162364Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8162442Z     ) -> None:
2025-04-11T04:23:18.8162698Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8162791Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8162904Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8162979Z         lr = 1e-3
2025-04-11T04:23:18.8163065Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8163147Z         eps = 1e-8
2025-04-11T04:23:18.8163249Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8163482Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8163703Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8163709Z 
2025-04-11T04:23:18.8163821Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8163934Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8163938Z 
2025-04-11T04:23:18.8164018Z self = HybridAdam (
2025-04-11T04:23:18.8164102Z Parameter Group 0
2025-04-11T04:23:18.8164180Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8164265Z     bias_correction: True
2025-04-11T04:23:18.8164342Z     eps: 1e-08
2025-04-11T04:23:18.8164415Z     lr: 0.001
2025-04-11T04:23:18.8164504Z     weig...arameter Group 1
2025-04-11T04:23:18.8164580Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8164666Z     bias_correction: True
2025-04-11T04:23:18.8164740Z     eps: 1e-08
2025-04-11T04:23:18.8164863Z     lr: 0.001
2025-04-11T04:23:18.8164947Z     weight_decay: 0.0
2025-04-11T04:23:18.8165016Z )
2025-04-11T04:23:18.8165334Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8165484Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8165647Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8165740Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8166116Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8166122Z 
2025-04-11T04:23:18.8166195Z     def __init__(
2025-04-11T04:23:18.8166267Z         self,
2025-04-11T04:23:18.8166348Z         model_params,
2025-04-11T04:23:18.8166421Z         lr=1e-3,
2025-04-11T04:23:18.8166506Z         bias_correction=True,
2025-04-11T04:23:18.8166583Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8166657Z         eps=1e-8,
2025-04-11T04:23:18.8166737Z         weight_decay=0,
2025-04-11T04:23:18.8166814Z         adamw_mode=True,
2025-04-11T04:23:18.8166963Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8167070Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8167150Z         **defaults: Any,
2025-04-11T04:23:18.8167219Z     ):
2025-04-11T04:23:18.8167298Z         super().__init__(
2025-04-11T04:23:18.8167380Z             model_params,
2025-04-11T04:23:18.8167453Z             lr,
2025-04-11T04:23:18.8167542Z             bias_correction,
2025-04-11T04:23:18.8167664Z             betas,
2025-04-11T04:23:18.8167737Z             eps,
2025-04-11T04:23:18.8167818Z             weight_decay,
2025-04-11T04:23:18.8167894Z             adamw_mode,
2025-04-11T04:23:18.8167982Z             nvme_offload_fraction,
2025-04-11T04:23:18.8168060Z             nvme_offload_dir,
2025-04-11T04:23:18.8168132Z         )
2025-04-11T04:23:18.8168228Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8168343Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8168467Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8168726Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8168842Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8169131Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8169269Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8169437Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8169441Z 
2025-04-11T04:23:18.8169575Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8169760Z _____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] ______
2025-04-11T04:23:18.8169763Z 
2025-04-11T04:23:18.8169926Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8170095Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T04:23:18.8170181Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8170185Z 
2025-04-11T04:23:18.8170356Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8170478Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8170631Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8170724Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8170879Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8170964Z         device: torch.device,
2025-04-11T04:23:18.8171040Z         adamw: bool,
2025-04-11T04:23:18.8171123Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8171260Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8171337Z     ) -> None:
2025-04-11T04:23:18.8171594Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8171690Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8171694Z 
2025-04-11T04:23:18.8171806Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8171917Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8172160Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8172256Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8172482Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8172574Z     return self._apply(convert)
2025-04-11T04:23:18.8172809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8172893Z     module._apply(fn)
2025-04-11T04:23:18.8173126Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8173205Z     module._apply(fn)
2025-04-11T04:23:18.8173500Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8173576Z     module._apply(fn)
2025-04-11T04:23:18.8173807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8173892Z     param_applied = fn(param)
2025-04-11T04:23:18.8174004Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8174008Z 
2025-04-11T04:23:18.8174145Z t = Parameter containing:
2025-04-11T04:23:18.8174279Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8174377Z         [ 0.0281,  0.0026,...4, -0.0037,  0.0294],
2025-04-11T04:23:18.8174493Z         [ 0.0003,  0.0104, -0.0075,  ...,  0.0078,  0.0005, -0.0179]],
2025-04-11T04:23:18.8174581Z        requires_grad=True)
2025-04-11T04:23:18.8174585Z 
2025-04-11T04:23:18.8174662Z     def convert(t):
2025-04-11T04:23:18.8174792Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8174972Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8175136Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8175348Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8175456Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8175746Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8175882Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8176042Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8176048Z 
2025-04-11T04:23:18.8176297Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8176475Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] _______
2025-04-11T04:23:18.8176481Z 
2025-04-11T04:23:18.8176643Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T04:23:18.8176817Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8176903Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8176907Z 
2025-04-11T04:23:18.8177071Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8177198Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8177353Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8177447Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8177598Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8177737Z         device: torch.device,
2025-04-11T04:23:18.8177817Z         adamw: bool,
2025-04-11T04:23:18.8177901Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8177986Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8178063Z     ) -> None:
2025-04-11T04:23:18.8178320Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8178413Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8178417Z 
2025-04-11T04:23:18.8178524Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8178634Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8178876Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8178971Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8179195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8179286Z     return self._apply(convert)
2025-04-11T04:23:18.8179513Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8179593Z     module._apply(fn)
2025-04-11T04:23:18.8179874Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8179955Z     module._apply(fn)
2025-04-11T04:23:18.8180185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8180264Z     module._apply(fn)
2025-04-11T04:23:18.8180495Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8180632Z     param_applied = fn(param)
2025-04-11T04:23:18.8180740Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8180747Z 
2025-04-11T04:23:18.8180834Z t = Parameter containing:
2025-04-11T04:23:18.8180968Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8181067Z         [ 0.0240, -0.0152,...6, -0.0175, -0.0244],
2025-04-11T04:23:18.8181186Z         [-0.0064, -0.0248,  0.0195,  ..., -0.0030, -0.0263,  0.0248]],
2025-04-11T04:23:18.8181273Z        requires_grad=True)
2025-04-11T04:23:18.8181277Z 
2025-04-11T04:23:18.8181353Z     def convert(t):
2025-04-11T04:23:18.8181529Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8181712Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8181827Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8182034Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8182144Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8182431Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8182566Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8182725Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8182731Z 
2025-04-11T04:23:18.8182984Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8183162Z _______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] ________
2025-04-11T04:23:18.8183169Z 
2025-04-11T04:23:18.8183319Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T04:23:18.8183484Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8183576Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8183580Z 
2025-04-11T04:23:18.8183746Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8183866Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8184090Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8184189Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8184341Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8184426Z         device: torch.device,
2025-04-11T04:23:18.8184505Z         adamw: bool,
2025-04-11T04:23:18.8184590Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8184676Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8184749Z     ) -> None:
2025-04-11T04:23:18.8184996Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8185092Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8185098Z 
2025-04-11T04:23:18.8185205Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8185318Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8185559Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8185654Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8185879Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8186017Z     return self._apply(convert)
2025-04-11T04:23:18.8186256Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8186335Z     module._apply(fn)
2025-04-11T04:23:18.8186568Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8186647Z     module._apply(fn)
2025-04-11T04:23:18.8186880Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8187007Z     module._apply(fn)
2025-04-11T04:23:18.8187232Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8187325Z     param_applied = fn(param)
2025-04-11T04:23:18.8187435Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8187438Z 
2025-04-11T04:23:18.8187529Z t = Parameter containing:
2025-04-11T04:23:18.8187660Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8187758Z         [ 0.0105,  0.0235,...3,  0.0204, -0.0137],
2025-04-11T04:23:18.8187924Z         [ 0.0001, -0.0009, -0.0197,  ...,  0.0352, -0.0017,  0.0075]],
2025-04-11T04:23:18.8188011Z        requires_grad=True)
2025-04-11T04:23:18.8188020Z 
2025-04-11T04:23:18.8188096Z     def convert(t):
2025-04-11T04:23:18.8188225Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8188403Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8188550Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8188760Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8188870Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8189157Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8189293Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8189451Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8189455Z 
2025-04-11T04:23:18.8189711Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8189890Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] ______
2025-04-11T04:23:18.8189895Z 
2025-04-11T04:23:18.8190063Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8190215Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8190304Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8190308Z 
2025-04-11T04:23:18.8190532Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8190657Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8190809Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8190901Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8191062Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8191146Z         device: torch.device,
2025-04-11T04:23:18.8191227Z         adamw: bool,
2025-04-11T04:23:18.8191309Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8191391Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8191469Z     ) -> None:
2025-04-11T04:23:18.8191719Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8191815Z         torch_model = model_fn().to(device)
2025-04-11T04:23:18.8191923Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T04:23:18.8192000Z         lr = 1e-3
2025-04-11T04:23:18.8192086Z         beta1, beta2 = 0.9, 0.999
2025-04-11T04:23:18.8192166Z         eps = 1e-8
2025-04-11T04:23:18.8192271Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T04:23:18.8192551Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T04:23:18.8192769Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8192773Z 
2025-04-11T04:23:18.8192881Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T04:23:18.8192994Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8193052Z 
2025-04-11T04:23:18.8193133Z self = HybridAdam (
2025-04-11T04:23:18.8193212Z Parameter Group 0
2025-04-11T04:23:18.8193295Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8193380Z     bias_correction: True
2025-04-11T04:23:18.8193457Z     eps: 1e-08
2025-04-11T04:23:18.8193531Z     lr: 0.001
2025-04-11T04:23:18.8193620Z     weig...arameter Group 1
2025-04-11T04:23:18.8193700Z     betas: (0.9, 0.999)
2025-04-11T04:23:18.8193781Z     bias_correction: True
2025-04-11T04:23:18.8193857Z     eps: 1e-08
2025-04-11T04:23:18.8193930Z     lr: 0.001
2025-04-11T04:23:18.8194008Z     weight_decay: 0.0
2025-04-11T04:23:18.8194080Z )
2025-04-11T04:23:18.8194444Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T04:23:18.8194596Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T04:23:18.8194745Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T04:23:18.8194844Z nvme_offload_dir = None, defaults = {}
2025-04-11T04:23:18.8195220Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8195224Z 
2025-04-11T04:23:18.8195304Z     def __init__(
2025-04-11T04:23:18.8195377Z         self,
2025-04-11T04:23:18.8195455Z         model_params,
2025-04-11T04:23:18.8195534Z         lr=1e-3,
2025-04-11T04:23:18.8195620Z         bias_correction=True,
2025-04-11T04:23:18.8195705Z         betas=(0.9, 0.999),
2025-04-11T04:23:18.8195782Z         eps=1e-8,
2025-04-11T04:23:18.8195862Z         weight_decay=0,
2025-04-11T04:23:18.8195946Z         adamw_mode=True,
2025-04-11T04:23:18.8196041Z         nvme_offload_fraction: float = 0.0,
2025-04-11T04:23:18.8196148Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T04:23:18.8196227Z         **defaults: Any,
2025-04-11T04:23:18.8196298Z     ):
2025-04-11T04:23:18.8196385Z         super().__init__(
2025-04-11T04:23:18.8196465Z             model_params,
2025-04-11T04:23:18.8196539Z             lr,
2025-04-11T04:23:18.8196622Z             bias_correction,
2025-04-11T04:23:18.8196698Z             betas,
2025-04-11T04:23:18.8196772Z             eps,
2025-04-11T04:23:18.8196903Z             weight_decay,
2025-04-11T04:23:18.8196985Z             adamw_mode,
2025-04-11T04:23:18.8197072Z             nvme_offload_fraction,
2025-04-11T04:23:18.8197156Z             nvme_offload_dir,
2025-04-11T04:23:18.8197230Z         )
2025-04-11T04:23:18.8197324Z         if torch.cuda.is_available():
2025-04-11T04:23:18.8197446Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T04:23:18.8197570Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T04:23:18.8197783Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T04:23:18.8197894Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8198188Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8198327Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8198490Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8198494Z 
2025-04-11T04:23:18.8198635Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T04:23:18.8198817Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] ______
2025-04-11T04:23:18.8198870Z 
2025-04-11T04:23:18.8199043Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T04:23:18.8199209Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T04:23:18.8199298Z g_dtype = torch.bfloat16
2025-04-11T04:23:18.8199302Z 
2025-04-11T04:23:18.8199469Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T04:23:18.8199591Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T04:23:18.8199794Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T04:23:18.8199885Z     def test_adam_optim_on_bert(
2025-04-11T04:23:18.8200045Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T04:23:18.8200132Z         device: torch.device,
2025-04-11T04:23:18.8200216Z         adamw: bool,
2025-04-11T04:23:18.8200301Z         p_dtype: torch.dtype,
2025-04-11T04:23:18.8200381Z         g_dtype: torch.dtype,
2025-04-11T04:23:18.8200461Z     ) -> None:
2025-04-11T04:23:18.8200762Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T04:23:18.8200863Z >       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8200867Z 
2025-04-11T04:23:18.8200976Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T04:23:18.8201088Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8201337Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T04:23:18.8201430Z     return super().to(*args, **kwargs)
2025-04-11T04:23:18.8201662Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T04:23:18.8201751Z     return self._apply(convert)
2025-04-11T04:23:18.8201988Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8202069Z     module._apply(fn)
2025-04-11T04:23:18.8202305Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8202386Z     module._apply(fn)
2025-04-11T04:23:18.8202611Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8202695Z     module._apply(fn)
2025-04-11T04:23:18.8202920Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8203013Z     param_applied = fn(param)
2025-04-11T04:23:18.8203120Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8203124Z 
2025-04-11T04:23:18.8203215Z t = Parameter containing:
2025-04-11T04:23:18.8203401Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T04:23:18.8203497Z         [ 0.0140, -0.0115,...1,  0.0094,  0.0310],
2025-04-11T04:23:18.8203622Z         [ 0.0050,  0.0139, -0.0004,  ...,  0.0203, -0.0216, -0.0075]],
2025-04-11T04:23:18.8203711Z        requires_grad=True)
2025-04-11T04:23:18.8203715Z 
2025-04-11T04:23:18.8203797Z     def convert(t):
2025-04-11T04:23:18.8203929Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T04:23:18.8204112Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T04:23:18.8204231Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T04:23:18.8204440Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8204554Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8204837Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8204981Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8205138Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8205193Z 
2025-04-11T04:23:18.8205451Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T04:23:18.8205589Z _____________________________ test_dist_adafactor ______________________________
2025-04-11T04:23:18.8205593Z 
2025-04-11T04:23:18.8205687Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8206304Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8206356Z 
2025-04-11T04:23:18.8206463Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8206542Z         try_count = 0
2025-04-11T04:23:18.8206644Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8206727Z             max_try, int
2025-04-11T04:23:18.8206873Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8206948Z     
2025-04-11T04:23:18.8207060Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8207135Z             try:
2025-04-11T04:23:18.8207285Z                 try_count += 1
2025-04-11T04:23:18.8207377Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8207462Z                 return ret
2025-04-11T04:23:18.8207554Z             except exception_type as e:
2025-04-11T04:23:18.8207654Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8207844Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8207962Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8208113Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8208267Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8208353Z                     continue
2025-04-11T04:23:18.8208429Z                 else:
2025-04-11T04:23:18.8208654Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8208731Z >                   raise e
2025-04-11T04:23:18.8208736Z 
2025-04-11T04:23:18.8208828Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8208939Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8209070Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8209160Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8209334Z tests/test_optimizer/test_dist_adafactor.py:468: in test_dist_adafactor
2025-04-11T04:23:18.8209424Z     spawn(run_dist, nprocs=4)
2025-04-11T04:23:18.8209523Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8209673Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8209938Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8210121Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8210412Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8210497Z     while not context.join():
2025-04-11T04:23:18.8210610Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8210614Z 
2025-04-11T04:23:18.8210833Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a8aa10>
2025-04-11T04:23:18.8210911Z timeout = None
2025-04-11T04:23:18.8210919Z 
2025-04-11T04:23:18.8211012Z     def join(self, timeout=None):
2025-04-11T04:23:18.8211137Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8211210Z     
2025-04-11T04:23:18.8211355Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8211503Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8211668Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8211810Z         of the first process exiting.
2025-04-11T04:23:18.8211886Z     
2025-04-11T04:23:18.8212034Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8212174Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8212242Z     
2025-04-11T04:23:18.8212319Z         Args:
2025-04-11T04:23:18.8212463Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8212585Z         """
2025-04-11T04:23:18.8212727Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8212820Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8212903Z             return True
2025-04-11T04:23:18.8212975Z     
2025-04-11T04:23:18.8213110Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8213233Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8213328Z             self.sentinels.keys(),
2025-04-11T04:23:18.8213415Z             timeout=timeout,
2025-04-11T04:23:18.8213487Z         )
2025-04-11T04:23:18.8213604Z     
2025-04-11T04:23:18.8213693Z         error_index = None
2025-04-11T04:23:18.8213778Z         for sentinel in ready:
2025-04-11T04:23:18.8213887Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8213985Z             process = self.processes[index]
2025-04-11T04:23:18.8214071Z             process.join()
2025-04-11T04:23:18.8214166Z             if process.exitcode != 0:
2025-04-11T04:23:18.8214251Z                 error_index = index
2025-04-11T04:23:18.8214329Z                 break
2025-04-11T04:23:18.8214397Z     
2025-04-11T04:23:18.8214490Z         # Return if there was no error.
2025-04-11T04:23:18.8214578Z         if error_index is None:
2025-04-11T04:23:18.8214712Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8214814Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8214887Z     
2025-04-11T04:23:18.8215037Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8215134Z         for process in self.processes:
2025-04-11T04:23:18.8215222Z             if process.is_alive():
2025-04-11T04:23:18.8215316Z                 process.terminate()
2025-04-11T04:23:18.8215400Z             process.join()
2025-04-11T04:23:18.8215473Z     
2025-04-11T04:23:18.8215611Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8215732Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8215836Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8215953Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8216090Z             if exitcode < 0:
2025-04-11T04:23:18.8216199Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8216308Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8216459Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8216562Z                     error_index=error_index,
2025-04-11T04:23:18.8216664Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8216752Z                     exit_code=exitcode,
2025-04-11T04:23:18.8216840Z                     signal_name=name,
2025-04-11T04:23:18.8216913Z                 )
2025-04-11T04:23:18.8216990Z             else:
2025-04-11T04:23:18.8217098Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8217261Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8217357Z                     error_index=error_index,
2025-04-11T04:23:18.8217453Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8217545Z                     exit_code=exitcode,
2025-04-11T04:23:18.8217618Z                 )
2025-04-11T04:23:18.8217693Z     
2025-04-11T04:23:18.8217827Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8218049Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8218140Z         msg += original_trace
2025-04-11T04:23:18.8218313Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8218474Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8218548Z E       
2025-04-11T04:23:18.8218672Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8218827Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8219124Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8219207Z E           fn(i, *args)
2025-04-11T04:23:18.8219458Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
2025-04-11T04:23:18.8219553Z E           exam_dist_adafactor_base()
2025-04-11T04:23:18.8219809Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8219946Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8220204Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8220290Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8220583Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
2025-04-11T04:23:18.8220740Z E           model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
2025-04-11T04:23:18.8221011Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.8221108Z E           return self._apply(convert)
2025-04-11T04:23:18.8221384Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8221482Z E           param_applied = fn(param)
2025-04-11T04:23:18.8221758Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.8221974Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8222081Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8222371Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8222506Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8222667Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8222671Z 
2025-04-11T04:23:18.8223021Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8223179Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8223340Z [04/11/25 04:18:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8223470Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8223579Z                              :75 launch                                         
2025-04-11T04:23:18.8223714Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8223843Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8223928Z Base Test Passed
2025-04-11T04:23:18.8224011Z Base Test Passed
2025-04-11T04:23:18.8224088Z Base Test Passed
2025-04-11T04:23:18.8224290Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8224437Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8225591Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8225816Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8226980Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8227148Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8228309Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8228516Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8229657Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8229823Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8230510Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8230597Z   warnings.warn(
2025-04-11T04:23:18.8231270Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8231358Z   warnings.warn(
2025-04-11T04:23:18.8232082Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8232167Z   warnings.warn(
2025-04-11T04:23:18.8232837Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8232916Z   warnings.warn(
2025-04-11T04:23:18.8233743Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8233822Z   warnings.warn(
2025-04-11T04:23:18.8234626Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8234762Z   warnings.warn(
2025-04-11T04:23:18.8235574Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8235722Z   warnings.warn(
2025-04-11T04:23:18.8236518Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8236599Z   warnings.warn(
2025-04-11T04:23:18.8237541Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8237619Z   warnings.warn(
2025-04-11T04:23:18.8238414Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8238492Z   warnings.warn(
2025-04-11T04:23:18.8239288Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8239364Z   warnings.warn(
2025-04-11T04:23:18.8240159Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8240236Z   warnings.warn(
2025-04-11T04:23:18.8241120Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8241198Z   warnings.warn(
2025-04-11T04:23:18.8241985Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8242064Z   warnings.warn(
2025-04-11T04:23:18.8242848Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8242924Z   warnings.warn(
2025-04-11T04:23:18.8243224Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:21639 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8244018Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8244147Z   warnings.warn(
2025-04-11T04:23:18.8244436Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:21639 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8244572Z ________________________________ test_dist_came ________________________________
2025-04-11T04:23:18.8244580Z 
2025-04-11T04:23:18.8244669Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8245332Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8245342Z 
2025-04-11T04:23:18.8245444Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8245524Z         try_count = 0
2025-04-11T04:23:18.8245628Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8245711Z             max_try, int
2025-04-11T04:23:18.8245912Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8245982Z     
2025-04-11T04:23:18.8246095Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8246171Z             try:
2025-04-11T04:23:18.8246254Z                 try_count += 1
2025-04-11T04:23:18.8246347Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8246426Z                 return ret
2025-04-11T04:23:18.8246521Z             except exception_type as e:
2025-04-11T04:23:18.8246624Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8246811Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8246931Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8247079Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8247235Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8247318Z                     continue
2025-04-11T04:23:18.8247395Z                 else:
2025-04-11T04:23:18.8247623Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8247702Z >                   raise e
2025-04-11T04:23:18.8247706Z 
2025-04-11T04:23:18.8247802Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8247916Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8248051Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8248138Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8248333Z tests/test_optimizer/test_dist_came.py:357: in test_dist_came
2025-04-11T04:23:18.8248428Z     spawn(run_dist, nprocs=4)
2025-04-11T04:23:18.8248527Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8248627Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8248885Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8249068Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8249353Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8249438Z     while not context.join():
2025-04-11T04:23:18.8249551Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8249557Z 
2025-04-11T04:23:18.8249752Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8692950>
2025-04-11T04:23:18.8249832Z timeout = None
2025-04-11T04:23:18.8249836Z 
2025-04-11T04:23:18.8249925Z     def join(self, timeout=None):
2025-04-11T04:23:18.8250052Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8250120Z     
2025-04-11T04:23:18.8250263Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8250461Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8250625Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8250717Z         of the first process exiting.
2025-04-11T04:23:18.8250786Z     
2025-04-11T04:23:18.8250938Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8251072Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8251192Z     
2025-04-11T04:23:18.8251268Z         Args:
2025-04-11T04:23:18.8251407Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8251482Z         """
2025-04-11T04:23:18.8251623Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8251713Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8251794Z             return True
2025-04-11T04:23:18.8251862Z     
2025-04-11T04:23:18.8251999Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8252115Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8252253Z             self.sentinels.keys(),
2025-04-11T04:23:18.8252340Z             timeout=timeout,
2025-04-11T04:23:18.8252410Z         )
2025-04-11T04:23:18.8252481Z     
2025-04-11T04:23:18.8252561Z         error_index = None
2025-04-11T04:23:18.8252648Z         for sentinel in ready:
2025-04-11T04:23:18.8252754Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8252852Z             process = self.processes[index]
2025-04-11T04:23:18.8252941Z             process.join()
2025-04-11T04:23:18.8253030Z             if process.exitcode != 0:
2025-04-11T04:23:18.8253118Z                 error_index = index
2025-04-11T04:23:18.8253194Z                 break
2025-04-11T04:23:18.8253263Z     
2025-04-11T04:23:18.8253356Z         # Return if there was no error.
2025-04-11T04:23:18.8253440Z         if error_index is None:
2025-04-11T04:23:18.8253576Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8253673Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8253744Z     
2025-04-11T04:23:18.8253883Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8253977Z         for process in self.processes:
2025-04-11T04:23:18.8254067Z             if process.is_alive():
2025-04-11T04:23:18.8254157Z                 process.terminate()
2025-04-11T04:23:18.8254246Z             process.join()
2025-04-11T04:23:18.8254315Z     
2025-04-11T04:23:18.8254454Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8254573Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8254728Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8254853Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8254936Z             if exitcode < 0:
2025-04-11T04:23:18.8255042Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8255149Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8255298Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8255396Z                     error_index=error_index,
2025-04-11T04:23:18.8255495Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8255584Z                     exit_code=exitcode,
2025-04-11T04:23:18.8255667Z                     signal_name=name,
2025-04-11T04:23:18.8255743Z                 )
2025-04-11T04:23:18.8255820Z             else:
2025-04-11T04:23:18.8255920Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8256086Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8256180Z                     error_index=error_index,
2025-04-11T04:23:18.8256279Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8256364Z                     exit_code=exitcode,
2025-04-11T04:23:18.8256485Z                 )
2025-04-11T04:23:18.8256557Z     
2025-04-11T04:23:18.8256691Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8256867Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8256950Z         msg += original_trace
2025-04-11T04:23:18.8257124Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8257287Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8257408Z E       
2025-04-11T04:23:18.8257538Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8257636Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8257936Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8258017Z E           fn(i, *args)
2025-04-11T04:23:18.8258249Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
2025-04-11T04:23:18.8258442Z E           exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
2025-04-11T04:23:18.8258703Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8258797Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8259102Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
2025-04-11T04:23:18.8259330Z E           ) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
2025-04-11T04:23:18.8259655Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
2025-04-11T04:23:18.8259754Z E           org_model = org_model.cuda()
2025-04-11T04:23:18.8260044Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.8260148Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.8260419Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8260537Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8260814Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8260901Z E           module._apply(fn)
2025-04-11T04:23:18.8261175Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8261259Z E           module._apply(fn)
2025-04-11T04:23:18.8261581Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8261677Z E           param_applied = fn(param)
2025-04-11T04:23:18.8261948Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8262070Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8262175Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8262461Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8262595Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8262759Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8262763Z 
2025-04-11T04:23:18.8263065Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8263218Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8263378Z [04/11/25 04:18:11] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8263577Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8263688Z                              :75 launch                                         
2025-04-11T04:23:18.8263825Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8263951Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8264147Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8264344Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8265491Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8265711Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8266842Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8267011Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8268123Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8268287Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8269434Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8269655Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8270334Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8270420Z   warnings.warn(
2025-04-11T04:23:18.8271092Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8271177Z   warnings.warn(
2025-04-11T04:23:18.8271842Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8271926Z   warnings.warn(
2025-04-11T04:23:18.8272592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8272731Z   warnings.warn(
2025-04-11T04:23:18.8273574Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8273704Z   warnings.warn(
2025-04-11T04:23:18.8274518Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8274595Z   warnings.warn(
2025-04-11T04:23:18.8275455Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8275536Z   warnings.warn(
2025-04-11T04:23:18.8276328Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8276406Z   warnings.warn(
2025-04-11T04:23:18.8277199Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8277276Z   warnings.warn(
2025-04-11T04:23:18.8278067Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8278142Z   warnings.warn(
2025-04-11T04:23:18.8278929Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8279057Z   warnings.warn(
2025-04-11T04:23:18.8279847Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8279928Z   warnings.warn(
2025-04-11T04:23:18.8280714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8280791Z   warnings.warn(
2025-04-11T04:23:18.8281577Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8281659Z   warnings.warn(
2025-04-11T04:23:18.8282444Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8282579Z   warnings.warn(
2025-04-11T04:23:18.8283371Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8283500Z   warnings.warn(
2025-04-11T04:23:18.8283797Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8284080Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8284402Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8284546Z _______________________________ test_dist_galore _______________________________
2025-04-11T04:23:18.8284550Z 
2025-04-11T04:23:18.8284640Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8285245Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8285251Z 
2025-04-11T04:23:18.8285351Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8285435Z         try_count = 0
2025-04-11T04:23:18.8285536Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8285616Z             max_try, int
2025-04-11T04:23:18.8285767Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8285839Z     
2025-04-11T04:23:18.8285952Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8286025Z             try:
2025-04-11T04:23:18.8286107Z                 try_count += 1
2025-04-11T04:23:18.8286202Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8286282Z                 return ret
2025-04-11T04:23:18.8286379Z             except exception_type as e:
2025-04-11T04:23:18.8286477Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8286670Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8286786Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8286982Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8287144Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8287225Z                     continue
2025-04-11T04:23:18.8287306Z                 else:
2025-04-11T04:23:18.8287531Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8287612Z >                   raise e
2025-04-11T04:23:18.8287616Z 
2025-04-11T04:23:18.8287709Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8287819Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8287955Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8288042Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8288202Z tests/test_optimizer/test_dist_galore.py:298: in test_dist_galore
2025-04-11T04:23:18.8288298Z     spawn(check_dist_galore, nprocs=4)
2025-04-11T04:23:18.8288404Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8288502Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8288756Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8288989Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8289272Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8289361Z     while not context.join():
2025-04-11T04:23:18.8289475Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8289479Z 
2025-04-11T04:23:18.8289729Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a8b2e0>
2025-04-11T04:23:18.8289807Z timeout = None
2025-04-11T04:23:18.8289811Z 
2025-04-11T04:23:18.8289900Z     def join(self, timeout=None):
2025-04-11T04:23:18.8290027Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8290100Z     
2025-04-11T04:23:18.8290253Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8290402Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8290570Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8290710Z         of the first process exiting.
2025-04-11T04:23:18.8290781Z     
2025-04-11T04:23:18.8290932Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8291068Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8291141Z     
2025-04-11T04:23:18.8291214Z         Args:
2025-04-11T04:23:18.8291354Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8291430Z         """
2025-04-11T04:23:18.8291566Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8291659Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8291740Z             return True
2025-04-11T04:23:18.8291812Z     
2025-04-11T04:23:18.8291941Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8292057Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8292152Z             self.sentinels.keys(),
2025-04-11T04:23:18.8292234Z             timeout=timeout,
2025-04-11T04:23:18.8292309Z         )
2025-04-11T04:23:18.8292377Z     
2025-04-11T04:23:18.8292456Z         error_index = None
2025-04-11T04:23:18.8292544Z         for sentinel in ready:
2025-04-11T04:23:18.8292648Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8292749Z             process = self.processes[index]
2025-04-11T04:23:18.8292835Z             process.join()
2025-04-11T04:23:18.8292924Z             if process.exitcode != 0:
2025-04-11T04:23:18.8293013Z                 error_index = index
2025-04-11T04:23:18.8293088Z                 break
2025-04-11T04:23:18.8293158Z     
2025-04-11T04:23:18.8293312Z         # Return if there was no error.
2025-04-11T04:23:18.8293403Z         if error_index is None:
2025-04-11T04:23:18.8293535Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8293630Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8293702Z     
2025-04-11T04:23:18.8293842Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8293939Z         for process in self.processes:
2025-04-11T04:23:18.8294025Z             if process.is_alive():
2025-04-11T04:23:18.8294115Z                 process.terminate()
2025-04-11T04:23:18.8294201Z             process.join()
2025-04-11T04:23:18.8294268Z     
2025-04-11T04:23:18.8294412Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8294525Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8294635Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8294754Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8294837Z             if exitcode < 0:
2025-04-11T04:23:18.8294945Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8295048Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8295256Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8295352Z                     error_index=error_index,
2025-04-11T04:23:18.8295453Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8295544Z                     exit_code=exitcode,
2025-04-11T04:23:18.8295627Z                     signal_name=name,
2025-04-11T04:23:18.8295700Z                 )
2025-04-11T04:23:18.8295772Z             else:
2025-04-11T04:23:18.8295929Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8296090Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8296180Z                     error_index=error_index,
2025-04-11T04:23:18.8296281Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8296366Z                     exit_code=exitcode,
2025-04-11T04:23:18.8296438Z                 )
2025-04-11T04:23:18.8296506Z     
2025-04-11T04:23:18.8296637Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8296855Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8296942Z         msg += original_trace
2025-04-11T04:23:18.8297115Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8297272Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8297347Z E       
2025-04-11T04:23:18.8297473Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8297569Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8297877Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8297959Z E           fn(i, *args)
2025-04-11T04:23:18.8298228Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
2025-04-11T04:23:18.8298310Z E           dist.barrier()
2025-04-11T04:23:18.8298615Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.8298707Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.8299025Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:18.8299131Z E           work = default_pg.barrier(opts=opts)
2025-04-11T04:23:18.8299237Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8299525Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8299657Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8299866Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8299870Z 
2025-04-11T04:23:18.8300174Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8300331Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8300486Z [04/11/25 04:18:19] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8300612Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8300729Z                              :75 launch                                         
2025-04-11T04:23:18.8300869Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8300995Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8301131Z Skipping forward-backward tests due to SVD instability
2025-04-11T04:23:18.8301546Z Running bert tests, which are expected to produce minor errors due to instability in SVD convergence.             For example, a 1e-9 grad diff causes drastic difference in SVD output.
2025-04-11T04:23:18.8301687Z CUDA error: out of memory
2025-04-11T04:23:18.8301967Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8302099Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8302256Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8302260Z 
2025-04-11T04:23:18.8302349Z CUDA error: out of memory
2025-04-11T04:23:18.8302618Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8302796Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8302949Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8302953Z 
2025-04-11T04:23:18.8303040Z CUDA error: out of memory
2025-04-11T04:23:18.8303310Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8303436Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8303642Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8303646Z 
2025-04-11T04:23:18.8304016Z [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T04:23:18.8304419Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T04:23:18.8304856Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f10aa7d3d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T04:23:18.8305202Z frame #1: <unknown function> + 0x5522c2e (0x7f10ef0bcc2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8305704Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f10ef0b7440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8306075Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f10ef0b7782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8306420Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f10ef0b85b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8306776Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8307265Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8307619Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8307977Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8308533Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f10ab998a59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8309138Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f10ab99fa4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8309979Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f10ab9b5e4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8310369Z frame #12: <unknown function> + 0x54c7dbd (0x7f10ef061dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8310692Z frame #13: <unknown function> + 0x54d1cb8 (0x7f10ef06bcb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8311062Z frame #14: <unknown function> + 0x4b16e6c (0x7f10ee6b0e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8311382Z frame #15: <unknown function> + 0x1696528 (0x7f10eb230528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8311697Z frame #16: <unknown function> + 0x54d94d3 (0x7f10ef0734d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8312085Z frame #17: <unknown function> + 0x54e48bf (0x7f10ef07e8bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8312959Z frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f10ef0e4f2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8313298Z frame #19: <unknown function> + 0xc55ad1 (0x7f10f6c8fad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8313631Z frame #20: <unknown function> + 0x413ea4 (0x7f10f644dea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8313765Z frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T04:23:18.8313972Z frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8314188Z frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8314389Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8314593Z frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8334470Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8334791Z frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8334940Z frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8335218Z frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T04:23:18.8335402Z frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8335621Z frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8335822Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336038Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336168Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8336356Z frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336566Z frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336757Z frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8336962Z frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337149Z frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337417Z frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337606Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337806Z frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8337993Z frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8338245Z frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8338434Z frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8338607Z frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8338738Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T04:23:18.8338922Z frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339095Z frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339348Z frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339542Z frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339744Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8339934Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8340103Z frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8340231Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T04:23:18.8340418Z frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8340616Z frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8340811Z frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341008Z frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341193Z frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341392Z frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341578Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8341775Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8342052Z . This may indicate a possible application crash on rank 0 or a network set up issue.
2025-04-11T04:23:18.8342435Z [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T04:23:18.8342842Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T04:23:18.8343240Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f6608535d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T04:23:18.8343578Z frame #1: <unknown function> + 0x5522c2e (0x7f664ce1ec2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8344211Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f664ce19440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8344581Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f664ce19782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8344984Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f664ce1a5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8345335Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8345682Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8346084Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8346433Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8346944Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f66096faa59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8347591Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f6609701a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8348472Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f6609717e4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8348813Z frame #12: <unknown function> + 0x54c7dbd (0x7f664cdc3dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8349137Z frame #13: <unknown function> + 0x54d1cb8 (0x7f664cdcdcb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8349455Z frame #14: <unknown function> + 0x4b16e6c (0x7f664c412e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8349773Z frame #15: <unknown function> + 0x1696528 (0x7f6648f92528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8350093Z frame #16: <unknown function> + 0x54d94d3 (0x7f664cdd54d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8350471Z frame #17: <unknown function> + 0x54e48bf (0x7f664cde08bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8351333Z frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f664ce46f2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8351671Z frame #19: <unknown function> + 0xc55ad1 (0x7f66549f1ad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8351997Z frame #20: <unknown function> + 0x413ea4 (0x7f66541afea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8352134Z frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T04:23:18.8352325Z frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8352538Z frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8352728Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8352990Z frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8353181Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8353393Z frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8353524Z frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8353704Z frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T04:23:18.8353881Z frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8354084Z frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8354277Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8354487Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8354614Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8354859Z frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355063Z frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355255Z frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355453Z frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355645Z frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8355844Z frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356033Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356229Z frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356420Z frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356624Z frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356813Z frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8356984Z frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8357110Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T04:23:18.8357297Z frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8357514Z frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8357719Z frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8357908Z frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358109Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358299Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358461Z frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358586Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T04:23:18.8358770Z frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8358969Z frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359154Z frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359349Z frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359534Z frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359783Z frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8359973Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8360169Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8360374Z . This may indicate a possible application crash on rank 0 or a network set up issue.
2025-04-11T04:23:18.8361022Z Failed to replace attention.self.query of type Linear with Linear1D_Col with the exception: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T04:23:18.8361419Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T04:23:18.8361853Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f10aa7d3d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T04:23:18.8362187Z frame #1: <unknown function> + 0x5522c2e (0x7f10ef0bcc2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8362672Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f10ef0b7440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8363029Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f10ef0b7782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8363373Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f10ef0b85b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8363721Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8364077Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8364418Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8364765Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8365308Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f10ab998a59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8365911Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f10ab99fa4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8366736Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f10ab9b5e4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T04:23:18.8367071Z frame #12: <unknown function> + 0x54c7dbd (0x7f10ef061dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8367387Z frame #13: <unknown function> + 0x54d1cb8 (0x7f10ef06bcb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8367709Z frame #14: <unknown function> + 0x4b16e6c (0x7f10ee6b0e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8368080Z frame #15: <unknown function> + 0x1696528 (0x7f10eb230528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8368397Z frame #16: <unknown function> + 0x54d94d3 (0x7f10ef0734d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8368712Z frame #17: <unknown function> + 0x54e48bf (0x7f10ef07e8bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T04:23:18.8369090Z frame #18: <unknown function> + 0xca3fae (0x7f10f6cddfae in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8369418Z frame #19: <unknown function> + 0x413ea4 (0x7f10f644dea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T04:23:18.8369549Z frame #20: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T04:23:18.8369745Z frame #21: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8369931Z frame #22: /opt/conda/envs/pytorch/bin/python() [0x509cbf]
2025-04-11T04:23:18.8370139Z frame #23: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8370329Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8370529Z frame #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8370720Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8370922Z frame #27: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371110Z frame #28: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371310Z frame #29: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371498Z frame #30: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371697Z frame #31: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8371884Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8372095Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8372226Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8372349Z frame #35: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T04:23:18.8372524Z frame #36: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8372775Z frame #37: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8372969Z frame #38: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373138Z frame #39: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373338Z frame #40: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373528Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373724Z frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8373855Z frame #43: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8374050Z frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8374178Z frame #45: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8374375Z frame #46: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8374502Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8374750Z frame #48: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8374874Z frame #49: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8375073Z frame #50: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8375195Z frame #51: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8375392Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8375631Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8375830Z frame #54: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8375955Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T04:23:18.8376150Z frame #56: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8376341Z frame #57: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8376596Z frame #58: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8376725Z frame #59: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T04:23:18.8376910Z frame #60: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8377110Z frame #61: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8377301Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8377499Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T04:23:18.8377958Z . This may indicate a possible application crash on rank 0 or a network set up issue.. Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
2025-04-11T04:23:18.8378157Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8378320Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8379470Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8379647Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8380811Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8380986Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8382112Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8382282Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8383401Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8383611Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8384317Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8384448Z   warnings.warn(
2025-04-11T04:23:18.8385173Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8385261Z   warnings.warn(
2025-04-11T04:23:18.8386004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8386090Z   warnings.warn(
2025-04-11T04:23:18.8386798Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8386883Z   warnings.warn(
2025-04-11T04:23:18.8387720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8387804Z   warnings.warn(
2025-04-11T04:23:18.8388656Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8388739Z   warnings.warn(
2025-04-11T04:23:18.8389591Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8389672Z   warnings.warn(
2025-04-11T04:23:18.8390474Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8390554Z   warnings.warn(
2025-04-11T04:23:18.8391346Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8391425Z   warnings.warn(
2025-04-11T04:23:18.8392231Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8392306Z   warnings.warn(
2025-04-11T04:23:18.8393154Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8393229Z   warnings.warn(
2025-04-11T04:23:18.8394018Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8394148Z   warnings.warn(
2025-04-11T04:23:18.8394935Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8395011Z   warnings.warn(
2025-04-11T04:23:18.8395855Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8395935Z   warnings.warn(
2025-04-11T04:23:18.8396721Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8396797Z   warnings.warn(
2025-04-11T04:23:18.8397584Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8397660Z   warnings.warn(
2025-04-11T04:23:18.8397959Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:36049 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8398242Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:36049 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8398524Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:36049 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8398712Z ________________________________ test_dist_lamb ________________________________
2025-04-11T04:23:18.8398719Z 
2025-04-11T04:23:18.8398815Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8399429Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8399436Z 
2025-04-11T04:23:18.8399543Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8399628Z         try_count = 0
2025-04-11T04:23:18.8399731Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8399820Z             max_try, int
2025-04-11T04:23:18.8399967Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8400039Z     
2025-04-11T04:23:18.8400152Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8400224Z             try:
2025-04-11T04:23:18.8400314Z                 try_count += 1
2025-04-11T04:23:18.8400406Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8400491Z                 return ret
2025-04-11T04:23:18.8400587Z             except exception_type as e:
2025-04-11T04:23:18.8400738Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8400933Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8401052Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8401203Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8401357Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8401513Z                     continue
2025-04-11T04:23:18.8401589Z                 else:
2025-04-11T04:23:18.8401815Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8401900Z >                   raise e
2025-04-11T04:23:18.8401904Z 
2025-04-11T04:23:18.8401998Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8402117Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8402252Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8402481Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8402630Z tests/test_optimizer/test_dist_lamb.py:276: in test_dist_lamb
2025-04-11T04:23:18.8402723Z     spawn(check_dist_lamb, nprocs=4)
2025-04-11T04:23:18.8402827Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8402925Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8403190Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8403370Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8403659Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8403748Z     while not context.join():
2025-04-11T04:23:18.8403856Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8403862Z 
2025-04-11T04:23:18.8404068Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8690c40>
2025-04-11T04:23:18.8404148Z timeout = None
2025-04-11T04:23:18.8404152Z 
2025-04-11T04:23:18.8404245Z     def join(self, timeout=None):
2025-04-11T04:23:18.8404370Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8404442Z     
2025-04-11T04:23:18.8404586Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8404733Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8404899Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8404990Z         of the first process exiting.
2025-04-11T04:23:18.8405062Z     
2025-04-11T04:23:18.8405257Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8405398Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8405469Z     
2025-04-11T04:23:18.8405541Z         Args:
2025-04-11T04:23:18.8405682Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8405755Z         """
2025-04-11T04:23:18.8405897Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8405988Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8406065Z             return True
2025-04-11T04:23:18.8406136Z     
2025-04-11T04:23:18.8406268Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8406390Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8406481Z             self.sentinels.keys(),
2025-04-11T04:23:18.8406567Z             timeout=timeout,
2025-04-11T04:23:18.8406637Z         )
2025-04-11T04:23:18.8406706Z     
2025-04-11T04:23:18.8406792Z         error_index = None
2025-04-11T04:23:18.8406876Z         for sentinel in ready:
2025-04-11T04:23:18.8406983Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8407132Z             process = self.processes[index]
2025-04-11T04:23:18.8407216Z             process.join()
2025-04-11T04:23:18.8407313Z             if process.exitcode != 0:
2025-04-11T04:23:18.8407400Z                 error_index = index
2025-04-11T04:23:18.8407476Z                 break
2025-04-11T04:23:18.8407543Z     
2025-04-11T04:23:18.8407633Z         # Return if there was no error.
2025-04-11T04:23:18.8407720Z         if error_index is None:
2025-04-11T04:23:18.8407852Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8408023Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8408091Z     
2025-04-11T04:23:18.8408230Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8408327Z         for process in self.processes:
2025-04-11T04:23:18.8408413Z             if process.is_alive():
2025-04-11T04:23:18.8408504Z                 process.terminate()
2025-04-11T04:23:18.8408586Z             process.join()
2025-04-11T04:23:18.8408658Z     
2025-04-11T04:23:18.8408798Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8408963Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8409076Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8409197Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8409284Z             if exitcode < 0:
2025-04-11T04:23:18.8409391Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8409502Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8409656Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8409751Z                     error_index=error_index,
2025-04-11T04:23:18.8409857Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8409944Z                     exit_code=exitcode,
2025-04-11T04:23:18.8410031Z                     signal_name=name,
2025-04-11T04:23:18.8410103Z                 )
2025-04-11T04:23:18.8410178Z             else:
2025-04-11T04:23:18.8410283Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8410449Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8410547Z                     error_index=error_index,
2025-04-11T04:23:18.8410644Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8410732Z                     exit_code=exitcode,
2025-04-11T04:23:18.8410804Z                 )
2025-04-11T04:23:18.8410871Z     
2025-04-11T04:23:18.8411007Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8411179Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8411268Z         msg += original_trace
2025-04-11T04:23:18.8411491Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8411659Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8411737Z E       
2025-04-11T04:23:18.8411864Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8411967Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8412269Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8412352Z E           fn(i, *args)
2025-04-11T04:23:18.8412610Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
2025-04-11T04:23:18.8412700Z E           run_dist_lamb_basic()
2025-04-11T04:23:18.8412964Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8413052Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8413307Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8413391Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8413692Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8413778Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8413992Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:18.8414094Z E           get_accelerator().synchronize()
2025-04-11T04:23:18.8414348Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:18.8414505Z E           torch.cuda.synchronize(device)
2025-04-11T04:23:18.8414782Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:18.8414894Z E           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8414997Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8415281Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8415417Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8415622Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8415628Z 
2025-04-11T04:23:18.8415932Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8416086Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8416248Z [04/11/25 04:18:28] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8416375Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8416492Z                              :75 launch                                         
2025-04-11T04:23:18.8416629Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8416755Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8416953Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8417097Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8418216Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8418382Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8419536Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8419708Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8420804Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8420967Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8422063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8422278Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8422980Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8423111Z   warnings.warn(
2025-04-11T04:23:18.8423802Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8423885Z   warnings.warn(
2025-04-11T04:23:18.8424612Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8424694Z   warnings.warn(
2025-04-11T04:23:18.8425362Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8425440Z   warnings.warn(
2025-04-11T04:23:18.8426274Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8426355Z   warnings.warn(
2025-04-11T04:23:18.8427148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8427225Z   warnings.warn(
2025-04-11T04:23:18.8428096Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8428175Z   warnings.warn(
2025-04-11T04:23:18.8429165Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8429243Z   warnings.warn(
2025-04-11T04:23:18.8430032Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8430110Z   warnings.warn(
2025-04-11T04:23:18.8430935Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8431084Z   warnings.warn(
2025-04-11T04:23:18.8431896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8431974Z   warnings.warn(
2025-04-11T04:23:18.8432773Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8432909Z   warnings.warn(
2025-04-11T04:23:18.8433691Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8433825Z   warnings.warn(
2025-04-11T04:23:18.8434613Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8434695Z   warnings.warn(
2025-04-11T04:23:18.8435485Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8435565Z   warnings.warn(
2025-04-11T04:23:18.8436358Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8436438Z   warnings.warn(
2025-04-11T04:23:18.8436734Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34628 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8437022Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34628 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8437356Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34628 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8437915Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8437995Z   warnings.warn(
2025-04-11T04:23:18.8438525Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8438608Z   warnings.warn(
2025-04-11T04:23:18.8439142Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8439224Z   warnings.warn(
2025-04-11T04:23:18.8439752Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8439834Z   warnings.warn(
2025-04-11T04:23:18.8440366Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8440499Z   warnings.warn(
2025-04-11T04:23:18.8441033Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8441161Z   warnings.warn(
2025-04-11T04:23:18.8441699Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8441774Z   warnings.warn(
2025-04-11T04:23:18.8442309Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:18.8442385Z   warnings.warn(
2025-04-11T04:23:18.8442576Z ______________________________ test_pipeline_p2p _______________________________
2025-04-11T04:23:18.8442581Z 
2025-04-11T04:23:18.8442678Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8443284Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8443291Z 
2025-04-11T04:23:18.8443396Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8443479Z         try_count = 0
2025-04-11T04:23:18.8443580Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8443665Z             max_try, int
2025-04-11T04:23:18.8443815Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8443883Z     
2025-04-11T04:23:18.8443998Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8444071Z             try:
2025-04-11T04:23:18.8444156Z                 try_count += 1
2025-04-11T04:23:18.8444248Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8444327Z                 return ret
2025-04-11T04:23:18.8444424Z             except exception_type as e:
2025-04-11T04:23:18.8444521Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8444710Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8444828Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8444971Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8445178Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8445262Z                     continue
2025-04-11T04:23:18.8445341Z                 else:
2025-04-11T04:23:18.8445567Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8445651Z >                   raise e
2025-04-11T04:23:18.8445657Z 
2025-04-11T04:23:18.8445750Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8445861Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8445993Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8446080Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8446260Z tests/test_pipeline/test_p2p_communication.py:79: in test_pipeline_p2p
2025-04-11T04:23:18.8446348Z     spawn(run_dist, WORLD_SIZE)
2025-04-11T04:23:18.8446450Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8446548Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8446809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8446993Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8447334Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8447428Z     while not context.join():
2025-04-11T04:23:18.8447536Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8447540Z 
2025-04-11T04:23:18.8447739Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f139a0>
2025-04-11T04:23:18.8447817Z timeout = None
2025-04-11T04:23:18.8447869Z 
2025-04-11T04:23:18.8447966Z     def join(self, timeout=None):
2025-04-11T04:23:18.8448089Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8448157Z     
2025-04-11T04:23:18.8448303Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8448444Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8448610Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8448702Z         of the first process exiting.
2025-04-11T04:23:18.8448770Z     
2025-04-11T04:23:18.8448961Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8449099Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8449170Z     
2025-04-11T04:23:18.8449242Z         Args:
2025-04-11T04:23:18.8449381Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8449455Z         """
2025-04-11T04:23:18.8449591Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8449685Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8449762Z             return True
2025-04-11T04:23:18.8449832Z     
2025-04-11T04:23:18.8449964Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8450078Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8450167Z             self.sentinels.keys(),
2025-04-11T04:23:18.8450251Z             timeout=timeout,
2025-04-11T04:23:18.8450323Z         )
2025-04-11T04:23:18.8450390Z     
2025-04-11T04:23:18.8450473Z         error_index = None
2025-04-11T04:23:18.8450560Z         for sentinel in ready:
2025-04-11T04:23:18.8450663Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8450762Z             process = self.processes[index]
2025-04-11T04:23:18.8450846Z             process.join()
2025-04-11T04:23:18.8450938Z             if process.exitcode != 0:
2025-04-11T04:23:18.8451026Z                 error_index = index
2025-04-11T04:23:18.8451100Z                 break
2025-04-11T04:23:18.8451172Z     
2025-04-11T04:23:18.8451261Z         # Return if there was no error.
2025-04-11T04:23:18.8451347Z         if error_index is None:
2025-04-11T04:23:18.8451528Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8451625Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8451698Z     
2025-04-11T04:23:18.8451836Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8451934Z         for process in self.processes:
2025-04-11T04:23:18.8452022Z             if process.is_alive():
2025-04-11T04:23:18.8452115Z                 process.terminate()
2025-04-11T04:23:18.8452195Z             process.join()
2025-04-11T04:23:18.8452264Z     
2025-04-11T04:23:18.8452404Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8452520Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8452632Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8452752Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8452832Z             if exitcode < 0:
2025-04-11T04:23:18.8452944Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8453049Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8453202Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8453347Z                     error_index=error_index,
2025-04-11T04:23:18.8453449Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8453536Z                     exit_code=exitcode,
2025-04-11T04:23:18.8453619Z                     signal_name=name,
2025-04-11T04:23:18.8453694Z                 )
2025-04-11T04:23:18.8453767Z             else:
2025-04-11T04:23:18.8453869Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8454084Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8454177Z                     error_index=error_index,
2025-04-11T04:23:18.8454277Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8454363Z                     exit_code=exitcode,
2025-04-11T04:23:18.8454438Z                 )
2025-04-11T04:23:18.8454505Z     
2025-04-11T04:23:18.8454637Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8454807Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8454894Z         msg += original_trace
2025-04-11T04:23:18.8455115Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8455281Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8455357Z E       
2025-04-11T04:23:18.8455483Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8455579Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8455885Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8455966Z E           fn(i, *args)
2025-04-11T04:23:18.8456226Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
2025-04-11T04:23:18.8456319Z E           check_p2p_communication()
2025-04-11T04:23:18.8456618Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
2025-04-11T04:23:18.8456786Z E           tensor = torch.ones(1, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8456894Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8457181Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8457315Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8457479Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8457484Z 
2025-04-11T04:23:18.8457789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8458016Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8458174Z [04/11/25 04:18:33] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8458305Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8458416Z                              :75 launch                                         
2025-04-11T04:23:18.8458558Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8458683Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8458883Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8459032Z _________________________ test_pipeline_stage_manager __________________________
2025-04-11T04:23:18.8459036Z 
2025-04-11T04:23:18.8459129Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8459739Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8459792Z 
2025-04-11T04:23:18.8459895Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8459975Z         try_count = 0
2025-04-11T04:23:18.8460077Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8460160Z             max_try, int
2025-04-11T04:23:18.8460305Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8460373Z     
2025-04-11T04:23:18.8460488Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8460612Z             try:
2025-04-11T04:23:18.8460699Z                 try_count += 1
2025-04-11T04:23:18.8460790Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8460870Z                 return ret
2025-04-11T04:23:18.8460965Z             except exception_type as e:
2025-04-11T04:23:18.8461066Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8461258Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8461374Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8461567Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8461725Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8461803Z                     continue
2025-04-11T04:23:18.8461882Z                 else:
2025-04-11T04:23:18.8462105Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8462189Z >                   raise e
2025-04-11T04:23:18.8462193Z 
2025-04-11T04:23:18.8462286Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8462398Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8462533Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8462618Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8462802Z tests/test_pipeline/test_stage_manager.py:74: in test_pipeline_stage_manager
2025-04-11T04:23:18.8462885Z     spawn(run_dist, 4)
2025-04-11T04:23:18.8462987Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8463087Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8463345Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8463524Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8463812Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8463902Z     while not context.join():
2025-04-11T04:23:18.8464008Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8464012Z 
2025-04-11T04:23:18.8464263Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f09de0>
2025-04-11T04:23:18.8464343Z timeout = None
2025-04-11T04:23:18.8464347Z 
2025-04-11T04:23:18.8464438Z     def join(self, timeout=None):
2025-04-11T04:23:18.8464567Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8464634Z     
2025-04-11T04:23:18.8464784Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8464926Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8465089Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8465179Z         of the first process exiting.
2025-04-11T04:23:18.8465251Z     
2025-04-11T04:23:18.8465396Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8465530Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8465602Z     
2025-04-11T04:23:18.8465675Z         Args:
2025-04-11T04:23:18.8465816Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8465887Z         """
2025-04-11T04:23:18.8466024Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8466173Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8466253Z             return True
2025-04-11T04:23:18.8466324Z     
2025-04-11T04:23:18.8466454Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8466569Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8466661Z             self.sentinels.keys(),
2025-04-11T04:23:18.8466745Z             timeout=timeout,
2025-04-11T04:23:18.8466871Z         )
2025-04-11T04:23:18.8466938Z     
2025-04-11T04:23:18.8467025Z         error_index = None
2025-04-11T04:23:18.8467110Z         for sentinel in ready:
2025-04-11T04:23:18.8467212Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8467315Z             process = self.processes[index]
2025-04-11T04:23:18.8467396Z             process.join()
2025-04-11T04:23:18.8467489Z             if process.exitcode != 0:
2025-04-11T04:23:18.8467575Z                 error_index = index
2025-04-11T04:23:18.8467650Z                 break
2025-04-11T04:23:18.8467722Z     
2025-04-11T04:23:18.8467811Z         # Return if there was no error.
2025-04-11T04:23:18.8467943Z         if error_index is None:
2025-04-11T04:23:18.8468078Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8468173Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8468245Z     
2025-04-11T04:23:18.8468382Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8468530Z         for process in self.processes:
2025-04-11T04:23:18.8468617Z             if process.is_alive():
2025-04-11T04:23:18.8468711Z                 process.terminate()
2025-04-11T04:23:18.8468792Z             process.join()
2025-04-11T04:23:18.8468859Z     
2025-04-11T04:23:18.8469003Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8469115Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8469224Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8469346Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8469431Z             if exitcode < 0:
2025-04-11T04:23:18.8469540Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8469643Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8469797Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8469890Z                     error_index=error_index,
2025-04-11T04:23:18.8469993Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8470082Z                     exit_code=exitcode,
2025-04-11T04:23:18.8470165Z                     signal_name=name,
2025-04-11T04:23:18.8470239Z                 )
2025-04-11T04:23:18.8470367Z             else:
2025-04-11T04:23:18.8470473Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8470634Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8470730Z                     error_index=error_index,
2025-04-11T04:23:18.8470826Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8470911Z                     exit_code=exitcode,
2025-04-11T04:23:18.8470985Z                 )
2025-04-11T04:23:18.8471054Z     
2025-04-11T04:23:18.8471186Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8471355Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8471441Z         msg += original_trace
2025-04-11T04:23:18.8471618Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8471773Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8471849Z E       
2025-04-11T04:23:18.8471975Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8472072Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8472369Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8472508Z E           fn(i, *args)
2025-04-11T04:23:18.8472751Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
2025-04-11T04:23:18.8472839Z E           check_stage_manager()
2025-04-11T04:23:18.8473106Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
2025-04-11T04:23:18.8473258Z E           dist.barrier(group=group)
2025-04-11T04:23:18.8473560Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.8473654Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.8473975Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
2025-04-11T04:23:18.8474078Z E           work = group.barrier(opts=opts)
2025-04-11T04:23:18.8474183Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8474540Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8474677Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8474840Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8474845Z 
2025-04-11T04:23:18.8475144Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8475297Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8475448Z [04/11/25 04:18:39] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8475576Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8475684Z                              :75 launch                                         
2025-04-11T04:23:18.8475820Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8475949Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8476141Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8476285Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8476581Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26717 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8476869Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26717 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8477050Z _______________________________ test_pp[2-12-4] ________________________________
2025-04-11T04:23:18.8477055Z 
2025-04-11T04:23:18.8477130Z args = ()
2025-04-11T04:23:18.8477293Z kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 2}
2025-04-11T04:23:18.8477371Z try_count = 1
2025-04-11T04:23:18.8477994Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8477998Z 
2025-04-11T04:23:18.8478099Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8478183Z         try_count = 0
2025-04-11T04:23:18.8478281Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8478360Z             max_try, int
2025-04-11T04:23:18.8478510Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8478577Z     
2025-04-11T04:23:18.8478695Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8478767Z             try:
2025-04-11T04:23:18.8478852Z                 try_count += 1
2025-04-11T04:23:18.8478944Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8479091Z                 return ret
2025-04-11T04:23:18.8479191Z             except exception_type as e:
2025-04-11T04:23:18.8479295Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8479488Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8479605Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8479751Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8479958Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8480037Z                     continue
2025-04-11T04:23:18.8480115Z                 else:
2025-04-11T04:23:18.8480341Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8480427Z >                   raise e
2025-04-11T04:23:18.8480431Z 
2025-04-11T04:23:18.8480525Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8480639Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8480827Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8480915Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8481088Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T04:23:18.8481161Z     spawn(
2025-04-11T04:23:18.8481263Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8481362Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8481620Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8481798Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8482084Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8482174Z     while not context.join():
2025-04-11T04:23:18.8482282Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8482286Z 
2025-04-11T04:23:18.8482486Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f09cf0>
2025-04-11T04:23:18.8482562Z timeout = None
2025-04-11T04:23:18.8482566Z 
2025-04-11T04:23:18.8482656Z     def join(self, timeout=None):
2025-04-11T04:23:18.8482781Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8482851Z     
2025-04-11T04:23:18.8482996Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8483138Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8483301Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8483445Z         of the first process exiting.
2025-04-11T04:23:18.8483516Z     
2025-04-11T04:23:18.8483665Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8483801Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8483873Z     
2025-04-11T04:23:18.8483946Z         Args:
2025-04-11T04:23:18.8484086Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8484158Z         """
2025-04-11T04:23:18.8484292Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8484386Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8484465Z             return True
2025-04-11T04:23:18.8484536Z     
2025-04-11T04:23:18.8484665Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8484777Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8484870Z             self.sentinels.keys(),
2025-04-11T04:23:18.8484954Z             timeout=timeout,
2025-04-11T04:23:18.8485026Z         )
2025-04-11T04:23:18.8485093Z     
2025-04-11T04:23:18.8485174Z         error_index = None
2025-04-11T04:23:18.8485261Z         for sentinel in ready:
2025-04-11T04:23:18.8485412Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8485515Z             process = self.processes[index]
2025-04-11T04:23:18.8485599Z             process.join()
2025-04-11T04:23:18.8485694Z             if process.exitcode != 0:
2025-04-11T04:23:18.8485780Z                 error_index = index
2025-04-11T04:23:18.8485855Z                 break
2025-04-11T04:23:18.8485928Z     
2025-04-11T04:23:18.8486017Z         # Return if there was no error.
2025-04-11T04:23:18.8486153Z         if error_index is None:
2025-04-11T04:23:18.8486284Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8486376Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8486448Z     
2025-04-11T04:23:18.8486589Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8486684Z         for process in self.processes:
2025-04-11T04:23:18.8486769Z             if process.is_alive():
2025-04-11T04:23:18.8486863Z                 process.terminate()
2025-04-11T04:23:18.8486945Z             process.join()
2025-04-11T04:23:18.8487012Z     
2025-04-11T04:23:18.8487206Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8487322Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8487430Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8487549Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8487632Z             if exitcode < 0:
2025-04-11T04:23:18.8487739Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8487845Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8487995Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8488090Z                     error_index=error_index,
2025-04-11T04:23:18.8488191Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8488277Z                     exit_code=exitcode,
2025-04-11T04:23:18.8488363Z                     signal_name=name,
2025-04-11T04:23:18.8488438Z                 )
2025-04-11T04:23:18.8488512Z             else:
2025-04-11T04:23:18.8488617Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8488781Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8488872Z                     error_index=error_index,
2025-04-11T04:23:18.8488973Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8489059Z                     exit_code=exitcode,
2025-04-11T04:23:18.8489134Z                 )
2025-04-11T04:23:18.8489202Z     
2025-04-11T04:23:18.8489332Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8489553Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8489642Z         msg += original_trace
2025-04-11T04:23:18.8489817Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8489975Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8490049Z E       
2025-04-11T04:23:18.8490175Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8490273Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8490574Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8490653Z E           fn(i, *args)
2025-04-11T04:23:18.8490928Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:18.8491024Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8491296Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8491413Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8491681Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8491817Z E           module._apply(fn)
2025-04-11T04:23:18.8492087Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8492175Z E           module._apply(fn)
2025-04-11T04:23:18.8492437Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8492583Z E           param_applied = fn(param)
2025-04-11T04:23:18.8492852Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8492969Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8493076Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8493359Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8493496Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8493701Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8493705Z 
2025-04-11T04:23:18.8494014Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8494162Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8494319Z [04/11/25 04:18:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8494446Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8494555Z                              :75 launch                                         
2025-04-11T04:23:18.8494690Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8494811Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8495012Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8495143Z _______________________________ test_pp[2-12-12] _______________________________
2025-04-11T04:23:18.8495147Z 
2025-04-11T04:23:18.8495226Z args = ()
2025-04-11T04:23:18.8495379Z kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 2}
2025-04-11T04:23:18.8495457Z try_count = 1
2025-04-11T04:23:18.8496048Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8496055Z 
2025-04-11T04:23:18.8496207Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8496288Z         try_count = 0
2025-04-11T04:23:18.8496385Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8496469Z             max_try, int
2025-04-11T04:23:18.8496615Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8496687Z     
2025-04-11T04:23:18.8496801Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8496873Z             try:
2025-04-11T04:23:18.8496959Z                 try_count += 1
2025-04-11T04:23:18.8497048Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8497129Z                 return ret
2025-04-11T04:23:18.8497220Z             except exception_type as e:
2025-04-11T04:23:18.8497322Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8497509Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8497621Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8497772Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8497926Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8498057Z                     continue
2025-04-11T04:23:18.8498133Z                 else:
2025-04-11T04:23:18.8498357Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8498435Z >                   raise e
2025-04-11T04:23:18.8498439Z 
2025-04-11T04:23:18.8498531Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8498644Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8498820Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8498909Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8499078Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T04:23:18.8499155Z     spawn(
2025-04-11T04:23:18.8499256Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8499353Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8499609Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8499785Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8500116Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8500205Z     while not context.join():
2025-04-11T04:23:18.8500314Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8500320Z 
2025-04-11T04:23:18.8500514Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5ad28f0>
2025-04-11T04:23:18.8500590Z timeout = None
2025-04-11T04:23:18.8500594Z 
2025-04-11T04:23:18.8500686Z     def join(self, timeout=None):
2025-04-11T04:23:18.8500810Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8500881Z     
2025-04-11T04:23:18.8501024Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8501166Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8501329Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8501422Z         of the first process exiting.
2025-04-11T04:23:18.8501493Z     
2025-04-11T04:23:18.8501638Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8501773Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8501843Z     
2025-04-11T04:23:18.8501916Z         Args:
2025-04-11T04:23:18.8502054Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8502125Z         """
2025-04-11T04:23:18.8502264Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8502402Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8502487Z             return True
2025-04-11T04:23:18.8502556Z     
2025-04-11T04:23:18.8502687Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8502809Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8502898Z             self.sentinels.keys(),
2025-04-11T04:23:18.8502985Z             timeout=timeout,
2025-04-11T04:23:18.8503056Z         )
2025-04-11T04:23:18.8503123Z     
2025-04-11T04:23:18.8503208Z         error_index = None
2025-04-11T04:23:18.8503292Z         for sentinel in ready:
2025-04-11T04:23:18.8503397Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8503494Z             process = self.processes[index]
2025-04-11T04:23:18.8503578Z             process.join()
2025-04-11T04:23:18.8503673Z             if process.exitcode != 0:
2025-04-11T04:23:18.8503758Z                 error_index = index
2025-04-11T04:23:18.8503834Z                 break
2025-04-11T04:23:18.8503901Z     
2025-04-11T04:23:18.8503993Z         # Return if there was no error.
2025-04-11T04:23:18.8504075Z         if error_index is None:
2025-04-11T04:23:18.8504209Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8504361Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8504429Z     
2025-04-11T04:23:18.8504568Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8504661Z         for process in self.processes:
2025-04-11T04:23:18.8504750Z             if process.is_alive():
2025-04-11T04:23:18.8504841Z                 process.terminate()
2025-04-11T04:23:18.8504922Z             process.join()
2025-04-11T04:23:18.8504993Z     
2025-04-11T04:23:18.8505195Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8505311Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8505415Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8505535Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8505620Z             if exitcode < 0:
2025-04-11T04:23:18.8505724Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8505828Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8505979Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8506120Z                     error_index=error_index,
2025-04-11T04:23:18.8506224Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8506310Z                     exit_code=exitcode,
2025-04-11T04:23:18.8506396Z                     signal_name=name,
2025-04-11T04:23:18.8506468Z                 )
2025-04-11T04:23:18.8506545Z             else:
2025-04-11T04:23:18.8506644Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8506805Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8506899Z                     error_index=error_index,
2025-04-11T04:23:18.8506997Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8507084Z                     exit_code=exitcode,
2025-04-11T04:23:18.8507153Z                 )
2025-04-11T04:23:18.8507220Z     
2025-04-11T04:23:18.8507355Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8507526Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8507614Z         msg += original_trace
2025-04-11T04:23:18.8507785Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8507949Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8508020Z E       
2025-04-11T04:23:18.8508148Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8508248Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8508586Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8508727Z E           fn(i, *args)
2025-04-11T04:23:18.8509002Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:18.8509102Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8509374Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8509489Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8509765Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8509849Z E           module._apply(fn)
2025-04-11T04:23:18.8510125Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8510209Z E           module._apply(fn)
2025-04-11T04:23:18.8510479Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8510573Z E           param_applied = fn(param)
2025-04-11T04:23:18.8510849Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8511022Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8511136Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8511431Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8511568Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8511740Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8511884Z 
2025-04-11T04:23:18.8512192Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8512351Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8512505Z [04/11/25 04:18:50] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8512632Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8512745Z                              :75 launch                                         
2025-04-11T04:23:18.8512935Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8513067Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8513261Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8513397Z _______________________________ test_pp[4-12-4] ________________________________
2025-04-11T04:23:18.8513402Z 
2025-04-11T04:23:18.8513476Z args = ()
2025-04-11T04:23:18.8513632Z kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 4}
2025-04-11T04:23:18.8513707Z try_count = 1
2025-04-11T04:23:18.8514316Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8514322Z 
2025-04-11T04:23:18.8514429Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8514509Z         try_count = 0
2025-04-11T04:23:18.8514613Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8514694Z             max_try, int
2025-04-11T04:23:18.8514840Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8514910Z     
2025-04-11T04:23:18.8515025Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8515101Z             try:
2025-04-11T04:23:18.8515182Z                 try_count += 1
2025-04-11T04:23:18.8515276Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8515357Z                 return ret
2025-04-11T04:23:18.8515500Z             except exception_type as e:
2025-04-11T04:23:18.8515606Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8515791Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8515911Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8516058Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8516212Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8516292Z                     continue
2025-04-11T04:23:18.8516366Z                 else:
2025-04-11T04:23:18.8516593Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8516672Z >                   raise e
2025-04-11T04:23:18.8516676Z 
2025-04-11T04:23:18.8516771Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8516882Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8517017Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8517101Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8517322Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T04:23:18.8517399Z     spawn(
2025-04-11T04:23:18.8517503Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8517604Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8517860Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8518035Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8518372Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8518458Z     while not context.join():
2025-04-11T04:23:18.8518571Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8518577Z 
2025-04-11T04:23:18.8518775Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3d810>
2025-04-11T04:23:18.8518853Z timeout = None
2025-04-11T04:23:18.8518860Z 
2025-04-11T04:23:18.8518947Z     def join(self, timeout=None):
2025-04-11T04:23:18.8519074Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8519190Z     
2025-04-11T04:23:18.8519336Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8519485Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8519647Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8519744Z         of the first process exiting.
2025-04-11T04:23:18.8519814Z     
2025-04-11T04:23:18.8519965Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8520101Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8520171Z     
2025-04-11T04:23:18.8520246Z         Args:
2025-04-11T04:23:18.8520385Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8520458Z         """
2025-04-11T04:23:18.8520599Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8520688Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8520771Z             return True
2025-04-11T04:23:18.8520839Z     
2025-04-11T04:23:18.8520973Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8521091Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8521185Z             self.sentinels.keys(),
2025-04-11T04:23:18.8521271Z             timeout=timeout,
2025-04-11T04:23:18.8521342Z         )
2025-04-11T04:23:18.8521414Z     
2025-04-11T04:23:18.8521496Z         error_index = None
2025-04-11T04:23:18.8521584Z         for sentinel in ready:
2025-04-11T04:23:18.8521689Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8521834Z             process = self.processes[index]
2025-04-11T04:23:18.8521925Z             process.join()
2025-04-11T04:23:18.8522018Z             if process.exitcode != 0:
2025-04-11T04:23:18.8522112Z                 error_index = index
2025-04-11T04:23:18.8522186Z                 break
2025-04-11T04:23:18.8522254Z     
2025-04-11T04:23:18.8522352Z         # Return if there was no error.
2025-04-11T04:23:18.8522436Z         if error_index is None:
2025-04-11T04:23:18.8522571Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8522665Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8522737Z     
2025-04-11T04:23:18.8522878Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8522970Z         for process in self.processes:
2025-04-11T04:23:18.8523061Z             if process.is_alive():
2025-04-11T04:23:18.8523150Z                 process.terminate()
2025-04-11T04:23:18.8523235Z             process.join()
2025-04-11T04:23:18.8523307Z     
2025-04-11T04:23:18.8523446Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8523561Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8523723Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8523847Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8523929Z             if exitcode < 0:
2025-04-11T04:23:18.8524035Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8524138Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8524285Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8524429Z                     error_index=error_index,
2025-04-11T04:23:18.8524530Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8524622Z                     exit_code=exitcode,
2025-04-11T04:23:18.8524705Z                     signal_name=name,
2025-04-11T04:23:18.8524779Z                 )
2025-04-11T04:23:18.8524855Z             else:
2025-04-11T04:23:18.8524957Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8525122Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8525216Z                     error_index=error_index,
2025-04-11T04:23:18.8525365Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8525457Z                     exit_code=exitcode,
2025-04-11T04:23:18.8525529Z                 )
2025-04-11T04:23:18.8525601Z     
2025-04-11T04:23:18.8525730Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8525900Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8525985Z         msg += original_trace
2025-04-11T04:23:18.8526163Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8526322Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8526395Z E       
2025-04-11T04:23:18.8526522Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8526618Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8526922Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8527004Z E           fn(i, *args)
2025-04-11T04:23:18.8527274Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:18.8527372Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8527643Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8527763Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8528031Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8528168Z E           module._apply(fn)
2025-04-11T04:23:18.8528433Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8528524Z E           module._apply(fn)
2025-04-11T04:23:18.8528792Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8528888Z E           param_applied = fn(param)
2025-04-11T04:23:18.8529169Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8529285Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8529399Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8529687Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8529829Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8529993Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8529998Z 
2025-04-11T04:23:18.8530300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8530517Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8530670Z [04/11/25 04:18:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8530799Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8530903Z                              :75 launch                                         
2025-04-11T04:23:18.8531078Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8531202Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8531402Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8531531Z _______________________________ test_pp[4-12-12] _______________________________
2025-04-11T04:23:18.8531535Z 
2025-04-11T04:23:18.8531609Z args = ()
2025-04-11T04:23:18.8531770Z kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 4}
2025-04-11T04:23:18.8531844Z try_count = 1
2025-04-11T04:23:18.8532495Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8532500Z 
2025-04-11T04:23:18.8532603Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8532685Z         try_count = 0
2025-04-11T04:23:18.8532785Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8532863Z             max_try, int
2025-04-11T04:23:18.8533011Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8533081Z     
2025-04-11T04:23:18.8533194Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8533267Z             try:
2025-04-11T04:23:18.8533353Z                 try_count += 1
2025-04-11T04:23:18.8533444Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8533522Z                 return ret
2025-04-11T04:23:18.8533616Z             except exception_type as e:
2025-04-11T04:23:18.8533713Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8533901Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8534014Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8534162Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8534315Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8534393Z                     continue
2025-04-11T04:23:18.8534518Z                 else:
2025-04-11T04:23:18.8534745Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8534826Z >                   raise e
2025-04-11T04:23:18.8534832Z 
2025-04-11T04:23:18.8534925Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8535037Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8535168Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8535253Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8535426Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T04:23:18.8535500Z     spawn(
2025-04-11T04:23:18.8535602Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8535700Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8535953Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8536135Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8536415Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8536556Z     while not context.join():
2025-04-11T04:23:18.8536665Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8536670Z 
2025-04-11T04:23:18.8536874Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b42475e0>
2025-04-11T04:23:18.8536950Z timeout = None
2025-04-11T04:23:18.8536954Z 
2025-04-11T04:23:18.8537047Z     def join(self, timeout=None):
2025-04-11T04:23:18.8537170Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8537286Z     
2025-04-11T04:23:18.8537435Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8537575Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8537742Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8537833Z         of the first process exiting.
2025-04-11T04:23:18.8537900Z     
2025-04-11T04:23:18.8538048Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8538185Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8538303Z     
2025-04-11T04:23:18.8538375Z         Args:
2025-04-11T04:23:18.8538519Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8538590Z         """
2025-04-11T04:23:18.8538725Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8538817Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8538897Z             return True
2025-04-11T04:23:18.8538967Z     
2025-04-11T04:23:18.8539097Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8539210Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8539302Z             self.sentinels.keys(),
2025-04-11T04:23:18.8539384Z             timeout=timeout,
2025-04-11T04:23:18.8539457Z         )
2025-04-11T04:23:18.8539524Z     
2025-04-11T04:23:18.8539606Z         error_index = None
2025-04-11T04:23:18.8539692Z         for sentinel in ready:
2025-04-11T04:23:18.8539793Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8539895Z             process = self.processes[index]
2025-04-11T04:23:18.8539977Z             process.join()
2025-04-11T04:23:18.8540067Z             if process.exitcode != 0:
2025-04-11T04:23:18.8540153Z                 error_index = index
2025-04-11T04:23:18.8540227Z                 break
2025-04-11T04:23:18.8540297Z     
2025-04-11T04:23:18.8540386Z         # Return if there was no error.
2025-04-11T04:23:18.8540473Z         if error_index is None:
2025-04-11T04:23:18.8540601Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8540694Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8540830Z     
2025-04-11T04:23:18.8540971Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8541066Z         for process in self.processes:
2025-04-11T04:23:18.8541152Z             if process.is_alive():
2025-04-11T04:23:18.8541247Z                 process.terminate()
2025-04-11T04:23:18.8541328Z             process.join()
2025-04-11T04:23:18.8541397Z     
2025-04-11T04:23:18.8541538Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8541651Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8541759Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8541877Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8541961Z             if exitcode < 0:
2025-04-11T04:23:18.8542068Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8542170Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8542323Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8542415Z                     error_index=error_index,
2025-04-11T04:23:18.8542516Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8542654Z                     exit_code=exitcode,
2025-04-11T04:23:18.8542737Z                     signal_name=name,
2025-04-11T04:23:18.8542815Z                 )
2025-04-11T04:23:18.8542887Z             else:
2025-04-11T04:23:18.8542990Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8543153Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8543243Z                     error_index=error_index,
2025-04-11T04:23:18.8543395Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8543479Z                     exit_code=exitcode,
2025-04-11T04:23:18.8543553Z                 )
2025-04-11T04:23:18.8543620Z     
2025-04-11T04:23:18.8543753Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8543923Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8544006Z         msg += original_trace
2025-04-11T04:23:18.8544180Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8544339Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8544460Z E       
2025-04-11T04:23:18.8544585Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8544684Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8544979Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8545060Z E           fn(i, *args)
2025-04-11T04:23:18.8545331Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:18.8545424Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8545696Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8545809Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8546081Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8546168Z E           module._apply(fn)
2025-04-11T04:23:18.8546433Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8546519Z E           module._apply(fn)
2025-04-11T04:23:18.8546781Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8546880Z E           param_applied = fn(param)
2025-04-11T04:23:18.8547152Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8547314Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8547421Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8547709Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8547849Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8548011Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8548015Z 
2025-04-11T04:23:18.8548320Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8548507Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8548670Z [04/11/25 04:19:00] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8548800Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8548913Z                              :75 launch                                         
2025-04-11T04:23:18.8549051Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8549175Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8549435Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8549580Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8549879Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:47696 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8550012Z _______________________________ test_pp[2-12-4] ________________________________
2025-04-11T04:23:18.8550069Z 
2025-04-11T04:23:18.8550229Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 2}
2025-04-11T04:23:18.8550304Z try_count = 1
2025-04-11T04:23:18.8550909Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8550916Z 
2025-04-11T04:23:18.8551016Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8551093Z         try_count = 0
2025-04-11T04:23:18.8551245Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8551327Z             max_try, int
2025-04-11T04:23:18.8551474Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8551543Z     
2025-04-11T04:23:18.8551656Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8551731Z             try:
2025-04-11T04:23:18.8551813Z                 try_count += 1
2025-04-11T04:23:18.8551906Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8551983Z                 return ret
2025-04-11T04:23:18.8552075Z             except exception_type as e:
2025-04-11T04:23:18.8552175Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8552359Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8552478Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8552622Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8552779Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8552858Z                     continue
2025-04-11T04:23:18.8552935Z                 else:
2025-04-11T04:23:18.8553156Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8553237Z >                   raise e
2025-04-11T04:23:18.8553244Z 
2025-04-11T04:23:18.8553335Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8553442Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8553644Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8553733Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8553899Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T04:23:18.8553974Z     spawn(
2025-04-11T04:23:18.8554072Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8554174Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8554428Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8554605Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8554886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8554975Z     while not context.join():
2025-04-11T04:23:18.8555082Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8555086Z 
2025-04-11T04:23:18.8555283Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3e200>
2025-04-11T04:23:18.8555359Z timeout = None
2025-04-11T04:23:18.8555362Z 
2025-04-11T04:23:18.8555450Z     def join(self, timeout=None):
2025-04-11T04:23:18.8555626Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8555695Z     
2025-04-11T04:23:18.8555844Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8555987Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8556146Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8556240Z         of the first process exiting.
2025-04-11T04:23:18.8556355Z     
2025-04-11T04:23:18.8556505Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8556641Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8556713Z     
2025-04-11T04:23:18.8556784Z         Args:
2025-04-11T04:23:18.8556923Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8556998Z         """
2025-04-11T04:23:18.8557136Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8557234Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8557311Z             return True
2025-04-11T04:23:18.8557424Z     
2025-04-11T04:23:18.8557558Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8557674Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8557765Z             self.sentinels.keys(),
2025-04-11T04:23:18.8557847Z             timeout=timeout,
2025-04-11T04:23:18.8557922Z         )
2025-04-11T04:23:18.8557990Z     
2025-04-11T04:23:18.8558069Z         error_index = None
2025-04-11T04:23:18.8558156Z         for sentinel in ready:
2025-04-11T04:23:18.8558259Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8558357Z             process = self.processes[index]
2025-04-11T04:23:18.8558442Z             process.join()
2025-04-11T04:23:18.8558531Z             if process.exitcode != 0:
2025-04-11T04:23:18.8558620Z                 error_index = index
2025-04-11T04:23:18.8558696Z                 break
2025-04-11T04:23:18.8558766Z     
2025-04-11T04:23:18.8558854Z         # Return if there was no error.
2025-04-11T04:23:18.8558940Z         if error_index is None:
2025-04-11T04:23:18.8559073Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8559167Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8559237Z     
2025-04-11T04:23:18.8559372Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8559469Z         for process in self.processes:
2025-04-11T04:23:18.8559553Z             if process.is_alive():
2025-04-11T04:23:18.8559642Z                 process.terminate()
2025-04-11T04:23:18.8559726Z             process.join()
2025-04-11T04:23:18.8559794Z     
2025-04-11T04:23:18.8559989Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8560105Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8560210Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8560334Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8560417Z             if exitcode < 0:
2025-04-11T04:23:18.8560526Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8560629Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8560780Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8560874Z                     error_index=error_index,
2025-04-11T04:23:18.8560974Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8561062Z                     exit_code=exitcode,
2025-04-11T04:23:18.8561146Z                     signal_name=name,
2025-04-11T04:23:18.8561220Z                 )
2025-04-11T04:23:18.8561291Z             else:
2025-04-11T04:23:18.8561392Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8561556Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8561702Z                     error_index=error_index,
2025-04-11T04:23:18.8561803Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8561888Z                     exit_code=exitcode,
2025-04-11T04:23:18.8561962Z                 )
2025-04-11T04:23:18.8562030Z     
2025-04-11T04:23:18.8562160Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8562329Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8562467Z         msg += original_trace
2025-04-11T04:23:18.8562641Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8562799Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8562873Z E       
2025-04-11T04:23:18.8562999Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8563095Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8563398Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8563478Z E           fn(i, *args)
2025-04-11T04:23:18.8563801Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:18.8563910Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:18.8564177Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:18.8564272Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8564535Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8564655Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8564922Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8565009Z E           module._apply(fn)
2025-04-11T04:23:18.8565273Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8565359Z E           module._apply(fn)
2025-04-11T04:23:18.8565619Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8565710Z E           param_applied = fn(param)
2025-04-11T04:23:18.8565982Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8566095Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8566210Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8566540Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8566681Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8566841Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8566848Z 
2025-04-11T04:23:18.8567157Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8567306Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8567460Z [04/11/25 04:19:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8567593Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8567701Z                              :75 launch                                         
2025-04-11T04:23:18.8567840Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8567963Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8568160Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8568354Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8568651Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:42298 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8568786Z _______________________________ test_pp[2-12-6] ________________________________
2025-04-11T04:23:18.8568790Z 
2025-04-11T04:23:18.8568943Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 2}
2025-04-11T04:23:18.8569073Z try_count = 1
2025-04-11T04:23:18.8569677Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8569683Z 
2025-04-11T04:23:18.8569787Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8569861Z         try_count = 0
2025-04-11T04:23:18.8569962Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8570042Z             max_try, int
2025-04-11T04:23:18.8570232Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8570305Z     
2025-04-11T04:23:18.8570416Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8570492Z             try:
2025-04-11T04:23:18.8570574Z                 try_count += 1
2025-04-11T04:23:18.8570667Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8570748Z                 return ret
2025-04-11T04:23:18.8570839Z             except exception_type as e:
2025-04-11T04:23:18.8570939Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8571124Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8571245Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8571388Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8571545Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8571628Z                     continue
2025-04-11T04:23:18.8571705Z                 else:
2025-04-11T04:23:18.8571929Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8572007Z >                   raise e
2025-04-11T04:23:18.8572011Z 
2025-04-11T04:23:18.8572105Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8572215Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8572345Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8572434Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8572643Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T04:23:18.8572720Z     spawn(
2025-04-11T04:23:18.8572820Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8572920Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8573180Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8573357Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8573647Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8573738Z     while not context.join():
2025-04-11T04:23:18.8573848Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8573854Z 
2025-04-11T04:23:18.8574051Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b4245e40>
2025-04-11T04:23:18.8574130Z timeout = None
2025-04-11T04:23:18.8574134Z 
2025-04-11T04:23:18.8574222Z     def join(self, timeout=None):
2025-04-11T04:23:18.8574349Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8574419Z     
2025-04-11T04:23:18.8574564Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8574761Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8574926Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8575018Z         of the first process exiting.
2025-04-11T04:23:18.8575088Z     
2025-04-11T04:23:18.8575233Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8575370Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8575490Z     
2025-04-11T04:23:18.8575564Z         Args:
2025-04-11T04:23:18.8575702Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8575776Z         """
2025-04-11T04:23:18.8575914Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8576004Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8576086Z             return True
2025-04-11T04:23:18.8576153Z     
2025-04-11T04:23:18.8576285Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8576402Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8576549Z             self.sentinels.keys(),
2025-04-11T04:23:18.8576640Z             timeout=timeout,
2025-04-11T04:23:18.8576710Z         )
2025-04-11T04:23:18.8576780Z     
2025-04-11T04:23:18.8576860Z         error_index = None
2025-04-11T04:23:18.8576944Z         for sentinel in ready:
2025-04-11T04:23:18.8577054Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8577151Z             process = self.processes[index]
2025-04-11T04:23:18.8577236Z             process.join()
2025-04-11T04:23:18.8577326Z             if process.exitcode != 0:
2025-04-11T04:23:18.8577415Z                 error_index = index
2025-04-11T04:23:18.8577490Z                 break
2025-04-11T04:23:18.8577557Z     
2025-04-11T04:23:18.8577649Z         # Return if there was no error.
2025-04-11T04:23:18.8577732Z         if error_index is None:
2025-04-11T04:23:18.8577867Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8577962Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8578029Z     
2025-04-11T04:23:18.8578169Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8578263Z         for process in self.processes:
2025-04-11T04:23:18.8578354Z             if process.is_alive():
2025-04-11T04:23:18.8578441Z                 process.terminate()
2025-04-11T04:23:18.8578528Z             process.join()
2025-04-11T04:23:18.8578596Z     
2025-04-11T04:23:18.8578736Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8578856Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8579007Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8579132Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8579214Z             if exitcode < 0:
2025-04-11T04:23:18.8579316Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8579425Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8579573Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8579669Z                     error_index=error_index,
2025-04-11T04:23:18.8579764Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8579852Z                     exit_code=exitcode,
2025-04-11T04:23:18.8579937Z                     signal_name=name,
2025-04-11T04:23:18.8580009Z                 )
2025-04-11T04:23:18.8580085Z             else:
2025-04-11T04:23:18.8580182Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8580343Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8580436Z                     error_index=error_index,
2025-04-11T04:23:18.8580532Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8580620Z                     exit_code=exitcode,
2025-04-11T04:23:18.8580739Z                 )
2025-04-11T04:23:18.8580810Z     
2025-04-11T04:23:18.8580944Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8581116Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8581199Z         msg += original_trace
2025-04-11T04:23:18.8581369Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8581582Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8581654Z E       
2025-04-11T04:23:18.8581779Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8581874Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8582169Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8582250Z E           fn(i, *args)
2025-04-11T04:23:18.8582519Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:18.8582632Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:18.8582943Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:18.8583041Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8583308Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8583428Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8583695Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8583778Z E           module._apply(fn)
2025-04-11T04:23:18.8584046Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8584128Z E           module._apply(fn)
2025-04-11T04:23:18.8584391Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8584483Z E           param_applied = fn(param)
2025-04-11T04:23:18.8584757Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8584868Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8584973Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8585263Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8585396Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8585606Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8585611Z 
2025-04-11T04:23:18.8585914Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8586068Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8586226Z [04/11/25 04:19:09] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8586356Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8586461Z                              :75 launch                                         
2025-04-11T04:23:18.8586598Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8586727Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8586921Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8587058Z _______________________________ test_pp[4-12-4] ________________________________
2025-04-11T04:23:18.8587061Z 
2025-04-11T04:23:18.8587217Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 4}
2025-04-11T04:23:18.8587348Z try_count = 1
2025-04-11T04:23:18.8587948Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8587952Z 
2025-04-11T04:23:18.8588053Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8588132Z         try_count = 0
2025-04-11T04:23:18.8588280Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8588361Z             max_try, int
2025-04-11T04:23:18.8588533Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8588605Z     
2025-04-11T04:23:18.8588717Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8588792Z             try:
2025-04-11T04:23:18.8588873Z                 try_count += 1
2025-04-11T04:23:18.8588961Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8589047Z                 return ret
2025-04-11T04:23:18.8589138Z             except exception_type as e:
2025-04-11T04:23:18.8589293Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8589478Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8589592Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8589742Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8589897Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8589981Z                     continue
2025-04-11T04:23:18.8590055Z                 else:
2025-04-11T04:23:18.8590280Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8590355Z >                   raise e
2025-04-11T04:23:18.8590360Z 
2025-04-11T04:23:18.8590452Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8590566Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8590696Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8590785Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8590949Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T04:23:18.8591025Z     spawn(
2025-04-11T04:23:18.8591124Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8591223Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8591481Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8591658Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8591999Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8592090Z     while not context.join():
2025-04-11T04:23:18.8592201Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8592205Z 
2025-04-11T04:23:18.8592401Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3f640>
2025-04-11T04:23:18.8592478Z timeout = None
2025-04-11T04:23:18.8592482Z 
2025-04-11T04:23:18.8592574Z     def join(self, timeout=None):
2025-04-11T04:23:18.8592697Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8592769Z     
2025-04-11T04:23:18.8592914Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8593057Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8593218Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8593310Z         of the first process exiting.
2025-04-11T04:23:18.8593382Z     
2025-04-11T04:23:18.8593525Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8593662Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8593785Z     
2025-04-11T04:23:18.8593860Z         Args:
2025-04-11T04:23:18.8593999Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8594071Z         """
2025-04-11T04:23:18.8594211Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8594301Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8594385Z             return True
2025-04-11T04:23:18.8594506Z     
2025-04-11T04:23:18.8594635Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8594755Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8594844Z             self.sentinels.keys(),
2025-04-11T04:23:18.8594931Z             timeout=timeout,
2025-04-11T04:23:18.8595001Z         )
2025-04-11T04:23:18.8595069Z     
2025-04-11T04:23:18.8595154Z         error_index = None
2025-04-11T04:23:18.8595236Z         for sentinel in ready:
2025-04-11T04:23:18.8595343Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8595439Z             process = self.processes[index]
2025-04-11T04:23:18.8595569Z             process.join()
2025-04-11T04:23:18.8595662Z             if process.exitcode != 0:
2025-04-11T04:23:18.8595746Z                 error_index = index
2025-04-11T04:23:18.8595824Z                 break
2025-04-11T04:23:18.8595894Z     
2025-04-11T04:23:18.8595985Z         # Return if there was no error.
2025-04-11T04:23:18.8596069Z         if error_index is None:
2025-04-11T04:23:18.8596200Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8596297Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8596365Z     
2025-04-11T04:23:18.8596508Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8596601Z         for process in self.processes:
2025-04-11T04:23:18.8596686Z             if process.is_alive():
2025-04-11T04:23:18.8596779Z                 process.terminate()
2025-04-11T04:23:18.8596862Z             process.join()
2025-04-11T04:23:18.8596934Z     
2025-04-11T04:23:18.8597073Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8597190Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8597294Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8597414Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8597500Z             if exitcode < 0:
2025-04-11T04:23:18.8597603Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8597710Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8597857Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8597999Z                     error_index=error_index,
2025-04-11T04:23:18.8598101Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8598187Z                     exit_code=exitcode,
2025-04-11T04:23:18.8598277Z                     signal_name=name,
2025-04-11T04:23:18.8598349Z                 )
2025-04-11T04:23:18.8598423Z             else:
2025-04-11T04:23:18.8598526Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8598687Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8598783Z                     error_index=error_index,
2025-04-11T04:23:18.8598879Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8598971Z                     exit_code=exitcode,
2025-04-11T04:23:18.8599044Z                 )
2025-04-11T04:23:18.8599116Z     
2025-04-11T04:23:18.8599249Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8599421Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8599510Z         msg += original_trace
2025-04-11T04:23:18.8599681Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8599901Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8599973Z E       
2025-04-11T04:23:18.8600098Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8600198Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8600491Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8600573Z E           fn(i, *args)
2025-04-11T04:23:18.8600912Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:18.8601020Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:18.8601290Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:18.8601384Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8601650Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8601764Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8602093Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8602180Z E           module._apply(fn)
2025-04-11T04:23:18.8602450Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8602534Z E           module._apply(fn)
2025-04-11T04:23:18.8602798Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8602891Z E           param_applied = fn(param)
2025-04-11T04:23:18.8603163Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8603279Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8603388Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8603677Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8603811Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8603974Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8603978Z 
2025-04-11T04:23:18.8604275Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8604429Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8604581Z [04/11/25 04:19:15] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8604754Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8604867Z                              :75 launch                                         
2025-04-11T04:23:18.8605008Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8605135Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8605331Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8605476Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8605772Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30189 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8605904Z _______________________________ test_pp[4-12-6] ________________________________
2025-04-11T04:23:18.8605912Z 
2025-04-11T04:23:18.8606067Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 4}
2025-04-11T04:23:18.8606144Z try_count = 1
2025-04-11T04:23:18.8606748Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8606806Z 
2025-04-11T04:23:18.8606908Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8606991Z         try_count = 0
2025-04-11T04:23:18.8607089Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8607170Z             max_try, int
2025-04-11T04:23:18.8607314Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8607416Z     
2025-04-11T04:23:18.8607532Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8607605Z             try:
2025-04-11T04:23:18.8607690Z                 try_count += 1
2025-04-11T04:23:18.8607782Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8607861Z                 return ret
2025-04-11T04:23:18.8607957Z             except exception_type as e:
2025-04-11T04:23:18.8608056Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8608249Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8608412Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8608563Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8608720Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8608800Z                     continue
2025-04-11T04:23:18.8608881Z                 else:
2025-04-11T04:23:18.8609101Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8609180Z >                   raise e
2025-04-11T04:23:18.8609184Z 
2025-04-11T04:23:18.8609275Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8609387Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8609516Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8609603Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8609772Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T04:23:18.8609846Z     spawn(
2025-04-11T04:23:18.8609948Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8610044Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8610300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8610478Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8610760Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8610852Z     while not context.join():
2025-04-11T04:23:18.8611005Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8611009Z 
2025-04-11T04:23:18.8611209Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b86920e0>
2025-04-11T04:23:18.8611286Z timeout = None
2025-04-11T04:23:18.8611290Z 
2025-04-11T04:23:18.8611382Z     def join(self, timeout=None):
2025-04-11T04:23:18.8611504Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8611571Z     
2025-04-11T04:23:18.8611716Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8611856Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8612019Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8612109Z         of the first process exiting.
2025-04-11T04:23:18.8612180Z     
2025-04-11T04:23:18.8612321Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8612455Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8612526Z     
2025-04-11T04:23:18.8612597Z         Args:
2025-04-11T04:23:18.8612736Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8612952Z         """
2025-04-11T04:23:18.8613091Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8613184Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8613262Z             return True
2025-04-11T04:23:18.8613333Z     
2025-04-11T04:23:18.8613462Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8613582Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8613721Z             self.sentinels.keys(),
2025-04-11T04:23:18.8613803Z             timeout=timeout,
2025-04-11T04:23:18.8613878Z         )
2025-04-11T04:23:18.8613945Z     
2025-04-11T04:23:18.8614027Z         error_index = None
2025-04-11T04:23:18.8614112Z         for sentinel in ready:
2025-04-11T04:23:18.8614213Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8614313Z             process = self.processes[index]
2025-04-11T04:23:18.8614395Z             process.join()
2025-04-11T04:23:18.8614490Z             if process.exitcode != 0:
2025-04-11T04:23:18.8614575Z                 error_index = index
2025-04-11T04:23:18.8614691Z                 break
2025-04-11T04:23:18.8614764Z     
2025-04-11T04:23:18.8614853Z         # Return if there was no error.
2025-04-11T04:23:18.8614938Z         if error_index is None:
2025-04-11T04:23:18.8615067Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8615163Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8615232Z     
2025-04-11T04:23:18.8615366Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8615463Z         for process in self.processes:
2025-04-11T04:23:18.8615549Z             if process.is_alive():
2025-04-11T04:23:18.8615644Z                 process.terminate()
2025-04-11T04:23:18.8615723Z             process.join()
2025-04-11T04:23:18.8615789Z     
2025-04-11T04:23:18.8615931Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8616045Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8616154Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8616273Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8616358Z             if exitcode < 0:
2025-04-11T04:23:18.8616461Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8616561Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8616712Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8616803Z                     error_index=error_index,
2025-04-11T04:23:18.8616902Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8616988Z                     exit_code=exitcode,
2025-04-11T04:23:18.8617116Z                     signal_name=name,
2025-04-11T04:23:18.8617194Z                 )
2025-04-11T04:23:18.8617266Z             else:
2025-04-11T04:23:18.8617367Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8617531Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8617626Z                     error_index=error_index,
2025-04-11T04:23:18.8617723Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8617807Z                     exit_code=exitcode,
2025-04-11T04:23:18.8617883Z                 )
2025-04-11T04:23:18.8617952Z     
2025-04-11T04:23:18.8618085Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8618256Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8618342Z         msg += original_trace
2025-04-11T04:23:18.8618515Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8618671Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8618750Z E       
2025-04-11T04:23:18.8618874Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8619024Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8619322Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8619402Z E           fn(i, *args)
2025-04-11T04:23:18.8619674Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:18.8619779Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:18.8620098Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:18.8620192Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.8620463Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8620577Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8620845Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8620931Z E           module._apply(fn)
2025-04-11T04:23:18.8621244Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8621334Z E           module._apply(fn)
2025-04-11T04:23:18.8621593Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8621691Z E           param_applied = fn(param)
2025-04-11T04:23:18.8621959Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8622076Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8622183Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8622466Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8622604Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8622765Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8622769Z 
2025-04-11T04:23:18.8623075Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8623225Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8623382Z [04/11/25 04:19:22] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8623507Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8623663Z                              :75 launch                                         
2025-04-11T04:23:18.8623800Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8623925Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8624126Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8624267Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8624560Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8624841Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8625117Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8625247Z ___________________________________ test_pp ____________________________________
2025-04-11T04:23:18.8625251Z 
2025-04-11T04:23:18.8625345Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8625938Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8626005Z 
2025-04-11T04:23:18.8626109Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8626186Z         try_count = 0
2025-04-11T04:23:18.8626285Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8626367Z             max_try, int
2025-04-11T04:23:18.8626545Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8626615Z     
2025-04-11T04:23:18.8626724Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8626797Z             try:
2025-04-11T04:23:18.8626881Z                 try_count += 1
2025-04-11T04:23:18.8626971Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8627052Z                 return ret
2025-04-11T04:23:18.8627141Z             except exception_type as e:
2025-04-11T04:23:18.8627242Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8627477Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8627594Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8627740Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8627895Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8627979Z                     continue
2025-04-11T04:23:18.8628054Z                 else:
2025-04-11T04:23:18.8628275Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8628353Z >                   raise e
2025-04-11T04:23:18.8628359Z 
2025-04-11T04:23:18.8628491Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8628606Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8628738Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8628827Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8629004Z tests/test_pipeline/test_schedule/test_zerobubble_pp.py:1077: in test_pp
2025-04-11T04:23:18.8629078Z     spawn(
2025-04-11T04:23:18.8629179Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8629276Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8629532Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8629711Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8629996Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8630140Z     while not context.join():
2025-04-11T04:23:18.8630252Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8630259Z 
2025-04-11T04:23:18.8630457Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b4246350>
2025-04-11T04:23:18.8630536Z timeout = None
2025-04-11T04:23:18.8630540Z 
2025-04-11T04:23:18.8630634Z     def join(self, timeout=None):
2025-04-11T04:23:18.8630758Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8630829Z     
2025-04-11T04:23:18.8630973Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8631119Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8631283Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8631372Z         of the first process exiting.
2025-04-11T04:23:18.8631443Z     
2025-04-11T04:23:18.8631590Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8631728Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8631796Z     
2025-04-11T04:23:18.8631868Z         Args:
2025-04-11T04:23:18.8632065Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8632137Z         """
2025-04-11T04:23:18.8632278Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8632370Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8632451Z             return True
2025-04-11T04:23:18.8632518Z     
2025-04-11T04:23:18.8632649Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8632822Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8632912Z             self.sentinels.keys(),
2025-04-11T04:23:18.8632996Z             timeout=timeout,
2025-04-11T04:23:18.8633068Z         )
2025-04-11T04:23:18.8633135Z     
2025-04-11T04:23:18.8633220Z         error_index = None
2025-04-11T04:23:18.8633305Z         for sentinel in ready:
2025-04-11T04:23:18.8633411Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8633508Z             process = self.processes[index]
2025-04-11T04:23:18.8633592Z             process.join()
2025-04-11T04:23:18.8633685Z             if process.exitcode != 0:
2025-04-11T04:23:18.8633820Z                 error_index = index
2025-04-11T04:23:18.8633899Z                 break
2025-04-11T04:23:18.8633966Z     
2025-04-11T04:23:18.8634058Z         # Return if there was no error.
2025-04-11T04:23:18.8634140Z         if error_index is None:
2025-04-11T04:23:18.8634271Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8634370Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8634438Z     
2025-04-11T04:23:18.8634577Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8634670Z         for process in self.processes:
2025-04-11T04:23:18.8634756Z             if process.is_alive():
2025-04-11T04:23:18.8634849Z                 process.terminate()
2025-04-11T04:23:18.8634930Z             process.join()
2025-04-11T04:23:18.8634999Z     
2025-04-11T04:23:18.8635136Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8635252Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8635360Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8635478Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8635562Z             if exitcode < 0:
2025-04-11T04:23:18.8635664Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8635770Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8635914Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8636006Z                     error_index=error_index,
2025-04-11T04:23:18.8636108Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8636240Z                     exit_code=exitcode,
2025-04-11T04:23:18.8636331Z                     signal_name=name,
2025-04-11T04:23:18.8636403Z                 )
2025-04-11T04:23:18.8636476Z             else:
2025-04-11T04:23:18.8636581Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8636746Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8636840Z                     error_index=error_index,
2025-04-11T04:23:18.8636937Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8637027Z                     exit_code=exitcode,
2025-04-11T04:23:18.8637096Z                 )
2025-04-11T04:23:18.8637166Z     
2025-04-11T04:23:18.8637302Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8637467Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8637553Z         msg += original_trace
2025-04-11T04:23:18.8637729Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8637892Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8637966Z E       
2025-04-11T04:23:18.8638139Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8638241Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8638542Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8638625Z E           fn(i, *args)
2025-04-11T04:23:18.8638909Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
2025-04-11T04:23:18.8639063Z E           run_with_booster_moehybridplugin()
2025-04-11T04:23:18.8639321Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8639407Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8639777Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
2025-04-11T04:23:18.8639905Z E           torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T04:23:18.8640245Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.8640349Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.8640622Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8640737Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8641012Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8641102Z E           module._apply(fn)
2025-04-11T04:23:18.8641373Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8641470Z E           param_applied = fn(param)
2025-04-11T04:23:18.8641740Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8641856Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8641964Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8642270Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8642402Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8642562Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8642567Z 
2025-04-11T04:23:18.8642877Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8643026Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8643235Z [04/11/25 04:19:27] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8643367Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8643478Z                              :75 launch                                         
2025-04-11T04:23:18.8643617Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8643744Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8643938Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8644080Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8645215Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.8646370Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.8647510Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.8648644Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.8648784Z _____________________________ test_flash_attn_func _____________________________
2025-04-11T04:23:18.8648788Z 
2025-04-11T04:23:18.8648873Z args = (), kwargs = {}
2025-04-11T04:23:18.8648877Z 
2025-04-11T04:23:18.8648970Z     def _clear_cache(*args, **kwargs):
2025-04-11T04:23:18.8649065Z         get_accelerator().empty_cache()
2025-04-11T04:23:18.8649174Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T04:23:18.8649288Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T04:23:18.8649396Z         get_accelerator().reset_max_memory_cached()
2025-04-11T04:23:18.8649490Z >       get_accelerator().synchronize()
2025-04-11T04:23:18.8649494Z 
2025-04-11T04:23:18.8649591Z colossalai/testing/utils.py:271: 
2025-04-11T04:23:18.8649702Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8649861Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8649953Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8650061Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8650069Z 
2025-04-11T04:23:18.8650145Z device = None
2025-04-11T04:23:18.8650149Z 
2025-04-11T04:23:18.8650269Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8650490Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8650562Z     
2025-04-11T04:23:18.8650638Z         Args:
2025-04-11T04:23:18.8650808Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8650979Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8651091Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8651162Z         """
2025-04-11T04:23:18.8651243Z         _lazy_init()
2025-04-11T04:23:18.8651334Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8651438Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8651545Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8651834Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8651974Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8652134Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8652138Z 
2025-04-11T04:23:18.8652382Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8652566Z ______________________________ test_release_layer ______________________________
2025-04-11T04:23:18.8652570Z 
2025-04-11T04:23:18.8652659Z     def test_release_layer():
2025-04-11T04:23:18.8652784Z         orig_cuda_allocated = torch.cuda.memory_allocated()
2025-04-11T04:23:18.8652871Z >       model = Net().cuda()
2025-04-11T04:23:18.8652875Z 
2025-04-11T04:23:18.8652988Z tests/test_shardformer/test_shard_utils.py:16: 
2025-04-11T04:23:18.8653126Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8653365Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
2025-04-11T04:23:18.8653473Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8653714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8653796Z     module._apply(fn)
2025-04-11T04:23:18.8654031Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T04:23:18.8654112Z     module._apply(fn)
2025-04-11T04:23:18.8654382Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T04:23:18.8654478Z     param_applied = fn(param)
2025-04-11T04:23:18.8654583Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8654587Z 
2025-04-11T04:23:18.8654681Z t = Parameter containing:
2025-04-11T04:23:18.8654759Z tensor([[-0.8151],
2025-04-11T04:23:18.8654846Z         [ 0.1839]], requires_grad=True)
2025-04-11T04:23:18.8654850Z 
2025-04-11T04:23:18.8654958Z >   return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8655060Z E   RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8655347Z E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8655479Z E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8655639Z E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8655644Z 
2025-04-11T04:23:18.8655891Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
2025-04-11T04:23:18.8656020Z __________________________________ test_gpt2 ___________________________________
2025-04-11T04:23:18.8656023Z 
2025-04-11T04:23:18.8656113Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8656120Z 
2025-04-11T04:23:18.8656219Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8656295Z         try_count = 0
2025-04-11T04:23:18.8656392Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8656474Z             max_try, int
2025-04-11T04:23:18.8656666Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8656740Z     
2025-04-11T04:23:18.8656850Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8656925Z             try:
2025-04-11T04:23:18.8657010Z                 try_count += 1
2025-04-11T04:23:18.8657101Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8657104Z 
2025-04-11T04:23:18.8657199Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8657304Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8657418Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8657512Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8657670Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8657766Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8658004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T04:23:18.8658102Z     with torch.cuda.device(device):
2025-04-11T04:23:18.8658207Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8658211Z 
2025-04-11T04:23:18.8658331Z self = <torch.cuda.device object at 0x7fb581d6d5d0>
2025-04-11T04:23:18.8658384Z 
2025-04-11T04:23:18.8658465Z     def __enter__(self):
2025-04-11T04:23:18.8658597Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T04:23:18.8658705Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8658992Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8659182Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8659339Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8659344Z 
2025-04-11T04:23:18.8659577Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T04:23:18.8659709Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T04:23:18.8659713Z 
2025-04-11T04:23:18.8659806Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8659812Z 
2025-04-11T04:23:18.8659909Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8659985Z         try_count = 0
2025-04-11T04:23:18.8660133Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8660214Z             max_try, int
2025-04-11T04:23:18.8660360Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8660428Z     
2025-04-11T04:23:18.8660540Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8660614Z             try:
2025-04-11T04:23:18.8660695Z                 try_count += 1
2025-04-11T04:23:18.8660786Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8660790Z 
2025-04-11T04:23:18.8660881Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8660992Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8661102Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8661194Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8661346Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8661437Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8661548Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8661552Z 
2025-04-11T04:23:18.8661627Z device = None
2025-04-11T04:23:18.8661631Z 
2025-04-11T04:23:18.8661753Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8661904Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8661977Z     
2025-04-11T04:23:18.8662048Z         Args:
2025-04-11T04:23:18.8662212Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8662439Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8662549Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8662624Z         """
2025-04-11T04:23:18.8662701Z         _lazy_init()
2025-04-11T04:23:18.8662792Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8662897Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8663002Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8663290Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8663427Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8663589Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8663593Z 
2025-04-11T04:23:18.8663827Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8663963Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T04:23:18.8663967Z 
2025-04-11T04:23:18.8664057Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8664061Z 
2025-04-11T04:23:18.8664208Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8664289Z         try_count = 0
2025-04-11T04:23:18.8664388Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8664469Z             max_try, int
2025-04-11T04:23:18.8664610Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8664680Z     
2025-04-11T04:23:18.8664787Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8664858Z             try:
2025-04-11T04:23:18.8665008Z                 try_count += 1
2025-04-11T04:23:18.8665096Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8665101Z 
2025-04-11T04:23:18.8665195Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8665305Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8665416Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8665509Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8665661Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8665759Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8665913Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8665917Z 
2025-04-11T04:23:18.8665998Z device = None
2025-04-11T04:23:18.8666002Z 
2025-04-11T04:23:18.8666121Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8666276Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8666346Z     
2025-04-11T04:23:18.8666417Z         Args:
2025-04-11T04:23:18.8666586Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8666754Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8666866Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8666939Z         """
2025-04-11T04:23:18.8667018Z         _lazy_init()
2025-04-11T04:23:18.8667118Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8667217Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8667326Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8667611Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8667750Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8667908Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8667914Z 
2025-04-11T04:23:18.8668148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8668335Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T04:23:18.8668340Z 
2025-04-11T04:23:18.8668461Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8668466Z 
2025-04-11T04:23:18.8668569Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8668648Z         try_count = 0
2025-04-11T04:23:18.8668747Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8668827Z             max_try, int
2025-04-11T04:23:18.8668972Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8669041Z     
2025-04-11T04:23:18.8669148Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8669224Z             try:
2025-04-11T04:23:18.8669305Z                 try_count += 1
2025-04-11T04:23:18.8669399Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8669403Z 
2025-04-11T04:23:18.8669495Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8669602Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8669718Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8669809Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8669965Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8670120Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8670235Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8670239Z 
2025-04-11T04:23:18.8670314Z device = None
2025-04-11T04:23:18.8670318Z 
2025-04-11T04:23:18.8670439Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8670593Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8670703Z     
2025-04-11T04:23:18.8670780Z         Args:
2025-04-11T04:23:18.8670947Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8671116Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8671229Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8671302Z         """
2025-04-11T04:23:18.8671382Z         _lazy_init()
2025-04-11T04:23:18.8671472Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8671576Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8671682Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8672037Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8672175Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8672331Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8672337Z 
2025-04-11T04:23:18.8672573Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8672710Z ____________________________ test_dist_crossentropy ____________________________
2025-04-11T04:23:18.8672714Z 
2025-04-11T04:23:18.8672808Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8673429Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8673437Z 
2025-04-11T04:23:18.8673538Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8673616Z         try_count = 0
2025-04-11T04:23:18.8673716Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8673793Z             max_try, int
2025-04-11T04:23:18.8673935Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8674009Z     
2025-04-11T04:23:18.8674117Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8674194Z             try:
2025-04-11T04:23:18.8674274Z                 try_count += 1
2025-04-11T04:23:18.8674415Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8674500Z                 return ret
2025-04-11T04:23:18.8674590Z             except exception_type as e:
2025-04-11T04:23:18.8674691Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8674878Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8674997Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8675138Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8675290Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8675373Z                     continue
2025-04-11T04:23:18.8675448Z                 else:
2025-04-11T04:23:18.8675672Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8675749Z >                   raise e
2025-04-11T04:23:18.8675753Z 
2025-04-11T04:23:18.8675847Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8675953Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8676079Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8676219Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8676442Z tests/test_shardformer/test_layer/test_dist_crossentropy.py:51: in test_dist_crossentropy
2025-04-11T04:23:18.8676589Z     spawn(check_dist_crossentropy, 2, ignore_index=ignore_index)
2025-04-11T04:23:18.8676687Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8676787Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8677037Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8677263Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8677554Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8677644Z     while not context.join():
2025-04-11T04:23:18.8677754Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8677758Z 
2025-04-11T04:23:18.8677977Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3e410>
2025-04-11T04:23:18.8678058Z timeout = None
2025-04-11T04:23:18.8678110Z 
2025-04-11T04:23:18.8678203Z     def join(self, timeout=None):
2025-04-11T04:23:18.8678328Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8678396Z     
2025-04-11T04:23:18.8678542Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8678689Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8678853Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8678945Z         of the first process exiting.
2025-04-11T04:23:18.8679013Z     
2025-04-11T04:23:18.8679160Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8679298Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8679366Z     
2025-04-11T04:23:18.8679441Z         Args:
2025-04-11T04:23:18.8679580Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8679654Z         """
2025-04-11T04:23:18.8679793Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8679884Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8679967Z             return True
2025-04-11T04:23:18.8680035Z     
2025-04-11T04:23:18.8680169Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8680288Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8680377Z             self.sentinels.keys(),
2025-04-11T04:23:18.8680462Z             timeout=timeout,
2025-04-11T04:23:18.8680533Z         )
2025-04-11T04:23:18.8680604Z     
2025-04-11T04:23:18.8680739Z         error_index = None
2025-04-11T04:23:18.8680826Z         for sentinel in ready:
2025-04-11T04:23:18.8680936Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8681034Z             process = self.processes[index]
2025-04-11T04:23:18.8681122Z             process.join()
2025-04-11T04:23:18.8681214Z             if process.exitcode != 0:
2025-04-11T04:23:18.8681306Z                 error_index = index
2025-04-11T04:23:18.8681380Z                 break
2025-04-11T04:23:18.8681449Z     
2025-04-11T04:23:18.8681541Z         # Return if there was no error.
2025-04-11T04:23:18.8681626Z         if error_index is None:
2025-04-11T04:23:18.8681762Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8681858Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8681926Z     
2025-04-11T04:23:18.8682069Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8682163Z         for process in self.processes:
2025-04-11T04:23:18.8682254Z             if process.is_alive():
2025-04-11T04:23:18.8682345Z                 process.terminate()
2025-04-11T04:23:18.8682432Z             process.join()
2025-04-11T04:23:18.8682499Z     
2025-04-11T04:23:18.8682690Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8682811Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8682916Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8683038Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8683121Z             if exitcode < 0:
2025-04-11T04:23:18.8683228Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8683382Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8683529Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8683626Z                     error_index=error_index,
2025-04-11T04:23:18.8683726Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8683818Z                     exit_code=exitcode,
2025-04-11T04:23:18.8683901Z                     signal_name=name,
2025-04-11T04:23:18.8683973Z                 )
2025-04-11T04:23:18.8684051Z             else:
2025-04-11T04:23:18.8684154Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8684371Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8684464Z                     error_index=error_index,
2025-04-11T04:23:18.8684563Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8684651Z                     exit_code=exitcode,
2025-04-11T04:23:18.8684723Z                 )
2025-04-11T04:23:18.8684798Z     
2025-04-11T04:23:18.8684928Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8685099Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8685184Z         msg += original_trace
2025-04-11T04:23:18.8685358Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8685519Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8685591Z E       
2025-04-11T04:23:18.8685721Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8685817Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8686122Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8686201Z E           fn(i, *args)
2025-04-11T04:23:18.8686537Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
2025-04-11T04:23:18.8686676Z E           pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
2025-04-11T04:23:18.8686780Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8687114Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8687250Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8687411Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8687417Z 
2025-04-11T04:23:18.8687723Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8687871Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8688029Z [04/11/25 04:19:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8688155Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8688265Z                              :75 launch                                         
2025-04-11T04:23:18.8688403Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8688532Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8688729Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8688873Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8689216Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26698 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8689345Z _________________________________ test_dropout _________________________________
2025-04-11T04:23:18.8689352Z 
2025-04-11T04:23:18.8689443Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8690033Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8690085Z 
2025-04-11T04:23:18.8690189Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8690268Z         try_count = 0
2025-04-11T04:23:18.8690369Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8690449Z             max_try, int
2025-04-11T04:23:18.8690594Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8690665Z     
2025-04-11T04:23:18.8690819Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8690898Z             try:
2025-04-11T04:23:18.8690981Z                 try_count += 1
2025-04-11T04:23:18.8691073Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8691150Z                 return ret
2025-04-11T04:23:18.8691242Z             except exception_type as e:
2025-04-11T04:23:18.8691344Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8691530Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8691645Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8691790Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8691946Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8692026Z                     continue
2025-04-11T04:23:18.8692100Z                 else:
2025-04-11T04:23:18.8692324Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8692401Z >                   raise e
2025-04-11T04:23:18.8692405Z 
2025-04-11T04:23:18.8692499Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8692605Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8692737Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8692820Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8692988Z tests/test_shardformer/test_layer/test_dropout.py:66: in test_dropout
2025-04-11T04:23:18.8693079Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8693222Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8693327Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8693582Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8693763Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8694044Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8694129Z     while not context.join():
2025-04-11T04:23:18.8694238Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8694244Z 
2025-04-11T04:23:18.8694439Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a898d0>
2025-04-11T04:23:18.8694517Z timeout = None
2025-04-11T04:23:18.8694521Z 
2025-04-11T04:23:18.8694609Z     def join(self, timeout=None):
2025-04-11T04:23:18.8694735Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8694802Z     
2025-04-11T04:23:18.8694946Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8695090Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8695300Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8695396Z         of the first process exiting.
2025-04-11T04:23:18.8695466Z     
2025-04-11T04:23:18.8695612Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8695745Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8695812Z     
2025-04-11T04:23:18.8695954Z         Args:
2025-04-11T04:23:18.8696090Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8696163Z         """
2025-04-11T04:23:18.8696298Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8696390Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8696470Z             return True
2025-04-11T04:23:18.8696537Z     
2025-04-11T04:23:18.8696668Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8696786Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8696879Z             self.sentinels.keys(),
2025-04-11T04:23:18.8697009Z             timeout=timeout,
2025-04-11T04:23:18.8697080Z         )
2025-04-11T04:23:18.8697151Z     
2025-04-11T04:23:18.8697232Z         error_index = None
2025-04-11T04:23:18.8697319Z         for sentinel in ready:
2025-04-11T04:23:18.8697422Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8697517Z             process = self.processes[index]
2025-04-11T04:23:18.8697605Z             process.join()
2025-04-11T04:23:18.8697696Z             if process.exitcode != 0:
2025-04-11T04:23:18.8697782Z                 error_index = index
2025-04-11T04:23:18.8697855Z                 break
2025-04-11T04:23:18.8697922Z     
2025-04-11T04:23:18.8698015Z         # Return if there was no error.
2025-04-11T04:23:18.8698098Z         if error_index is None:
2025-04-11T04:23:18.8698230Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8698324Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8698394Z     
2025-04-11T04:23:18.8698534Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8698626Z         for process in self.processes:
2025-04-11T04:23:18.8698715Z             if process.is_alive():
2025-04-11T04:23:18.8698802Z                 process.terminate()
2025-04-11T04:23:18.8698884Z             process.join()
2025-04-11T04:23:18.8698954Z     
2025-04-11T04:23:18.8699091Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8699208Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8699311Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8699479Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8699563Z             if exitcode < 0:
2025-04-11T04:23:18.8699670Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8699773Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8699922Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8700020Z                     error_index=error_index,
2025-04-11T04:23:18.8700118Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8700204Z                     exit_code=exitcode,
2025-04-11T04:23:18.8700288Z                     signal_name=name,
2025-04-11T04:23:18.8700358Z                 )
2025-04-11T04:23:18.8700436Z             else:
2025-04-11T04:23:18.8700536Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8700700Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8700791Z                     error_index=error_index,
2025-04-11T04:23:18.8700892Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8700978Z                     exit_code=exitcode,
2025-04-11T04:23:18.8701047Z                 )
2025-04-11T04:23:18.8701169Z     
2025-04-11T04:23:18.8701300Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8701473Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8701558Z         msg += original_trace
2025-04-11T04:23:18.8701730Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8701892Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8702010Z E       
2025-04-11T04:23:18.8702139Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8702237Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8702542Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8702621Z E           fn(i, *args)
2025-04-11T04:23:18.8702888Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
2025-04-11T04:23:18.8702989Z E           check_dropout_parallel_input()
2025-04-11T04:23:18.8703352Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
2025-04-11T04:23:18.8703582Z E           dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
2025-04-11T04:23:18.8703857Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
2025-04-11T04:23:18.8704076Z E           return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
2025-04-11T04:23:18.8704316Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
2025-04-11T04:23:18.8704522Z E           self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
2025-04-11T04:23:18.8704819Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
2025-04-11T04:23:18.8705007Z E           is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
2025-04-11T04:23:18.8705319Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
2025-04-11T04:23:18.8705557Z E           index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.8705667Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8705950Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8706087Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8706297Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8706302Z 
2025-04-11T04:23:18.8706613Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8706765Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8706919Z [04/11/25 04:19:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8707048Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8707152Z                              :75 launch                                         
2025-04-11T04:23:18.8707292Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8707417Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8707617Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8707751Z ______________________________ test_embedding_1d _______________________________
2025-04-11T04:23:18.8707755Z 
2025-04-11T04:23:18.8707849Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8708485Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8708544Z 
2025-04-11T04:23:18.8708648Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8708726Z         try_count = 0
2025-04-11T04:23:18.8708825Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8708950Z             max_try, int
2025-04-11T04:23:18.8709095Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8709167Z     
2025-04-11T04:23:18.8709276Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8709349Z             try:
2025-04-11T04:23:18.8709434Z                 try_count += 1
2025-04-11T04:23:18.8709522Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8709603Z                 return ret
2025-04-11T04:23:18.8709693Z             except exception_type as e:
2025-04-11T04:23:18.8709794Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8710026Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8710142Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8710288Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8710440Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8710525Z                     continue
2025-04-11T04:23:18.8710600Z                 else:
2025-04-11T04:23:18.8710824Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8710903Z >                   raise e
2025-04-11T04:23:18.8710907Z 
2025-04-11T04:23:18.8710998Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8711108Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8711237Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8711325Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8711508Z tests/test_shardformer/test_layer/test_embedding.py:52: in test_embedding_1d
2025-04-11T04:23:18.8711600Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8711696Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8711792Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8712053Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8712226Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8712656Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8712746Z     while not context.join():
2025-04-11T04:23:18.8712854Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8712861Z 
2025-04-11T04:23:18.8713055Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3fe80>
2025-04-11T04:23:18.8713133Z timeout = None
2025-04-11T04:23:18.8713137Z 
2025-04-11T04:23:18.8713227Z     def join(self, timeout=None):
2025-04-11T04:23:18.8713351Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8713422Z     
2025-04-11T04:23:18.8713564Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8713709Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8713868Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8713958Z         of the first process exiting.
2025-04-11T04:23:18.8714030Z     
2025-04-11T04:23:18.8714174Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8714310Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8714428Z     
2025-04-11T04:23:18.8714500Z         Args:
2025-04-11T04:23:18.8714642Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8714715Z         """
2025-04-11T04:23:18.8714853Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8714944Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8715024Z             return True
2025-04-11T04:23:18.8715092Z     
2025-04-11T04:23:18.8715270Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8715389Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8715479Z             self.sentinels.keys(),
2025-04-11T04:23:18.8715565Z             timeout=timeout,
2025-04-11T04:23:18.8715636Z         )
2025-04-11T04:23:18.8715706Z     
2025-04-11T04:23:18.8715791Z         error_index = None
2025-04-11T04:23:18.8715874Z         for sentinel in ready:
2025-04-11T04:23:18.8715981Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8716080Z             process = self.processes[index]
2025-04-11T04:23:18.8716162Z             process.join()
2025-04-11T04:23:18.8716303Z             if process.exitcode != 0:
2025-04-11T04:23:18.8716393Z                 error_index = index
2025-04-11T04:23:18.8716471Z                 break
2025-04-11T04:23:18.8716539Z     
2025-04-11T04:23:18.8716629Z         # Return if there was no error.
2025-04-11T04:23:18.8716712Z         if error_index is None:
2025-04-11T04:23:18.8716849Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8716948Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8717016Z     
2025-04-11T04:23:18.8717156Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8717250Z         for process in self.processes:
2025-04-11T04:23:18.8717335Z             if process.is_alive():
2025-04-11T04:23:18.8717428Z                 process.terminate()
2025-04-11T04:23:18.8717509Z             process.join()
2025-04-11T04:23:18.8717580Z     
2025-04-11T04:23:18.8717716Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8717833Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8717938Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8718054Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8718138Z             if exitcode < 0:
2025-04-11T04:23:18.8718241Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8718350Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8718497Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8718588Z                     error_index=error_index,
2025-04-11T04:23:18.8718735Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8718824Z                     exit_code=exitcode,
2025-04-11T04:23:18.8718913Z                     signal_name=name,
2025-04-11T04:23:18.8718988Z                 )
2025-04-11T04:23:18.8719062Z             else:
2025-04-11T04:23:18.8719165Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8719334Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8719429Z                     error_index=error_index,
2025-04-11T04:23:18.8719524Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8719613Z                     exit_code=exitcode,
2025-04-11T04:23:18.8719686Z                 )
2025-04-11T04:23:18.8719753Z     
2025-04-11T04:23:18.8719885Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8720055Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8720145Z         msg += original_trace
2025-04-11T04:23:18.8720318Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8720486Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8720625Z E       
2025-04-11T04:23:18.8720748Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8720849Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8721147Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8721228Z E           fn(i, *args)
2025-04-11T04:23:18.8721499Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
2025-04-11T04:23:18.8721641Z E           check_embedding_1d()
2025-04-11T04:23:18.8721898Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8721988Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8722284Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
2025-04-11T04:23:18.8722390Z E           embedding = nn.Embedding(32, 128).cuda()
2025-04-11T04:23:18.8722714Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8722834Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8723103Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8723195Z E           param_applied = fn(param)
2025-04-11T04:23:18.8723469Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8723585Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8723688Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8723974Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8724108Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8724271Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8724275Z 
2025-04-11T04:23:18.8724579Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8724730Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8724884Z [04/11/25 04:19:43] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8725009Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8725116Z                              :75 launch                                         
2025-04-11T04:23:18.8725301Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8725429Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8725623Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8725759Z _______________________________ test_linearconv ________________________________
2025-04-11T04:23:18.8725765Z 
2025-04-11T04:23:18.8725857Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8726464Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8726471Z 
2025-04-11T04:23:18.8726573Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8726655Z         try_count = 0
2025-04-11T04:23:18.8726753Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8726832Z             max_try, int
2025-04-11T04:23:18.8726982Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8727051Z     
2025-04-11T04:23:18.8727165Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8727303Z             try:
2025-04-11T04:23:18.8727386Z                 try_count += 1
2025-04-11T04:23:18.8727481Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8727559Z                 return ret
2025-04-11T04:23:18.8727652Z             except exception_type as e:
2025-04-11T04:23:18.8727752Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8727941Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8728109Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8728251Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8728411Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8728491Z                     continue
2025-04-11T04:23:18.8728569Z                 else:
2025-04-11T04:23:18.8728788Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8728869Z >                   raise e
2025-04-11T04:23:18.8728877Z 
2025-04-11T04:23:18.8729016Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8729127Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8729260Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8729344Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8729568Z tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py:209: in test_linearconv
2025-04-11T04:23:18.8729657Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8729757Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8729856Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8730108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8730283Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8730565Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8730655Z     while not context.join():
2025-04-11T04:23:18.8730759Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8730763Z 
2025-04-11T04:23:18.8730961Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3c910>
2025-04-11T04:23:18.8731037Z timeout = None
2025-04-11T04:23:18.8731043Z 
2025-04-11T04:23:18.8731131Z     def join(self, timeout=None):
2025-04-11T04:23:18.8731255Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8731325Z     
2025-04-11T04:23:18.8731472Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8731663Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8731832Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8731926Z         of the first process exiting.
2025-04-11T04:23:18.8731997Z     
2025-04-11T04:23:18.8732152Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8732293Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8732367Z     
2025-04-11T04:23:18.8732443Z         Args:
2025-04-11T04:23:18.8732583Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8732665Z         """
2025-04-11T04:23:18.8732805Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8732902Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8732982Z             return True
2025-04-11T04:23:18.8733053Z     
2025-04-11T04:23:18.8733191Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8733312Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8733408Z             self.sentinels.keys(),
2025-04-11T04:23:18.8733542Z             timeout=timeout,
2025-04-11T04:23:18.8733618Z         )
2025-04-11T04:23:18.8733687Z     
2025-04-11T04:23:18.8733769Z         error_index = None
2025-04-11T04:23:18.8733856Z         for sentinel in ready:
2025-04-11T04:23:18.8733958Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8734056Z             process = self.processes[index]
2025-04-11T04:23:18.8734139Z             process.join()
2025-04-11T04:23:18.8734228Z             if process.exitcode != 0:
2025-04-11T04:23:18.8734368Z                 error_index = index
2025-04-11T04:23:18.8734442Z                 break
2025-04-11T04:23:18.8734514Z     
2025-04-11T04:23:18.8734603Z         # Return if there was no error.
2025-04-11T04:23:18.8734685Z         if error_index is None:
2025-04-11T04:23:18.8734822Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8734915Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8734985Z     
2025-04-11T04:23:18.8735122Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8735219Z         for process in self.processes:
2025-04-11T04:23:18.8735352Z             if process.is_alive():
2025-04-11T04:23:18.8735444Z                 process.terminate()
2025-04-11T04:23:18.8735529Z             process.join()
2025-04-11T04:23:18.8735596Z     
2025-04-11T04:23:18.8735738Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8735851Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8735958Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8736079Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8736158Z             if exitcode < 0:
2025-04-11T04:23:18.8736266Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8736369Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8736518Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8736613Z                     error_index=error_index,
2025-04-11T04:23:18.8736709Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8736801Z                     exit_code=exitcode,
2025-04-11T04:23:18.8736885Z                     signal_name=name,
2025-04-11T04:23:18.8736959Z                 )
2025-04-11T04:23:18.8737030Z             else:
2025-04-11T04:23:18.8737133Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8737293Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8737386Z                     error_index=error_index,
2025-04-11T04:23:18.8737486Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8737570Z                     exit_code=exitcode,
2025-04-11T04:23:18.8737693Z                 )
2025-04-11T04:23:18.8737763Z     
2025-04-11T04:23:18.8737894Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8738070Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8738155Z         msg += original_trace
2025-04-11T04:23:18.8738332Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8738495Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8738569Z E       
2025-04-11T04:23:18.8738693Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8738790Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8739090Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8739169Z E           fn(i, *args)
2025-04-11T04:23:18.8739491Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
2025-04-11T04:23:18.8739591Z E           check_gpt2_qkv_fused_linear_1d()
2025-04-11T04:23:18.8739848Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8739984Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8740238Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8740328Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8740687Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
2025-04-11T04:23:18.8740876Z E           check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
2025-04-11T04:23:18.8741211Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
2025-04-11T04:23:18.8741309Z E           linear = Conv1D(192, 48).cuda()
2025-04-11T04:23:18.8741573Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8741696Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8742009Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8742102Z E           param_applied = fn(param)
2025-04-11T04:23:18.8742383Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8742497Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8742605Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8742887Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8743025Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8743182Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8743186Z 
2025-04-11T04:23:18.8743488Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8743642Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8743797Z [04/11/25 04:19:48] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8743926Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8744029Z                              :75 launch                                         
2025-04-11T04:23:18.8744171Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8744293Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8744555Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8744700Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8744996Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26338 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8745132Z ________________________________ test_layernorm ________________________________
2025-04-11T04:23:18.8745136Z 
2025-04-11T04:23:18.8745226Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8745820Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8745827Z 
2025-04-11T04:23:18.8745925Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8746006Z         try_count = 0
2025-04-11T04:23:18.8746103Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8746184Z             max_try, int
2025-04-11T04:23:18.8746326Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8746446Z     
2025-04-11T04:23:18.8746561Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8746635Z             try:
2025-04-11T04:23:18.8746721Z                 try_count += 1
2025-04-11T04:23:18.8746810Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8746888Z                 return ret
2025-04-11T04:23:18.8746984Z             except exception_type as e:
2025-04-11T04:23:18.8747081Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8747323Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8747438Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8747586Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8747743Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8747822Z                     continue
2025-04-11T04:23:18.8747899Z                 else:
2025-04-11T04:23:18.8748123Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8748252Z >                   raise e
2025-04-11T04:23:18.8748256Z 
2025-04-11T04:23:18.8748349Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8748489Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8748619Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8748707Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8748886Z tests/test_shardformer/test_layer/test_layernorm.py:50: in test_layernorm
2025-04-11T04:23:18.8748973Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8749073Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8749171Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8749429Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8749602Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8749888Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8749979Z     while not context.join():
2025-04-11T04:23:18.8750085Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8750088Z 
2025-04-11T04:23:18.8750291Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3e9e0>
2025-04-11T04:23:18.8750370Z timeout = None
2025-04-11T04:23:18.8750374Z 
2025-04-11T04:23:18.8750464Z     def join(self, timeout=None):
2025-04-11T04:23:18.8750587Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8750655Z     
2025-04-11T04:23:18.8750856Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8751001Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8751166Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8751258Z         of the first process exiting.
2025-04-11T04:23:18.8751331Z     
2025-04-11T04:23:18.8751477Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8751613Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8751687Z     
2025-04-11T04:23:18.8751759Z         Args:
2025-04-11T04:23:18.8751897Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8751971Z         """
2025-04-11T04:23:18.8752106Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8752201Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8752279Z             return True
2025-04-11T04:23:18.8752352Z     
2025-04-11T04:23:18.8752481Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8752601Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8752743Z             self.sentinels.keys(),
2025-04-11T04:23:18.8752825Z             timeout=timeout,
2025-04-11T04:23:18.8752900Z         )
2025-04-11T04:23:18.8752970Z     
2025-04-11T04:23:18.8753054Z         error_index = None
2025-04-11T04:23:18.8753136Z         for sentinel in ready:
2025-04-11T04:23:18.8753238Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8753336Z             process = self.processes[index]
2025-04-11T04:23:18.8753419Z             process.join()
2025-04-11T04:23:18.8753566Z             if process.exitcode != 0:
2025-04-11T04:23:18.8753653Z                 error_index = index
2025-04-11T04:23:18.8753727Z                 break
2025-04-11T04:23:18.8753798Z     
2025-04-11T04:23:18.8753887Z         # Return if there was no error.
2025-04-11T04:23:18.8753975Z         if error_index is None:
2025-04-11T04:23:18.8754105Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8754200Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8754270Z     
2025-04-11T04:23:18.8754406Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8754557Z         for process in self.processes:
2025-04-11T04:23:18.8754644Z             if process.is_alive():
2025-04-11T04:23:18.8754738Z                 process.terminate()
2025-04-11T04:23:18.8754818Z             process.join()
2025-04-11T04:23:18.8754885Z     
2025-04-11T04:23:18.8755024Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8755140Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8755246Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8755364Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8755446Z             if exitcode < 0:
2025-04-11T04:23:18.8755551Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8755654Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8755804Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8755898Z                     error_index=error_index,
2025-04-11T04:23:18.8756001Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8756086Z                     exit_code=exitcode,
2025-04-11T04:23:18.8756169Z                     signal_name=name,
2025-04-11T04:23:18.8756244Z                 )
2025-04-11T04:23:18.8756314Z             else:
2025-04-11T04:23:18.8756417Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8756580Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8756672Z                     error_index=error_index,
2025-04-11T04:23:18.8756770Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8756906Z                     exit_code=exitcode,
2025-04-11T04:23:18.8756985Z                 )
2025-04-11T04:23:18.8757052Z     
2025-04-11T04:23:18.8757184Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8757353Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8757444Z         msg += original_trace
2025-04-11T04:23:18.8757613Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8757774Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8757848Z E       
2025-04-11T04:23:18.8757970Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8758071Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8758369Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8758446Z E           fn(i, *args)
2025-04-11T04:23:18.8758722Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
2025-04-11T04:23:18.8758805Z E           check_layernorm()
2025-04-11T04:23:18.8759066Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8759203Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8759489Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
2025-04-11T04:23:18.8759594Z E           norm = nn.LayerNorm(128, 0.00001).cuda()
2025-04-11T04:23:18.8759871Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8760036Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8760304Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8760402Z E           param_applied = fn(param)
2025-04-11T04:23:18.8760672Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8760786Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8760893Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8761225Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8761362Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8761523Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8761529Z 
2025-04-11T04:23:18.8761836Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8761987Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8762149Z [04/11/25 04:19:52] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8762276Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8762384Z                              :75 launch                                         
2025-04-11T04:23:18.8762525Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8762650Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8762847Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8762974Z _________________________________ test_linear __________________________________
2025-04-11T04:23:18.8762979Z 
2025-04-11T04:23:18.8763074Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8763734Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8763740Z 
2025-04-11T04:23:18.8763844Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8763924Z         try_count = 0
2025-04-11T04:23:18.8764027Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8764106Z             max_try, int
2025-04-11T04:23:18.8764254Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8764323Z     
2025-04-11T04:23:18.8764432Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8764512Z             try:
2025-04-11T04:23:18.8764594Z                 try_count += 1
2025-04-11T04:23:18.8764694Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8764773Z                 return ret
2025-04-11T04:23:18.8764864Z             except exception_type as e:
2025-04-11T04:23:18.8764968Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8765152Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8765269Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8765411Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8765624Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8765704Z                     continue
2025-04-11T04:23:18.8765778Z                 else:
2025-04-11T04:23:18.8766003Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8766081Z >                   raise e
2025-04-11T04:23:18.8766132Z 
2025-04-11T04:23:18.8766230Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8766341Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8766474Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8766560Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8766730Z tests/test_shardformer/test_layer/test_linear_1d.py:284: in test_linear
2025-04-11T04:23:18.8766828Z     spawn(check_dist_linear, nprocs=2)
2025-04-11T04:23:18.8766930Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8767032Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8767349Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8767530Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8767816Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8767904Z     while not context.join():
2025-04-11T04:23:18.8768015Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8768019Z 
2025-04-11T04:23:18.8768228Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8693070>
2025-04-11T04:23:18.8768312Z timeout = None
2025-04-11T04:23:18.8768316Z 
2025-04-11T04:23:18.8768407Z     def join(self, timeout=None):
2025-04-11T04:23:18.8768532Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8768602Z     
2025-04-11T04:23:18.8768747Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8768896Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8769057Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8769150Z         of the first process exiting.
2025-04-11T04:23:18.8769218Z     
2025-04-11T04:23:18.8769366Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8769502Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8769571Z     
2025-04-11T04:23:18.8769647Z         Args:
2025-04-11T04:23:18.8769833Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8769909Z         """
2025-04-11T04:23:18.8770047Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8770136Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8770220Z             return True
2025-04-11T04:23:18.8770287Z     
2025-04-11T04:23:18.8770422Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8770538Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8770631Z             self.sentinels.keys(),
2025-04-11T04:23:18.8770713Z             timeout=timeout,
2025-04-11T04:23:18.8770782Z         )
2025-04-11T04:23:18.8770857Z     
2025-04-11T04:23:18.8770939Z         error_index = None
2025-04-11T04:23:18.8771025Z         for sentinel in ready:
2025-04-11T04:23:18.8771126Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8771221Z             process = self.processes[index]
2025-04-11T04:23:18.8771306Z             process.join()
2025-04-11T04:23:18.8771397Z             if process.exitcode != 0:
2025-04-11T04:23:18.8771486Z                 error_index = index
2025-04-11T04:23:18.8771561Z                 break
2025-04-11T04:23:18.8771630Z     
2025-04-11T04:23:18.8771782Z         # Return if there was no error.
2025-04-11T04:23:18.8771865Z         if error_index is None:
2025-04-11T04:23:18.8772003Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8772096Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8772169Z     
2025-04-11T04:23:18.8772306Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8772399Z         for process in self.processes:
2025-04-11T04:23:18.8772541Z             if process.is_alive():
2025-04-11T04:23:18.8772632Z                 process.terminate()
2025-04-11T04:23:18.8772715Z             process.join()
2025-04-11T04:23:18.8772783Z     
2025-04-11T04:23:18.8772923Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8773042Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8773148Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8773268Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8773351Z             if exitcode < 0:
2025-04-11T04:23:18.8773506Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8773612Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8773759Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8773855Z                     error_index=error_index,
2025-04-11T04:23:18.8773954Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8774047Z                     exit_code=exitcode,
2025-04-11T04:23:18.8774133Z                     signal_name=name,
2025-04-11T04:23:18.8774205Z                 )
2025-04-11T04:23:18.8774279Z             else:
2025-04-11T04:23:18.8774382Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8774547Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8774638Z                     error_index=error_index,
2025-04-11T04:23:18.8774740Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8774826Z                     exit_code=exitcode,
2025-04-11T04:23:18.8774899Z                 )
2025-04-11T04:23:18.8774970Z     
2025-04-11T04:23:18.8775100Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8775275Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8775359Z         msg += original_trace
2025-04-11T04:23:18.8775534Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8775698Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8775770Z E       
2025-04-11T04:23:18.8775896Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8776044Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8776349Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8776430Z E           fn(i, *args)
2025-04-11T04:23:18.8776730Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
2025-04-11T04:23:18.8776819Z E           run_dist_linear_test()
2025-04-11T04:23:18.8777078Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8777168Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8777424Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8777510Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8777756Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8777840Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8778141Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
2025-04-11T04:23:18.8778335Z E           check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
2025-04-11T04:23:18.8778630Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T04:23:18.8778731Z E           linear = nn.Linear(32, 128).cuda()
2025-04-11T04:23:18.8779002Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8779169Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8779443Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8779536Z E           param_applied = fn(param)
2025-04-11T04:23:18.8779813Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8779929Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8780037Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8780366Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8780502Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8780665Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8780670Z 
2025-04-11T04:23:18.8780980Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8781133Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8781288Z [04/11/25 04:19:56] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8781415Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8781525Z                              :75 launch                                         
2025-04-11T04:23:18.8781664Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8781790Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8781987Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8782122Z _______________________________ test_linearconv ________________________________
2025-04-11T04:23:18.8782127Z 
2025-04-11T04:23:18.8782217Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8782866Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8782872Z 
2025-04-11T04:23:18.8782973Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8783051Z         try_count = 0
2025-04-11T04:23:18.8783156Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8783236Z             max_try, int
2025-04-11T04:23:18.8783386Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8783454Z     
2025-04-11T04:23:18.8783567Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8783640Z             try:
2025-04-11T04:23:18.8783721Z                 try_count += 1
2025-04-11T04:23:18.8783814Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8783894Z                 return ret
2025-04-11T04:23:18.8783989Z             except exception_type as e:
2025-04-11T04:23:18.8784085Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8784272Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8784389Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8784533Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8784740Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8784822Z                     continue
2025-04-11T04:23:18.8784901Z                 else:
2025-04-11T04:23:18.8785119Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8785197Z >                   raise e
2025-04-11T04:23:18.8785200Z 
2025-04-11T04:23:18.8785347Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8785456Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8785590Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8785674Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8785883Z tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py:166: in test_linearconv
2025-04-11T04:23:18.8785971Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8786070Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8786174Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8786475Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8786657Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8786939Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8787026Z     while not context.join():
2025-04-11T04:23:18.8787136Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8787140Z 
2025-04-11T04:23:18.8787338Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3cfd0>
2025-04-11T04:23:18.8787418Z timeout = None
2025-04-11T04:23:18.8787424Z 
2025-04-11T04:23:18.8787511Z     def join(self, timeout=None):
2025-04-11T04:23:18.8787638Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8787706Z     
2025-04-11T04:23:18.8787852Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8787995Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8788156Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8788251Z         of the first process exiting.
2025-04-11T04:23:18.8788319Z     
2025-04-11T04:23:18.8788501Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8788640Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8788712Z     
2025-04-11T04:23:18.8788784Z         Args:
2025-04-11T04:23:18.8788919Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8789070Z         """
2025-04-11T04:23:18.8789211Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8789304Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8789381Z             return True
2025-04-11T04:23:18.8789450Z     
2025-04-11T04:23:18.8789583Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8789702Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8789792Z             self.sentinels.keys(),
2025-04-11T04:23:18.8789876Z             timeout=timeout,
2025-04-11T04:23:18.8789946Z         )
2025-04-11T04:23:18.8790017Z     
2025-04-11T04:23:18.8790097Z         error_index = None
2025-04-11T04:23:18.8790186Z         for sentinel in ready:
2025-04-11T04:23:18.8790287Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8790388Z             process = self.processes[index]
2025-04-11T04:23:18.8790470Z             process.join()
2025-04-11T04:23:18.8790562Z             if process.exitcode != 0:
2025-04-11T04:23:18.8790653Z                 error_index = index
2025-04-11T04:23:18.8790727Z                 break
2025-04-11T04:23:18.8790797Z     
2025-04-11T04:23:18.8790887Z         # Return if there was no error.
2025-04-11T04:23:18.8791027Z         if error_index is None:
2025-04-11T04:23:18.8791161Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8791257Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8791327Z     
2025-04-11T04:23:18.8791463Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8791560Z         for process in self.processes:
2025-04-11T04:23:18.8791645Z             if process.is_alive():
2025-04-11T04:23:18.8791801Z                 process.terminate()
2025-04-11T04:23:18.8791888Z             process.join()
2025-04-11T04:23:18.8791955Z     
2025-04-11T04:23:18.8792093Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8792207Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8792312Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8792435Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8792518Z             if exitcode < 0:
2025-04-11T04:23:18.8792624Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8792778Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8792933Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8793026Z                     error_index=error_index,
2025-04-11T04:23:18.8793124Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8793214Z                     exit_code=exitcode,
2025-04-11T04:23:18.8793299Z                     signal_name=name,
2025-04-11T04:23:18.8793376Z                 )
2025-04-11T04:23:18.8793447Z             else:
2025-04-11T04:23:18.8793547Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8793717Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8793805Z                     error_index=error_index,
2025-04-11T04:23:18.8793906Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8793992Z                     exit_code=exitcode,
2025-04-11T04:23:18.8794065Z                 )
2025-04-11T04:23:18.8794134Z     
2025-04-11T04:23:18.8794266Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8794441Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8794524Z         msg += original_trace
2025-04-11T04:23:18.8794698Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8794862Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8794932Z E       
2025-04-11T04:23:18.8795062Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8795158Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8795505Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8795587Z E           fn(i, *args)
2025-04-11T04:23:18.8795889Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
2025-04-11T04:23:18.8795976Z E           check_linear_1d_col()
2025-04-11T04:23:18.8796230Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8796322Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8796640Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T04:23:18.8796741Z E           linear = nn.Linear(8, 80).cuda()
2025-04-11T04:23:18.8797009Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8797131Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8797396Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8797543Z E           param_applied = fn(param)
2025-04-11T04:23:18.8797819Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8797932Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8798040Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8798321Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8798505Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8798665Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8798669Z 
2025-04-11T04:23:18.8798976Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8799126Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8799288Z [04/11/25 04:20:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8799459Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8799567Z                              :75 launch                                         
2025-04-11T04:23:18.8799709Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8799834Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8800037Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8800167Z ________________________________ test_ring_attn ________________________________
2025-04-11T04:23:18.8800171Z 
2025-04-11T04:23:18.8800267Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8800874Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8800882Z 
2025-04-11T04:23:18.8800983Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8801061Z         try_count = 0
2025-04-11T04:23:18.8801159Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8801241Z             max_try, int
2025-04-11T04:23:18.8801385Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8801458Z     
2025-04-11T04:23:18.8801567Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8801640Z             try:
2025-04-11T04:23:18.8801726Z                 try_count += 1
2025-04-11T04:23:18.8801817Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8801943Z                 return ret
2025-04-11T04:23:18.8802036Z             except exception_type as e:
2025-04-11T04:23:18.8802138Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8802326Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8802444Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8802593Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8802746Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8802829Z                     continue
2025-04-11T04:23:18.8802906Z                 else:
2025-04-11T04:23:18.8803130Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8803206Z >                   raise e
2025-04-11T04:23:18.8803210Z 
2025-04-11T04:23:18.8803303Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8803416Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8803545Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8803683Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8803838Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T04:23:18.8803925Z     partial_func(**kwargs)
2025-04-11T04:23:18.8804102Z tests/test_shardformer/test_layer/test_ring_attn.py:181: in test_ring_attn
2025-04-11T04:23:18.8804214Z     spawn(launch_single_ring, nprocs=world_size)
2025-04-11T04:23:18.8804315Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8804466Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8804726Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8804902Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8805188Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8805274Z     while not context.join():
2025-04-11T04:23:18.8805383Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8805389Z 
2025-04-11T04:23:18.8805638Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8692200>
2025-04-11T04:23:18.8805717Z timeout = None
2025-04-11T04:23:18.8805721Z 
2025-04-11T04:23:18.8805811Z     def join(self, timeout=None):
2025-04-11T04:23:18.8805933Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8806004Z     
2025-04-11T04:23:18.8806153Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8806294Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8806458Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8806548Z         of the first process exiting.
2025-04-11T04:23:18.8806620Z     
2025-04-11T04:23:18.8806763Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8806902Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8806972Z     
2025-04-11T04:23:18.8807044Z         Args:
2025-04-11T04:23:18.8807189Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8807260Z         """
2025-04-11T04:23:18.8807399Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8807488Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8807566Z             return True
2025-04-11T04:23:18.8807639Z     
2025-04-11T04:23:18.8807769Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8807887Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8807976Z             self.sentinels.keys(),
2025-04-11T04:23:18.8808059Z             timeout=timeout,
2025-04-11T04:23:18.8808184Z         )
2025-04-11T04:23:18.8808254Z     
2025-04-11T04:23:18.8808338Z         error_index = None
2025-04-11T04:23:18.8808423Z         for sentinel in ready:
2025-04-11T04:23:18.8808534Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8808633Z             process = self.processes[index]
2025-04-11T04:23:18.8808719Z             process.join()
2025-04-11T04:23:18.8808814Z             if process.exitcode != 0:
2025-04-11T04:23:18.8808900Z                 error_index = index
2025-04-11T04:23:18.8808979Z                 break
2025-04-11T04:23:18.8809046Z     
2025-04-11T04:23:18.8809136Z         # Return if there was no error.
2025-04-11T04:23:18.8809225Z         if error_index is None:
2025-04-11T04:23:18.8809353Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8809451Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8809518Z     
2025-04-11T04:23:18.8809658Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8809756Z         for process in self.processes:
2025-04-11T04:23:18.8809842Z             if process.is_alive():
2025-04-11T04:23:18.8809935Z                 process.terminate()
2025-04-11T04:23:18.8810067Z             process.join()
2025-04-11T04:23:18.8810138Z     
2025-04-11T04:23:18.8810282Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8810396Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8810505Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8810628Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8810712Z             if exitcode < 0:
2025-04-11T04:23:18.8810866Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8810969Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8811119Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8811214Z                     error_index=error_index,
2025-04-11T04:23:18.8811318Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8811403Z                     exit_code=exitcode,
2025-04-11T04:23:18.8811491Z                     signal_name=name,
2025-04-11T04:23:18.8811562Z                 )
2025-04-11T04:23:18.8811633Z             else:
2025-04-11T04:23:18.8811874Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8812043Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8812138Z                     error_index=error_index,
2025-04-11T04:23:18.8812237Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8812326Z                     exit_code=exitcode,
2025-04-11T04:23:18.8812397Z                 )
2025-04-11T04:23:18.8812464Z     
2025-04-11T04:23:18.8812595Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8812765Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8812849Z         msg += original_trace
2025-04-11T04:23:18.8813019Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8813181Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8813256Z E       
2025-04-11T04:23:18.8813379Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8813481Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8813776Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8813857Z E           fn(i, *args)
2025-04-11T04:23:18.8814158Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
2025-04-11T04:23:18.8814241Z E           check_packed_seq()
2025-04-11T04:23:18.8814552Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8814641Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8814892Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8814979Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8815228Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8815311Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8815412Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.8815698Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
2025-04-11T04:23:18.8815874Z E           padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
2025-04-11T04:23:18.8815982Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8816265Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8816403Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8816558Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8816623Z 
2025-04-11T04:23:18.8816936Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8817084Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8817235Z [04/11/25 04:20:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8817365Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8817521Z                              :75 launch                                         
2025-04-11T04:23:18.8817661Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8817790Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8817993Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8818128Z _______________________________ test_double_ring _______________________________
2025-04-11T04:23:18.8818133Z 
2025-04-11T04:23:18.8818233Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8818885Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8818890Z 
2025-04-11T04:23:18.8818992Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8819073Z         try_count = 0
2025-04-11T04:23:18.8819169Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8819251Z             max_try, int
2025-04-11T04:23:18.8819397Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8819470Z     
2025-04-11T04:23:18.8819579Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8819652Z             try:
2025-04-11T04:23:18.8819737Z                 try_count += 1
2025-04-11T04:23:18.8819828Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8819907Z                 return ret
2025-04-11T04:23:18.8820000Z             except exception_type as e:
2025-04-11T04:23:18.8820098Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8820287Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8820402Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8820552Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8820706Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8820789Z                     continue
2025-04-11T04:23:18.8820914Z                 else:
2025-04-11T04:23:18.8821136Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8821221Z >                   raise e
2025-04-11T04:23:18.8821225Z 
2025-04-11T04:23:18.8821315Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8821430Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8821559Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8821647Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8821797Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T04:23:18.8821883Z     partial_func(**kwargs)
2025-04-11T04:23:18.8822071Z tests/test_shardformer/test_layer/test_ring_attn.py:187: in test_double_ring
2025-04-11T04:23:18.8822181Z     spawn(launch_double_ring, nprocs=world_size)
2025-04-11T04:23:18.8822281Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8822379Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8822636Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8822812Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8823151Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8823240Z     while not context.join():
2025-04-11T04:23:18.8823348Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8823352Z 
2025-04-11T04:23:18.8823554Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a88c40>
2025-04-11T04:23:18.8823682Z timeout = None
2025-04-11T04:23:18.8823686Z 
2025-04-11T04:23:18.8823779Z     def join(self, timeout=None):
2025-04-11T04:23:18.8823901Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8823969Z     
2025-04-11T04:23:18.8824119Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8824259Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8824422Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8824514Z         of the first process exiting.
2025-04-11T04:23:18.8824583Z     
2025-04-11T04:23:18.8824773Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8824910Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8824982Z     
2025-04-11T04:23:18.8825053Z         Args:
2025-04-11T04:23:18.8825194Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8825267Z         """
2025-04-11T04:23:18.8825403Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8825497Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8825573Z             return True
2025-04-11T04:23:18.8825645Z     
2025-04-11T04:23:18.8825775Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8825895Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8825983Z             self.sentinels.keys(),
2025-04-11T04:23:18.8826066Z             timeout=timeout,
2025-04-11T04:23:18.8826140Z         )
2025-04-11T04:23:18.8826209Z     
2025-04-11T04:23:18.8826292Z         error_index = None
2025-04-11T04:23:18.8826375Z         for sentinel in ready:
2025-04-11T04:23:18.8826478Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8826578Z             process = self.processes[index]
2025-04-11T04:23:18.8826660Z             process.join()
2025-04-11T04:23:18.8826754Z             if process.exitcode != 0:
2025-04-11T04:23:18.8826840Z                 error_index = index
2025-04-11T04:23:18.8826914Z                 break
2025-04-11T04:23:18.8826982Z     
2025-04-11T04:23:18.8827070Z         # Return if there was no error.
2025-04-11T04:23:18.8827207Z         if error_index is None:
2025-04-11T04:23:18.8827340Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8827437Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8827507Z     
2025-04-11T04:23:18.8827643Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8827743Z         for process in self.processes:
2025-04-11T04:23:18.8827829Z             if process.is_alive():
2025-04-11T04:23:18.8827921Z                 process.terminate()
2025-04-11T04:23:18.8828001Z             process.join()
2025-04-11T04:23:18.8828068Z     
2025-04-11T04:23:18.8828207Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8828323Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8828462Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8828582Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8828668Z             if exitcode < 0:
2025-04-11T04:23:18.8828772Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8828875Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8829026Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8829181Z                     error_index=error_index,
2025-04-11T04:23:18.8829286Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8829375Z                     exit_code=exitcode,
2025-04-11T04:23:18.8829462Z                     signal_name=name,
2025-04-11T04:23:18.8829533Z                 )
2025-04-11T04:23:18.8829603Z             else:
2025-04-11T04:23:18.8829706Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8829924Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8830017Z                     error_index=error_index,
2025-04-11T04:23:18.8830114Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8830203Z                     exit_code=exitcode,
2025-04-11T04:23:18.8830277Z                 )
2025-04-11T04:23:18.8830345Z     
2025-04-11T04:23:18.8830477Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8830648Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8830783Z         msg += original_trace
2025-04-11T04:23:18.8830958Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8831121Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8831198Z E       
2025-04-11T04:23:18.8831321Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8831424Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8831723Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8831801Z E           fn(i, *args)
2025-04-11T04:23:18.8832103Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
2025-04-11T04:23:18.8832204Z E           check_ring_attn(inner_ring_size=2)
2025-04-11T04:23:18.8832465Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8832553Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8832808Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8832894Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8833146Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8833231Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8833335Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.8833669Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
2025-04-11T04:23:18.8833876Z E           qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
2025-04-11T04:23:18.8833984Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8834270Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8834406Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8834567Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8834571Z 
2025-04-11T04:23:18.8834874Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8835029Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8835182Z [04/11/25 04:20:10] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8835317Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8835423Z                              :75 launch                                         
2025-04-11T04:23:18.8835563Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8835736Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8835935Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8836073Z __________________________ test_all_to_all_attention ___________________________
2025-04-11T04:23:18.8836077Z 
2025-04-11T04:23:18.8836168Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8836800Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8836805Z 
2025-04-11T04:23:18.8836907Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8836988Z         try_count = 0
2025-04-11T04:23:18.8837087Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8837172Z             max_try, int
2025-04-11T04:23:18.8837316Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8837437Z     
2025-04-11T04:23:18.8837550Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8837623Z             try:
2025-04-11T04:23:18.8837707Z                 try_count += 1
2025-04-11T04:23:18.8837796Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8837877Z                 return ret
2025-04-11T04:23:18.8837971Z             except exception_type as e:
2025-04-11T04:23:18.8838069Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8839092Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8839209Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8839355Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8839508Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8839592Z                     continue
2025-04-11T04:23:18.8839669Z                 else:
2025-04-11T04:23:18.8839892Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8839975Z >                   raise e
2025-04-11T04:23:18.8839979Z 
2025-04-11T04:23:18.8840070Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8840185Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8840314Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8840402Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8840699Z tests/test_shardformer/test_layer/test_sequence_parallel.py:174: in test_all_to_all_attention
2025-04-11T04:23:18.8840799Z     spawn(check_all2all_attn, nprocs=4)
2025-04-11T04:23:18.8840900Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8840996Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8841259Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8841437Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8841723Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8841810Z     while not context.join():
2025-04-11T04:23:18.8841916Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8841926Z 
2025-04-11T04:23:18.8842127Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3c4c0>
2025-04-11T04:23:18.8842203Z timeout = None
2025-04-11T04:23:18.8842207Z 
2025-04-11T04:23:18.8842299Z     def join(self, timeout=None):
2025-04-11T04:23:18.8842423Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8842493Z     
2025-04-11T04:23:18.8842638Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8842830Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8842997Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8843087Z         of the first process exiting.
2025-04-11T04:23:18.8843160Z     
2025-04-11T04:23:18.8843306Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8843446Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8843565Z     
2025-04-11T04:23:18.8843637Z         Args:
2025-04-11T04:23:18.8843776Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8843848Z         """
2025-04-11T04:23:18.8843988Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8844077Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8844155Z             return True
2025-04-11T04:23:18.8844227Z     
2025-04-11T04:23:18.8844357Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8844522Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8844613Z             self.sentinels.keys(),
2025-04-11T04:23:18.8844700Z             timeout=timeout,
2025-04-11T04:23:18.8844770Z         )
2025-04-11T04:23:18.8844836Z     
2025-04-11T04:23:18.8844920Z         error_index = None
2025-04-11T04:23:18.8845003Z         for sentinel in ready:
2025-04-11T04:23:18.8845112Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8845207Z             process = self.processes[index]
2025-04-11T04:23:18.8845288Z             process.join()
2025-04-11T04:23:18.8845381Z             if process.exitcode != 0:
2025-04-11T04:23:18.8845467Z                 error_index = index
2025-04-11T04:23:18.8845545Z                 break
2025-04-11T04:23:18.8845611Z     
2025-04-11T04:23:18.8845699Z         # Return if there was no error.
2025-04-11T04:23:18.8845785Z         if error_index is None:
2025-04-11T04:23:18.8845917Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8846014Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8846081Z     
2025-04-11T04:23:18.8846220Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8846313Z         for process in self.processes:
2025-04-11T04:23:18.8846399Z             if process.is_alive():
2025-04-11T04:23:18.8846490Z                 process.terminate()
2025-04-11T04:23:18.8846572Z             process.join()
2025-04-11T04:23:18.8846643Z     
2025-04-11T04:23:18.8846779Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8846888Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8847046Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8847167Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8847249Z             if exitcode < 0:
2025-04-11T04:23:18.8847355Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8847459Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8847608Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8847700Z                     error_index=error_index,
2025-04-11T04:23:18.8847801Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8847888Z                     exit_code=exitcode,
2025-04-11T04:23:18.8847977Z                     signal_name=name,
2025-04-11T04:23:18.8848048Z                 )
2025-04-11T04:23:18.8848120Z             else:
2025-04-11T04:23:18.8848221Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8848384Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8848477Z                     error_index=error_index,
2025-04-11T04:23:18.8848574Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8848661Z                     exit_code=exitcode,
2025-04-11T04:23:18.8848783Z                 )
2025-04-11T04:23:18.8848850Z     
2025-04-11T04:23:18.8848986Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8849155Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8849241Z         msg += original_trace
2025-04-11T04:23:18.8849408Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8849622Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8849694Z E       
2025-04-11T04:23:18.8849814Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8849913Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8850212Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8850295Z E           fn(i, *args)
2025-04-11T04:23:18.8850619Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
2025-04-11T04:23:18.8850776Z E           run_seq_parallel_attn()
2025-04-11T04:23:18.8851037Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8851123Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8851375Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8851463Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8851713Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8851796Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8851901Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.8852231Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
2025-04-11T04:23:18.8852378Z E           seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
2025-04-11T04:23:18.8852694Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
2025-04-11T04:23:18.8852826Z E           x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
2025-04-11T04:23:18.8852936Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8853219Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8853357Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8853515Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8853568Z 
2025-04-11T04:23:18.8853881Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8854030Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8854187Z [04/11/25 04:20:15] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8854318Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8854421Z                              :75 launch                                         
2025-04-11T04:23:18.8854561Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8854686Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8854884Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8855018Z _____________________________ test_vocab_embedding _____________________________
2025-04-11T04:23:18.8855022Z 
2025-04-11T04:23:18.8855113Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8855718Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8855773Z 
2025-04-11T04:23:18.8855875Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8855961Z         try_count = 0
2025-04-11T04:23:18.8856060Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8856145Z             max_try, int
2025-04-11T04:23:18.8856326Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8856399Z     
2025-04-11T04:23:18.8856510Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8856583Z             try:
2025-04-11T04:23:18.8856669Z                 try_count += 1
2025-04-11T04:23:18.8856760Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8856842Z                 return ret
2025-04-11T04:23:18.8856933Z             except exception_type as e:
2025-04-11T04:23:18.8857035Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8857272Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8857389Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8857543Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8857698Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8857782Z                     continue
2025-04-11T04:23:18.8857857Z                 else:
2025-04-11T04:23:18.8858077Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8858162Z >                   raise e
2025-04-11T04:23:18.8858168Z 
2025-04-11T04:23:18.8858260Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8858370Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8858505Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8858595Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8858835Z tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py:54: in test_vocab_embedding
2025-04-11T04:23:18.8858923Z     spawn(run_dist, nprocs=2)
2025-04-11T04:23:18.8859025Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8859122Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8859382Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8859557Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8859894Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8859983Z     while not context.join():
2025-04-11T04:23:18.8860091Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8860097Z 
2025-04-11T04:23:18.8860298Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581d79540>
2025-04-11T04:23:18.8860375Z timeout = None
2025-04-11T04:23:18.8860382Z 
2025-04-11T04:23:18.8860475Z     def join(self, timeout=None):
2025-04-11T04:23:18.8860598Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8860669Z     
2025-04-11T04:23:18.8860811Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8860954Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8861119Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8861207Z         of the first process exiting.
2025-04-11T04:23:18.8861277Z     
2025-04-11T04:23:18.8861419Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8861557Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8861624Z     
2025-04-11T04:23:18.8861753Z         Args:
2025-04-11T04:23:18.8861895Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8861970Z         """
2025-04-11T04:23:18.8862114Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8862203Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8862279Z             return True
2025-04-11T04:23:18.8862351Z     
2025-04-11T04:23:18.8862480Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8862650Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8862741Z             self.sentinels.keys(),
2025-04-11T04:23:18.8862827Z             timeout=timeout,
2025-04-11T04:23:18.8862898Z         )
2025-04-11T04:23:18.8862966Z     
2025-04-11T04:23:18.8863053Z         error_index = None
2025-04-11T04:23:18.8863137Z         for sentinel in ready:
2025-04-11T04:23:18.8863244Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8863343Z             process = self.processes[index]
2025-04-11T04:23:18.8863428Z             process.join()
2025-04-11T04:23:18.8863586Z             if process.exitcode != 0:
2025-04-11T04:23:18.8863676Z                 error_index = index
2025-04-11T04:23:18.8863754Z                 break
2025-04-11T04:23:18.8863823Z     
2025-04-11T04:23:18.8863914Z         # Return if there was no error.
2025-04-11T04:23:18.8864002Z         if error_index is None:
2025-04-11T04:23:18.8864135Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8864233Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8864301Z     
2025-04-11T04:23:18.8864443Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8864539Z         for process in self.processes:
2025-04-11T04:23:18.8864629Z             if process.is_alive():
2025-04-11T04:23:18.8864720Z                 process.terminate()
2025-04-11T04:23:18.8864803Z             process.join()
2025-04-11T04:23:18.8864876Z     
2025-04-11T04:23:18.8865015Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8865129Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8865238Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8865359Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8865443Z             if exitcode < 0:
2025-04-11T04:23:18.8865550Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8865659Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8865808Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8865899Z                     error_index=error_index,
2025-04-11T04:23:18.8866058Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8866148Z                     exit_code=exitcode,
2025-04-11T04:23:18.8866237Z                     signal_name=name,
2025-04-11T04:23:18.8866309Z                 )
2025-04-11T04:23:18.8866382Z             else:
2025-04-11T04:23:18.8866490Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8866656Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8866752Z                     error_index=error_index,
2025-04-11T04:23:18.8866850Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8866938Z                     exit_code=exitcode,
2025-04-11T04:23:18.8867009Z                 )
2025-04-11T04:23:18.8867079Z     
2025-04-11T04:23:18.8867213Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8867382Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8867470Z         msg += original_trace
2025-04-11T04:23:18.8867645Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8867802Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8867927Z E       
2025-04-11T04:23:18.8868052Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8868156Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8868479Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8868563Z E           fn(i, *args)
2025-04-11T04:23:18.8868887Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
2025-04-11T04:23:18.8869038Z E           check_vocab_embedding_1d()
2025-04-11T04:23:18.8869305Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8869392Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8869776Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
2025-04-11T04:23:18.8869892Z E           embedding = nn.Embedding(128, 32).to("cuda")
2025-04-11T04:23:18.8870225Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.8870320Z E           return self._apply(convert)
2025-04-11T04:23:18.8870593Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8870685Z E           param_applied = fn(param)
2025-04-11T04:23:18.8870962Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.8871182Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.8871289Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8871592Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8871730Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8871893Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8871897Z 
2025-04-11T04:23:18.8872198Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8872353Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8872506Z [04/11/25 04:20:20] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8872634Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8872743Z                              :75 launch                                         
2025-04-11T04:23:18.8872933Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8873063Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.8873264Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8873395Z __________________________________ test_bert ___________________________________
2025-04-11T04:23:18.8873399Z 
2025-04-11T04:23:18.8873490Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8873493Z 
2025-04-11T04:23:18.8873596Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8873673Z         try_count = 0
2025-04-11T04:23:18.8873769Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8873854Z             max_try, int
2025-04-11T04:23:18.8873998Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8874068Z     
2025-04-11T04:23:18.8874176Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8874251Z             try:
2025-04-11T04:23:18.8874337Z                 try_count += 1
2025-04-11T04:23:18.8874428Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8874432Z 
2025-04-11T04:23:18.8874585Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8874695Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8874811Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8874903Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8875055Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8875153Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8875312Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8875316Z 
2025-04-11T04:23:18.8875395Z device = None
2025-04-11T04:23:18.8875399Z 
2025-04-11T04:23:18.8875520Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8875675Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8875744Z     
2025-04-11T04:23:18.8875815Z         Args:
2025-04-11T04:23:18.8875991Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8876160Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8876318Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8876393Z         """
2025-04-11T04:23:18.8876472Z         _lazy_init()
2025-04-11T04:23:18.8876568Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8876670Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8876782Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8877073Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8877215Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8877377Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8877381Z 
2025-04-11T04:23:18.8877624Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8877758Z __________________________________ test_blip2 __________________________________
2025-04-11T04:23:18.8877762Z 
2025-04-11T04:23:18.8877850Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8877854Z 
2025-04-11T04:23:18.8877955Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8878032Z         try_count = 0
2025-04-11T04:23:18.8878131Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8878212Z             max_try, int
2025-04-11T04:23:18.8878359Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8878429Z     
2025-04-11T04:23:18.8878535Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8878612Z             try:
2025-04-11T04:23:18.8878743Z                 try_count += 1
2025-04-11T04:23:18.8878839Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8878843Z 
2025-04-11T04:23:18.8878934Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8879048Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8879162Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8879255Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8879413Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8879506Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8879612Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8879618Z 
2025-04-11T04:23:18.8879692Z device = None
2025-04-11T04:23:18.8879696Z 
2025-04-11T04:23:18.8879819Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8879971Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8880038Z     
2025-04-11T04:23:18.8880115Z         Args:
2025-04-11T04:23:18.8880282Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8880506Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8880613Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8880684Z         """
2025-04-11T04:23:18.8880764Z         _lazy_init()
2025-04-11T04:23:18.8880855Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8880957Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8881061Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8881399Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8881536Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8881693Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8881700Z 
2025-04-11T04:23:18.8881939Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8882067Z __________________________________ test_bloom __________________________________
2025-04-11T04:23:18.8882071Z 
2025-04-11T04:23:18.8882212Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8882217Z 
2025-04-11T04:23:18.8882318Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8882398Z         try_count = 0
2025-04-11T04:23:18.8882495Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8882575Z             max_try, int
2025-04-11T04:23:18.8882720Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8882787Z     
2025-04-11T04:23:18.8882898Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8882970Z             try:
2025-04-11T04:23:18.8883057Z                 try_count += 1
2025-04-11T04:23:18.8883145Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8883149Z 
2025-04-11T04:23:18.8883240Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8883349Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8883464Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8883564Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8883716Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8883812Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8883919Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8883925Z 
2025-04-11T04:23:18.8883997Z device = None
2025-04-11T04:23:18.8884005Z 
2025-04-11T04:23:18.8884120Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8884268Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8884340Z     
2025-04-11T04:23:18.8884460Z         Args:
2025-04-11T04:23:18.8884631Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8884796Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8884903Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8884978Z         """
2025-04-11T04:23:18.8885056Z         _lazy_init()
2025-04-11T04:23:18.8885151Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8885250Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8885357Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8885642Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8885775Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8885936Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8885940Z 
2025-04-11T04:23:18.8886175Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8886308Z _________________________________ test_chatglm _________________________________
2025-04-11T04:23:18.8886362Z 
2025-04-11T04:23:18.8886455Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8886459Z 
2025-04-11T04:23:18.8886558Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8886633Z         try_count = 0
2025-04-11T04:23:18.8886731Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8886808Z             max_try, int
2025-04-11T04:23:18.8886949Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8887089Z     
2025-04-11T04:23:18.8887196Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8887270Z             try:
2025-04-11T04:23:18.8887351Z                 try_count += 1
2025-04-11T04:23:18.8887441Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8887445Z 
2025-04-11T04:23:18.8887540Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8887646Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8887764Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8887901Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8888060Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8888151Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8888254Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8888258Z 
2025-04-11T04:23:18.8888338Z device = None
2025-04-11T04:23:18.8888341Z 
2025-04-11T04:23:18.8888456Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8888606Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8888674Z     
2025-04-11T04:23:18.8888748Z         Args:
2025-04-11T04:23:18.8888914Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8889078Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8889190Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8889260Z         """
2025-04-11T04:23:18.8889340Z         _lazy_init()
2025-04-11T04:23:18.8889430Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8889532Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8889635Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8889912Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8890054Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8890208Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8890212Z 
2025-04-11T04:23:18.8890501Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8890634Z _________________________________ test_command _________________________________
2025-04-11T04:23:18.8890640Z 
2025-04-11T04:23:18.8890734Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8890738Z 
2025-04-11T04:23:18.8890836Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8890918Z         try_count = 0
2025-04-11T04:23:18.8891015Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8891093Z             max_try, int
2025-04-11T04:23:18.8891237Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8891308Z     
2025-04-11T04:23:18.8891417Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8891488Z             try:
2025-04-11T04:23:18.8891567Z                 try_count += 1
2025-04-11T04:23:18.8891659Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8891665Z 
2025-04-11T04:23:18.8891756Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.8891862Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8892025Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.8892119Z     get_accelerator().synchronize()
2025-04-11T04:23:18.8892273Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.8892362Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.8892468Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8892472Z 
2025-04-11T04:23:18.8892544Z device = None
2025-04-11T04:23:18.8892598Z 
2025-04-11T04:23:18.8892717Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.8892864Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8892935Z     
2025-04-11T04:23:18.8893006Z         Args:
2025-04-11T04:23:18.8893171Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.8893339Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.8893444Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.8893517Z         """
2025-04-11T04:23:18.8893592Z         _lazy_init()
2025-04-11T04:23:18.8893731Z         with torch.cuda.device(device):
2025-04-11T04:23:18.8893835Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.8893939Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8894225Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8894361Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8894518Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8894522Z 
2025-04-11T04:23:18.8894754Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.8894888Z _______________________________ test_deepseek[4] _______________________________
2025-04-11T04:23:18.8894894Z 
2025-04-11T04:23:18.8895009Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.8895615Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8895620Z 
2025-04-11T04:23:18.8895717Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8895795Z         try_count = 0
2025-04-11T04:23:18.8895894Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8895973Z             max_try, int
2025-04-11T04:23:18.8896117Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8896184Z     
2025-04-11T04:23:18.8896344Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8896418Z             try:
2025-04-11T04:23:18.8896498Z                 try_count += 1
2025-04-11T04:23:18.8896589Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8896669Z                 return ret
2025-04-11T04:23:18.8896761Z             except exception_type as e:
2025-04-11T04:23:18.8896860Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8897045Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8897165Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8897309Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8897468Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8897548Z                     continue
2025-04-11T04:23:18.8897624Z                 else:
2025-04-11T04:23:18.8897848Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8897926Z >                   raise e
2025-04-11T04:23:18.8897930Z 
2025-04-11T04:23:18.8898077Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8898186Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8898318Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8898402Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8898595Z tests/test_shardformer/test_model/test_shard_deepseek.py:228: in test_deepseek
2025-04-11T04:23:18.8898688Z     spawn(check_deepseek, world_size)
2025-04-11T04:23:18.8898838Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8898939Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8899197Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8899378Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8899663Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8899757Z     while not context.join():
2025-04-11T04:23:18.8899865Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8899918Z 
2025-04-11T04:23:18.8900120Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581bc8a30>
2025-04-11T04:23:18.8900200Z timeout = None
2025-04-11T04:23:18.8900204Z 
2025-04-11T04:23:18.8900291Z     def join(self, timeout=None):
2025-04-11T04:23:18.8900416Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8900486Z     
2025-04-11T04:23:18.8900632Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8900776Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8900940Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8901033Z         of the first process exiting.
2025-04-11T04:23:18.8901101Z     
2025-04-11T04:23:18.8901249Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8901387Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8901459Z     
2025-04-11T04:23:18.8901530Z         Args:
2025-04-11T04:23:18.8901668Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8901742Z         """
2025-04-11T04:23:18.8901877Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8901970Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8902047Z             return True
2025-04-11T04:23:18.8902114Z     
2025-04-11T04:23:18.8902254Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8902372Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8902516Z             self.sentinels.keys(),
2025-04-11T04:23:18.8902601Z             timeout=timeout,
2025-04-11T04:23:18.8902672Z         )
2025-04-11T04:23:18.8902743Z     
2025-04-11T04:23:18.8902823Z         error_index = None
2025-04-11T04:23:18.8902914Z         for sentinel in ready:
2025-04-11T04:23:18.8903017Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8903119Z             process = self.processes[index]
2025-04-11T04:23:18.8903200Z             process.join()
2025-04-11T04:23:18.8903291Z             if process.exitcode != 0:
2025-04-11T04:23:18.8903380Z                 error_index = index
2025-04-11T04:23:18.8903455Z                 break
2025-04-11T04:23:18.8903525Z     
2025-04-11T04:23:18.8903620Z         # Return if there was no error.
2025-04-11T04:23:18.8903704Z         if error_index is None:
2025-04-11T04:23:18.8903842Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8903935Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8904009Z     
2025-04-11T04:23:18.8904149Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8904244Z         for process in self.processes:
2025-04-11T04:23:18.8904330Z             if process.is_alive():
2025-04-11T04:23:18.8904507Z                 process.terminate()
2025-04-11T04:23:18.8904592Z             process.join()
2025-04-11T04:23:18.8904663Z     
2025-04-11T04:23:18.8904804Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8904917Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8905023Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8905145Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8905278Z             if exitcode < 0:
2025-04-11T04:23:18.8905386Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8905489Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8905642Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8905736Z                     error_index=error_index,
2025-04-11T04:23:18.8905835Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8905927Z                     exit_code=exitcode,
2025-04-11T04:23:18.8906011Z                     signal_name=name,
2025-04-11T04:23:18.8906134Z                 )
2025-04-11T04:23:18.8906208Z             else:
2025-04-11T04:23:18.8906310Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8906481Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8906574Z                     error_index=error_index,
2025-04-11T04:23:18.8906680Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8906765Z                     exit_code=exitcode,
2025-04-11T04:23:18.8906839Z                 )
2025-04-11T04:23:18.8906907Z     
2025-04-11T04:23:18.8907037Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8907211Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8907294Z         msg += original_trace
2025-04-11T04:23:18.8907467Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8907627Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8907701Z E       
2025-04-11T04:23:18.8907828Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8907924Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8908229Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8908310Z E           fn(i, *args)
2025-04-11T04:23:18.8908655Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
2025-04-11T04:23:18.8908742Z E           run_deepseek_test()
2025-04-11T04:23:18.8909052Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8909145Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8909450Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
2025-04-11T04:23:18.8909551Z E           run_deepseek_commom(config)
2025-04-11T04:23:18.8909852Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
2025-04-11T04:23:18.8910056Z E           torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
2025-04-11T04:23:18.8910347Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.8910449Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.8910718Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8910835Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8911107Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8911267Z E           module._apply(fn)
2025-04-11T04:23:18.8911542Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8911634Z E           param_applied = fn(param)
2025-04-11T04:23:18.8911916Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8912191Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8912296Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8912579Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8912714Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8912877Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8912881Z 
2025-04-11T04:23:18.8913187Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8913394Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8913550Z [04/11/25 04:20:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8913681Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8913788Z                              :75 launch                                         
2025-04-11T04:23:18.8913924Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8914050Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8914138Z rank 0 testing (0, 1, 4, 1, 1)
2025-04-11T04:23:18.8914337Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8914479Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8915195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8915279Z   warnings.warn(
2025-04-11T04:23:18.8915956Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8916038Z   warnings.warn(
2025-04-11T04:23:18.8916776Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8916858Z   warnings.warn(
2025-04-11T04:23:18.8917547Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8917629Z   warnings.warn(
2025-04-11T04:23:18.8918455Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8918539Z   warnings.warn(
2025-04-11T04:23:18.8919343Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8919478Z   warnings.warn(
2025-04-11T04:23:18.8920278Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8920411Z   warnings.warn(
2025-04-11T04:23:18.8921216Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8921297Z   warnings.warn(
2025-04-11T04:23:18.8922147Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8922228Z   warnings.warn(
2025-04-11T04:23:18.8923027Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8923107Z   warnings.warn(
2025-04-11T04:23:18.8923897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8923980Z   warnings.warn(
2025-04-11T04:23:18.8924790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8924868Z   warnings.warn(
2025-04-11T04:23:18.8925670Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8925797Z   warnings.warn(
2025-04-11T04:23:18.8926581Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8926661Z   warnings.warn(
2025-04-11T04:23:18.8926949Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8927042Z - configuration_deepseek.py
2025-04-11T04:23:18.8927396Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8928192Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8928270Z   warnings.warn(
2025-04-11T04:23:18.8928544Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8928687Z - configuration_deepseek.py
2025-04-11T04:23:18.8929034Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8929853Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8929980Z   warnings.warn(
2025-04-11T04:23:18.8930255Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8930347Z - configuration_deepseek.py
2025-04-11T04:23:18.8930687Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8931539Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8931616Z   warnings.warn(
2025-04-11T04:23:18.8931882Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8931971Z - configuration_deepseek.py
2025-04-11T04:23:18.8932307Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8933097Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8933178Z   warnings.warn(
2025-04-11T04:23:18.8933961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8934041Z   warnings.warn(
2025-04-11T04:23:18.8934873Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8934954Z   warnings.warn(
2025-04-11T04:23:18.8935743Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8935821Z   warnings.warn(
2025-04-11T04:23:18.8936607Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8936683Z   warnings.warn(
2025-04-11T04:23:18.8937472Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8937599Z   warnings.warn(
2025-04-11T04:23:18.8937870Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8937955Z - modeling_deepseek.py
2025-04-11T04:23:18.8938288Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8938552Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8938689Z - modeling_deepseek.py
2025-04-11T04:23:18.8939017Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8939276Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8939362Z - modeling_deepseek.py
2025-04-11T04:23:18.8939739Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8940003Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T04:23:18.8940085Z - modeling_deepseek.py
2025-04-11T04:23:18.8940418Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8941217Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8941298Z   warnings.warn(
2025-04-11T04:23:18.8942087Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8942169Z   warnings.warn(
2025-04-11T04:23:18.8942950Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8943032Z   warnings.warn(
2025-04-11T04:23:18.8943168Z _____________________________ test_deepseek_v3[4] ______________________________
2025-04-11T04:23:18.8943172Z 
2025-04-11T04:23:18.8943352Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.8943964Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8943973Z 
2025-04-11T04:23:18.8944076Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.8944159Z         try_count = 0
2025-04-11T04:23:18.8944258Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.8944340Z             max_try, int
2025-04-11T04:23:18.8944486Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8944558Z     
2025-04-11T04:23:18.8944671Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.8944743Z             try:
2025-04-11T04:23:18.8944828Z                 try_count += 1
2025-04-11T04:23:18.8944921Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.8945002Z                 return ret
2025-04-11T04:23:18.8945094Z             except exception_type as e:
2025-04-11T04:23:18.8945194Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.8945438Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.8945557Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.8945710Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.8945865Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.8945999Z                     continue
2025-04-11T04:23:18.8946075Z                 else:
2025-04-11T04:23:18.8946299Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.8946380Z >                   raise e
2025-04-11T04:23:18.8946384Z 
2025-04-11T04:23:18.8946481Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.8946595Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8946728Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.8946817Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.8947066Z tests/test_shardformer/test_model/test_shard_deepseek_v3.py:100: in test_deepseek_v3
2025-04-11T04:23:18.8947166Z     spawn(check_deepseek_v3, world_size)
2025-04-11T04:23:18.8947268Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.8947367Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.8947630Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.8947811Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.8948099Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.8948188Z     while not context.join():
2025-04-11T04:23:18.8948298Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.8948302Z 
2025-04-11T04:23:18.8948530Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581e0dde0>
2025-04-11T04:23:18.8948607Z timeout = None
2025-04-11T04:23:18.8948611Z 
2025-04-11T04:23:18.8948706Z     def join(self, timeout=None):
2025-04-11T04:23:18.8948830Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8948901Z     
2025-04-11T04:23:18.8949044Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.8949184Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.8949353Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.8949445Z         of the first process exiting.
2025-04-11T04:23:18.8949516Z     
2025-04-11T04:23:18.8949718Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.8949861Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8949931Z     
2025-04-11T04:23:18.8950002Z         Args:
2025-04-11T04:23:18.8950146Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.8950217Z         """
2025-04-11T04:23:18.8950357Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.8950449Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.8950527Z             return True
2025-04-11T04:23:18.8950599Z     
2025-04-11T04:23:18.8950728Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.8950849Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.8950938Z             self.sentinels.keys(),
2025-04-11T04:23:18.8951021Z             timeout=timeout,
2025-04-11T04:23:18.8951094Z         )
2025-04-11T04:23:18.8951162Z     
2025-04-11T04:23:18.8951247Z         error_index = None
2025-04-11T04:23:18.8951332Z         for sentinel in ready:
2025-04-11T04:23:18.8951441Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.8951538Z             process = self.processes[index]
2025-04-11T04:23:18.8951679Z             process.join()
2025-04-11T04:23:18.8951774Z             if process.exitcode != 0:
2025-04-11T04:23:18.8951862Z                 error_index = index
2025-04-11T04:23:18.8951940Z                 break
2025-04-11T04:23:18.8952006Z     
2025-04-11T04:23:18.8952095Z         # Return if there was no error.
2025-04-11T04:23:18.8952181Z         if error_index is None:
2025-04-11T04:23:18.8952315Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.8952465Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.8952534Z     
2025-04-11T04:23:18.8952673Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.8952771Z         for process in self.processes:
2025-04-11T04:23:18.8952860Z             if process.is_alive():
2025-04-11T04:23:18.8952953Z                 process.terminate()
2025-04-11T04:23:18.8953034Z             process.join()
2025-04-11T04:23:18.8953104Z     
2025-04-11T04:23:18.8953245Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.8953359Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.8953519Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.8953640Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.8953726Z             if exitcode < 0:
2025-04-11T04:23:18.8953832Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.8953938Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8954090Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.8954182Z                     error_index=error_index,
2025-04-11T04:23:18.8954285Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8954371Z                     exit_code=exitcode,
2025-04-11T04:23:18.8954458Z                     signal_name=name,
2025-04-11T04:23:18.8954529Z                 )
2025-04-11T04:23:18.8954600Z             else:
2025-04-11T04:23:18.8954707Z                 raise ProcessExitedException(
2025-04-11T04:23:18.8954871Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.8954965Z                     error_index=error_index,
2025-04-11T04:23:18.8955064Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.8955152Z                     exit_code=exitcode,
2025-04-11T04:23:18.8955222Z                 )
2025-04-11T04:23:18.8955292Z     
2025-04-11T04:23:18.8955424Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.8955593Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.8955682Z         msg += original_trace
2025-04-11T04:23:18.8955902Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.8956067Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.8956143Z E       
2025-04-11T04:23:18.8956270Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.8956371Z E       Traceback (most recent call last):
2025-04-11T04:23:18.8956671Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.8956753Z E           fn(i, *args)
2025-04-11T04:23:18.8957081Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
2025-04-11T04:23:18.8957170Z E           run_deepseek_v3_test()
2025-04-11T04:23:18.8957429Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.8957515Z E           partial_func(**kwargs)
2025-04-11T04:23:18.8957836Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
2025-04-11T04:23:18.8957923Z E           check_forward_backward(
2025-04-11T04:23:18.8958298Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
2025-04-11T04:23:18.8958570Z E           org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
2025-04-11T04:23:18.8958870Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
2025-04-11T04:23:18.8959013Z E           org_model = org_model.cuda()
2025-04-11T04:23:18.8959300Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.8959404Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.8959669Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.8959791Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8960061Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8960197Z E           module._apply(fn)
2025-04-11T04:23:18.8960464Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.8960550Z E           module._apply(fn)
2025-04-11T04:23:18.8960816Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.8960912Z E           param_applied = fn(param)
2025-04-11T04:23:18.8961185Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.8961303Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.8961412Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.8961696Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.8961836Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.8962001Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8962005Z 
2025-04-11T04:23:18.8962308Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.8962463Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.8962620Z [04/11/25 04:20:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.8962752Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.8962906Z                              :75 launch                                         
2025-04-11T04:23:18.8963049Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.8963175Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.8963372Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.8963520Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.8964670Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8964847Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8965958Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8966175Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8967288Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8967503Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8968659Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.8968826Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.8969512Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8969595Z   warnings.warn(
2025-04-11T04:23:18.8970277Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8970360Z   warnings.warn(
2025-04-11T04:23:18.8971027Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8971108Z   warnings.warn(
2025-04-11T04:23:18.8971780Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.8971911Z   warnings.warn(
2025-04-11T04:23:18.8972741Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8972826Z   warnings.warn(
2025-04-11T04:23:18.8973640Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8973723Z   warnings.warn(
2025-04-11T04:23:18.8974533Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8974614Z   warnings.warn(
2025-04-11T04:23:18.8975419Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8975547Z   warnings.warn(
2025-04-11T04:23:18.8976355Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8976482Z   warnings.warn(
2025-04-11T04:23:18.8977285Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8977366Z   warnings.warn(
2025-04-11T04:23:18.8978210Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8978288Z   warnings.warn(
2025-04-11T04:23:18.8979112Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8979188Z   warnings.warn(
2025-04-11T04:23:18.8980012Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8980089Z   warnings.warn(
2025-04-11T04:23:18.8980892Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8980969Z   warnings.warn(
2025-04-11T04:23:18.8981840Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8981916Z   warnings.warn(
2025-04-11T04:23:18.8982216Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44807 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.8983015Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8983101Z   warnings.warn(
2025-04-11T04:23:18.8983895Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8983973Z   warnings.warn(
2025-04-11T04:23:18.8984759Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8984891Z   warnings.warn(
2025-04-11T04:23:18.8985673Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8985806Z   warnings.warn(
2025-04-11T04:23:18.8986595Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8986673Z   warnings.warn(
2025-04-11T04:23:18.8987516Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8987595Z   warnings.warn(
2025-04-11T04:23:18.8988400Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8988522Z   warnings.warn(
2025-04-11T04:23:18.8989314Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8989391Z   warnings.warn(
2025-04-11T04:23:18.8990179Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8990256Z   warnings.warn(
2025-04-11T04:23:18.8991139Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8991217Z   warnings.warn(
2025-04-11T04:23:18.8992009Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8992086Z   warnings.warn(
2025-04-11T04:23:18.8992342Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.8992433Z - configuration_deepseek.py
2025-04-11T04:23:18.8992780Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8993574Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8993652Z   warnings.warn(
2025-04-11T04:23:18.8993968Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.8994061Z - configuration_deepseek.py
2025-04-11T04:23:18.8994409Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8995213Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8995348Z   warnings.warn(
2025-04-11T04:23:18.8995595Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.8995687Z - configuration_deepseek.py
2025-04-11T04:23:18.8996021Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8996316Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.8996407Z - configuration_deepseek.py
2025-04-11T04:23:18.8996742Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.8997555Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8997634Z   warnings.warn(
2025-04-11T04:23:18.8998432Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8998509Z   warnings.warn(
2025-04-11T04:23:18.8999301Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.8999379Z   warnings.warn(
2025-04-11T04:23:18.9000218Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9000295Z   warnings.warn(
2025-04-11T04:23:18.9001084Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9001162Z   warnings.warn(
2025-04-11T04:23:18.9001954Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9002031Z   warnings.warn(
2025-04-11T04:23:18.9002829Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9002955Z   warnings.warn(
2025-04-11T04:23:18.9003207Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.9003290Z - modeling_deepseek.py
2025-04-11T04:23:18.9003624Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.9003865Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.9004016Z - modeling_deepseek.py
2025-04-11T04:23:18.9004348Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.9004586Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.9004671Z - modeling_deepseek.py
2025-04-11T04:23:18.9005008Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.9005293Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T04:23:18.9005378Z - modeling_deepseek.py
2025-04-11T04:23:18.9005708Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T04:23:18.9005857Z _________________________________ test_falcon __________________________________
2025-04-11T04:23:18.9005862Z 
2025-04-11T04:23:18.9005953Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9005957Z 
2025-04-11T04:23:18.9006067Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9006146Z         try_count = 0
2025-04-11T04:23:18.9006250Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9006330Z             max_try, int
2025-04-11T04:23:18.9006482Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9006560Z     
2025-04-11T04:23:18.9006674Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9006754Z             try:
2025-04-11T04:23:18.9006839Z                 try_count += 1
2025-04-11T04:23:18.9006932Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9006941Z 
2025-04-11T04:23:18.9007034Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9007149Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9007268Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9007362Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9007578Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9007675Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9007785Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9007796Z 
2025-04-11T04:23:18.9007872Z device = None
2025-04-11T04:23:18.9007876Z 
2025-04-11T04:23:18.9008000Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9008159Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9008229Z     
2025-04-11T04:23:18.9008305Z         Args:
2025-04-11T04:23:18.9008479Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9008646Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9008761Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9008834Z         """
2025-04-11T04:23:18.9008916Z         _lazy_init()
2025-04-11T04:23:18.9009012Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9009116Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9009222Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9009518Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9009719Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9009879Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9009884Z 
2025-04-11T04:23:18.9010128Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9010296Z __________________________________ test_gpt2 ___________________________________
2025-04-11T04:23:18.9010300Z 
2025-04-11T04:23:18.9010399Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9010403Z 
2025-04-11T04:23:18.9010502Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9010590Z         try_count = 0
2025-04-11T04:23:18.9010691Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9010771Z             max_try, int
2025-04-11T04:23:18.9010919Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9010989Z     
2025-04-11T04:23:18.9011150Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9011227Z             try:
2025-04-11T04:23:18.9011310Z                 try_count += 1
2025-04-11T04:23:18.9011408Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9011412Z 
2025-04-11T04:23:18.9011504Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9011618Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9011733Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9011832Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9011987Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9012082Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9012194Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9012199Z 
2025-04-11T04:23:18.9012273Z device = None
2025-04-11T04:23:18.9012279Z 
2025-04-11T04:23:18.9012403Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9012556Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9012630Z     
2025-04-11T04:23:18.9012704Z         Args:
2025-04-11T04:23:18.9012872Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9013041Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9013151Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9013228Z         """
2025-04-11T04:23:18.9013307Z         _lazy_init()
2025-04-11T04:23:18.9013407Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9013557Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9013667Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9013960Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9014103Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9014269Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9014273Z 
2025-04-11T04:23:18.9014509Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9014646Z __________________________________ test_llama __________________________________
2025-04-11T04:23:18.9014651Z 
2025-04-11T04:23:18.9014741Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9014745Z 
2025-04-11T04:23:18.9014847Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9014925Z         try_count = 0
2025-04-11T04:23:18.9015026Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9015113Z             max_try, int
2025-04-11T04:23:18.9015260Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9015386Z     
2025-04-11T04:23:18.9015498Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9015575Z             try:
2025-04-11T04:23:18.9015661Z                 try_count += 1
2025-04-11T04:23:18.9015752Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9015756Z 
2025-04-11T04:23:18.9015855Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9015965Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9016140Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9016234Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9016389Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9016486Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9016595Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9016599Z 
2025-04-11T04:23:18.9016683Z device = None
2025-04-11T04:23:18.9016687Z 
2025-04-11T04:23:18.9016811Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9017010Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9017083Z     
2025-04-11T04:23:18.9017158Z         Args:
2025-04-11T04:23:18.9017333Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9017501Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9017619Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9017693Z         """
2025-04-11T04:23:18.9017777Z         _lazy_init()
2025-04-11T04:23:18.9017870Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9017969Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9018083Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9018369Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9018514Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9018676Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9018680Z 
2025-04-11T04:23:18.9018920Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9019052Z _________________________________ test_mistral _________________________________
2025-04-11T04:23:18.9019058Z 
2025-04-11T04:23:18.9019148Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9019158Z 
2025-04-11T04:23:18.9019256Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9019335Z         try_count = 0
2025-04-11T04:23:18.9019487Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9019570Z             max_try, int
2025-04-11T04:23:18.9019720Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9019790Z     
2025-04-11T04:23:18.9019902Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9019982Z             try:
2025-04-11T04:23:18.9020066Z                 try_count += 1
2025-04-11T04:23:18.9020237Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9020242Z 
2025-04-11T04:23:18.9020427Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9020586Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9020856Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9020985Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9021169Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9021312Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9021478Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9021482Z 
2025-04-11T04:23:18.9021634Z device = None
2025-04-11T04:23:18.9021638Z 
2025-04-11T04:23:18.9021785Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9022048Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9022159Z     
2025-04-11T04:23:18.9022247Z         Args:
2025-04-11T04:23:18.9022493Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9022685Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9022852Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9023011Z         """
2025-04-11T04:23:18.9023137Z         _lazy_init()
2025-04-11T04:23:18.9023271Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9023413Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9023580Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9023893Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9024098Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9024319Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9024324Z 
2025-04-11T04:23:18.9024734Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9024898Z _______________________________ test_mixtral[4] ________________________________
2025-04-11T04:23:18.9024902Z 
2025-04-11T04:23:18.9025091Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9025732Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9025737Z 
2025-04-11T04:23:18.9025884Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9025999Z         try_count = 0
2025-04-11T04:23:18.9026138Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9026290Z             max_try, int
2025-04-11T04:23:18.9026471Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9026599Z     
2025-04-11T04:23:18.9026723Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9026874Z             try:
2025-04-11T04:23:18.9026988Z                 try_count += 1
2025-04-11T04:23:18.9027115Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9027253Z                 return ret
2025-04-11T04:23:18.9027375Z             except exception_type as e:
2025-04-11T04:23:18.9027525Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9027805Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9027959Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9028163Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9028351Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9028614Z                     continue
2025-04-11T04:23:18.9028730Z                 else:
2025-04-11T04:23:18.9029039Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9029150Z >                   raise e
2025-04-11T04:23:18.9029155Z 
2025-04-11T04:23:18.9029310Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9029445Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9029592Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9029769Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9029987Z tests/test_shardformer/test_model/test_shard_mixtral.py:222: in test_mixtral
2025-04-11T04:23:18.9030146Z     spawn(check_mixtral, world_size)
2025-04-11T04:23:18.9030280Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9030483Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9030788Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9031009Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9031352Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9031473Z     while not context.join():
2025-04-11T04:23:18.9031692Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9031696Z 
2025-04-11T04:23:18.9031920Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a214b0>
2025-04-11T04:23:18.9032084Z timeout = None
2025-04-11T04:23:18.9032091Z 
2025-04-11T04:23:18.9032212Z     def join(self, timeout=None):
2025-04-11T04:23:18.9032368Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9032577Z     
2025-04-11T04:23:18.9032767Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9033022Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9033228Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9033386Z         of the first process exiting.
2025-04-11T04:23:18.9033487Z     
2025-04-11T04:23:18.9033671Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9033860Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9033967Z     
2025-04-11T04:23:18.9034113Z         Args:
2025-04-11T04:23:18.9034279Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9034420Z         """
2025-04-11T04:23:18.9034586Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9034697Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9034858Z             return True
2025-04-11T04:23:18.9034958Z     
2025-04-11T04:23:18.9035158Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9035310Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9035449Z             self.sentinels.keys(),
2025-04-11T04:23:18.9035570Z             timeout=timeout,
2025-04-11T04:23:18.9035684Z         )
2025-04-11T04:23:18.9035812Z     
2025-04-11T04:23:18.9036021Z         error_index = None
2025-04-11T04:23:18.9036171Z         for sentinel in ready:
2025-04-11T04:23:18.9036292Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9036427Z             process = self.processes[index]
2025-04-11T04:23:18.9036584Z             process.join()
2025-04-11T04:23:18.9036714Z             if process.exitcode != 0:
2025-04-11T04:23:18.9036926Z                 error_index = index
2025-04-11T04:23:18.9037032Z                 break
2025-04-11T04:23:18.9037154Z     
2025-04-11T04:23:18.9037285Z         # Return if there was no error.
2025-04-11T04:23:18.9037412Z         if error_index is None:
2025-04-11T04:23:18.9037603Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9037731Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9037843Z     
2025-04-11T04:23:18.9038020Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9038155Z         for process in self.processes:
2025-04-11T04:23:18.9038313Z             if process.is_alive():
2025-04-11T04:23:18.9038438Z                 process.terminate()
2025-04-11T04:23:18.9038586Z             process.join()
2025-04-11T04:23:18.9038671Z     
2025-04-11T04:23:18.9038899Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9039045Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9039262Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9039444Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9039613Z             if exitcode < 0:
2025-04-11T04:23:18.9039781Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9039942Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9040158Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9040282Z                     error_index=error_index,
2025-04-11T04:23:18.9040413Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9040550Z                     exit_code=exitcode,
2025-04-11T04:23:18.9040729Z                     signal_name=name,
2025-04-11T04:23:18.9040872Z                 )
2025-04-11T04:23:18.9040974Z             else:
2025-04-11T04:23:18.9041107Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9041336Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9041453Z                     error_index=error_index,
2025-04-11T04:23:18.9041634Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9041753Z                     exit_code=exitcode,
2025-04-11T04:23:18.9041882Z                 )
2025-04-11T04:23:18.9041979Z     
2025-04-11T04:23:18.9042198Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9042424Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9042632Z         msg += original_trace
2025-04-11T04:23:18.9042865Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9043056Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9043200Z E       
2025-04-11T04:23:18.9043346Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9043482Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9043858Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9043968Z E           fn(i, *args)
2025-04-11T04:23:18.9044342Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
2025-04-11T04:23:18.9044459Z E           run_mixtral_test()
2025-04-11T04:23:18.9044775Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9044906Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9045263Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
2025-04-11T04:23:18.9045395Z E           run_mixtral_commom(config)
2025-04-11T04:23:18.9045726Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
2025-04-11T04:23:18.9045951Z E           torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T04:23:18.9046281Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9046456Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9046761Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9046938Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9047233Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9047374Z E           module._apply(fn)
2025-04-11T04:23:18.9047754Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9047878Z E           param_applied = fn(param)
2025-04-11T04:23:18.9048227Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9048377Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9048529Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9048902Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9049118Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9049308Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9049312Z 
2025-04-11T04:23:18.9049673Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9049903Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9050075Z [04/11/25 04:20:47] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9050296Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9050434Z                              :75 launch                                         
2025-04-11T04:23:18.9050634Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9050834Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9051080Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9051282Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9052459Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.9053584Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.9054771Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.9055958Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T04:23:18.9056184Z ________________________________ test_OPTModel _________________________________
2025-04-11T04:23:18.9056188Z 
2025-04-11T04:23:18.9056324Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9056360Z 
2025-04-11T04:23:18.9056491Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9056599Z         try_count = 0
2025-04-11T04:23:18.9056768Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9056869Z             max_try, int
2025-04-11T04:23:18.9057099Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9057199Z     
2025-04-11T04:23:18.9057341Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9057541Z             try:
2025-04-11T04:23:18.9057652Z                 try_count += 1
2025-04-11T04:23:18.9057795Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9057799Z 
2025-04-11T04:23:18.9057931Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9058100Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9058244Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9058423Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9058627Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9058759Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9058933Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9058940Z 
2025-04-11T04:23:18.9059041Z device = None
2025-04-11T04:23:18.9059046Z 
2025-04-11T04:23:18.9059317Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9059501Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9059625Z     
2025-04-11T04:23:18.9059744Z         Args:
2025-04-11T04:23:18.9059996Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9060242Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9060379Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9060496Z         """
2025-04-11T04:23:18.9060613Z         _lazy_init()
2025-04-11T04:23:18.9060796Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9060928Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9061065Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9061422Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9061575Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9061815Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9061819Z 
2025-04-11T04:23:18.9062103Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9062290Z __________________________________ test_qwen2 __________________________________
2025-04-11T04:23:18.9062295Z 
2025-04-11T04:23:18.9062414Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9062421Z 
2025-04-11T04:23:18.9062564Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9062681Z         try_count = 0
2025-04-11T04:23:18.9062829Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9063052Z             max_try, int
2025-04-11T04:23:18.9063277Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9063404Z     
2025-04-11T04:23:18.9063533Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9070355Z             try:
2025-04-11T04:23:18.9070467Z                 try_count += 1
2025-04-11T04:23:18.9070604Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9070613Z 
2025-04-11T04:23:18.9070752Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9070904Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9071036Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9071139Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9071314Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9071417Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9071539Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9071544Z 
2025-04-11T04:23:18.9071628Z device = None
2025-04-11T04:23:18.9071633Z 
2025-04-11T04:23:18.9071768Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9071932Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9072109Z     
2025-04-11T04:23:18.9072193Z         Args:
2025-04-11T04:23:18.9072372Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9072551Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9072665Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9072744Z         """
2025-04-11T04:23:18.9072829Z         _lazy_init()
2025-04-11T04:23:18.9073008Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9073125Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9073244Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9073560Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9073706Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9073875Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9073882Z 
2025-04-11T04:23:18.9074197Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9074336Z ___________________________________ test_sam ___________________________________
2025-04-11T04:23:18.9074341Z 
2025-04-11T04:23:18.9074442Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9074446Z 
2025-04-11T04:23:18.9074556Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9074642Z         try_count = 0
2025-04-11T04:23:18.9074744Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9074833Z             max_try, int
2025-04-11T04:23:18.9074988Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9075064Z     
2025-04-11T04:23:18.9075186Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9075266Z             try:
2025-04-11T04:23:18.9075359Z                 try_count += 1
2025-04-11T04:23:18.9075460Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9075464Z 
2025-04-11T04:23:18.9075570Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9075689Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9075811Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9075916Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9076085Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9076193Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9076310Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9076314Z 
2025-04-11T04:23:18.9076401Z device = None
2025-04-11T04:23:18.9076405Z 
2025-04-11T04:23:18.9076617Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9076778Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9076859Z     
2025-04-11T04:23:18.9076935Z         Args:
2025-04-11T04:23:18.9077111Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9077285Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9077399Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9077475Z         """
2025-04-11T04:23:18.9077555Z         _lazy_init()
2025-04-11T04:23:18.9077658Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9077765Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9077882Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9078173Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9078314Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9078482Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9078536Z 
2025-04-11T04:23:18.9078787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9078926Z ___________________________________ test_t5 ____________________________________
2025-04-11T04:23:18.9078929Z 
2025-04-11T04:23:18.9079023Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9079027Z 
2025-04-11T04:23:18.9079133Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9079258Z         try_count = 0
2025-04-11T04:23:18.9079365Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9079449Z             max_try, int
2025-04-11T04:23:18.9079595Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9079673Z     
2025-04-11T04:23:18.9079789Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9079869Z             try:
2025-04-11T04:23:18.9079957Z                 try_count += 1
2025-04-11T04:23:18.9080053Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9080062Z 
2025-04-11T04:23:18.9080157Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9080320Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9080444Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9080542Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9080710Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9080812Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9080926Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9080934Z 
2025-04-11T04:23:18.9081017Z device = None
2025-04-11T04:23:18.9081021Z 
2025-04-11T04:23:18.9081145Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9081310Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9081384Z     
2025-04-11T04:23:18.9081467Z         Args:
2025-04-11T04:23:18.9081642Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9081820Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9081937Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9082015Z         """
2025-04-11T04:23:18.9082103Z         _lazy_init()
2025-04-11T04:23:18.9082204Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9082318Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9082427Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9082721Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9082922Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9103657Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9103669Z 
2025-04-11T04:23:18.9103944Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9104078Z ___________________________________ test_vit ___________________________________
2025-04-11T04:23:18.9104082Z 
2025-04-11T04:23:18.9104180Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9104184Z 
2025-04-11T04:23:18.9104285Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9104369Z         try_count = 0
2025-04-11T04:23:18.9104473Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9104554Z             max_try, int
2025-04-11T04:23:18.9104706Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9104776Z     
2025-04-11T04:23:18.9104894Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9104969Z             try:
2025-04-11T04:23:18.9105053Z                 try_count += 1
2025-04-11T04:23:18.9105150Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9105215Z 
2025-04-11T04:23:18.9105311Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9105433Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9105549Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9105648Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9105817Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9105919Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9106122Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9106127Z 
2025-04-11T04:23:18.9106205Z device = None
2025-04-11T04:23:18.9106209Z 
2025-04-11T04:23:18.9106335Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9106489Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9106562Z     
2025-04-11T04:23:18.9106635Z         Args:
2025-04-11T04:23:18.9106804Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9107041Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9107159Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9107233Z         """
2025-04-11T04:23:18.9107312Z         _lazy_init()
2025-04-11T04:23:18.9107409Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9107509Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9107615Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9107909Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9108048Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9108206Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9108211Z 
2025-04-11T04:23:18.9108485Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9108628Z _________________________________ test_whisper _________________________________
2025-04-11T04:23:18.9108632Z 
2025-04-11T04:23:18.9108723Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9108727Z 
2025-04-11T04:23:18.9108829Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9108906Z         try_count = 0
2025-04-11T04:23:18.9109004Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9109087Z             max_try, int
2025-04-11T04:23:18.9109232Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9109304Z     
2025-04-11T04:23:18.9109415Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9109549Z             try:
2025-04-11T04:23:18.9109638Z                 try_count += 1
2025-04-11T04:23:18.9109727Z >               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9109731Z 
2025-04-11T04:23:18.9109828Z colossalai/testing/utils.py:133: 
2025-04-11T04:23:18.9109939Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9110057Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T04:23:18.9110148Z     get_accelerator().synchronize()
2025-04-11T04:23:18.9110299Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T04:23:18.9110396Z     torch.cuda.synchronize(device)
2025-04-11T04:23:18.9110503Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9110508Z 
2025-04-11T04:23:18.9110586Z device = None
2025-04-11T04:23:18.9110590Z 
2025-04-11T04:23:18.9110708Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T04:23:18.9110861Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9110928Z     
2025-04-11T04:23:18.9110998Z         Args:
2025-04-11T04:23:18.9111169Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T04:23:18.9111389Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T04:23:18.9111500Z                 if :attr:`device` is ``None`` (default).
2025-04-11T04:23:18.9111571Z         """
2025-04-11T04:23:18.9111650Z         _lazy_init()
2025-04-11T04:23:18.9111741Z         with torch.cuda.device(device):
2025-04-11T04:23:18.9111840Z >           return torch._C._cuda_synchronize()
2025-04-11T04:23:18.9112002Z E           RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9112291Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9112433Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9112592Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9112595Z 
2025-04-11T04:23:18.9112839Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T04:23:18.9112973Z ________________________________ test_comm_spec ________________________________
2025-04-11T04:23:18.9113043Z 
2025-04-11T04:23:18.9113139Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9113766Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9113773Z 
2025-04-11T04:23:18.9113873Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9113954Z         try_count = 0
2025-04-11T04:23:18.9114051Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9114135Z             max_try, int
2025-04-11T04:23:18.9114278Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9114348Z     
2025-04-11T04:23:18.9114457Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9114532Z             try:
2025-04-11T04:23:18.9114617Z                 try_count += 1
2025-04-11T04:23:18.9114709Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9114792Z                 return ret
2025-04-11T04:23:18.9114885Z             except exception_type as e:
2025-04-11T04:23:18.9114982Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9115174Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9115293Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9115440Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9115644Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9115731Z                     continue
2025-04-11T04:23:18.9115806Z                 else:
2025-04-11T04:23:18.9116033Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9116121Z >                   raise e
2025-04-11T04:23:18.9116129Z 
2025-04-11T04:23:18.9116223Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9116332Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9116467Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9116554Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9116720Z tests/test_tensor/test_comm_spec_apply.py:211: in test_comm_spec
2025-04-11T04:23:18.9116811Z     spawn(check_comm, world_size)
2025-04-11T04:23:18.9116911Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9117014Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9117274Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9117458Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9117798Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9117893Z     while not context.join():
2025-04-11T04:23:18.9118002Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9118006Z 
2025-04-11T04:23:18.9118210Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5ad0f40>
2025-04-11T04:23:18.9118287Z timeout = None
2025-04-11T04:23:18.9118339Z 
2025-04-11T04:23:18.9118431Z     def join(self, timeout=None):
2025-04-11T04:23:18.9118559Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9118628Z     
2025-04-11T04:23:18.9118778Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9118922Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9119089Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9119189Z         of the first process exiting.
2025-04-11T04:23:18.9119257Z     
2025-04-11T04:23:18.9119456Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9119595Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9119667Z     
2025-04-11T04:23:18.9119740Z         Args:
2025-04-11T04:23:18.9119876Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9119953Z         """
2025-04-11T04:23:18.9120092Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9120189Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9120268Z             return True
2025-04-11T04:23:18.9120338Z     
2025-04-11T04:23:18.9120474Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9120594Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9120692Z             self.sentinels.keys(),
2025-04-11T04:23:18.9120777Z             timeout=timeout,
2025-04-11T04:23:18.9120857Z         )
2025-04-11T04:23:18.9120926Z     
2025-04-11T04:23:18.9121009Z         error_index = None
2025-04-11T04:23:18.9121101Z         for sentinel in ready:
2025-04-11T04:23:18.9121208Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9121311Z             process = self.processes[index]
2025-04-11T04:23:18.9121397Z             process.join()
2025-04-11T04:23:18.9121490Z             if process.exitcode != 0:
2025-04-11T04:23:18.9121587Z                 error_index = index
2025-04-11T04:23:18.9121662Z                 break
2025-04-11T04:23:18.9121734Z     
2025-04-11T04:23:18.9121824Z         # Return if there was no error.
2025-04-11T04:23:18.9121909Z         if error_index is None:
2025-04-11T04:23:18.9122098Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9122198Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9122274Z     
2025-04-11T04:23:18.9122414Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9122517Z         for process in self.processes:
2025-04-11T04:23:18.9122607Z             if process.is_alive():
2025-04-11T04:23:18.9122701Z                 process.terminate()
2025-04-11T04:23:18.9122790Z             process.join()
2025-04-11T04:23:18.9122860Z     
2025-04-11T04:23:18.9123003Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9123120Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9123229Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9123355Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9123439Z             if exitcode < 0:
2025-04-11T04:23:18.9123554Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9123662Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9123815Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9123962Z                     error_index=error_index,
2025-04-11T04:23:18.9124064Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9124159Z                     exit_code=exitcode,
2025-04-11T04:23:18.9124246Z                     signal_name=name,
2025-04-11T04:23:18.9124325Z                 )
2025-04-11T04:23:18.9124399Z             else:
2025-04-11T04:23:18.9124502Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9124671Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9124814Z                     error_index=error_index,
2025-04-11T04:23:18.9124925Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9125014Z                     exit_code=exitcode,
2025-04-11T04:23:18.9125096Z                 )
2025-04-11T04:23:18.9125169Z     
2025-04-11T04:23:18.9125305Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9125486Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9125580Z         msg += original_trace
2025-04-11T04:23:18.9125813Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9125981Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9126064Z E       
2025-04-11T04:23:18.9126193Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9126293Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9126608Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9126692Z E           fn(i, *args)
2025-04-11T04:23:18.9126960Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
2025-04-11T04:23:18.9127067Z E           check_all_gather(device_mesh, rank)
2025-04-11T04:23:18.9127333Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
2025-04-11T04:23:18.9127459Z E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T04:23:18.9127567Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9127859Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9127997Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9128162Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9128168Z 
2025-04-11T04:23:18.9128479Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9128698Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9128858Z [04/11/25 04:20:53] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9128992Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9129108Z                              :75 launch                                         
2025-04-11T04:23:18.9129250Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9129383Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9129578Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9129731Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9130028Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9130320Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9130456Z ______________________________ test_padded_tensor ______________________________
2025-04-11T04:23:18.9130512Z 
2025-04-11T04:23:18.9130607Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9131218Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9131223Z 
2025-04-11T04:23:18.9131327Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9131463Z         try_count = 0
2025-04-11T04:23:18.9131562Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9131648Z             max_try, int
2025-04-11T04:23:18.9131792Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9131868Z     
2025-04-11T04:23:18.9131979Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9132053Z             try:
2025-04-11T04:23:18.9132141Z                 try_count += 1
2025-04-11T04:23:18.9132232Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9132317Z                 return ret
2025-04-11T04:23:18.9132411Z             except exception_type as e:
2025-04-11T04:23:18.9132558Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9132757Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9132873Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9133025Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9133183Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9133270Z                     continue
2025-04-11T04:23:18.9133347Z                 else:
2025-04-11T04:23:18.9133573Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9133660Z >                   raise e
2025-04-11T04:23:18.9133664Z 
2025-04-11T04:23:18.9133759Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9133877Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9134009Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9134100Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9134265Z tests/test_tensor/test_padded_tensor.py:42: in test_padded_tensor
2025-04-11T04:23:18.9134365Z     spawn(check_padded_tensor, world_size)
2025-04-11T04:23:18.9134472Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9134572Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9134838Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9135065Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9135362Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9135453Z     while not context.join():
2025-04-11T04:23:18.9135562Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9135567Z 
2025-04-11T04:23:18.9135771Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581eca8f0>
2025-04-11T04:23:18.9135851Z timeout = None
2025-04-11T04:23:18.9135854Z 
2025-04-11T04:23:18.9135952Z     def join(self, timeout=None):
2025-04-11T04:23:18.9136077Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9136154Z     
2025-04-11T04:23:18.9136301Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9136446Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9136620Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9136712Z         of the first process exiting.
2025-04-11T04:23:18.9136789Z     
2025-04-11T04:23:18.9136936Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9137138Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9137209Z     
2025-04-11T04:23:18.9137286Z         Args:
2025-04-11T04:23:18.9137433Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9137507Z         """
2025-04-11T04:23:18.9137653Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9137747Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9137894Z             return True
2025-04-11T04:23:18.9137978Z     
2025-04-11T04:23:18.9138110Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9138236Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9138330Z             self.sentinels.keys(),
2025-04-11T04:23:18.9138423Z             timeout=timeout,
2025-04-11T04:23:18.9138495Z         )
2025-04-11T04:23:18.9138567Z     
2025-04-11T04:23:18.9138657Z         error_index = None
2025-04-11T04:23:18.9138747Z         for sentinel in ready:
2025-04-11T04:23:18.9138859Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9139004Z             process = self.processes[index]
2025-04-11T04:23:18.9139095Z             process.join()
2025-04-11T04:23:18.9139198Z             if process.exitcode != 0:
2025-04-11T04:23:18.9139288Z                 error_index = index
2025-04-11T04:23:18.9139372Z                 break
2025-04-11T04:23:18.9139445Z     
2025-04-11T04:23:18.9139541Z         # Return if there was no error.
2025-04-11T04:23:18.9139629Z         if error_index is None:
2025-04-11T04:23:18.9139763Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9139863Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9139934Z     
2025-04-11T04:23:18.9140082Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9140179Z         for process in self.processes:
2025-04-11T04:23:18.9140268Z             if process.is_alive():
2025-04-11T04:23:18.9140369Z                 process.terminate()
2025-04-11T04:23:18.9140453Z             process.join()
2025-04-11T04:23:18.9140531Z     
2025-04-11T04:23:18.9140675Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9140789Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9140903Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9141025Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9141117Z             if exitcode < 0:
2025-04-11T04:23:18.9141222Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9141331Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9141532Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9141633Z                     error_index=error_index,
2025-04-11T04:23:18.9141741Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9141832Z                     exit_code=exitcode,
2025-04-11T04:23:18.9141926Z                     signal_name=name,
2025-04-11T04:23:18.9142002Z                 )
2025-04-11T04:23:18.9142078Z             else:
2025-04-11T04:23:18.9142192Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9142365Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9142468Z                     error_index=error_index,
2025-04-11T04:23:18.9142575Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9142671Z                     exit_code=exitcode,
2025-04-11T04:23:18.9142746Z                 )
2025-04-11T04:23:18.9142820Z     
2025-04-11T04:23:18.9142963Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9143141Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9143235Z         msg += original_trace
2025-04-11T04:23:18.9143412Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9143635Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9143713Z E       
2025-04-11T04:23:18.9143842Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9143944Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9144244Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9144384Z E           fn(i, *args)
2025-04-11T04:23:18.9144654Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
2025-04-11T04:23:18.9144773Z E           original_tensor = torch.rand(32, 64).to("cuda")
2025-04-11T04:23:18.9144885Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9145171Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9145314Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9145523Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9145528Z 
2025-04-11T04:23:18.9145845Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9145997Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9146160Z [04/11/25 04:20:59] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9146291Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9146398Z                              :75 launch                                         
2025-04-11T04:23:18.9146544Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9146668Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9146875Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9147021Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9147319Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:39213 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9147450Z __________________________________ test_apply __________________________________
2025-04-11T04:23:18.9147456Z 
2025-04-11T04:23:18.9147555Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9148215Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9148221Z 
2025-04-11T04:23:18.9148330Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9148436Z         try_count = 0
2025-04-11T04:23:18.9148538Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9148623Z             max_try, int
2025-04-11T04:23:18.9148769Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9148847Z     
2025-04-11T04:23:18.9148961Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9149040Z             try:
2025-04-11T04:23:18.9149134Z                 try_count += 1
2025-04-11T04:23:18.9149228Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9149316Z                 return ret
2025-04-11T04:23:18.9149412Z             except exception_type as e:
2025-04-11T04:23:18.9149520Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9149712Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9149833Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9149986Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9150200Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9150293Z                     continue
2025-04-11T04:23:18.9150368Z                 else:
2025-04-11T04:23:18.9150592Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9150669Z >                   raise e
2025-04-11T04:23:18.9150726Z 
2025-04-11T04:23:18.9150819Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9150932Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9151062Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9151154Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9151321Z tests/test_tensor/test_shape_consistency_apply.py:72: in test_apply
2025-04-11T04:23:18.9151415Z     spawn(check_apply, world_size)
2025-04-11T04:23:18.9151513Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9151612Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9151927Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9152103Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9152389Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9152479Z     while not context.join():
2025-04-11T04:23:18.9152592Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9152596Z 
2025-04-11T04:23:18.9152791Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a6f6a0>
2025-04-11T04:23:18.9152868Z timeout = None
2025-04-11T04:23:18.9152875Z 
2025-04-11T04:23:18.9152966Z     def join(self, timeout=None):
2025-04-11T04:23:18.9153089Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9153160Z     
2025-04-11T04:23:18.9153304Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9153449Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9153608Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9153698Z         of the first process exiting.
2025-04-11T04:23:18.9153769Z     
2025-04-11T04:23:18.9153912Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9154050Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9154117Z     
2025-04-11T04:23:18.9154189Z         Args:
2025-04-11T04:23:18.9154327Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9154451Z         """
2025-04-11T04:23:18.9154593Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9154685Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9154766Z             return True
2025-04-11T04:23:18.9154836Z     
2025-04-11T04:23:18.9154965Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9155084Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9155173Z             self.sentinels.keys(),
2025-04-11T04:23:18.9155258Z             timeout=timeout,
2025-04-11T04:23:18.9155328Z         )
2025-04-11T04:23:18.9155394Z     
2025-04-11T04:23:18.9155478Z         error_index = None
2025-04-11T04:23:18.9155562Z         for sentinel in ready:
2025-04-11T04:23:18.9155669Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9155767Z             process = self.processes[index]
2025-04-11T04:23:18.9155850Z             process.join()
2025-04-11T04:23:18.9155942Z             if process.exitcode != 0:
2025-04-11T04:23:18.9156028Z                 error_index = index
2025-04-11T04:23:18.9156106Z                 break
2025-04-11T04:23:18.9156173Z     
2025-04-11T04:23:18.9156264Z         # Return if there was no error.
2025-04-11T04:23:18.9156399Z         if error_index is None:
2025-04-11T04:23:18.9156533Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9156631Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9156699Z     
2025-04-11T04:23:18.9156838Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9156935Z         for process in self.processes:
2025-04-11T04:23:18.9157020Z             if process.is_alive():
2025-04-11T04:23:18.9157164Z                 process.terminate()
2025-04-11T04:23:18.9157245Z             process.join()
2025-04-11T04:23:18.9157317Z     
2025-04-11T04:23:18.9157457Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9157576Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9157682Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9157801Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9157886Z             if exitcode < 0:
2025-04-11T04:23:18.9157991Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9158160Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9158310Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9158403Z                     error_index=error_index,
2025-04-11T04:23:18.9158507Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9158595Z                     exit_code=exitcode,
2025-04-11T04:23:18.9158680Z                     signal_name=name,
2025-04-11T04:23:18.9158751Z                 )
2025-04-11T04:23:18.9158826Z             else:
2025-04-11T04:23:18.9158924Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9159088Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9159182Z                     error_index=error_index,
2025-04-11T04:23:18.9159279Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9159368Z                     exit_code=exitcode,
2025-04-11T04:23:18.9159438Z                 )
2025-04-11T04:23:18.9159508Z     
2025-04-11T04:23:18.9159641Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9159807Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9159896Z         msg += original_trace
2025-04-11T04:23:18.9160068Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9160234Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9160305Z E       
2025-04-11T04:23:18.9160433Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9160585Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9160880Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9160964Z E           fn(i, *args)
2025-04-11T04:23:18.9161238Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
2025-04-11T04:23:18.9161415Z E           tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
2025-04-11T04:23:18.9161518Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9161802Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9161941Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9162098Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9162102Z 
2025-04-11T04:23:18.9162410Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9162560Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9162715Z [04/11/25 04:21:03] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9162910Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9163018Z                              :75 launch                                         
2025-04-11T04:23:18.9163152Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9163274Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9163507Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9163640Z ________________________________ test_comm_spec ________________________________
2025-04-11T04:23:18.9163644Z 
2025-04-11T04:23:18.9163740Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9164344Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9164351Z 
2025-04-11T04:23:18.9164502Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9164582Z         try_count = 0
2025-04-11T04:23:18.9164683Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9164761Z             max_try, int
2025-04-11T04:23:18.9164904Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9164978Z     
2025-04-11T04:23:18.9165090Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9165165Z             try:
2025-04-11T04:23:18.9165246Z                 try_count += 1
2025-04-11T04:23:18.9165334Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9165418Z                 return ret
2025-04-11T04:23:18.9165509Z             except exception_type as e:
2025-04-11T04:23:18.9165610Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9165794Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9165913Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9166055Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9166207Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9166290Z                     continue
2025-04-11T04:23:18.9166365Z                 else:
2025-04-11T04:23:18.9166590Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9166668Z >                   raise e
2025-04-11T04:23:18.9166672Z 
2025-04-11T04:23:18.9166765Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9166923Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9167055Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9167142Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9167316Z tests/test_tensor/test_dtensor/test_comm_spec.py:157: in test_comm_spec
2025-04-11T04:23:18.9167410Z     spawn(check_comm, world_size)
2025-04-11T04:23:18.9167509Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9167610Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9167867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9168039Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9168327Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9168415Z     while not context.join():
2025-04-11T04:23:18.9168530Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9168534Z 
2025-04-11T04:23:18.9168735Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a6f700>
2025-04-11T04:23:18.9168863Z timeout = None
2025-04-11T04:23:18.9168867Z 
2025-04-11T04:23:18.9168955Z     def join(self, timeout=None):
2025-04-11T04:23:18.9169079Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9169150Z     
2025-04-11T04:23:18.9169294Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9169439Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9169603Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9169742Z         of the first process exiting.
2025-04-11T04:23:18.9169811Z     
2025-04-11T04:23:18.9169956Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9170096Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9170165Z     
2025-04-11T04:23:18.9170239Z         Args:
2025-04-11T04:23:18.9170373Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9170447Z         """
2025-04-11T04:23:18.9170590Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9170725Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9170809Z             return True
2025-04-11T04:23:18.9170877Z     
2025-04-11T04:23:18.9171013Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9171129Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9171220Z             self.sentinels.keys(),
2025-04-11T04:23:18.9171304Z             timeout=timeout,
2025-04-11T04:23:18.9171375Z         )
2025-04-11T04:23:18.9171445Z     
2025-04-11T04:23:18.9171526Z         error_index = None
2025-04-11T04:23:18.9171609Z         for sentinel in ready:
2025-04-11T04:23:18.9171718Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9171814Z             process = self.processes[index]
2025-04-11T04:23:18.9171898Z             process.join()
2025-04-11T04:23:18.9171988Z             if process.exitcode != 0:
2025-04-11T04:23:18.9172075Z                 error_index = index
2025-04-11T04:23:18.9172151Z                 break
2025-04-11T04:23:18.9172221Z     
2025-04-11T04:23:18.9172313Z         # Return if there was no error.
2025-04-11T04:23:18.9172396Z         if error_index is None:
2025-04-11T04:23:18.9172529Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9172622Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9172692Z     
2025-04-11T04:23:18.9172834Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9172928Z         for process in self.processes:
2025-04-11T04:23:18.9173016Z             if process.is_alive():
2025-04-11T04:23:18.9173104Z                 process.terminate()
2025-04-11T04:23:18.9173233Z             process.join()
2025-04-11T04:23:18.9173306Z     
2025-04-11T04:23:18.9173445Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9173563Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9173670Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9173793Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9173874Z             if exitcode < 0:
2025-04-11T04:23:18.9173977Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9174083Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9174233Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9174329Z                     error_index=error_index,
2025-04-11T04:23:18.9174429Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9174513Z                     exit_code=exitcode,
2025-04-11T04:23:18.9174601Z                     signal_name=name,
2025-04-11T04:23:18.9174672Z                 )
2025-04-11T04:23:18.9174748Z             else:
2025-04-11T04:23:18.9174847Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9175065Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9175158Z                     error_index=error_index,
2025-04-11T04:23:18.9175255Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9175343Z                     exit_code=exitcode,
2025-04-11T04:23:18.9175414Z                 )
2025-04-11T04:23:18.9175485Z     
2025-04-11T04:23:18.9175615Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9175831Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9175918Z         msg += original_trace
2025-04-11T04:23:18.9176087Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9176250Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9176322Z E       
2025-04-11T04:23:18.9176449Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9176547Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9176958Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9177045Z E           fn(i, *args)
2025-04-11T04:23:18.9177312Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
2025-04-11T04:23:18.9177428Z E           check_all_gather(process_group_dict, rank)
2025-04-11T04:23:18.9177701Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
2025-04-11T04:23:18.9177822Z E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T04:23:18.9177926Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9178210Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9178345Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9178506Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9178510Z 
2025-04-11T04:23:18.9178819Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9178969Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9179123Z [04/11/25 04:21:09] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9179252Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9179359Z                              :75 launch                                         
2025-04-11T04:23:18.9179541Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9179666Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9179867Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9180012Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9180319Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9180608Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9180745Z _________________________________ test_dtensor _________________________________
2025-04-11T04:23:18.9180749Z 
2025-04-11T04:23:18.9180838Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9181471Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9181476Z 
2025-04-11T04:23:18.9181623Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9181705Z         try_count = 0
2025-04-11T04:23:18.9181807Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9181886Z             max_try, int
2025-04-11T04:23:18.9182035Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9182104Z     
2025-04-11T04:23:18.9182218Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9182290Z             try:
2025-04-11T04:23:18.9182421Z                 try_count += 1
2025-04-11T04:23:18.9182516Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9182595Z                 return ret
2025-04-11T04:23:18.9182690Z             except exception_type as e:
2025-04-11T04:23:18.9182790Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9182978Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9183091Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9183238Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9183438Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9183520Z                     continue
2025-04-11T04:23:18.9183599Z                 else:
2025-04-11T04:23:18.9183824Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9183907Z >                   raise e
2025-04-11T04:23:18.9183912Z 
2025-04-11T04:23:18.9184003Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9184112Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9184252Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9184336Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9184502Z tests/test_tensor/test_dtensor/test_dtensor.py:83: in test_dtensor
2025-04-11T04:23:18.9184597Z     spawn(check_dtensor, world_size)
2025-04-11T04:23:18.9184698Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9184797Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9185052Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9185231Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9185518Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9185608Z     while not context.join():
2025-04-11T04:23:18.9185714Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9185718Z 
2025-04-11T04:23:18.9185977Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581db6ec0>
2025-04-11T04:23:18.9186057Z timeout = None
2025-04-11T04:23:18.9186061Z 
2025-04-11T04:23:18.9186147Z     def join(self, timeout=None):
2025-04-11T04:23:18.9186277Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9186345Z     
2025-04-11T04:23:18.9186496Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9186638Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9186801Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9186891Z         of the first process exiting.
2025-04-11T04:23:18.9186959Z     
2025-04-11T04:23:18.9187109Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9187245Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9187315Z     
2025-04-11T04:23:18.9187386Z         Args:
2025-04-11T04:23:18.9187523Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9187596Z         """
2025-04-11T04:23:18.9187734Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9187876Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9187953Z             return True
2025-04-11T04:23:18.9188028Z     
2025-04-11T04:23:18.9188156Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9188271Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9188365Z             self.sentinels.keys(),
2025-04-11T04:23:18.9188474Z             timeout=timeout,
2025-04-11T04:23:18.9188602Z         )
2025-04-11T04:23:18.9188670Z     
2025-04-11T04:23:18.9188753Z         error_index = None
2025-04-11T04:23:18.9188842Z         for sentinel in ready:
2025-04-11T04:23:18.9188947Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9189049Z             process = self.processes[index]
2025-04-11T04:23:18.9189133Z             process.join()
2025-04-11T04:23:18.9189224Z             if process.exitcode != 0:
2025-04-11T04:23:18.9189313Z                 error_index = index
2025-04-11T04:23:18.9189387Z                 break
2025-04-11T04:23:18.9189459Z     
2025-04-11T04:23:18.9189550Z         # Return if there was no error.
2025-04-11T04:23:18.9189683Z         if error_index is None:
2025-04-11T04:23:18.9189819Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9189912Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9189983Z     
2025-04-11T04:23:18.9190120Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9190221Z         for process in self.processes:
2025-04-11T04:23:18.9190306Z             if process.is_alive():
2025-04-11T04:23:18.9190393Z                 process.terminate()
2025-04-11T04:23:18.9190477Z             process.join()
2025-04-11T04:23:18.9190545Z     
2025-04-11T04:23:18.9190687Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9190800Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9190909Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9191028Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9191109Z             if exitcode < 0:
2025-04-11T04:23:18.9191218Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9191320Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9191470Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9191563Z                     error_index=error_index,
2025-04-11T04:23:18.9191663Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9191751Z                     exit_code=exitcode,
2025-04-11T04:23:18.9191835Z                     signal_name=name,
2025-04-11T04:23:18.9191910Z                 )
2025-04-11T04:23:18.9191982Z             else:
2025-04-11T04:23:18.9192134Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9192301Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9192395Z                     error_index=error_index,
2025-04-11T04:23:18.9192497Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9192582Z                     exit_code=exitcode,
2025-04-11T04:23:18.9192656Z                 )
2025-04-11T04:23:18.9192724Z     
2025-04-11T04:23:18.9192854Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9193026Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9193114Z         msg += original_trace
2025-04-11T04:23:18.9193289Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9193448Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9193523Z E       
2025-04-11T04:23:18.9193649Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9193745Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9194043Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9194174Z E           fn(i, *args)
2025-04-11T04:23:18.9194444Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
2025-04-11T04:23:18.9194549Z E           test_model = TestModel(8, 8).to("cuda")
2025-04-11T04:23:18.9194815Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.9194961Z E           return self._apply(convert)
2025-04-11T04:23:18.9195228Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9195316Z E           module._apply(fn)
2025-04-11T04:23:18.9195580Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9195675Z E           param_applied = fn(param)
2025-04-11T04:23:18.9195951Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.9196218Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.9196325Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9196614Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9196750Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9196911Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9196915Z 
2025-04-11T04:23:18.9197227Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9197376Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9197533Z [04/11/25 04:21:13] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9197664Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9197774Z                              :75 launch                                         
2025-04-11T04:23:18.9197909Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9198035Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9198231Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9198366Z ____________________________ test_layout_converter _____________________________
2025-04-11T04:23:18.9198369Z 
2025-04-11T04:23:18.9198464Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9199096Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9199104Z 
2025-04-11T04:23:18.9199208Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9199287Z         try_count = 0
2025-04-11T04:23:18.9199388Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9199468Z             max_try, int
2025-04-11T04:23:18.9199618Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9199686Z     
2025-04-11T04:23:18.9199799Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9199876Z             try:
2025-04-11T04:23:18.9199961Z                 try_count += 1
2025-04-11T04:23:18.9200052Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9200130Z                 return ret
2025-04-11T04:23:18.9200221Z             except exception_type as e:
2025-04-11T04:23:18.9200324Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9200508Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9200676Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9200824Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9200981Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9201060Z                     continue
2025-04-11T04:23:18.9201134Z                 else:
2025-04-11T04:23:18.9201393Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9201472Z >                   raise e
2025-04-11T04:23:18.9201476Z 
2025-04-11T04:23:18.9201570Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9201683Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9201815Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9201899Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9202106Z tests/test_tensor/test_dtensor/test_layout_converter.py:180: in test_layout_converter
2025-04-11T04:23:18.9202272Z     spawn(check_layout_converting_apply, world_size)
2025-04-11T04:23:18.9202373Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9202473Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9202727Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9202907Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9203194Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9203279Z     while not context.join():
2025-04-11T04:23:18.9203393Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9203397Z 
2025-04-11T04:23:18.9203594Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b85701f0>
2025-04-11T04:23:18.9203677Z timeout = None
2025-04-11T04:23:18.9203681Z 
2025-04-11T04:23:18.9203769Z     def join(self, timeout=None):
2025-04-11T04:23:18.9203899Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9203969Z     
2025-04-11T04:23:18.9204113Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9204260Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9204422Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9204517Z         of the first process exiting.
2025-04-11T04:23:18.9204585Z     
2025-04-11T04:23:18.9204733Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9204915Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9204985Z     
2025-04-11T04:23:18.9205062Z         Args:
2025-04-11T04:23:18.9205199Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9205277Z         """
2025-04-11T04:23:18.9205414Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9205507Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9205589Z             return True
2025-04-11T04:23:18.9205657Z     
2025-04-11T04:23:18.9205790Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9205905Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9205998Z             self.sentinels.keys(),
2025-04-11T04:23:18.9206081Z             timeout=timeout,
2025-04-11T04:23:18.9206152Z         )
2025-04-11T04:23:18.9206224Z     
2025-04-11T04:23:18.9206304Z         error_index = None
2025-04-11T04:23:18.9206391Z         for sentinel in ready:
2025-04-11T04:23:18.9206497Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9206595Z             process = self.processes[index]
2025-04-11T04:23:18.9206679Z             process.join()
2025-04-11T04:23:18.9206769Z             if process.exitcode != 0:
2025-04-11T04:23:18.9206911Z                 error_index = index
2025-04-11T04:23:18.9206985Z                 break
2025-04-11T04:23:18.9207055Z     
2025-04-11T04:23:18.9207149Z         # Return if there was no error.
2025-04-11T04:23:18.9207232Z         if error_index is None:
2025-04-11T04:23:18.9207365Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9207457Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9207574Z     
2025-04-11T04:23:18.9207713Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9207809Z         for process in self.processes:
2025-04-11T04:23:18.9207899Z             if process.is_alive():
2025-04-11T04:23:18.9207986Z                 process.terminate()
2025-04-11T04:23:18.9208071Z             process.join()
2025-04-11T04:23:18.9208138Z     
2025-04-11T04:23:18.9208275Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9208395Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9208501Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9208677Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9208760Z             if exitcode < 0:
2025-04-11T04:23:18.9208869Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9208969Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9209116Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9209215Z                     error_index=error_index,
2025-04-11T04:23:18.9209314Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9209403Z                     exit_code=exitcode,
2025-04-11T04:23:18.9209488Z                     signal_name=name,
2025-04-11T04:23:18.9209560Z                 )
2025-04-11T04:23:18.9209634Z             else:
2025-04-11T04:23:18.9209732Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9209901Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9209992Z                     error_index=error_index,
2025-04-11T04:23:18.9210095Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9210179Z                     exit_code=exitcode,
2025-04-11T04:23:18.9210248Z                 )
2025-04-11T04:23:18.9210319Z     
2025-04-11T04:23:18.9210448Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9210618Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9210701Z         msg += original_trace
2025-04-11T04:23:18.9210869Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9211075Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9211149Z E       
2025-04-11T04:23:18.9211277Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9211376Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9211690Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9211771Z E           fn(i, *args)
2025-04-11T04:23:18.9212109Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
2025-04-11T04:23:18.9212236Z E           original_tensor = torch.rand(global_shape).cuda()
2025-04-11T04:23:18.9212339Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9212632Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9212769Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9212931Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9212935Z 
2025-04-11T04:23:18.9213245Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9213448Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9213602Z [04/11/25 04:21:19] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9213729Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9213839Z                              :75 launch                                         
2025-04-11T04:23:18.9214008Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9214134Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9214279Z [04/11/25 04:21:24] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9214407Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9214509Z                              :75 launch                                         
2025-04-11T04:23:18.9214685Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9214813Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9214953Z [04/11/25 04:21:28] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9215078Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9215181Z                              :75 launch                                         
2025-04-11T04:23:18.9215314Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9215436Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9215634Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9215777Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9216075Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9216364Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9216638Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:32601 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9216919Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:32601 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9217238Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24558 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9217515Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24558 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9217649Z ____________________________ test_chunk_manager[2] _____________________________
2025-04-11T04:23:18.9217653Z 
2025-04-11T04:23:18.9217773Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.9218379Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9218385Z 
2025-04-11T04:23:18.9218489Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9218566Z         try_count = 0
2025-04-11T04:23:18.9218665Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9218747Z             max_try, int
2025-04-11T04:23:18.9218894Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9218964Z     
2025-04-11T04:23:18.9219074Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9219150Z             try:
2025-04-11T04:23:18.9219282Z                 try_count += 1
2025-04-11T04:23:18.9219373Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9219461Z                 return ret
2025-04-11T04:23:18.9219553Z             except exception_type as e:
2025-04-11T04:23:18.9219655Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9219840Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9220018Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9220166Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9220320Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9220406Z                     continue
2025-04-11T04:23:18.9220481Z                 else:
2025-04-11T04:23:18.9220707Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9220788Z >                   raise e
2025-04-11T04:23:18.9220792Z 
2025-04-11T04:23:18.9220926Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9221040Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9221169Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9221257Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9221433Z tests/test_zero/test_gemini/test_chunk_mgrv2.py:60: in test_chunk_manager
2025-04-11T04:23:18.9221525Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9221626Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9221726Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9221990Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9222166Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9222454Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9222543Z     while not context.join():
2025-04-11T04:23:18.9222656Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9222660Z 
2025-04-11T04:23:18.9222854Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581d6cc70>
2025-04-11T04:23:18.9222930Z timeout = None
2025-04-11T04:23:18.9222938Z 
2025-04-11T04:23:18.9223031Z     def join(self, timeout=None):
2025-04-11T04:23:18.9223152Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9223223Z     
2025-04-11T04:23:18.9223369Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9223564Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9223728Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9223817Z         of the first process exiting.
2025-04-11T04:23:18.9223892Z     
2025-04-11T04:23:18.9224039Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9224176Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9224243Z     
2025-04-11T04:23:18.9224317Z         Args:
2025-04-11T04:23:18.9224454Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9224528Z         """
2025-04-11T04:23:18.9224669Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9224759Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9224839Z             return True
2025-04-11T04:23:18.9224908Z     
2025-04-11T04:23:18.9225037Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9225158Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9225247Z             self.sentinels.keys(),
2025-04-11T04:23:18.9225332Z             timeout=timeout,
2025-04-11T04:23:18.9225403Z         )
2025-04-11T04:23:18.9225518Z     
2025-04-11T04:23:18.9225604Z         error_index = None
2025-04-11T04:23:18.9225689Z         for sentinel in ready:
2025-04-11T04:23:18.9225801Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9225898Z             process = self.processes[index]
2025-04-11T04:23:18.9225984Z             process.join()
2025-04-11T04:23:18.9226074Z             if process.exitcode != 0:
2025-04-11T04:23:18.9226159Z                 error_index = index
2025-04-11T04:23:18.9226285Z                 break
2025-04-11T04:23:18.9226353Z     
2025-04-11T04:23:18.9226445Z         # Return if there was no error.
2025-04-11T04:23:18.9226527Z         if error_index is None:
2025-04-11T04:23:18.9226656Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9226755Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9226822Z     
2025-04-11T04:23:18.9226961Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9227057Z         for process in self.processes:
2025-04-11T04:23:18.9227145Z             if process.is_alive():
2025-04-11T04:23:18.9227277Z                 process.terminate()
2025-04-11T04:23:18.9227361Z             process.join()
2025-04-11T04:23:18.9227433Z     
2025-04-11T04:23:18.9227574Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9227689Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9227797Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9227916Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9228003Z             if exitcode < 0:
2025-04-11T04:23:18.9228107Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9228214Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9228361Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9228484Z                     error_index=error_index,
2025-04-11T04:23:18.9228587Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9228672Z                     exit_code=exitcode,
2025-04-11T04:23:18.9228761Z                     signal_name=name,
2025-04-11T04:23:18.9228832Z                 )
2025-04-11T04:23:18.9228907Z             else:
2025-04-11T04:23:18.9229008Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9229168Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9229263Z                     error_index=error_index,
2025-04-11T04:23:18.9229361Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9229448Z                     exit_code=exitcode,
2025-04-11T04:23:18.9229519Z                 )
2025-04-11T04:23:18.9229591Z     
2025-04-11T04:23:18.9229777Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9229948Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9230037Z         msg += original_trace
2025-04-11T04:23:18.9230206Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9230371Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9230442Z E       
2025-04-11T04:23:18.9230564Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9230665Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9230954Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9231038Z E           fn(i, *args)
2025-04-11T04:23:18.9231287Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
2025-04-11T04:23:18.9231375Z E           exam_chunk_memory()
2025-04-11T04:23:18.9231630Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9231718Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9232027Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9232112Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9232389Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
2025-04-11T04:23:18.9232492Z E           chunk_manager = ChunkManager(config)
2025-04-11T04:23:18.9232797Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T04:23:18.9233050Z E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.9233159Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9233442Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9233576Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9233741Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9233793Z 
2025-04-11T04:23:18.9234097Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9234251Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9234407Z [04/11/25 04:21:33] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9234544Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9234653Z                              :75 launch                                         
2025-04-11T04:23:18.9234798Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9234925Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9235125Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9235278Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9236162Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:18.9236271Z   return tensor.storage().size() == 0
2025-04-11T04:23:18.9236409Z ____________________________ test_chunk_function[1] ____________________________
2025-04-11T04:23:18.9236413Z 
2025-04-11T04:23:18.9236582Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T04:23:18.9237186Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9237193Z 
2025-04-11T04:23:18.9237300Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9237378Z         try_count = 0
2025-04-11T04:23:18.9237478Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9237557Z             max_try, int
2025-04-11T04:23:18.9237702Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9237776Z     
2025-04-11T04:23:18.9237887Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9237963Z             try:
2025-04-11T04:23:18.9238044Z                 try_count += 1
2025-04-11T04:23:18.9238134Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9238219Z                 return ret
2025-04-11T04:23:18.9238310Z             except exception_type as e:
2025-04-11T04:23:18.9238411Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9238648Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9238766Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9238909Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9239061Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9239144Z                     continue
2025-04-11T04:23:18.9239267Z                 else:
2025-04-11T04:23:18.9239489Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9239566Z >                   raise e
2025-04-11T04:23:18.9239571Z 
2025-04-11T04:23:18.9239667Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9239779Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9239909Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9240000Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9240168Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T04:23:18.9240298Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9240398Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9240498Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9240758Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9240938Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9241229Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9241316Z     while not context.join():
2025-04-11T04:23:18.9241429Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9241433Z 
2025-04-11T04:23:18.9241629Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c77700>
2025-04-11T04:23:18.9241711Z timeout = None
2025-04-11T04:23:18.9241715Z 
2025-04-11T04:23:18.9241802Z     def join(self, timeout=None):
2025-04-11T04:23:18.9241927Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9242001Z     
2025-04-11T04:23:18.9242144Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9242287Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9242452Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9242545Z         of the first process exiting.
2025-04-11T04:23:18.9242613Z     
2025-04-11T04:23:18.9242757Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9242944Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9243015Z     
2025-04-11T04:23:18.9243089Z         Args:
2025-04-11T04:23:18.9243227Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9243301Z         """
2025-04-11T04:23:18.9243445Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9243536Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9243622Z             return True
2025-04-11T04:23:18.9243690Z     
2025-04-11T04:23:18.9243821Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9243938Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9244027Z             self.sentinels.keys(),
2025-04-11T04:23:18.9244113Z             timeout=timeout,
2025-04-11T04:23:18.9244184Z         )
2025-04-11T04:23:18.9244255Z     
2025-04-11T04:23:18.9244336Z         error_index = None
2025-04-11T04:23:18.9244418Z         for sentinel in ready:
2025-04-11T04:23:18.9244527Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9244625Z             process = self.processes[index]
2025-04-11T04:23:18.9244712Z             process.join()
2025-04-11T04:23:18.9244853Z             if process.exitcode != 0:
2025-04-11T04:23:18.9244937Z                 error_index = index
2025-04-11T04:23:18.9245015Z                 break
2025-04-11T04:23:18.9245083Z     
2025-04-11T04:23:18.9245173Z         # Return if there was no error.
2025-04-11T04:23:18.9245254Z         if error_index is None:
2025-04-11T04:23:18.9245387Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9245481Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9245597Z     
2025-04-11T04:23:18.9245742Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9245834Z         for process in self.processes:
2025-04-11T04:23:18.9245923Z             if process.is_alive():
2025-04-11T04:23:18.9246013Z                 process.terminate()
2025-04-11T04:23:18.9246095Z             process.join()
2025-04-11T04:23:18.9246166Z     
2025-04-11T04:23:18.9246306Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9246427Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9246575Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9246698Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9246779Z             if exitcode < 0:
2025-04-11T04:23:18.9246880Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9246986Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9247134Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9247231Z                     error_index=error_index,
2025-04-11T04:23:18.9247328Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9247412Z                     exit_code=exitcode,
2025-04-11T04:23:18.9247501Z                     signal_name=name,
2025-04-11T04:23:18.9247572Z                 )
2025-04-11T04:23:18.9247648Z             else:
2025-04-11T04:23:18.9247746Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9247912Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9248004Z                     error_index=error_index,
2025-04-11T04:23:18.9248100Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9248187Z                     exit_code=exitcode,
2025-04-11T04:23:18.9248256Z                 )
2025-04-11T04:23:18.9248327Z     
2025-04-11T04:23:18.9248459Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9248629Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9248717Z         msg += original_trace
2025-04-11T04:23:18.9248887Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9249098Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9249171Z E       
2025-04-11T04:23:18.9249300Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9249397Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9249701Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9249783Z E           fn(i, *args)
2025-04-11T04:23:18.9250031Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:18.9250119Z E           exam_chunk_basic()
2025-04-11T04:23:18.9250375Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9250463Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9250716Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9250801Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9251049Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9251179Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9251284Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.9251543Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:18.9251629Z E           my_chunk = Chunk(
2025-04-11T04:23:18.9251862Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:18.9252111Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:18.9252219Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9252506Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9252644Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9252804Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9252809Z 
2025-04-11T04:23:18.9253162Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9253316Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9253473Z [04/11/25 04:21:37] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9253601Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9253709Z                              :75 launch                                         
2025-04-11T04:23:18.9253846Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9253970Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.9254168Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9254301Z ____________________________ test_chunk_function[2] ____________________________
2025-04-11T04:23:18.9254307Z 
2025-04-11T04:23:18.9254426Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.9255024Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9255029Z 
2025-04-11T04:23:18.9255136Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9255213Z         try_count = 0
2025-04-11T04:23:18.9255309Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9255392Z             max_try, int
2025-04-11T04:23:18.9255580Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9255656Z     
2025-04-11T04:23:18.9255767Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9255844Z             try:
2025-04-11T04:23:18.9255928Z                 try_count += 1
2025-04-11T04:23:18.9256018Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9256102Z                 return ret
2025-04-11T04:23:18.9256193Z             except exception_type as e:
2025-04-11T04:23:18.9256293Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9256475Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9256589Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9256736Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9256889Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9256973Z                     continue
2025-04-11T04:23:18.9257049Z                 else:
2025-04-11T04:23:18.9257276Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9257417Z >                   raise e
2025-04-11T04:23:18.9257421Z 
2025-04-11T04:23:18.9257515Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9257627Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9257755Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9257841Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9258011Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T04:23:18.9258148Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9258246Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9258343Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9258603Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9258779Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9259062Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9259152Z     while not context.join():
2025-04-11T04:23:18.9259305Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9259309Z 
2025-04-11T04:23:18.9259515Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5be2230>
2025-04-11T04:23:18.9259595Z timeout = None
2025-04-11T04:23:18.9259599Z 
2025-04-11T04:23:18.9259688Z     def join(self, timeout=None):
2025-04-11T04:23:18.9259817Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9259889Z     
2025-04-11T04:23:18.9260033Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9260177Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9260339Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9260430Z         of the first process exiting.
2025-04-11T04:23:18.9260501Z     
2025-04-11T04:23:18.9260650Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9260792Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9260860Z     
2025-04-11T04:23:18.9260933Z         Args:
2025-04-11T04:23:18.9261071Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9261143Z         """
2025-04-11T04:23:18.9261284Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9261376Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9261456Z             return True
2025-04-11T04:23:18.9261524Z     
2025-04-11T04:23:18.9261655Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9261823Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9261915Z             self.sentinels.keys(),
2025-04-11T04:23:18.9262004Z             timeout=timeout,
2025-04-11T04:23:18.9262075Z         )
2025-04-11T04:23:18.9262147Z     
2025-04-11T04:23:18.9262231Z         error_index = None
2025-04-11T04:23:18.9262314Z         for sentinel in ready:
2025-04-11T04:23:18.9262426Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9262523Z             process = self.processes[index]
2025-04-11T04:23:18.9262606Z             process.join()
2025-04-11T04:23:18.9262697Z             if process.exitcode != 0:
2025-04-11T04:23:18.9262782Z                 error_index = index
2025-04-11T04:23:18.9262862Z                 break
2025-04-11T04:23:18.9262930Z     
2025-04-11T04:23:18.9263024Z         # Return if there was no error.
2025-04-11T04:23:18.9263106Z         if error_index is None:
2025-04-11T04:23:18.9263238Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9263336Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9263405Z     
2025-04-11T04:23:18.9263547Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9263642Z         for process in self.processes:
2025-04-11T04:23:18.9263781Z             if process.is_alive():
2025-04-11T04:23:18.9263871Z                 process.terminate()
2025-04-11T04:23:18.9263955Z             process.join()
2025-04-11T04:23:18.9264026Z     
2025-04-11T04:23:18.9264163Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9264281Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9264384Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9264551Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9264635Z             if exitcode < 0:
2025-04-11T04:23:18.9264740Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9264846Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9264997Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9265095Z                     error_index=error_index,
2025-04-11T04:23:18.9265194Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9265284Z                     exit_code=exitcode,
2025-04-11T04:23:18.9265417Z                     signal_name=name,
2025-04-11T04:23:18.9265492Z                 )
2025-04-11T04:23:18.9265568Z             else:
2025-04-11T04:23:18.9265670Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9265833Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9265929Z                     error_index=error_index,
2025-04-11T04:23:18.9266024Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9266115Z                     exit_code=exitcode,
2025-04-11T04:23:18.9266184Z                 )
2025-04-11T04:23:18.9266256Z     
2025-04-11T04:23:18.9266391Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9266558Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9266647Z         msg += original_trace
2025-04-11T04:23:18.9266817Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9266978Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9267048Z E       
2025-04-11T04:23:18.9267175Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9267271Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9267565Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9267650Z E           fn(i, *args)
2025-04-11T04:23:18.9267900Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:18.9267987Z E           exam_chunk_basic()
2025-04-11T04:23:18.9268284Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9268375Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9268669Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9268758Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9269008Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9269091Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9269196Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.9269456Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:18.9269541Z E           my_chunk = Chunk(
2025-04-11T04:23:18.9269773Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:18.9269975Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:18.9270084Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9270422Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9270561Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9270718Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9270722Z 
2025-04-11T04:23:18.9271026Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9271226Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9271382Z [04/11/25 04:21:42] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9271510Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9271615Z                              :75 launch                                         
2025-04-11T04:23:18.9271755Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9271879Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9272124Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9272270Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9272567Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34935 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9272703Z ____________________________ test_chunk_function[4] ____________________________
2025-04-11T04:23:18.9272707Z 
2025-04-11T04:23:18.9272829Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9273431Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9273436Z 
2025-04-11T04:23:18.9273538Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9273621Z         try_count = 0
2025-04-11T04:23:18.9273719Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9273801Z             max_try, int
2025-04-11T04:23:18.9273946Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9274018Z     
2025-04-11T04:23:18.9274128Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9274202Z             try:
2025-04-11T04:23:18.9274288Z                 try_count += 1
2025-04-11T04:23:18.9274376Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9274458Z                 return ret
2025-04-11T04:23:18.9274599Z             except exception_type as e:
2025-04-11T04:23:18.9274699Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9274886Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9275003Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9275153Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9275307Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9275390Z                     continue
2025-04-11T04:23:18.9275466Z                 else:
2025-04-11T04:23:18.9275688Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9275773Z >                   raise e
2025-04-11T04:23:18.9275777Z 
2025-04-11T04:23:18.9275868Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9275984Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9276116Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9276202Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9276374Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T04:23:18.9276508Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9276611Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9276711Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9276967Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9277141Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9277546Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9277633Z     while not context.join():
2025-04-11T04:23:18.9277742Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9277751Z 
2025-04-11T04:23:18.9277950Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c77700>
2025-04-11T04:23:18.9278028Z timeout = None
2025-04-11T04:23:18.9278032Z 
2025-04-11T04:23:18.9278125Z     def join(self, timeout=None):
2025-04-11T04:23:18.9278247Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9278362Z     
2025-04-11T04:23:18.9278506Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9278646Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9278812Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9278907Z         of the first process exiting.
2025-04-11T04:23:18.9278980Z     
2025-04-11T04:23:18.9279124Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9279263Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9279333Z     
2025-04-11T04:23:18.9279406Z         Args:
2025-04-11T04:23:18.9279546Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9279617Z         """
2025-04-11T04:23:18.9279759Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9279850Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9279927Z             return True
2025-04-11T04:23:18.9279998Z     
2025-04-11T04:23:18.9280126Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9280246Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9280333Z             self.sentinels.keys(),
2025-04-11T04:23:18.9280419Z             timeout=timeout,
2025-04-11T04:23:18.9280489Z         )
2025-04-11T04:23:18.9280557Z     
2025-04-11T04:23:18.9280640Z         error_index = None
2025-04-11T04:23:18.9280722Z         for sentinel in ready:
2025-04-11T04:23:18.9280829Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9280984Z             process = self.processes[index]
2025-04-11T04:23:18.9281069Z             process.join()
2025-04-11T04:23:18.9281164Z             if process.exitcode != 0:
2025-04-11T04:23:18.9281251Z                 error_index = index
2025-04-11T04:23:18.9281325Z                 break
2025-04-11T04:23:18.9281392Z     
2025-04-11T04:23:18.9281482Z         # Return if there was no error.
2025-04-11T04:23:18.9281567Z         if error_index is None:
2025-04-11T04:23:18.9281697Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9281793Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9281860Z     
2025-04-11T04:23:18.9282000Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9282097Z         for process in self.processes:
2025-04-11T04:23:18.9282183Z             if process.is_alive():
2025-04-11T04:23:18.9282274Z                 process.terminate()
2025-04-11T04:23:18.9282355Z             process.join()
2025-04-11T04:23:18.9282426Z     
2025-04-11T04:23:18.9282564Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9282676Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9282848Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9282968Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9283052Z             if exitcode < 0:
2025-04-11T04:23:18.9283155Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9283262Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9283408Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9283546Z                     error_index=error_index,
2025-04-11T04:23:18.9283647Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9283733Z                     exit_code=exitcode,
2025-04-11T04:23:18.9283820Z                     signal_name=name,
2025-04-11T04:23:18.9283894Z                 )
2025-04-11T04:23:18.9283966Z             else:
2025-04-11T04:23:18.9284068Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9284227Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9284324Z                     error_index=error_index,
2025-04-11T04:23:18.9284464Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9284555Z                     exit_code=exitcode,
2025-04-11T04:23:18.9284625Z                 )
2025-04-11T04:23:18.9284694Z     
2025-04-11T04:23:18.9284826Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9284993Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9285084Z         msg += original_trace
2025-04-11T04:23:18.9285252Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9285410Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9285483Z E       
2025-04-11T04:23:18.9285607Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9285707Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9286005Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9286089Z E           fn(i, *args)
2025-04-11T04:23:18.9286338Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:18.9286421Z E           exam_chunk_basic()
2025-04-11T04:23:18.9286678Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9286766Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9287016Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9287101Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9287401Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9287489Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9287597Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.9287857Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:18.9287940Z E           my_chunk = Chunk(
2025-04-11T04:23:18.9288177Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:18.9288379Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:18.9288492Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9288774Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9288913Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9289072Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9289076Z 
2025-04-11T04:23:18.9289378Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9289580Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9289737Z [04/11/25 04:21:48] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9289868Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9289973Z                              :75 launch                                         
2025-04-11T04:23:18.9290143Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9290265Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9290465Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9290607Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9290901Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38965 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9291240Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38965 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9291380Z ____________________________ test_grad_accumulation ____________________________
2025-04-11T04:23:18.9291384Z 
2025-04-11T04:23:18.9291479Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9292079Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9292083Z 
2025-04-11T04:23:18.9292189Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9292267Z         try_count = 0
2025-04-11T04:23:18.9292366Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9292447Z             max_try, int
2025-04-11T04:23:18.9292591Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9292665Z     
2025-04-11T04:23:18.9292775Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9292852Z             try:
2025-04-11T04:23:18.9292934Z                 try_count += 1
2025-04-11T04:23:18.9293025Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9293101Z                 return ret
2025-04-11T04:23:18.9293195Z             except exception_type as e:
2025-04-11T04:23:18.9293295Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9293480Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9293647Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9293796Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9293951Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9294036Z                     continue
2025-04-11T04:23:18.9294112Z                 else:
2025-04-11T04:23:18.9294337Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9294414Z >                   raise e
2025-04-11T04:23:18.9294418Z 
2025-04-11T04:23:18.9294512Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9294624Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9294756Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9294842Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9295030Z tests/test_zero/test_gemini/test_grad_accum.py:158: in test_grad_accumulation
2025-04-11T04:23:18.9295116Z     spawn(run_dist, 2)
2025-04-11T04:23:18.9295215Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9295315Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9295627Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9295804Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9296094Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9296181Z     while not context.join():
2025-04-11T04:23:18.9296290Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9296339Z 
2025-04-11T04:23:18.9296544Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76290>
2025-04-11T04:23:18.9296624Z timeout = None
2025-04-11T04:23:18.9296628Z 
2025-04-11T04:23:18.9296716Z     def join(self, timeout=None):
2025-04-11T04:23:18.9296845Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9296914Z     
2025-04-11T04:23:18.9297057Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9297203Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9297405Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9297501Z         of the first process exiting.
2025-04-11T04:23:18.9297570Z     
2025-04-11T04:23:18.9297714Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9297851Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9297923Z     
2025-04-11T04:23:18.9297997Z         Args:
2025-04-11T04:23:18.9298136Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9298210Z         """
2025-04-11T04:23:18.9298350Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9298440Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9298522Z             return True
2025-04-11T04:23:18.9298589Z     
2025-04-11T04:23:18.9298723Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9298839Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9298931Z             self.sentinels.keys(),
2025-04-11T04:23:18.9299017Z             timeout=timeout,
2025-04-11T04:23:18.9299087Z         )
2025-04-11T04:23:18.9299159Z     
2025-04-11T04:23:18.9299239Z         error_index = None
2025-04-11T04:23:18.9299323Z         for sentinel in ready:
2025-04-11T04:23:18.9299429Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9299527Z             process = self.processes[index]
2025-04-11T04:23:18.9299612Z             process.join()
2025-04-11T04:23:18.9299703Z             if process.exitcode != 0:
2025-04-11T04:23:18.9299794Z                 error_index = index
2025-04-11T04:23:18.9299916Z                 break
2025-04-11T04:23:18.9299986Z     
2025-04-11T04:23:18.9300083Z         # Return if there was no error.
2025-04-11T04:23:18.9300167Z         if error_index is None:
2025-04-11T04:23:18.9300307Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9300401Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9300474Z     
2025-04-11T04:23:18.9300619Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9300716Z         for process in self.processes:
2025-04-11T04:23:18.9300806Z             if process.is_alive():
2025-04-11T04:23:18.9300897Z                 process.terminate()
2025-04-11T04:23:18.9300985Z             process.join()
2025-04-11T04:23:18.9301053Z     
2025-04-11T04:23:18.9301191Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9301310Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9301418Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9301540Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9301620Z             if exitcode < 0:
2025-04-11T04:23:18.9301725Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9301881Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9302033Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9302131Z                     error_index=error_index,
2025-04-11T04:23:18.9302230Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9302318Z                     exit_code=exitcode,
2025-04-11T04:23:18.9302401Z                     signal_name=name,
2025-04-11T04:23:18.9302520Z                 )
2025-04-11T04:23:18.9302598Z             else:
2025-04-11T04:23:18.9302700Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9302866Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9302961Z                     error_index=error_index,
2025-04-11T04:23:18.9303058Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9303144Z                     exit_code=exitcode,
2025-04-11T04:23:18.9303217Z                 )
2025-04-11T04:23:18.9303287Z     
2025-04-11T04:23:18.9303485Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9303657Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9303742Z         msg += original_trace
2025-04-11T04:23:18.9303916Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9304078Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9304151Z E       
2025-04-11T04:23:18.9304276Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9304373Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9304678Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9304757Z E           fn(i, *args)
2025-04-11T04:23:18.9305010Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
2025-04-11T04:23:18.9305101Z E           exam_gemini_grad_acc()
2025-04-11T04:23:18.9305355Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9305446Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9305693Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9305783Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9306029Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9306112Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9306221Z E         [Previous line repeated 4 more times]
2025-04-11T04:23:18.9306546Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
2025-04-11T04:23:18.9306654Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9306949Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9307052Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9307316Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9307436Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9307715Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9307799Z E           module._apply(fn)
2025-04-11T04:23:18.9308072Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9308156Z E           module._apply(fn)
2025-04-11T04:23:18.9308457Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9308607Z E           param_applied = fn(param)
2025-04-11T04:23:18.9308882Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9309001Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9309105Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9309393Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9309578Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9309740Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9309744Z 
2025-04-11T04:23:18.9310046Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9310199Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9310357Z [04/11/25 04:21:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9310533Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9310644Z                              :75 launch                                         
2025-04-11T04:23:18.9310782Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9310910Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9311111Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9311261Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9312422Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9312602Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9313727Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9313953Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9314655Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9314744Z   warnings.warn(
2025-04-11T04:23:18.9315406Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9315491Z   warnings.warn(
2025-04-11T04:23:18.9316306Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9316389Z   warnings.warn(
2025-04-11T04:23:18.9317192Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9317316Z   warnings.warn(
2025-04-11T04:23:18.9318119Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9318244Z   warnings.warn(
2025-04-11T04:23:18.9319060Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9319139Z   warnings.warn(
2025-04-11T04:23:18.9320007Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9320086Z   warnings.warn(
2025-04-11T04:23:18.9320871Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9320953Z   warnings.warn(
2025-04-11T04:23:18.9321840Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:18.9321941Z   return tensor.storage().size() == 0
2025-04-11T04:23:18.9322892Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9323062Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9323247Z ______________________________ test_grad_clip[1] _______________________________
2025-04-11T04:23:18.9323252Z 
2025-04-11T04:23:18.9323370Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T04:23:18.9323979Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9323985Z 
2025-04-11T04:23:18.9324087Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9324169Z         try_count = 0
2025-04-11T04:23:18.9324268Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9324346Z             max_try, int
2025-04-11T04:23:18.9324495Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9324565Z     
2025-04-11T04:23:18.9324677Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9324749Z             try:
2025-04-11T04:23:18.9324832Z                 try_count += 1
2025-04-11T04:23:18.9324927Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9325007Z                 return ret
2025-04-11T04:23:18.9325103Z             except exception_type as e:
2025-04-11T04:23:18.9325251Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9325445Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9325561Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9325704Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9325860Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9325988Z                     continue
2025-04-11T04:23:18.9326067Z                 else:
2025-04-11T04:23:18.9326293Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9326375Z >                   raise e
2025-04-11T04:23:18.9326379Z 
2025-04-11T04:23:18.9326471Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9326580Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9326717Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9326844Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9327014Z tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
2025-04-11T04:23:18.9327100Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9327201Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9327297Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9327554Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9327732Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9328016Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9328106Z     while not context.join():
2025-04-11T04:23:18.9328213Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9328219Z 
2025-04-11T04:23:18.9328421Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c77b20>
2025-04-11T04:23:18.9328499Z timeout = None
2025-04-11T04:23:18.9328503Z 
2025-04-11T04:23:18.9328590Z     def join(self, timeout=None):
2025-04-11T04:23:18.9328717Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9328786Z     
2025-04-11T04:23:18.9328931Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9329074Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9329240Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9329329Z         of the first process exiting.
2025-04-11T04:23:18.9329398Z     
2025-04-11T04:23:18.9329591Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9329728Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9329800Z     
2025-04-11T04:23:18.9329874Z         Args:
2025-04-11T04:23:18.9330012Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9330089Z         """
2025-04-11T04:23:18.9330226Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9330322Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9330401Z             return True
2025-04-11T04:23:18.9330473Z     
2025-04-11T04:23:18.9330602Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9330722Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9330816Z             self.sentinels.keys(),
2025-04-11T04:23:18.9330897Z             timeout=timeout,
2025-04-11T04:23:18.9330970Z         )
2025-04-11T04:23:18.9331039Z     
2025-04-11T04:23:18.9331121Z         error_index = None
2025-04-11T04:23:18.9331210Z         for sentinel in ready:
2025-04-11T04:23:18.9331317Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9331465Z             process = self.processes[index]
2025-04-11T04:23:18.9331547Z             process.join()
2025-04-11T04:23:18.9331642Z             if process.exitcode != 0:
2025-04-11T04:23:18.9331728Z                 error_index = index
2025-04-11T04:23:18.9331803Z                 break
2025-04-11T04:23:18.9331874Z     
2025-04-11T04:23:18.9331963Z         # Return if there was no error.
2025-04-11T04:23:18.9332047Z         if error_index is None:
2025-04-11T04:23:18.9332178Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9332328Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9332400Z     
2025-04-11T04:23:18.9332540Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9332639Z         for process in self.processes:
2025-04-11T04:23:18.9332727Z             if process.is_alive():
2025-04-11T04:23:18.9332817Z                 process.terminate()
2025-04-11T04:23:18.9332902Z             process.join()
2025-04-11T04:23:18.9332971Z     
2025-04-11T04:23:18.9333113Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9333273Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9333385Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9333505Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9333585Z             if exitcode < 0:
2025-04-11T04:23:18.9333691Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9333799Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9333951Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9334046Z                     error_index=error_index,
2025-04-11T04:23:18.9334148Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9334236Z                     exit_code=exitcode,
2025-04-11T04:23:18.9334319Z                     signal_name=name,
2025-04-11T04:23:18.9334394Z                 )
2025-04-11T04:23:18.9334467Z             else:
2025-04-11T04:23:18.9334571Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9334737Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9334829Z                     error_index=error_index,
2025-04-11T04:23:18.9334931Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9335016Z                     exit_code=exitcode,
2025-04-11T04:23:18.9335090Z                 )
2025-04-11T04:23:18.9335158Z     
2025-04-11T04:23:18.9335288Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9335463Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9335546Z         msg += original_trace
2025-04-11T04:23:18.9335772Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9335935Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9336015Z E       
2025-04-11T04:23:18.9336144Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9336247Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9336553Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9336636Z E           fn(i, *args)
2025-04-11T04:23:18.9336899Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T04:23:18.9336993Z E           exam_grad_clipping()
2025-04-11T04:23:18.9337256Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9337348Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9337603Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9337696Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9337992Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9338082Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9338187Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.9338462Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T04:23:18.9338563Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9338894Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9338996Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9339265Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9339384Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9339653Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9339743Z E           module._apply(fn)
2025-04-11T04:23:18.9340051Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9340139Z E           module._apply(fn)
2025-04-11T04:23:18.9340401Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9340497Z E           param_applied = fn(param)
2025-04-11T04:23:18.9340770Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9340884Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9340996Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9341285Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9341424Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9341586Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9341591Z 
2025-04-11T04:23:18.9341895Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9342044Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9342199Z [04/11/25 04:22:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9342330Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9342434Z                              :75 launch                                         
2025-04-11T04:23:18.9342617Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9342744Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.9342944Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9343088Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9344215Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9344385Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9345071Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9345199Z   warnings.warn(
2025-04-11T04:23:18.9346007Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9346090Z   warnings.warn(
2025-04-11T04:23:18.9346930Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9347010Z   warnings.warn(
2025-04-11T04:23:18.9347835Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9347921Z   warnings.warn(
2025-04-11T04:23:18.9348056Z ______________________________ test_grad_clip[2] _______________________________
2025-04-11T04:23:18.9348060Z 
2025-04-11T04:23:18.9348181Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T04:23:18.9348826Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9348832Z 
2025-04-11T04:23:18.9348937Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9349016Z         try_count = 0
2025-04-11T04:23:18.9349118Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9349197Z             max_try, int
2025-04-11T04:23:18.9349345Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9349417Z     
2025-04-11T04:23:18.9349529Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9349606Z             try:
2025-04-11T04:23:18.9349688Z                 try_count += 1
2025-04-11T04:23:18.9349778Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9349867Z                 return ret
2025-04-11T04:23:18.9349960Z             except exception_type as e:
2025-04-11T04:23:18.9350063Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9350251Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9350465Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9350613Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9350769Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9350857Z                     continue
2025-04-11T04:23:18.9350932Z                 else:
2025-04-11T04:23:18.9351158Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9351235Z >                   raise e
2025-04-11T04:23:18.9351239Z 
2025-04-11T04:23:18.9351334Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9351446Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9351581Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9351669Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9351831Z tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
2025-04-11T04:23:18.9351924Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9352023Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9352124Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9352378Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9352611Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9352896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9352983Z     while not context.join():
2025-04-11T04:23:18.9353094Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9353147Z 
2025-04-11T04:23:18.9353346Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b841a9b0>
2025-04-11T04:23:18.9353425Z timeout = None
2025-04-11T04:23:18.9353429Z 
2025-04-11T04:23:18.9353516Z     def join(self, timeout=None):
2025-04-11T04:23:18.9353640Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9353711Z     
2025-04-11T04:23:18.9353852Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9353997Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9354205Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9354299Z         of the first process exiting.
2025-04-11T04:23:18.9354368Z     
2025-04-11T04:23:18.9354512Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9354648Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9354718Z     
2025-04-11T04:23:18.9354793Z         Args:
2025-04-11T04:23:18.9354929Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9355000Z         """
2025-04-11T04:23:18.9355139Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9355230Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9355311Z             return True
2025-04-11T04:23:18.9355378Z     
2025-04-11T04:23:18.9355510Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9355630Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9355720Z             self.sentinels.keys(),
2025-04-11T04:23:18.9355805Z             timeout=timeout,
2025-04-11T04:23:18.9355875Z         )
2025-04-11T04:23:18.9355945Z     
2025-04-11T04:23:18.9356025Z         error_index = None
2025-04-11T04:23:18.9356107Z         for sentinel in ready:
2025-04-11T04:23:18.9356216Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9356314Z             process = self.processes[index]
2025-04-11T04:23:18.9356400Z             process.join()
2025-04-11T04:23:18.9356491Z             if process.exitcode != 0:
2025-04-11T04:23:18.9356576Z                 error_index = index
2025-04-11T04:23:18.9356653Z                 break
2025-04-11T04:23:18.9356767Z     
2025-04-11T04:23:18.9356863Z         # Return if there was no error.
2025-04-11T04:23:18.9356945Z         if error_index is None:
2025-04-11T04:23:18.9357081Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9357177Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9357245Z     
2025-04-11T04:23:18.9357390Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9357483Z         for process in self.processes:
2025-04-11T04:23:18.9357570Z             if process.is_alive():
2025-04-11T04:23:18.9357659Z                 process.terminate()
2025-04-11T04:23:18.9357740Z             process.join()
2025-04-11T04:23:18.9357813Z     
2025-04-11T04:23:18.9357952Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9358068Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9358173Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9358295Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9358375Z             if exitcode < 0:
2025-04-11T04:23:18.9358478Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9358650Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9358801Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9358898Z                     error_index=error_index,
2025-04-11T04:23:18.9358999Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9359083Z                     exit_code=exitcode,
2025-04-11T04:23:18.9359171Z                     signal_name=name,
2025-04-11T04:23:18.9359289Z                 )
2025-04-11T04:23:18.9359364Z             else:
2025-04-11T04:23:18.9359462Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9359626Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9359720Z                     error_index=error_index,
2025-04-11T04:23:18.9359816Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9359905Z                     exit_code=exitcode,
2025-04-11T04:23:18.9359976Z                 )
2025-04-11T04:23:18.9360050Z     
2025-04-11T04:23:18.9360183Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9360395Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9360486Z         msg += original_trace
2025-04-11T04:23:18.9360657Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9360816Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9360889Z E       
2025-04-11T04:23:18.9361017Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9361115Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9361418Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9361499Z E           fn(i, *args)
2025-04-11T04:23:18.9361753Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T04:23:18.9361848Z E           exam_grad_clipping()
2025-04-11T04:23:18.9362102Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9362191Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9362439Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9362525Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9362775Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9362858Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9362964Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.9363288Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T04:23:18.9363393Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9363679Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9363780Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9364046Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9364160Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9364430Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9364517Z E           module._apply(fn)
2025-04-11T04:23:18.9364782Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9364866Z E           module._apply(fn)
2025-04-11T04:23:18.9365130Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9365221Z E           param_applied = fn(param)
2025-04-11T04:23:18.9365542Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9365660Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9365763Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9366047Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9366229Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9366392Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9366396Z 
2025-04-11T04:23:18.9366696Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9366846Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9367000Z [04/11/25 04:22:07] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9367174Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9367286Z                              :75 launch                                         
2025-04-11T04:23:18.9367421Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9367544Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9367740Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9367884Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9369006Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9369178Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9370277Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9370488Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9371186Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9371273Z   warnings.warn(
2025-04-11T04:23:18.9371962Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9372045Z   warnings.warn(
2025-04-11T04:23:18.9372867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9372944Z   warnings.warn(
2025-04-11T04:23:18.9373752Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9373877Z   warnings.warn(
2025-04-11T04:23:18.9374679Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9374801Z   warnings.warn(
2025-04-11T04:23:18.9375611Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9375686Z   warnings.warn(
2025-04-11T04:23:18.9376542Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9376619Z   warnings.warn(
2025-04-11T04:23:18.9377416Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9377495Z   warnings.warn(
2025-04-11T04:23:18.9378449Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9378622Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9378759Z ______________________________ test_inference[1] _______________________________
2025-04-11T04:23:18.9378763Z 
2025-04-11T04:23:18.9378881Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T04:23:18.9379479Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9379484Z 
2025-04-11T04:23:18.9379633Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9379719Z         try_count = 0
2025-04-11T04:23:18.9379818Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9379897Z             max_try, int
2025-04-11T04:23:18.9380047Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9380116Z     
2025-04-11T04:23:18.9380233Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9380304Z             try:
2025-04-11T04:23:18.9380386Z                 try_count += 1
2025-04-11T04:23:18.9380480Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9380558Z                 return ret
2025-04-11T04:23:18.9380656Z             except exception_type as e:
2025-04-11T04:23:18.9380751Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9380938Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9381055Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9381199Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9381357Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9381487Z                     continue
2025-04-11T04:23:18.9381566Z                 else:
2025-04-11T04:23:18.9381794Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9381873Z >                   raise e
2025-04-11T04:23:18.9381877Z 
2025-04-11T04:23:18.9381967Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9382077Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9382258Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9382344Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9382511Z tests/test_zero/test_gemini/test_inference.py:118: in test_inference
2025-04-11T04:23:18.9382599Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9382696Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9382797Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9383052Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9383275Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9383560Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9383649Z     while not context.join():
2025-04-11T04:23:18.9383759Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9383765Z 
2025-04-11T04:23:18.9383966Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c74af0>
2025-04-11T04:23:18.9384042Z timeout = None
2025-04-11T04:23:18.9384046Z 
2025-04-11T04:23:18.9384134Z     def join(self, timeout=None):
2025-04-11T04:23:18.9384263Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9384331Z     
2025-04-11T04:23:18.9384475Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9384619Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9384787Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9384878Z         of the first process exiting.
2025-04-11T04:23:18.9384945Z     
2025-04-11T04:23:18.9385092Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9385229Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9385301Z     
2025-04-11T04:23:18.9385373Z         Args:
2025-04-11T04:23:18.9385510Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9385584Z         """
2025-04-11T04:23:18.9385723Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9385874Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9385954Z             return True
2025-04-11T04:23:18.9386026Z     
2025-04-11T04:23:18.9386158Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9386276Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9386372Z             self.sentinels.keys(),
2025-04-11T04:23:18.9386454Z             timeout=timeout,
2025-04-11T04:23:18.9386528Z         )
2025-04-11T04:23:18.9386595Z     
2025-04-11T04:23:18.9386676Z         error_index = None
2025-04-11T04:23:18.9386763Z         for sentinel in ready:
2025-04-11T04:23:18.9386868Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9386970Z             process = self.processes[index]
2025-04-11T04:23:18.9387052Z             process.join()
2025-04-11T04:23:18.9387143Z             if process.exitcode != 0:
2025-04-11T04:23:18.9387231Z                 error_index = index
2025-04-11T04:23:18.9387303Z                 break
2025-04-11T04:23:18.9387375Z     
2025-04-11T04:23:18.9387464Z         # Return if there was no error.
2025-04-11T04:23:18.9387546Z         if error_index is None:
2025-04-11T04:23:18.9387680Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9387892Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9387965Z     
2025-04-11T04:23:18.9388104Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9388200Z         for process in self.processes:
2025-04-11T04:23:18.9388286Z             if process.is_alive():
2025-04-11T04:23:18.9388375Z                 process.terminate()
2025-04-11T04:23:18.9388493Z             process.join()
2025-04-11T04:23:18.9388612Z     
2025-04-11T04:23:18.9388753Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9388867Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9388972Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9389097Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9389178Z             if exitcode < 0:
2025-04-11T04:23:18.9389286Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9389392Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9389591Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9389688Z                     error_index=error_index,
2025-04-11T04:23:18.9389787Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9389878Z                     exit_code=exitcode,
2025-04-11T04:23:18.9389962Z                     signal_name=name,
2025-04-11T04:23:18.9390041Z                 )
2025-04-11T04:23:18.9390112Z             else:
2025-04-11T04:23:18.9390216Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9390378Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9390470Z                     error_index=error_index,
2025-04-11T04:23:18.9390570Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9390657Z                     exit_code=exitcode,
2025-04-11T04:23:18.9390732Z                 )
2025-04-11T04:23:18.9390799Z     
2025-04-11T04:23:18.9390930Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9391107Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9391191Z         msg += original_trace
2025-04-11T04:23:18.9391364Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9391524Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9391601Z E       
2025-04-11T04:23:18.9391725Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9391820Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9392169Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9392252Z E           fn(i, *args)
2025-04-11T04:23:18.9392505Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T04:23:18.9392590Z E           exam_inference()
2025-04-11T04:23:18.9392849Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9392936Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9393180Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9393272Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9393517Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9393602Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9393859Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T04:23:18.9393963Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9394246Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9394394Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9394664Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9394781Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9395051Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9395182Z E           module._apply(fn)
2025-04-11T04:23:18.9395451Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9395534Z E           module._apply(fn)
2025-04-11T04:23:18.9395805Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9395901Z E           param_applied = fn(param)
2025-04-11T04:23:18.9396175Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9396340Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9396449Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9396732Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9396863Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9397026Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9397031Z 
2025-04-11T04:23:18.9397329Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9397478Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9397631Z [04/11/25 04:22:14] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9397758Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9397867Z                              :75 launch                                         
2025-04-11T04:23:18.9398002Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9398126Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.9398322Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9398468Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9399626Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9399804Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9400488Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9400573Z   warnings.warn(
2025-04-11T04:23:18.9401393Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9401473Z   warnings.warn(
2025-04-11T04:23:18.9402287Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9402410Z   warnings.warn(
2025-04-11T04:23:18.9403217Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9403339Z   warnings.warn(
2025-04-11T04:23:18.9403476Z ______________________________ test_inference[4] _______________________________
2025-04-11T04:23:18.9403482Z 
2025-04-11T04:23:18.9403599Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9404244Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9404251Z 
2025-04-11T04:23:18.9404353Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9404435Z         try_count = 0
2025-04-11T04:23:18.9404534Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9404613Z             max_try, int
2025-04-11T04:23:18.9404760Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9404832Z     
2025-04-11T04:23:18.9404943Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9405014Z             try:
2025-04-11T04:23:18.9405099Z                 try_count += 1
2025-04-11T04:23:18.9405190Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9405267Z                 return ret
2025-04-11T04:23:18.9405362Z             except exception_type as e:
2025-04-11T04:23:18.9405458Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9405646Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9405762Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9405906Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9406064Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9406146Z                     continue
2025-04-11T04:23:18.9406223Z                 else:
2025-04-11T04:23:18.9406444Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9406525Z >                   raise e
2025-04-11T04:23:18.9406529Z 
2025-04-11T04:23:18.9406667Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9406782Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9406914Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9407001Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9407170Z tests/test_zero/test_gemini/test_inference.py:118: in test_inference
2025-04-11T04:23:18.9407257Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9407358Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9407454Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9407703Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9407882Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9408161Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9408253Z     while not context.join():
2025-04-11T04:23:18.9408359Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9408362Z 
2025-04-11T04:23:18.9408566Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8490bb0>
2025-04-11T04:23:18.9408689Z timeout = None
2025-04-11T04:23:18.9408693Z 
2025-04-11T04:23:18.9408790Z     def join(self, timeout=None):
2025-04-11T04:23:18.9408913Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9408981Z     
2025-04-11T04:23:18.9409126Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9409268Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9409481Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9409573Z         of the first process exiting.
2025-04-11T04:23:18.9409642Z     
2025-04-11T04:23:18.9409792Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9409928Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9410000Z     
2025-04-11T04:23:18.9410073Z         Args:
2025-04-11T04:23:18.9410215Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9410286Z         """
2025-04-11T04:23:18.9410481Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9410579Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9410656Z             return True
2025-04-11T04:23:18.9410727Z     
2025-04-11T04:23:18.9410856Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9410973Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9411065Z             self.sentinels.keys(),
2025-04-11T04:23:18.9411146Z             timeout=timeout,
2025-04-11T04:23:18.9411219Z         )
2025-04-11T04:23:18.9411286Z     
2025-04-11T04:23:18.9411369Z         error_index = None
2025-04-11T04:23:18.9411456Z         for sentinel in ready:
2025-04-11T04:23:18.9411558Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9411659Z             process = self.processes[index]
2025-04-11T04:23:18.9411743Z             process.join()
2025-04-11T04:23:18.9411834Z             if process.exitcode != 0:
2025-04-11T04:23:18.9411921Z                 error_index = index
2025-04-11T04:23:18.9411994Z                 break
2025-04-11T04:23:18.9412064Z     
2025-04-11T04:23:18.9412153Z         # Return if there was no error.
2025-04-11T04:23:18.9412238Z         if error_index is None:
2025-04-11T04:23:18.9412368Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9412464Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9412534Z     
2025-04-11T04:23:18.9412669Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9412765Z         for process in self.processes:
2025-04-11T04:23:18.9412900Z             if process.is_alive():
2025-04-11T04:23:18.9412994Z                 process.terminate()
2025-04-11T04:23:18.9413076Z             process.join()
2025-04-11T04:23:18.9413143Z     
2025-04-11T04:23:18.9413285Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9413400Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9413509Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9413629Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9413710Z             if exitcode < 0:
2025-04-11T04:23:18.9413818Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9413921Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9414073Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9414166Z                     error_index=error_index,
2025-04-11T04:23:18.9414268Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9414356Z                     exit_code=exitcode,
2025-04-11T04:23:18.9414438Z                     signal_name=name,
2025-04-11T04:23:18.9414513Z                 )
2025-04-11T04:23:18.9414585Z             else:
2025-04-11T04:23:18.9414741Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9414909Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9415003Z                     error_index=error_index,
2025-04-11T04:23:18.9415108Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9415196Z                     exit_code=exitcode,
2025-04-11T04:23:18.9415273Z                 )
2025-04-11T04:23:18.9415345Z     
2025-04-11T04:23:18.9415531Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9415698Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9415781Z         msg += original_trace
2025-04-11T04:23:18.9415960Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9416122Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9416197Z E       
2025-04-11T04:23:18.9416320Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9416422Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9416783Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9416865Z E           fn(i, *args)
2025-04-11T04:23:18.9417121Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T04:23:18.9417207Z E           exam_inference()
2025-04-11T04:23:18.9417468Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9417554Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9417809Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9417895Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9418141Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9418230Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9418488Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T04:23:18.9418591Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9418874Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9418977Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9419241Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9419355Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9419681Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9419771Z E           module._apply(fn)
2025-04-11T04:23:18.9420040Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9420123Z E           module._apply(fn)
2025-04-11T04:23:18.9420386Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9420478Z E           param_applied = fn(param)
2025-04-11T04:23:18.9420746Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9420862Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9420968Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9421255Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9421388Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9421553Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9421611Z 
2025-04-11T04:23:18.9421915Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9422067Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9422219Z [04/11/25 04:22:24] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9422343Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9422502Z                              :75 launch                                         
2025-04-11T04:23:18.9422638Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9422765Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9422962Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9423105Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9424298Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9424469Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9425569Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9425737Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9426846Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9427010Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9428153Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9428322Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9429045Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9429132Z   warnings.warn(
2025-04-11T04:23:18.9429816Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9429899Z   warnings.warn(
2025-04-11T04:23:18.9430574Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9430713Z   warnings.warn(
2025-04-11T04:23:18.9431404Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9431537Z   warnings.warn(
2025-04-11T04:23:18.9432382Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9432458Z   warnings.warn(
2025-04-11T04:23:18.9433339Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9433417Z   warnings.warn(
2025-04-11T04:23:18.9434222Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9434299Z   warnings.warn(
2025-04-11T04:23:18.9435096Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9435172Z   warnings.warn(
2025-04-11T04:23:18.9435966Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9436043Z   warnings.warn(
2025-04-11T04:23:18.9436895Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9436972Z   warnings.warn(
2025-04-11T04:23:18.9437772Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9437848Z   warnings.warn(
2025-04-11T04:23:18.9438651Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9438728Z   warnings.warn(
2025-04-11T04:23:18.9439534Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9439609Z   warnings.warn(
2025-04-11T04:23:18.9440463Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9440538Z   warnings.warn(
2025-04-11T04:23:18.9440834Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9441177Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9441452Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9442299Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9442380Z   warnings.warn(
2025-04-11T04:23:18.9443185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9443263Z   warnings.warn(
2025-04-11T04:23:18.9443548Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9443822Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9444765Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9444935Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9445899Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9446069Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9446996Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9447159Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9447298Z ________________________________ test_optim[4] _________________________________
2025-04-11T04:23:18.9447304Z 
2025-04-11T04:23:18.9447421Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9448027Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9448031Z 
2025-04-11T04:23:18.9448132Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9448268Z         try_count = 0
2025-04-11T04:23:18.9448369Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9448451Z             max_try, int
2025-04-11T04:23:18.9448600Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9448669Z     
2025-04-11T04:23:18.9448784Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9448856Z             try:
2025-04-11T04:23:18.9448993Z                 try_count += 1
2025-04-11T04:23:18.9449087Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9449165Z                 return ret
2025-04-11T04:23:18.9449262Z             except exception_type as e:
2025-04-11T04:23:18.9449359Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9449551Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9449669Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9449814Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9450018Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9450101Z                     continue
2025-04-11T04:23:18.9450179Z                 else:
2025-04-11T04:23:18.9450401Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9450482Z >                   raise e
2025-04-11T04:23:18.9450486Z 
2025-04-11T04:23:18.9450577Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9450687Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9450823Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9450909Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9451057Z tests/test_zero/test_gemini/test_optim.py:193: in test_optim
2025-04-11T04:23:18.9451144Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9451247Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9451344Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9451596Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9451774Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9452055Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9452146Z     while not context.join():
2025-04-11T04:23:18.9452253Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9452257Z 
2025-04-11T04:23:18.9452509Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76260>
2025-04-11T04:23:18.9452588Z timeout = None
2025-04-11T04:23:18.9452592Z 
2025-04-11T04:23:18.9452684Z     def join(self, timeout=None):
2025-04-11T04:23:18.9452806Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9452877Z     
2025-04-11T04:23:18.9453026Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9453169Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9453332Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9453422Z         of the first process exiting.
2025-04-11T04:23:18.9453489Z     
2025-04-11T04:23:18.9453638Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9453772Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9453843Z     
2025-04-11T04:23:18.9453914Z         Args:
2025-04-11T04:23:18.9454054Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9454125Z         """
2025-04-11T04:23:18.9454262Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9454356Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9454487Z             return True
2025-04-11T04:23:18.9454558Z     
2025-04-11T04:23:18.9454691Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9454806Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9454898Z             self.sentinels.keys(),
2025-04-11T04:23:18.9454981Z             timeout=timeout,
2025-04-11T04:23:18.9455052Z         )
2025-04-11T04:23:18.9455119Z     
2025-04-11T04:23:18.9455251Z         error_index = None
2025-04-11T04:23:18.9455339Z         for sentinel in ready:
2025-04-11T04:23:18.9455441Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9455542Z             process = self.processes[index]
2025-04-11T04:23:18.9455625Z             process.join()
2025-04-11T04:23:18.9455722Z             if process.exitcode != 0:
2025-04-11T04:23:18.9455807Z                 error_index = index
2025-04-11T04:23:18.9455880Z                 break
2025-04-11T04:23:18.9455951Z     
2025-04-11T04:23:18.9456041Z         # Return if there was no error.
2025-04-11T04:23:18.9456125Z         if error_index is None:
2025-04-11T04:23:18.9456304Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9456400Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9456472Z     
2025-04-11T04:23:18.9456613Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9456711Z         for process in self.processes:
2025-04-11T04:23:18.9456799Z             if process.is_alive():
2025-04-11T04:23:18.9456889Z                 process.terminate()
2025-04-11T04:23:18.9456974Z             process.join()
2025-04-11T04:23:18.9457041Z     
2025-04-11T04:23:18.9457181Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9457296Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9457403Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9457521Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9457604Z             if exitcode < 0:
2025-04-11T04:23:18.9457714Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9457819Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9457970Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9458063Z                     error_index=error_index,
2025-04-11T04:23:18.9458165Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9458251Z                     exit_code=exitcode,
2025-04-11T04:23:18.9458335Z                     signal_name=name,
2025-04-11T04:23:18.9458410Z                 )
2025-04-11T04:23:18.9458481Z             else:
2025-04-11T04:23:18.9458635Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9458799Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9458890Z                     error_index=error_index,
2025-04-11T04:23:18.9458993Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9459078Z                     exit_code=exitcode,
2025-04-11T04:23:18.9459153Z                 )
2025-04-11T04:23:18.9459222Z     
2025-04-11T04:23:18.9459356Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9459525Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9459608Z         msg += original_trace
2025-04-11T04:23:18.9459789Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9459951Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9460024Z E       
2025-04-11T04:23:18.9460147Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9460247Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9460548Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9460677Z E           fn(i, *args)
2025-04-11T04:23:18.9460927Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
2025-04-11T04:23:18.9461011Z E           exam_model_step()
2025-04-11T04:23:18.9461271Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9461358Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9461658Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9461748Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9461993Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9462084Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9462188Z E         [Previous line repeated 2 more times]
2025-04-11T04:23:18.9462439Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
2025-04-11T04:23:18.9462541Z E           torch_model = model_builder().cuda()
2025-04-11T04:23:18.9462878Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:18.9462978Z E           return super().cuda(*args, **kwargs)
2025-04-11T04:23:18.9463241Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9463363Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9463629Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9463715Z E           module._apply(fn)
2025-04-11T04:23:18.9463982Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9464065Z E           module._apply(fn)
2025-04-11T04:23:18.9464327Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9464420Z E           param_applied = fn(param)
2025-04-11T04:23:18.9464695Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9464809Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9464917Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9465203Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9465339Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9465550Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9465555Z 
2025-04-11T04:23:18.9465859Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9466010Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9466165Z [04/11/25 04:22:32] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9466298Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9466402Z                              :75 launch                                         
2025-04-11T04:23:18.9466544Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9466667Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9466864Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9467008Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9468138Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9468373Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9469518Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9469738Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9470886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9471049Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9472153Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9472310Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9473004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9473084Z   warnings.warn(
2025-04-11T04:23:18.9473756Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9473838Z   warnings.warn(
2025-04-11T04:23:18.9474564Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9474645Z   warnings.warn(
2025-04-11T04:23:18.9475306Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9475385Z   warnings.warn(
2025-04-11T04:23:18.9476198Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9476281Z   warnings.warn(
2025-04-11T04:23:18.9477071Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9477208Z   warnings.warn(
2025-04-11T04:23:18.9478000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9478130Z   warnings.warn(
2025-04-11T04:23:18.9478911Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9478990Z   warnings.warn(
2025-04-11T04:23:18.9479812Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9479895Z   warnings.warn(
2025-04-11T04:23:18.9480683Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9480764Z   warnings.warn(
2025-04-11T04:23:18.9481551Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9481630Z   warnings.warn(
2025-04-11T04:23:18.9482417Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9482494Z   warnings.warn(
2025-04-11T04:23:18.9483280Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9483405Z   warnings.warn(
2025-04-11T04:23:18.9484199Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9484276Z   warnings.warn(
2025-04-11T04:23:18.9485078Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9485155Z   warnings.warn(
2025-04-11T04:23:18.9485952Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9486027Z   warnings.warn(
2025-04-11T04:23:18.9486976Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9487195Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9488154Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9488321Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9489280Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T04:23:18.9489447Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T04:23:18.9489590Z ________________________________ test_search[1] ________________________________
2025-04-11T04:23:18.9489594Z 
2025-04-11T04:23:18.9489712Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T04:23:18.9490318Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9490322Z 
2025-04-11T04:23:18.9490428Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9490509Z         try_count = 0
2025-04-11T04:23:18.9490614Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9490697Z             max_try, int
2025-04-11T04:23:18.9490844Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9490912Z     
2025-04-11T04:23:18.9491029Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9491101Z             try:
2025-04-11T04:23:18.9491188Z                 try_count += 1
2025-04-11T04:23:18.9491278Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9491358Z                 return ret
2025-04-11T04:23:18.9491456Z             except exception_type as e:
2025-04-11T04:23:18.9491602Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9491794Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9491909Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9492060Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9492217Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9492298Z                     continue
2025-04-11T04:23:18.9492377Z                 else:
2025-04-11T04:23:18.9492597Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9492680Z >                   raise e
2025-04-11T04:23:18.9492684Z 
2025-04-11T04:23:18.9492777Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9492891Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9493023Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9493108Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9493254Z tests/test_zero/test_gemini/test_search.py:68: in test_search
2025-04-11T04:23:18.9493388Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9493489Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9493588Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9493844Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9494027Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9494312Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9494467Z     while not context.join():
2025-04-11T04:23:18.9494579Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9494582Z 
2025-04-11T04:23:18.9494789Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a6dcf0>
2025-04-11T04:23:18.9494867Z timeout = None
2025-04-11T04:23:18.9494871Z 
2025-04-11T04:23:18.9494962Z     def join(self, timeout=None):
2025-04-11T04:23:18.9495087Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9495156Z     
2025-04-11T04:23:18.9495351Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9495496Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9495661Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9495753Z         of the first process exiting.
2025-04-11T04:23:18.9495826Z     
2025-04-11T04:23:18.9495971Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9496105Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9496177Z     
2025-04-11T04:23:18.9496250Z         Args:
2025-04-11T04:23:18.9496390Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9496462Z         """
2025-04-11T04:23:18.9496599Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9496697Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9496775Z             return True
2025-04-11T04:23:18.9496846Z     
2025-04-11T04:23:18.9496977Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9497094Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9497186Z             self.sentinels.keys(),
2025-04-11T04:23:18.9497268Z             timeout=timeout,
2025-04-11T04:23:18.9497343Z         )
2025-04-11T04:23:18.9497413Z     
2025-04-11T04:23:18.9497496Z         error_index = None
2025-04-11T04:23:18.9497582Z         for sentinel in ready:
2025-04-11T04:23:18.9497685Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9497787Z             process = self.processes[index]
2025-04-11T04:23:18.9497933Z             process.join()
2025-04-11T04:23:18.9498030Z             if process.exitcode != 0:
2025-04-11T04:23:18.9498118Z                 error_index = index
2025-04-11T04:23:18.9498193Z                 break
2025-04-11T04:23:18.9498267Z     
2025-04-11T04:23:18.9498357Z         # Return if there was no error.
2025-04-11T04:23:18.9498443Z         if error_index is None:
2025-04-11T04:23:18.9498578Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9498673Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9498744Z     
2025-04-11T04:23:18.9498882Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9498982Z         for process in self.processes:
2025-04-11T04:23:18.9499069Z             if process.is_alive():
2025-04-11T04:23:18.9499163Z                 process.terminate()
2025-04-11T04:23:18.9499246Z             process.join()
2025-04-11T04:23:18.9499313Z     
2025-04-11T04:23:18.9499459Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9499573Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9499681Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9499852Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9499935Z             if exitcode < 0:
2025-04-11T04:23:18.9500044Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9500147Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9500300Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9500394Z                     error_index=error_index,
2025-04-11T04:23:18.9500548Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9500635Z                     exit_code=exitcode,
2025-04-11T04:23:18.9500719Z                     signal_name=name,
2025-04-11T04:23:18.9500794Z                 )
2025-04-11T04:23:18.9500866Z             else:
2025-04-11T04:23:18.9500971Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9501132Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9501226Z                     error_index=error_index,
2025-04-11T04:23:18.9501326Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9501456Z                     exit_code=exitcode,
2025-04-11T04:23:18.9501531Z                 )
2025-04-11T04:23:18.9501599Z     
2025-04-11T04:23:18.9501733Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9501903Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9501987Z         msg += original_trace
2025-04-11T04:23:18.9502162Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9502323Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9502397Z E       
2025-04-11T04:23:18.9502525Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9502627Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9502931Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9503012Z E           fn(i, *args)
2025-04-11T04:23:18.9503259Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T04:23:18.9503344Z E           exam_chunk_manager()
2025-04-11T04:23:18.9503610Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T04:23:18.9503713Z E           chunk_manager = init_chunk_manager(
2025-04-11T04:23:18.9503983Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T04:23:18.9504065Z E           dist.barrier()
2025-04-11T04:23:18.9504487Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.9504590Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.9504910Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:18.9505019Z E           work = default_pg.barrier(opts=opts)
2025-04-11T04:23:18.9505124Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9505408Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9505542Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9505705Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9505713Z 
2025-04-11T04:23:18.9506016Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9506171Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9506331Z [04/11/25 04:22:36] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9506457Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9506627Z                              :75 launch                                         
2025-04-11T04:23:18.9506764Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9506888Z                              environment is initialized, world size: 1          
2025-04-11T04:23:18.9507083Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9507246Z ________________________________ test_search[4] ________________________________
2025-04-11T04:23:18.9507253Z 
2025-04-11T04:23:18.9507368Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9507964Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9507974Z 
2025-04-11T04:23:18.9508075Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9508151Z         try_count = 0
2025-04-11T04:23:18.9508301Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9508382Z             max_try, int
2025-04-11T04:23:18.9508558Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9508627Z     
2025-04-11T04:23:18.9508738Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9508815Z             try:
2025-04-11T04:23:18.9508896Z                 try_count += 1
2025-04-11T04:23:18.9508989Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9509067Z                 return ret
2025-04-11T04:23:18.9509159Z             except exception_type as e:
2025-04-11T04:23:18.9509261Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9509443Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9509565Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9509709Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9509864Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9509944Z                     continue
2025-04-11T04:23:18.9510018Z                 else:
2025-04-11T04:23:18.9510242Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9510323Z >                   raise e
2025-04-11T04:23:18.9510327Z 
2025-04-11T04:23:18.9510421Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9510530Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9510721Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9510809Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9510954Z tests/test_zero/test_gemini/test_search.py:68: in test_search
2025-04-11T04:23:18.9511044Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9511143Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9511245Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9511505Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9511681Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9511966Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9512054Z     while not context.join():
2025-04-11T04:23:18.9512167Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9512170Z 
2025-04-11T04:23:18.9512367Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c75d50>
2025-04-11T04:23:18.9512447Z timeout = None
2025-04-11T04:23:18.9512451Z 
2025-04-11T04:23:18.9512540Z     def join(self, timeout=None):
2025-04-11T04:23:18.9512722Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9512791Z     
2025-04-11T04:23:18.9512938Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9513084Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9513247Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9513340Z         of the first process exiting.
2025-04-11T04:23:18.9513461Z     
2025-04-11T04:23:18.9513606Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9513739Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9513807Z     
2025-04-11T04:23:18.9513885Z         Args:
2025-04-11T04:23:18.9514020Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9514094Z         """
2025-04-11T04:23:18.9514230Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9514323Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9514403Z             return True
2025-04-11T04:23:18.9514521Z     
2025-04-11T04:23:18.9514653Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9514770Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9514864Z             self.sentinels.keys(),
2025-04-11T04:23:18.9514945Z             timeout=timeout,
2025-04-11T04:23:18.9515018Z         )
2025-04-11T04:23:18.9515089Z     
2025-04-11T04:23:18.9515169Z         error_index = None
2025-04-11T04:23:18.9515256Z         for sentinel in ready:
2025-04-11T04:23:18.9515359Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9515456Z             process = self.processes[index]
2025-04-11T04:23:18.9515546Z             process.join()
2025-04-11T04:23:18.9515637Z             if process.exitcode != 0:
2025-04-11T04:23:18.9515726Z                 error_index = index
2025-04-11T04:23:18.9515802Z                 break
2025-04-11T04:23:18.9515869Z     
2025-04-11T04:23:18.9515960Z         # Return if there was no error.
2025-04-11T04:23:18.9516043Z         if error_index is None:
2025-04-11T04:23:18.9516180Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9516273Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9516343Z     
2025-04-11T04:23:18.9516481Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9516576Z         for process in self.processes:
2025-04-11T04:23:18.9516667Z             if process.is_alive():
2025-04-11T04:23:18.9516757Z                 process.terminate()
2025-04-11T04:23:18.9516841Z             process.join()
2025-04-11T04:23:18.9516908Z     
2025-04-11T04:23:18.9517096Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9517217Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9517324Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9517451Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9517539Z             if exitcode < 0:
2025-04-11T04:23:18.9517651Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9517758Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9517910Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9518012Z                     error_index=error_index,
2025-04-11T04:23:18.9518116Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9518209Z                     exit_code=exitcode,
2025-04-11T04:23:18.9518296Z                     signal_name=name,
2025-04-11T04:23:18.9518369Z                 )
2025-04-11T04:23:18.9518448Z             else:
2025-04-11T04:23:18.9518551Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9518721Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9518865Z                     error_index=error_index,
2025-04-11T04:23:18.9518964Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9519052Z                     exit_code=exitcode,
2025-04-11T04:23:18.9519123Z                 )
2025-04-11T04:23:18.9519193Z     
2025-04-11T04:23:18.9519322Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9519495Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9519633Z         msg += original_trace
2025-04-11T04:23:18.9519806Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9519961Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9520033Z E       
2025-04-11T04:23:18.9520162Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9520260Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9520559Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9520640Z E           fn(i, *args)
2025-04-11T04:23:18.9520939Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T04:23:18.9521032Z E           exam_chunk_manager()
2025-04-11T04:23:18.9521294Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T04:23:18.9521402Z E           chunk_manager = init_chunk_manager(
2025-04-11T04:23:18.9521667Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T04:23:18.9521750Z E           dist.barrier()
2025-04-11T04:23:18.9522046Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:18.9522141Z E           return func(*args, **kwargs)
2025-04-11T04:23:18.9522462Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:18.9522567Z E           work = default_pg.barrier(opts=opts)
2025-04-11T04:23:18.9522672Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9522954Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9523089Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9523249Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9523254Z 
2025-04-11T04:23:18.9523561Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9523760Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9523916Z [04/11/25 04:22:42] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9524050Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9524155Z                              :75 launch                                         
2025-04-11T04:23:18.9524297Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9524418Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9524615Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9524762Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9525083Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43909 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9525212Z _______________________________ test_zero_ddp[4] _______________________________
2025-04-11T04:23:18.9525216Z 
2025-04-11T04:23:18.9525330Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T04:23:18.9525991Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9525996Z 
2025-04-11T04:23:18.9526096Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9526178Z         try_count = 0
2025-04-11T04:23:18.9526325Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9526411Z             max_try, int
2025-04-11T04:23:18.9526562Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9526637Z     
2025-04-11T04:23:18.9526751Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9526829Z             try:
2025-04-11T04:23:18.9526919Z                 try_count += 1
2025-04-11T04:23:18.9527014Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9527099Z                 return ret
2025-04-11T04:23:18.9527197Z             except exception_type as e:
2025-04-11T04:23:18.9527347Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9527537Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9527652Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9527798Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9527954Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9528037Z                     continue
2025-04-11T04:23:18.9528111Z                 else:
2025-04-11T04:23:18.9528329Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9528411Z >                   raise e
2025-04-11T04:23:18.9528415Z 
2025-04-11T04:23:18.9528507Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9528621Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9528752Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9528840Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9529017Z tests/test_zero/test_gemini/test_zeroddp_state_dict.py:85: in test_zero_ddp
2025-04-11T04:23:18.9529103Z     spawn(run_dist, world_size)
2025-04-11T04:23:18.9529205Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9529305Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9529561Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9529734Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9530065Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9530155Z     while not context.join():
2025-04-11T04:23:18.9530265Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9530271Z 
2025-04-11T04:23:18.9530470Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840b160>
2025-04-11T04:23:18.9530549Z timeout = None
2025-04-11T04:23:18.9530553Z 
2025-04-11T04:23:18.9530644Z     def join(self, timeout=None):
2025-04-11T04:23:18.9530767Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9530839Z     
2025-04-11T04:23:18.9530987Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9531129Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9531295Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9531387Z         of the first process exiting.
2025-04-11T04:23:18.9531457Z     
2025-04-11T04:23:18.9531601Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9531739Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9531857Z     
2025-04-11T04:23:18.9531930Z         Args:
2025-04-11T04:23:18.9532077Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9532150Z         """
2025-04-11T04:23:18.9532292Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9532383Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9532461Z             return True
2025-04-11T04:23:18.9532582Z     
2025-04-11T04:23:18.9532714Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9532831Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9532920Z             self.sentinels.keys(),
2025-04-11T04:23:18.9533006Z             timeout=timeout,
2025-04-11T04:23:18.9533081Z         )
2025-04-11T04:23:18.9533148Z     
2025-04-11T04:23:18.9533234Z         error_index = None
2025-04-11T04:23:18.9533318Z         for sentinel in ready:
2025-04-11T04:23:18.9533429Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9533526Z             process = self.processes[index]
2025-04-11T04:23:18.9533658Z             process.join()
2025-04-11T04:23:18.9533755Z             if process.exitcode != 0:
2025-04-11T04:23:18.9533840Z                 error_index = index
2025-04-11T04:23:18.9533916Z                 break
2025-04-11T04:23:18.9533984Z     
2025-04-11T04:23:18.9534073Z         # Return if there was no error.
2025-04-11T04:23:18.9534162Z         if error_index is None:
2025-04-11T04:23:18.9534295Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9534394Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9534462Z     
2025-04-11T04:23:18.9534605Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9534701Z         for process in self.processes:
2025-04-11T04:23:18.9534789Z             if process.is_alive():
2025-04-11T04:23:18.9534881Z                 process.terminate()
2025-04-11T04:23:18.9534969Z             process.join()
2025-04-11T04:23:18.9535040Z     
2025-04-11T04:23:18.9535178Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9535293Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9535401Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9535520Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9535606Z             if exitcode < 0:
2025-04-11T04:23:18.9535712Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9535819Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9535966Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9536107Z                     error_index=error_index,
2025-04-11T04:23:18.9536214Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9536300Z                     exit_code=exitcode,
2025-04-11T04:23:18.9536390Z                     signal_name=name,
2025-04-11T04:23:18.9536462Z                 )
2025-04-11T04:23:18.9536533Z             else:
2025-04-11T04:23:18.9536639Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9536800Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9536895Z                     error_index=error_index,
2025-04-11T04:23:18.9536992Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9537082Z                     exit_code=exitcode,
2025-04-11T04:23:18.9537153Z                 )
2025-04-11T04:23:18.9537221Z     
2025-04-11T04:23:18.9537354Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9537524Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9537610Z         msg += original_trace
2025-04-11T04:23:18.9537780Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9537991Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9538066Z E       
2025-04-11T04:23:18.9538193Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9538295Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9538593Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9538674Z E           fn(i, *args)
2025-04-11T04:23:18.9538999Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
2025-04-11T04:23:18.9539083Z E           exam_state_dict()
2025-04-11T04:23:18.9539343Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9539432Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9539686Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9539774Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9540067Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9540154Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9540255Z E         [Previous line repeated 1 more time]
2025-04-11T04:23:18.9540542Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
2025-04-11T04:23:18.9540797Z E           model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
2025-04-11T04:23:18.9541036Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
2025-04-11T04:23:18.9541139Z E           self.chunk_manager = ChunkManager(
2025-04-11T04:23:18.9541379Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T04:23:18.9541628Z E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:18.9541740Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9542029Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9542164Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9542326Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9542332Z 
2025-04-11T04:23:18.9542638Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9542842Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9542997Z [04/11/25 04:22:49] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9543125Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9543235Z                              :75 launch                                         
2025-04-11T04:23:18.9543376Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9543497Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9543693Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9543839Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9544980Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9545200Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9546305Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9546532Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9547674Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9547883Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9549022Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9549184Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9549864Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9549948Z   warnings.warn(
2025-04-11T04:23:18.9550620Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9550703Z   warnings.warn(
2025-04-11T04:23:18.9551363Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9551446Z   warnings.warn(
2025-04-11T04:23:18.9552153Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9552239Z   warnings.warn(
2025-04-11T04:23:18.9553084Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9553166Z   warnings.warn(
2025-04-11T04:23:18.9553976Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9554054Z   warnings.warn(
2025-04-11T04:23:18.9554857Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9554988Z   warnings.warn(
2025-04-11T04:23:18.9555792Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9555922Z   warnings.warn(
2025-04-11T04:23:18.9556710Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9556788Z   warnings.warn(
2025-04-11T04:23:18.9557618Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9557697Z   warnings.warn(
2025-04-11T04:23:18.9558517Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9558594Z   warnings.warn(
2025-04-11T04:23:18.9559397Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9559473Z   warnings.warn(
2025-04-11T04:23:18.9560278Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9560354Z   warnings.warn(
2025-04-11T04:23:18.9561199Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9561276Z   warnings.warn(
2025-04-11T04:23:18.9562068Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9562145Z   warnings.warn(
2025-04-11T04:23:18.9562925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9563001Z   warnings.warn(
2025-04-11T04:23:18.9563168Z Exception ignored in: <function GeminiDDP.__del__ at 0x7ff5876a5fc0>
2025-04-11T04:23:18.9563261Z Traceback (most recent call last):
2025-04-11T04:23:18.9563500Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T04:23:18.9563588Z     self.remove_hooks()
2025-04-11T04:23:18.9563831Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T04:23:18.9563999Z     for p in self.module.parameters():
2025-04-11T04:23:18.9564300Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T04:23:18.9564494Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T04:23:18.9564644Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T04:23:18.9565590Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:18.9565687Z   return tensor.storage().size() == 0
2025-04-11T04:23:18.9566608Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:18.9566705Z   return tensor.storage().size() == 0
2025-04-11T04:23:18.9566841Z _________________________________ test_comm_nd _________________________________
2025-04-11T04:23:18.9566851Z 
2025-04-11T04:23:18.9566938Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9567543Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9567552Z 
2025-04-11T04:23:18.9567654Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9567732Z         try_count = 0
2025-04-11T04:23:18.9567836Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9567915Z             max_try, int
2025-04-11T04:23:18.9568065Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9568134Z     
2025-04-11T04:23:18.9568244Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9568320Z             try:
2025-04-11T04:23:18.9568402Z                 try_count += 1
2025-04-11T04:23:18.9568496Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9568573Z                 return ret
2025-04-11T04:23:18.9568667Z             except exception_type as e:
2025-04-11T04:23:18.9568770Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9569003Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9569127Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9569272Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9569434Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9569513Z                     continue
2025-04-11T04:23:18.9569587Z                 else:
2025-04-11T04:23:18.9569813Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9569890Z >                   raise e
2025-04-11T04:23:18.9569896Z 
2025-04-11T04:23:18.9569990Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9570101Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9570234Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9570319Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9570478Z tests/test_zero/test_low_level/test_coll_nd.py:38: in test_comm_nd
2025-04-11T04:23:18.9570562Z     spawn(run_dist, 4)
2025-04-11T04:23:18.9570662Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9570810Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9571068Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9571249Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9571536Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9571671Z     while not context.join():
2025-04-11T04:23:18.9571785Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9571789Z 
2025-04-11T04:23:18.9571983Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76dd0>
2025-04-11T04:23:18.9572062Z timeout = None
2025-04-11T04:23:18.9572068Z 
2025-04-11T04:23:18.9572157Z     def join(self, timeout=None):
2025-04-11T04:23:18.9572282Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9572350Z     
2025-04-11T04:23:18.9572495Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9572690Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9572855Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9572950Z         of the first process exiting.
2025-04-11T04:23:18.9573019Z     
2025-04-11T04:23:18.9573166Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9573303Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9573372Z     
2025-04-11T04:23:18.9573448Z         Args:
2025-04-11T04:23:18.9573584Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9573659Z         """
2025-04-11T04:23:18.9573798Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9573890Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9573972Z             return True
2025-04-11T04:23:18.9574041Z     
2025-04-11T04:23:18.9574173Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9574294Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9574384Z             self.sentinels.keys(),
2025-04-11T04:23:18.9574467Z             timeout=timeout,
2025-04-11T04:23:18.9574538Z         )
2025-04-11T04:23:18.9574610Z     
2025-04-11T04:23:18.9574691Z         error_index = None
2025-04-11T04:23:18.9574780Z         for sentinel in ready:
2025-04-11T04:23:18.9574885Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9574982Z             process = self.processes[index]
2025-04-11T04:23:18.9575071Z             process.join()
2025-04-11T04:23:18.9575161Z             if process.exitcode != 0:
2025-04-11T04:23:18.9575297Z                 error_index = index
2025-04-11T04:23:18.9575372Z                 break
2025-04-11T04:23:18.9575439Z     
2025-04-11T04:23:18.9575533Z         # Return if there was no error.
2025-04-11T04:23:18.9575620Z         if error_index is None:
2025-04-11T04:23:18.9575758Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9575853Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9575923Z     
2025-04-11T04:23:18.9576059Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9576152Z         for process in self.processes:
2025-04-11T04:23:18.9576244Z             if process.is_alive():
2025-04-11T04:23:18.9576336Z                 process.terminate()
2025-04-11T04:23:18.9576420Z             process.join()
2025-04-11T04:23:18.9576487Z     
2025-04-11T04:23:18.9576624Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9576744Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9576849Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9576968Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9577185Z             if exitcode < 0:
2025-04-11T04:23:18.9577294Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9577403Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9577550Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9577649Z                     error_index=error_index,
2025-04-11T04:23:18.9577749Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9577887Z                     exit_code=exitcode,
2025-04-11T04:23:18.9577973Z                     signal_name=name,
2025-04-11T04:23:18.9578044Z                 )
2025-04-11T04:23:18.9578118Z             else:
2025-04-11T04:23:18.9578219Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9578389Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9578480Z                     error_index=error_index,
2025-04-11T04:23:18.9578582Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9578669Z                     exit_code=exitcode,
2025-04-11T04:23:18.9578738Z                 )
2025-04-11T04:23:18.9578855Z     
2025-04-11T04:23:18.9578989Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9579161Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9579246Z         msg += original_trace
2025-04-11T04:23:18.9579422Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9579584Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9579655Z E       
2025-04-11T04:23:18.9579783Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9579880Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9580179Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9580257Z E           fn(i, *args)
2025-04-11T04:23:18.9580507Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
2025-04-11T04:23:18.9580597Z E           check_all_gather_2d()
2025-04-11T04:23:18.9580876Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
2025-04-11T04:23:18.9581011Z E           tensor = torch.rand(128, device=get_current_device())
2025-04-11T04:23:18.9581116Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9581400Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9581532Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9581742Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9581747Z 
2025-04-11T04:23:18.9582061Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9582211Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9582371Z [04/11/25 04:22:56] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9582498Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9582607Z                              :75 launch                                         
2025-04-11T04:23:18.9582745Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9582869Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9583067Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9583213Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9583503Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44260 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9583698Z ____________________________ test_grad_accumulation ____________________________
2025-04-11T04:23:18.9583702Z 
2025-04-11T04:23:18.9583788Z     @pytest.mark.dist
2025-04-11T04:23:18.9583878Z     def test_grad_accumulation():
2025-04-11T04:23:18.9583965Z >       spawn(run_dist, 2)
2025-04-11T04:23:18.9583968Z 
2025-04-11T04:23:18.9584097Z tests/test_zero/test_low_level/test_grad_acc.py:146: 
2025-04-11T04:23:18.9584243Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9584341Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9584437Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9584696Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9584869Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9585150Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9585238Z     while not context.join():
2025-04-11T04:23:18.9585391Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9585396Z 
2025-04-11T04:23:18.9585598Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5be2fe0>
2025-04-11T04:23:18.9585676Z timeout = None
2025-04-11T04:23:18.9585683Z 
2025-04-11T04:23:18.9585774Z     def join(self, timeout=None):
2025-04-11T04:23:18.9585894Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9585966Z     
2025-04-11T04:23:18.9586108Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9586253Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9586415Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9586504Z         of the first process exiting.
2025-04-11T04:23:18.9586578Z     
2025-04-11T04:23:18.9586723Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9586862Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9586928Z     
2025-04-11T04:23:18.9587003Z         Args:
2025-04-11T04:23:18.9587140Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9587211Z         """
2025-04-11T04:23:18.9587353Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9587445Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9587527Z             return True
2025-04-11T04:23:18.9587595Z     
2025-04-11T04:23:18.9587723Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9587890Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9587982Z             self.sentinels.keys(),
2025-04-11T04:23:18.9588070Z             timeout=timeout,
2025-04-11T04:23:18.9588141Z         )
2025-04-11T04:23:18.9588211Z     
2025-04-11T04:23:18.9588296Z         error_index = None
2025-04-11T04:23:18.9588380Z         for sentinel in ready:
2025-04-11T04:23:18.9588513Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9588611Z             process = self.processes[index]
2025-04-11T04:23:18.9588696Z             process.join()
2025-04-11T04:23:18.9588787Z             if process.exitcode != 0:
2025-04-11T04:23:18.9588873Z                 error_index = index
2025-04-11T04:23:18.9588952Z                 break
2025-04-11T04:23:18.9589021Z     
2025-04-11T04:23:18.9589113Z         # Return if there was no error.
2025-04-11T04:23:18.9589195Z         if error_index is None:
2025-04-11T04:23:18.9589327Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9589426Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9589493Z     
2025-04-11T04:23:18.9589633Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9589783Z         for process in self.processes:
2025-04-11T04:23:18.9589871Z             if process.is_alive():
2025-04-11T04:23:18.9589964Z                 process.terminate()
2025-04-11T04:23:18.9590045Z             process.join()
2025-04-11T04:23:18.9590117Z     
2025-04-11T04:23:18.9590254Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9590370Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9590530Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9590651Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9590738Z             if exitcode < 0:
2025-04-11T04:23:18.9590843Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9590953Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9591101Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9591198Z                     error_index=error_index,
2025-04-11T04:23:18.9591301Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9591387Z                     exit_code=exitcode,
2025-04-11T04:23:18.9591528Z                     signal_name=name,
2025-04-11T04:23:18.9591602Z                 )
2025-04-11T04:23:18.9591678Z             else:
2025-04-11T04:23:18.9591782Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9591943Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9592039Z                     error_index=error_index,
2025-04-11T04:23:18.9592137Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9592225Z                     exit_code=exitcode,
2025-04-11T04:23:18.9592296Z                 )
2025-04-11T04:23:18.9592367Z     
2025-04-11T04:23:18.9592501Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9592669Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9592761Z         msg += original_trace
2025-04-11T04:23:18.9592934Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9593098Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9593170Z E       
2025-04-11T04:23:18.9593292Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9593392Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9593687Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9593773Z E           fn(i, *args)
2025-04-11T04:23:18.9594027Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
2025-04-11T04:23:18.9594177Z E           exam_zero_1_grad_acc(sync=True)
2025-04-11T04:23:18.9594459Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
2025-04-11T04:23:18.9594564Z E           zero_model = zero_model.to(device)
2025-04-11T04:23:18.9594833Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:18.9594924Z E           return self._apply(convert)
2025-04-11T04:23:18.9595201Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9595285Z E           module._apply(fn)
2025-04-11T04:23:18.9595559Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9595650Z E           param_applied = fn(param)
2025-04-11T04:23:18.9595925Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:18.9596136Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:18.9596240Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9596577Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9596713Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9596876Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9596880Z 
2025-04-11T04:23:18.9597188Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9597386Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9597539Z [04/11/25 04:23:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9597669Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9597778Z                              :75 launch                                         
2025-04-11T04:23:18.9597918Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9598090Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9598233Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9598528Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43283 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9598656Z ________________________________ test_zero_1_2 _________________________________
2025-04-11T04:23:18.9598660Z 
2025-04-11T04:23:18.9598751Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9599353Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9599357Z 
2025-04-11T04:23:18.9599461Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9599539Z         try_count = 0
2025-04-11T04:23:18.9599639Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9599720Z             max_try, int
2025-04-11T04:23:18.9599863Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9599936Z     
2025-04-11T04:23:18.9600046Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9600122Z             try:
2025-04-11T04:23:18.9600204Z                 try_count += 1
2025-04-11T04:23:18.9600294Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9600376Z                 return ret
2025-04-11T04:23:18.9600467Z             except exception_type as e:
2025-04-11T04:23:18.9600635Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9600826Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9600940Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9601089Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9601244Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9601327Z                     continue
2025-04-11T04:23:18.9601403Z                 else:
2025-04-11T04:23:18.9601626Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9601707Z >                   raise e
2025-04-11T04:23:18.9601711Z 
2025-04-11T04:23:18.9601801Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9601912Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9602044Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9602132Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9602296Z tests/test_zero/test_low_level/test_mem_leak.py:57: in test_zero_1_2
2025-04-11T04:23:18.9602429Z     spawn(run_dist, 2)
2025-04-11T04:23:18.9602528Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9602627Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9602888Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9603068Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9603354Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9603488Z     while not context.join():
2025-04-11T04:23:18.9603601Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9603605Z 
2025-04-11T04:23:18.9603802Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c74e50>
2025-04-11T04:23:18.9603880Z timeout = None
2025-04-11T04:23:18.9603888Z 
2025-04-11T04:23:18.9603975Z     def join(self, timeout=None):
2025-04-11T04:23:18.9604097Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9604174Z     
2025-04-11T04:23:18.9604364Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9604510Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9604673Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9604761Z         of the first process exiting.
2025-04-11T04:23:18.9604834Z     
2025-04-11T04:23:18.9604977Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9605115Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9605182Z     
2025-04-11T04:23:18.9605257Z         Args:
2025-04-11T04:23:18.9605394Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9605464Z         """
2025-04-11T04:23:18.9605606Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9605701Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9605782Z             return True
2025-04-11T04:23:18.9605850Z     
2025-04-11T04:23:18.9605980Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9606102Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9606190Z             self.sentinels.keys(),
2025-04-11T04:23:18.9606274Z             timeout=timeout,
2025-04-11T04:23:18.9606345Z         )
2025-04-11T04:23:18.9606418Z     
2025-04-11T04:23:18.9606498Z         error_index = None
2025-04-11T04:23:18.9606583Z         for sentinel in ready:
2025-04-11T04:23:18.9606690Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9606785Z             process = self.processes[index]
2025-04-11T04:23:18.9606920Z             process.join()
2025-04-11T04:23:18.9607013Z             if process.exitcode != 0:
2025-04-11T04:23:18.9607100Z                 error_index = index
2025-04-11T04:23:18.9607178Z                 break
2025-04-11T04:23:18.9607247Z     
2025-04-11T04:23:18.9607340Z         # Return if there was no error.
2025-04-11T04:23:18.9607424Z         if error_index is None:
2025-04-11T04:23:18.9607556Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9607655Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9607724Z     
2025-04-11T04:23:18.9607863Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9607958Z         for process in self.processes:
2025-04-11T04:23:18.9608048Z             if process.is_alive():
2025-04-11T04:23:18.9608138Z                 process.terminate()
2025-04-11T04:23:18.9608220Z             process.join()
2025-04-11T04:23:18.9608292Z     
2025-04-11T04:23:18.9608431Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9608548Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9608653Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9608818Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9608902Z             if exitcode < 0:
2025-04-11T04:23:18.9609009Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9609117Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9609264Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9609359Z                     error_index=error_index,
2025-04-11T04:23:18.9609507Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9609593Z                     exit_code=exitcode,
2025-04-11T04:23:18.9609681Z                     signal_name=name,
2025-04-11T04:23:18.9609752Z                 )
2025-04-11T04:23:18.9609826Z             else:
2025-04-11T04:23:18.9609927Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9610088Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9610185Z                     error_index=error_index,
2025-04-11T04:23:18.9610285Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9610493Z                     exit_code=exitcode,
2025-04-11T04:23:18.9610566Z                 )
2025-04-11T04:23:18.9610636Z     
2025-04-11T04:23:18.9610768Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9610936Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9611028Z         msg += original_trace
2025-04-11T04:23:18.9611201Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9611369Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9611442Z E       
2025-04-11T04:23:18.9611566Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9611666Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9611965Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9612051Z E           fn(i, *args)
2025-04-11T04:23:18.9612302Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
2025-04-11T04:23:18.9612408Z E           exam_mem_leak(world_size=world_size)
2025-04-11T04:23:18.9612667Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
2025-04-11T04:23:18.9612763Z E           zero_model = MlpModel().cuda()
2025-04-11T04:23:18.9613042Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9613160Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9613483Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9613571Z E           module._apply(fn)
2025-04-11T04:23:18.9613837Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9613931Z E           param_applied = fn(param)
2025-04-11T04:23:18.9614206Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9614320Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9614425Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9614714Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9614847Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9615012Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9615016Z 
2025-04-11T04:23:18.9615316Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9615467Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9615677Z [04/11/25 04:23:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9615809Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9615914Z                              :75 launch                                         
2025-04-11T04:23:18.9616047Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9616208Z                              environment is initialized, world size: 2          
2025-04-11T04:23:18.9616402Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9616535Z ________________________________ test_zero_1_2 _________________________________
2025-04-11T04:23:18.9616539Z 
2025-04-11T04:23:18.9616629Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9617271Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9617278Z 
2025-04-11T04:23:18.9617381Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9617460Z         try_count = 0
2025-04-11T04:23:18.9617558Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9617636Z             max_try, int
2025-04-11T04:23:18.9617785Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9617853Z     
2025-04-11T04:23:18.9617967Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9618040Z             try:
2025-04-11T04:23:18.9618123Z                 try_count += 1
2025-04-11T04:23:18.9618215Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9618292Z                 return ret
2025-04-11T04:23:18.9618386Z             except exception_type as e:
2025-04-11T04:23:18.9618487Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9618678Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9618794Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9618939Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9619096Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9619177Z                     continue
2025-04-11T04:23:18.9619255Z                 else:
2025-04-11T04:23:18.9619474Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9619595Z >                   raise e
2025-04-11T04:23:18.9619600Z 
2025-04-11T04:23:18.9619694Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9619805Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9619941Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9620025Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9620192Z tests/test_zero/test_low_level/test_zero1_2.py:224: in test_zero_1_2
2025-04-11T04:23:18.9620270Z     spawn(run_dist, 4)
2025-04-11T04:23:18.9620371Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9620469Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9620726Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9620904Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9621184Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9621273Z     while not context.join():
2025-04-11T04:23:18.9621380Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9621383Z 
2025-04-11T04:23:18.9621629Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581d6cd00>
2025-04-11T04:23:18.9621705Z timeout = None
2025-04-11T04:23:18.9621712Z 
2025-04-11T04:23:18.9621800Z     def join(self, timeout=None):
2025-04-11T04:23:18.9621928Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9621996Z     
2025-04-11T04:23:18.9622139Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9622282Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9622496Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9622586Z         of the first process exiting.
2025-04-11T04:23:18.9622654Z     
2025-04-11T04:23:18.9622801Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9622934Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9623005Z     
2025-04-11T04:23:18.9623079Z         Args:
2025-04-11T04:23:18.9623211Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9623345Z         """
2025-04-11T04:23:18.9623485Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9623579Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9623656Z             return True
2025-04-11T04:23:18.9623727Z     
2025-04-11T04:23:18.9623856Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9623974Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9624069Z             self.sentinels.keys(),
2025-04-11T04:23:18.9624151Z             timeout=timeout,
2025-04-11T04:23:18.9624224Z         )
2025-04-11T04:23:18.9624291Z     
2025-04-11T04:23:18.9624372Z         error_index = None
2025-04-11T04:23:18.9624459Z         for sentinel in ready:
2025-04-11T04:23:18.9624562Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9624663Z             process = self.processes[index]
2025-04-11T04:23:18.9624747Z             process.join()
2025-04-11T04:23:18.9624841Z             if process.exitcode != 0:
2025-04-11T04:23:18.9624931Z                 error_index = index
2025-04-11T04:23:18.9625005Z                 break
2025-04-11T04:23:18.9625079Z     
2025-04-11T04:23:18.9625168Z         # Return if there was no error.
2025-04-11T04:23:18.9625255Z         if error_index is None:
2025-04-11T04:23:18.9625386Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9625482Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9625552Z     
2025-04-11T04:23:18.9625689Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9625786Z         for process in self.processes:
2025-04-11T04:23:18.9625938Z             if process.is_alive():
2025-04-11T04:23:18.9626030Z                 process.terminate()
2025-04-11T04:23:18.9626115Z             process.join()
2025-04-11T04:23:18.9626184Z     
2025-04-11T04:23:18.9626326Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9626441Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9626549Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9626669Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9626752Z             if exitcode < 0:
2025-04-11T04:23:18.9626862Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9626969Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9627119Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9627212Z                     error_index=error_index,
2025-04-11T04:23:18.9627317Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9627404Z                     exit_code=exitcode,
2025-04-11T04:23:18.9627490Z                     signal_name=name,
2025-04-11T04:23:18.9627566Z                 )
2025-04-11T04:23:18.9627684Z             else:
2025-04-11T04:23:18.9627788Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9627951Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9628040Z                     error_index=error_index,
2025-04-11T04:23:18.9628143Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9628228Z                     exit_code=exitcode,
2025-04-11T04:23:18.9628303Z                 )
2025-04-11T04:23:18.9628447Z     
2025-04-11T04:23:18.9628582Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9628755Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9628839Z         msg += original_trace
2025-04-11T04:23:18.9629019Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9629177Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9629251Z E       
2025-04-11T04:23:18.9629378Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9629474Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9629821Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9629904Z E           fn(i, *args)
2025-04-11T04:23:18.9630165Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
2025-04-11T04:23:18.9630254Z E           exam_zero_1_torch_ddp()
2025-04-11T04:23:18.9630512Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9630599Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9630853Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9630942Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9631192Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9631281Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9631566Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
2025-04-11T04:23:18.9631678Z E           torch_model = MlpModel().cuda().to(dtype)
2025-04-11T04:23:18.9631946Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9632070Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9632343Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9632481Z E           module._apply(fn)
2025-04-11T04:23:18.9632753Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9632845Z E           param_applied = fn(param)
2025-04-11T04:23:18.9633120Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9633235Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9633344Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9633628Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9633764Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9633927Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9633932Z 
2025-04-11T04:23:18.9634232Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9634384Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9634536Z [04/11/25 04:23:11] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9634722Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9634827Z                              :75 launch                                         
2025-04-11T04:23:18.9634966Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9635088Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9635326Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9635471Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9635768Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:63920 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9635904Z ________________________________ test_zero_ckpt ________________________________
2025-04-11T04:23:18.9635908Z 
2025-04-11T04:23:18.9636001Z args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9636653Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9636658Z 
2025-04-11T04:23:18.9636761Z     def _run_until_success(*args, **kwargs):
2025-04-11T04:23:18.9636841Z         try_count = 0
2025-04-11T04:23:18.9636941Z         assert max_try is None or isinstance(
2025-04-11T04:23:18.9637019Z             max_try, int
2025-04-11T04:23:18.9637166Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9637234Z     
2025-04-11T04:23:18.9637349Z         while max_try is None or try_count < max_try:
2025-04-11T04:23:18.9637423Z             try:
2025-04-11T04:23:18.9637505Z                 try_count += 1
2025-04-11T04:23:18.9637598Z                 ret = func(*args, **kwargs)
2025-04-11T04:23:18.9637676Z                 return ret
2025-04-11T04:23:18.9637772Z             except exception_type as e:
2025-04-11T04:23:18.9637872Z                 error_lines = str(e).split("\n")
2025-04-11T04:23:18.9638057Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T04:23:18.9638169Z                     print("Exception is caught, retrying...")
2025-04-11T04:23:18.9638310Z                     # when pattern is not specified, we always skip the exception
2025-04-11T04:23:18.9638470Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T04:23:18.9638549Z                     continue
2025-04-11T04:23:18.9638627Z                 else:
2025-04-11T04:23:18.9638891Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T04:23:18.9638974Z >                   raise e
2025-04-11T04:23:18.9638978Z 
2025-04-11T04:23:18.9639072Z colossalai/testing/utils.py:144: 
2025-04-11T04:23:18.9639181Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9639317Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T04:23:18.9639401Z     ret = func(*args, **kwargs)
2025-04-11T04:23:18.9639574Z tests/test_zero/test_low_level/test_zero_ckpt.py:129: in test_zero_ckpt
2025-04-11T04:23:18.9639653Z     spawn(run_dist, 4)
2025-04-11T04:23:18.9639754Z colossalai/testing/utils.py:252: in spawn
2025-04-11T04:23:18.9639853Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T04:23:18.9640111Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T04:23:18.9640292Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T04:23:18.9640572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T04:23:18.9640662Z     while not context.join():
2025-04-11T04:23:18.9640818Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T04:23:18.9640821Z 
2025-04-11T04:23:18.9641026Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76dd0>
2025-04-11T04:23:18.9641106Z timeout = None
2025-04-11T04:23:18.9641110Z 
2025-04-11T04:23:18.9641206Z     def join(self, timeout=None):
2025-04-11T04:23:18.9641337Z         r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9641454Z     
2025-04-11T04:23:18.9641601Z         Attempt to join one or more processes in this spawn context.
2025-04-11T04:23:18.9641745Z         If one of them exited with a non-zero exit status, this function
2025-04-11T04:23:18.9641910Z         kills the remaining processes and raises an exception with the cause
2025-04-11T04:23:18.9642002Z         of the first process exiting.
2025-04-11T04:23:18.9642070Z     
2025-04-11T04:23:18.9642220Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T04:23:18.9642357Z         ``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9642428Z     
2025-04-11T04:23:18.9642544Z         Args:
2025-04-11T04:23:18.9642686Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T04:23:18.9642758Z         """
2025-04-11T04:23:18.9642893Z         # Ensure this function can be called even when we're done.
2025-04-11T04:23:18.9642988Z         if len(self.sentinels) == 0:
2025-04-11T04:23:18.9643067Z             return True
2025-04-11T04:23:18.9643138Z     
2025-04-11T04:23:18.9643268Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T04:23:18.9643384Z         ready = multiprocessing.connection.wait(
2025-04-11T04:23:18.9643477Z             self.sentinels.keys(),
2025-04-11T04:23:18.9643562Z             timeout=timeout,
2025-04-11T04:23:18.9643636Z         )
2025-04-11T04:23:18.9643704Z     
2025-04-11T04:23:18.9643785Z         error_index = None
2025-04-11T04:23:18.9643874Z         for sentinel in ready:
2025-04-11T04:23:18.9643979Z             index = self.sentinels.pop(sentinel)
2025-04-11T04:23:18.9644081Z             process = self.processes[index]
2025-04-11T04:23:18.9644163Z             process.join()
2025-04-11T04:23:18.9644255Z             if process.exitcode != 0:
2025-04-11T04:23:18.9644345Z                 error_index = index
2025-04-11T04:23:18.9644419Z                 break
2025-04-11T04:23:18.9644493Z     
2025-04-11T04:23:18.9644582Z         # Return if there was no error.
2025-04-11T04:23:18.9644671Z         if error_index is None:
2025-04-11T04:23:18.9644803Z             # Return whether or not all processes have been joined.
2025-04-11T04:23:18.9644896Z             return len(self.sentinels) == 0
2025-04-11T04:23:18.9644967Z     
2025-04-11T04:23:18.9645152Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T04:23:18.9645253Z         for process in self.processes:
2025-04-11T04:23:18.9645339Z             if process.is_alive():
2025-04-11T04:23:18.9645430Z                 process.terminate()
2025-04-11T04:23:18.9645517Z             process.join()
2025-04-11T04:23:18.9645584Z     
2025-04-11T04:23:18.9645727Z         # There won't be an error on the queue if the process crashed.
2025-04-11T04:23:18.9645843Z         failed_process = self.processes[error_index]
2025-04-11T04:23:18.9645952Z         if self.error_queues[error_index].empty():
2025-04-11T04:23:18.9646073Z             exitcode = self.processes[error_index].exitcode
2025-04-11T04:23:18.9646155Z             if exitcode < 0:
2025-04-11T04:23:18.9646261Z                 name = signal.Signals(-exitcode).name
2025-04-11T04:23:18.9646363Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9646513Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T04:23:18.9646607Z                     error_index=error_index,
2025-04-11T04:23:18.9646707Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9646793Z                     exit_code=exitcode,
2025-04-11T04:23:18.9646930Z                     signal_name=name,
2025-04-11T04:23:18.9647005Z                 )
2025-04-11T04:23:18.9647080Z             else:
2025-04-11T04:23:18.9647184Z                 raise ProcessExitedException(
2025-04-11T04:23:18.9647346Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T04:23:18.9647437Z                     error_index=error_index,
2025-04-11T04:23:18.9647537Z                     error_pid=failed_process.pid,
2025-04-11T04:23:18.9647696Z                     exit_code=exitcode,
2025-04-11T04:23:18.9647771Z                 )
2025-04-11T04:23:18.9647839Z     
2025-04-11T04:23:18.9647971Z         original_trace = self.error_queues[error_index].get()
2025-04-11T04:23:18.9648143Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T04:23:18.9648226Z         msg += original_trace
2025-04-11T04:23:18.9648402Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T04:23:18.9648565Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:18.9648639Z E       
2025-04-11T04:23:18.9648808Z E       -- Process 0 terminated with the following error:
2025-04-11T04:23:18.9648907Z E       Traceback (most recent call last):
2025-04-11T04:23:18.9649211Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:18.9649291Z E           fn(i, *args)
2025-04-11T04:23:18.9649557Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
2025-04-11T04:23:18.9649652Z E           exam_zero_1_torch_ddp_ckpt()
2025-04-11T04:23:18.9649913Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:18.9649999Z E           partial_func(**kwargs)
2025-04-11T04:23:18.9650304Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
2025-04-11T04:23:18.9650402Z E           torch_model = MlpModel().cuda()
2025-04-11T04:23:18.9650673Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:18.9650795Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9651066Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:18.9651157Z E           module._apply(fn)
2025-04-11T04:23:18.9651426Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:18.9651520Z E           param_applied = fn(param)
2025-04-11T04:23:18.9651840Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:18.9651956Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:18.9652069Z E       RuntimeError: CUDA error: out of memory
2025-04-11T04:23:18.9652357Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:18.9652497Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:18.9652659Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9652663Z 
2025-04-11T04:23:18.9652973Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T04:23:18.9653126Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T04:23:18.9653285Z [04/11/25 04:23:17] INFO     colossalai - colossalai - INFO:                    
2025-04-11T04:23:18.9653416Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T04:23:18.9653522Z                              :75 launch                                         
2025-04-11T04:23:18.9653713Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T04:23:18.9653841Z                              environment is initialized, world size: 4          
2025-04-11T04:23:18.9654042Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T04:23:18.9654186Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T04:23:18.9654480Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9654809Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
2025-04-11T04:23:18.9654923Z =============================== warnings summary ===============================
2025-04-11T04:23:18.9655018Z colossalai/interface/model.py:45
2025-04-11T04:23:18.9655307Z   /__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\S'
2025-04-11T04:23:18.9655572Z     to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T04:23:18.9655576Z 
2025-04-11T04:23:18.9655672Z colossalai/interface/model.py:45
2025-04-11T04:23:18.9655959Z   /__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T04:23:18.9656161Z     to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T04:23:18.9656167Z 
2025-04-11T04:23:18.9656267Z colossalai/checkpoint_io/utils.py:862
2025-04-11T04:23:18.9656567Z   /__w/ColossalAI/ColossalAI/colossalai/checkpoint_io/utils.py:862: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T04:23:18.9656681Z     reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T04:23:18.9656685Z 
2025-04-11T04:23:18.9656786Z colossalai/nn/optimizer/cpu_adam.py:12
2025-04-11T04:23:18.9657087Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/cpu_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T04:23:18.9657165Z     """
2025-04-11T04:23:18.9657171Z 
2025-04-11T04:23:18.9657271Z colossalai/nn/optimizer/fused_adam.py:15
2025-04-11T04:23:18.9657575Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/fused_adam.py:15: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T04:23:18.9657664Z     """Implements Adam algorithm.
2025-04-11T04:23:18.9657670Z 
2025-04-11T04:23:18.9657773Z colossalai/nn/optimizer/hybrid_adam.py:12
2025-04-11T04:23:18.9658080Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/hybrid_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T04:23:18.9658168Z     """Implements Adam algorithm.
2025-04-11T04:23:18.9658177Z 
2025-04-11T04:23:18.9658505Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34
2025-04-11T04:23:18.9659654Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T04:23:18.9659829Z     deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9659836Z 
2025-04-11T04:23:18.9660071Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896
2025-04-11T04:23:18.9660217Z tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T04:23:18.9660381Z tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T04:23:18.9660546Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9661249Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T04:23:18.9661383Z     warnings.warn(
2025-04-11T04:23:18.9661387Z 
2025-04-11T04:23:18.9661605Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9661820Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9662066Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9662276Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9662490Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T04:23:18.9662608Z tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T04:23:18.9662771Z tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T04:23:18.9662930Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9663814Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T04:23:18.9663901Z     warnings.warn(
2025-04-11T04:23:18.9663905Z 
2025-04-11T04:23:18.9664000Z <frozen importlib._bootstrap>:283
2025-04-11T04:23:18.9664135Z tests/test_config/test_load_config.py::test_load_config
2025-04-11T04:23:18.9664292Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9664451Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9664601Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9664754Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:18.9665138Z   <frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead
2025-04-11T04:23:18.9665143Z 
2025-04-11T04:23:18.9665244Z colossalai/fx/profiler/dataflow.py:20
2025-04-11T04:23:18.9665556Z   /__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:20: DeprecationWarning: invalid escape sequence '\_'
2025-04-11T04:23:18.9665634Z     """
2025-04-11T04:23:18.9665638Z 
2025-04-11T04:23:18.9665733Z colossalai/fx/profiler/dataflow.py:77
2025-04-11T04:23:18.9666028Z   /__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:77: DeprecationWarning: invalid escape sequence '\_'
2025-04-11T04:23:18.9666257Z     """Analyze the autograd node dependencies and find out the memory usage.
2025-04-11T04:23:18.9666262Z 
2025-04-11T04:23:18.9666415Z colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31
2025-04-11T04:23:19.1713725Z   /__w/ColossalAI/ColossalAI/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T04:23:19.1714450Z     """A wrapper for optimizer. ``ShardedOptimizerV2`` and ``ShardedModelV2`` implement Zero Redundancy Optimizer (ZeRO).
2025-04-11T04:23:19.1714790Z 
2025-04-11T04:23:19.1714898Z colossalai/inference/utils.py:80
2025-04-11T04:23:19.1715355Z   /__w/ColossalAI/ColossalAI/colossalai/inference/utils.py:80: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T04:23:19.1715818Z     reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T04:23:19.1715994Z 
2025-04-11T04:23:19.1716121Z colossalai/inference/executor/rpc_worker.py:188
2025-04-11T04:23:19.1716665Z   /__w/ColossalAI/ColossalAI/colossalai/inference/executor/rpc_worker.py:188: SyntaxWarning: "is" with a literal. Did you mean "=="?
2025-04-11T04:23:19.1717180Z     if arch is "BaichuanForCausalLM":
2025-04-11T04:23:19.1717345Z 
2025-04-11T04:23:19.1717489Z tests/test_infer/test_async_engine/test_async_engine.py:49
2025-04-11T04:23:19.1718511Z   /__w/ColossalAI/ColossalAI/tests/test_infer/test_async_engine/test_async_engine.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
2025-04-11T04:23:19.1719378Z     @pytest.mark.asyncio
2025-04-11T04:23:19.1719517Z 
2025-04-11T04:23:19.1719635Z tests/test_tensor/test_dtensor/test_dtensor.py:10
2025-04-11T04:23:19.1720468Z   /__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py:10: PytestCollectionWarning: cannot collect test class 'TestModel' because it has a __init__ constructor (from: tests/test_tensor/test_dtensor/test_dtensor.py)
2025-04-11T04:23:19.1721235Z     class TestModel(torch.nn.Module):
2025-04-11T04:23:19.1721484Z 
2025-04-11T04:23:19.1721608Z tests/test_zero/test_low_level/test_mem_leak.py:23
2025-04-11T04:23:19.1722470Z   /__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py:23: PytestCollectionWarning: cannot collect test class 'TestLowLevelZeroOptimizer' because it has a __init__ constructor (from: tests/test_zero/test_low_level/test_mem_leak.py)
2025-04-11T04:23:19.1723312Z     class TestLowLevelZeroOptimizer(LowLevelZeroOptimizer):
2025-04-11T04:23:19.1723525Z 
2025-04-11T04:23:19.1723650Z tests/test_booster/test_accelerator.py: 1 warning
2025-04-11T04:23:19.1723989Z tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
2025-04-11T04:23:19.1724375Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
2025-04-11T04:23:19.1724762Z tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
2025-04-11T04:23:19.1725079Z tests/test_fp8/test_fp8_cast.py: 1 warning
2025-04-11T04:23:19.1725440Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
2025-04-11T04:23:19.1725836Z tests/test_shardformer/test_flash_attention.py: 1 warning
2025-04-11T04:23:19.1726161Z tests/test_shardformer/test_with_torch_ddp.py: 1 warning
2025-04-11T04:23:19.1726572Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
2025-04-11T04:23:19.1727087Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
2025-04-11T04:23:19.1727590Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
2025-04-11T04:23:19.1728021Z tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
2025-04-11T04:23:19.1728393Z tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
2025-04-11T04:23:19.1728751Z tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
2025-04-11T04:23:19.1729120Z tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
2025-04-11T04:23:19.1729564Z tests/test_shardformer/test_model/test_shard_command.py: 1 warning
2025-04-11T04:23:19.1729945Z tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
2025-04-11T04:23:19.1730303Z tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
2025-04-11T04:23:19.1730659Z tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
2025-04-11T04:23:19.1731022Z tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
2025-04-11T04:23:19.1731395Z tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
2025-04-11T04:23:19.1731765Z tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
2025-04-11T04:23:19.1732111Z tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
2025-04-11T04:23:19.1732458Z tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
2025-04-11T04:23:19.1732832Z tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
2025-04-11T04:23:19.1733190Z tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
2025-04-11T04:23:19.1733975Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:19.1734786Z     warnings.warn(
2025-04-11T04:23:19.1734904Z 
2025-04-11T04:23:19.1735019Z tests/test_booster/test_accelerator.py: 1 warning
2025-04-11T04:23:19.1735353Z tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
2025-04-11T04:23:19.1735754Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
2025-04-11T04:23:19.1736145Z tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
2025-04-11T04:23:19.1736458Z tests/test_fp8/test_fp8_cast.py: 1 warning
2025-04-11T04:23:19.1736861Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
2025-04-11T04:23:19.1737267Z tests/test_shardformer/test_flash_attention.py: 1 warning
2025-04-11T04:23:19.1737610Z tests/test_shardformer/test_with_torch_ddp.py: 1 warning
2025-04-11T04:23:19.1738027Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
2025-04-11T04:23:19.1738535Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
2025-04-11T04:23:19.1739084Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
2025-04-11T04:23:19.1739571Z tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
2025-04-11T04:23:19.1739927Z tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
2025-04-11T04:23:19.1740305Z tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
2025-04-11T04:23:19.1740671Z tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
2025-04-11T04:23:19.1741049Z tests/test_shardformer/test_model/test_shard_command.py: 1 warning
2025-04-11T04:23:19.1741414Z tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
2025-04-11T04:23:19.1741792Z tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
2025-04-11T04:23:19.1742148Z tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
2025-04-11T04:23:19.1742511Z tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
2025-04-11T04:23:19.1742874Z tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
2025-04-11T04:23:19.1743229Z tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
2025-04-11T04:23:19.1743581Z tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
2025-04-11T04:23:19.1743921Z tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
2025-04-11T04:23:19.1744262Z tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
2025-04-11T04:23:19.1744618Z tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
2025-04-11T04:23:19.1745385Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T04:23:19.1746123Z     warnings.warn(
2025-04-11T04:23:19.1746244Z 
2025-04-11T04:23:19.1746449Z tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
2025-04-11T04:23:19.1747187Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/_pytest/python.py:183: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
2025-04-11T04:23:19.1747903Z   You need to install a suitable plugin for your async framework, for example:
2025-04-11T04:23:19.1748227Z     - anyio
2025-04-11T04:23:19.1748478Z     - pytest-asyncio
2025-04-11T04:23:19.1748691Z     - pytest-tornasync
2025-04-11T04:23:19.1748895Z     - pytest-trio
2025-04-11T04:23:19.1749082Z     - pytest-twisted
2025-04-11T04:23:19.1749389Z     warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))
2025-04-11T04:23:19.1749631Z 
2025-04-11T04:23:19.1749916Z tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T04:23:19.1751155Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T04:23:19.1752247Z     numel += p.storage().size()
2025-04-11T04:23:19.1752392Z 
2025-04-11T04:23:19.1752566Z tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T04:23:19.1753953Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
2025-04-11T04:23:19.1755380Z     warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2025-04-11T04:23:19.1755638Z 
2025-04-11T04:23:19.1755805Z tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T04:23:19.1756544Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
2025-04-11T04:23:19.1757237Z     warnings.warn("To get the last learning rate computed by the scheduler, "
2025-04-11T04:23:19.1757476Z 
2025-04-11T04:23:19.1757652Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2025-04-11T04:23:19.1758016Z ============================== slowest durations ===============================
2025-04-11T04:23:19.1758395Z 17.15s call     tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:19.1758826Z 15.35s call     tests/test_infer/test_continuous_batching.py::test_continuous_batching
2025-04-11T04:23:19.1759285Z 15.10s call     tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
2025-04-11T04:23:19.1759764Z 10.96s call     tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
2025-04-11T04:23:19.1760217Z 9.67s call     tests/test_zero/test_gemini/test_inference.py::test_inference[4]
2025-04-11T04:23:19.1760624Z 8.90s call     tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
2025-04-11T04:23:19.1761007Z 8.86s call     tests/test_optimizer/test_dist_came.py::test_dist_came
2025-04-11T04:23:19.1761410Z 8.72s call     tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
2025-04-11T04:23:19.1761833Z 8.72s call     tests/test_optimizer/test_dist_galore.py::test_dist_galore
2025-04-11T04:23:19.1762292Z 8.67s call     tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
2025-04-11T04:23:19.1762841Z 8.57s call     tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
2025-04-11T04:23:19.1763267Z 8.45s call     tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
2025-04-11T04:23:19.1763685Z 8.35s call     tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
2025-04-11T04:23:19.1764081Z 8.16s call     tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
2025-04-11T04:23:19.1764514Z 7.92s call     tests/test_zero/test_gemini/test_optim.py::test_optim[4]
2025-04-11T04:23:19.1765000Z 7.53s call     tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
2025-04-11T04:23:19.1765541Z 7.41s call     tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
2025-04-11T04:23:19.1765955Z 7.38s call     tests/test_lora/test_lora.py::test_torch_ddp_lora
2025-04-11T04:23:19.1766339Z 7.25s call     tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
2025-04-11T04:23:19.1766734Z 7.08s call     tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
2025-04-11T04:23:19.1767145Z 6.86s call     tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
2025-04-11T04:23:19.1767580Z 6.67s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
2025-04-11T04:23:19.1768075Z 6.46s call     tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
2025-04-11T04:23:19.1768534Z 6.42s call     tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
2025-04-11T04:23:19.1769011Z 6.40s call     tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
2025-04-11T04:23:19.1769465Z 6.31s call     tests/test_zero/test_gemini/test_search.py::test_search[4]
2025-04-11T04:23:19.1769898Z 6.29s call     tests/test_zero/test_gemini/test_inference.py::test_inference[1]
2025-04-11T04:23:19.1770297Z 6.25s call     tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
2025-04-11T04:23:19.1770712Z 6.22s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
2025-04-11T04:23:19.1771148Z 6.21s call     tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
2025-04-11T04:23:19.1771570Z 6.11s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
2025-04-11T04:23:19.1772042Z 6.02s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
2025-04-11T04:23:19.1772453Z 6.01s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
2025-04-11T04:23:19.1772863Z 5.96s call     tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
2025-04-11T04:23:19.1773228Z 5.89s call     tests/test_infer/test_streamingllm.py::test_engine
2025-04-11T04:23:19.1773596Z 5.81s call     tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
2025-04-11T04:23:19.1773980Z 5.79s call     tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
2025-04-11T04:23:19.1774361Z 5.73s call     tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
2025-04-11T04:23:19.1774813Z 5.53s call     tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
2025-04-11T04:23:19.1775252Z 5.40s call     tests/test_fp8/test_fp8_allgather.py::test_all_gather
2025-04-11T04:23:19.1775627Z 5.40s call     tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
2025-04-11T04:23:19.1776014Z 5.37s call     tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
2025-04-11T04:23:19.1776428Z 5.35s call     tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
2025-04-11T04:23:19.1776902Z 5.31s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
2025-04-11T04:23:19.1777299Z 5.28s call     tests/test_infer/test_drafter.py::test_spec_dec
2025-04-11T04:23:19.1777669Z 5.22s call     tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
2025-04-11T04:23:19.1778085Z 5.21s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
2025-04-11T04:23:19.1778533Z 5.15s call     tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
2025-04-11T04:23:19.1778968Z 5.15s call     tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
2025-04-11T04:23:19.1779471Z 5.14s call     tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T04:23:19.1779976Z 5.14s call     tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
2025-04-11T04:23:19.1780447Z 5.09s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
2025-04-11T04:23:19.1780855Z 5.08s call     tests/test_device/test_init_logical_pg.py::test_logical_pg
2025-04-11T04:23:19.1781245Z 5.06s call     tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
2025-04-11T04:23:19.1781652Z 5.04s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
2025-04-11T04:23:19.1782076Z 5.04s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
2025-04-11T04:23:19.1782488Z 5.03s call     tests/test_tensor/test_padded_tensor.py::test_padded_tensor
2025-04-11T04:23:19.1782893Z 5.02s call     tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
2025-04-11T04:23:19.1783368Z 4.99s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
2025-04-11T04:23:19.1783815Z 4.96s call     tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
2025-04-11T04:23:19.1784255Z 4.72s call     tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T04:23:19.1784636Z 4.68s call     tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T04:23:19.1785031Z 4.66s call     tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
2025-04-11T04:23:19.1785540Z 4.48s call     tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
2025-04-11T04:23:19.1785952Z 4.41s call     tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
2025-04-11T04:23:19.1786320Z 4.38s call     tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
2025-04-11T04:23:19.1786716Z 4.31s call     tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
2025-04-11T04:23:19.1787112Z 4.29s call     tests/test_tensor/test_shape_consistency_apply.py::test_apply
2025-04-11T04:23:19.1787509Z 4.18s call     tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
2025-04-11T04:23:19.1787948Z 4.16s call     tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
2025-04-11T04:23:19.1788365Z 4.11s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
2025-04-11T04:23:19.1788865Z 4.10s call     tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
2025-04-11T04:23:19.1789378Z 4.10s call     tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
2025-04-11T04:23:19.1789867Z 4.10s call     tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
2025-04-11T04:23:19.1790309Z 4.09s call     tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
2025-04-11T04:23:19.1790761Z 4.08s call     tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T04:23:19.1791208Z 4.07s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
2025-04-11T04:23:19.1791642Z 4.07s call     tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
2025-04-11T04:23:19.1792062Z 4.05s call     tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
2025-04-11T04:23:19.1792474Z 4.04s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
2025-04-11T04:23:19.1792889Z 4.03s call     tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
2025-04-11T04:23:19.1793294Z 4.03s call     tests/test_shardformer/test_layer/test_dropout.py::test_dropout
2025-04-11T04:23:19.1793685Z 3.82s call     tests/test_infer/test_kvcache_manager.py::test_cache_manager
2025-04-11T04:23:19.1794192Z 3.80s call     tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
2025-04-11T04:23:19.1794624Z 3.79s call     tests/test_zero/test_gemini/test_search.py::test_search[1]
2025-04-11T04:23:19.1795019Z 3.69s call     tests/test_infer/test_config_and_struct.py::test_config_and_inference
2025-04-11T04:23:19.1795437Z 3.48s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
2025-04-11T04:23:19.1795925Z 0.66s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
2025-04-11T04:23:19.1796491Z 0.57s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
2025-04-11T04:23:19.1797060Z 0.56s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
2025-04-11T04:23:19.1797528Z 0.55s setup    tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T04:23:19.1798011Z 0.47s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
2025-04-11T04:23:19.1798593Z 0.36s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
2025-04-11T04:23:19.1799169Z 0.35s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
2025-04-11T04:23:19.1799807Z 0.33s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
2025-04-11T04:23:19.1800367Z 0.32s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
2025-04-11T04:23:19.1800885Z 0.27s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
2025-04-11T04:23:19.1801412Z 0.27s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
2025-04-11T04:23:19.1801830Z 0.22s call     tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
2025-04-11T04:23:19.1802230Z 0.21s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
2025-04-11T04:23:19.1802616Z 0.20s call     tests/test_fp8/test_fp8_cast.py::test_fp8_cast
2025-04-11T04:23:19.1802995Z 0.19s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
2025-04-11T04:23:19.1803417Z 0.19s setup    tests/test_infer/test_kvcache_manager.py::test_logical_blocks
2025-04-11T04:23:19.1803979Z 0.18s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_distribution
2025-04-11T04:23:19.1804474Z 0.17s setup    tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
2025-04-11T04:23:19.1804845Z 0.17s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
2025-04-11T04:23:19.1805319Z 0.17s call     tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
2025-04-11T04:23:19.1805777Z 0.16s setup    tests/test_optimizer/test_dist_came.py::test_dist_came
2025-04-11T04:23:19.1806147Z 0.16s setup    tests/test_optimizer/test_dist_galore.py::test_dist_galore
2025-04-11T04:23:19.1806616Z 0.16s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T04:23:19.1807108Z 0.16s setup    tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
2025-04-11T04:23:19.1807554Z 0.16s setup    tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T04:23:19.1808056Z 0.16s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_distribution
2025-04-11T04:23:19.1808650Z 0.16s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T04:23:19.1809255Z 0.16s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
2025-04-11T04:23:19.1809756Z 0.15s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
2025-04-11T04:23:19.1810302Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
2025-04-11T04:23:19.1810867Z 0.15s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T04:23:19.1811440Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
2025-04-11T04:23:19.1812052Z 0.15s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
2025-04-11T04:23:19.1812660Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
2025-04-11T04:23:19.1813257Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
2025-04-11T04:23:19.1813819Z 0.15s setup    tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T04:23:19.1814380Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
2025-04-11T04:23:19.1814978Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
2025-04-11T04:23:19.1815633Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
2025-04-11T04:23:19.1816150Z 0.15s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
2025-04-11T04:23:19.1816672Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
2025-04-11T04:23:19.1817196Z 0.14s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
2025-04-11T04:23:19.1817686Z 0.14s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
2025-04-11T04:23:19.1818105Z 0.14s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
2025-04-11T04:23:19.1818630Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
2025-04-11T04:23:19.1819166Z 0.14s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
2025-04-11T04:23:19.1819615Z 0.14s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
2025-04-11T04:23:19.1820190Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
2025-04-11T04:23:19.1820811Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
2025-04-11T04:23:19.1821309Z 0.14s setup    tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
2025-04-11T04:23:19.1821802Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
2025-04-11T04:23:19.1822324Z 0.14s setup    tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
2025-04-11T04:23:19.1822846Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
2025-04-11T04:23:19.1823441Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
2025-04-11T04:23:19.1824038Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
2025-04-11T04:23:19.1824633Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
2025-04-11T04:23:19.1825192Z 0.14s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_batch_size
2025-04-11T04:23:19.1825662Z 0.14s setup    tests/test_tensor/test_shape_consistency.py::test_one_step_transform
2025-04-11T04:23:19.1826093Z 0.14s setup    tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
2025-04-11T04:23:19.1826658Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
2025-04-11T04:23:19.1827181Z 0.14s setup    tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
2025-04-11T04:23:19.1827592Z 0.14s setup    tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
2025-04-11T04:23:19.1827986Z 0.14s setup    tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
2025-04-11T04:23:19.1828520Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
2025-04-11T04:23:19.1828999Z 0.14s setup    tests/test_infer/test_drafter.py::test_spec_dec
2025-04-11T04:23:19.1829394Z 0.14s setup    tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
2025-04-11T04:23:19.1829908Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
2025-04-11T04:23:19.1830533Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
2025-04-11T04:23:19.1831098Z 0.14s setup    tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
2025-04-11T04:23:19.1831578Z 0.14s setup    tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
2025-04-11T04:23:19.1832058Z 0.14s setup    tests/test_tensor/test_padded_tensor.py::test_padded_tensor
2025-04-11T04:23:19.1832456Z 0.14s setup    tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
2025-04-11T04:23:19.1832862Z 0.14s setup    tests/test_shardformer/test_model/test_shard_bert.py::test_bert
2025-04-11T04:23:19.1833272Z 0.14s setup    tests/test_shardformer/test_layer/test_dropout.py::test_dropout
2025-04-11T04:23:19.1833942Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
2025-04-11T04:23:19.1834449Z 0.14s setup    tests/test_infer/test_kvcache_manager.py::test_cache_manager
2025-04-11T04:23:19.1834840Z 0.14s setup    tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
2025-04-11T04:23:19.1835350Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
2025-04-11T04:23:19.1835867Z 0.14s setup    tests/test_tensor/test_sharding_spec.py::test_sharding_spec
2025-04-11T04:23:19.1836392Z 0.14s setup    tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
2025-04-11T04:23:19.1836914Z 0.14s setup    tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py::test_dtensor_sharding_spec
2025-04-11T04:23:19.1837498Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
2025-04-11T04:23:19.1838027Z 0.14s setup    tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
2025-04-11T04:23:19.1838467Z 0.14s setup    tests/test_zero/test_gemini/test_search.py::test_search[4]
2025-04-11T04:23:19.1838880Z 0.14s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
2025-04-11T04:23:19.1839366Z 0.14s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
2025-04-11T04:23:19.1839803Z 0.14s setup    tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
2025-04-11T04:23:19.1840231Z 0.14s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
2025-04-11T04:23:19.1840733Z 0.14s setup    tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
2025-04-11T04:23:19.1841224Z 0.14s setup    tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
2025-04-11T04:23:19.1841636Z 0.14s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
2025-04-11T04:23:19.1842054Z 0.14s setup    tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
2025-04-11T04:23:19.1842507Z 0.14s setup    tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T04:23:19.1842961Z 0.14s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
2025-04-11T04:23:19.1843449Z 0.14s setup    tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
2025-04-11T04:23:19.1843872Z 0.14s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
2025-04-11T04:23:19.1844300Z 0.14s setup    tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
2025-04-11T04:23:19.1844729Z 0.14s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[1]
2025-04-11T04:23:19.1845148Z 0.14s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
2025-04-11T04:23:19.1845538Z 0.14s setup    tests/test_zero/test_gemini/test_search.py::test_search[1]
2025-04-11T04:23:19.1845940Z 0.14s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[4]
2025-04-11T04:23:19.1846360Z 0.14s setup    tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
2025-04-11T04:23:19.1846773Z 0.14s setup    tests/test_zero/test_gemini/test_optim.py::test_optim[4]
2025-04-11T04:23:19.1847221Z 0.14s setup    tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
2025-04-11T04:23:19.1847818Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
2025-04-11T04:23:19.1848540Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
2025-04-11T04:23:19.1849159Z 0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
2025-04-11T04:23:19.1849779Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
2025-04-11T04:23:19.1850448Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
2025-04-11T04:23:19.1851078Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
2025-04-11T04:23:19.1851699Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
2025-04-11T04:23:19.1852306Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
2025-04-11T04:23:19.1852935Z 0.14s setup    tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
2025-04-11T04:23:19.1853498Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
2025-04-11T04:23:19.1854116Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
2025-04-11T04:23:19.1854695Z 0.13s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T04:23:19.1855311Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
2025-04-11T04:23:19.1855935Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
2025-04-11T04:23:19.1856533Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
2025-04-11T04:23:19.1857128Z 0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
2025-04-11T04:23:19.1857749Z 0.13s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
2025-04-11T04:23:19.1858364Z 0.13s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
2025-04-11T04:23:19.1858963Z 0.13s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
2025-04-11T04:23:19.1859421Z 0.13s setup    tests/test_lazy/test_ops.py::test_lazy_ops
2025-04-11T04:23:19.1859949Z 0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
2025-04-11T04:23:19.1860570Z 0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
2025-04-11T04:23:19.1861153Z 0.12s setup    tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
2025-04-11T04:23:19.1861738Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
2025-04-11T04:23:19.1862348Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
2025-04-11T04:23:19.1862960Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
2025-04-11T04:23:19.1863556Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
2025-04-11T04:23:19.1864151Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
2025-04-11T04:23:19.1864731Z 0.12s setup    tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
2025-04-11T04:23:19.1865303Z 0.12s setup    tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
2025-04-11T04:23:19.1865770Z 0.12s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
2025-04-11T04:23:19.1866314Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
2025-04-11T04:23:19.1866787Z 0.12s setup    tests/test_fp8/test_fp8_cast.py::test_fp8_cast
2025-04-11T04:23:19.1867304Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
2025-04-11T04:23:19.1867780Z 0.12s setup    tests/test_lora/test_lora.py::test_torch_ddp_lora
2025-04-11T04:23:19.1868242Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
2025-04-11T04:23:19.1868873Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
2025-04-11T04:23:19.1869356Z 0.12s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
2025-04-11T04:23:19.1869812Z 0.12s setup    tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
2025-04-11T04:23:19.1870314Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
2025-04-11T04:23:19.1870929Z 0.11s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
2025-04-11T04:23:19.1871514Z 0.11s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T04:23:19.1872039Z 0.11s setup    tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
2025-04-11T04:23:19.1872568Z 0.11s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T04:23:19.1873078Z 0.11s setup    tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
2025-04-11T04:23:19.1873562Z 0.11s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
2025-04-11T04:23:19.1874066Z 0.11s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T04:23:19.1874561Z 0.11s setup    tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
2025-04-11T04:23:19.1874971Z 0.11s setup    tests/test_config/test_load_config.py::test_load_config
2025-04-11T04:23:19.1875466Z 0.11s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
2025-04-11T04:23:19.1876142Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
2025-04-11T04:23:19.1876757Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
2025-04-11T04:23:19.1877403Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
2025-04-11T04:23:19.1877934Z 0.11s setup    tests/test_infer/test_streamingllm.py::test_engine
2025-04-11T04:23:19.1878360Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
2025-04-11T04:23:19.1878857Z 0.11s setup    tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
2025-04-11T04:23:19.1879349Z 0.11s setup    tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
2025-04-11T04:23:19.1879780Z 0.11s setup    tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
2025-04-11T04:23:19.1880204Z 0.11s setup    tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
2025-04-11T04:23:19.1880756Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
2025-04-11T04:23:19.1881364Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
2025-04-11T04:23:19.1882036Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
2025-04-11T04:23:19.1882646Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
2025-04-11T04:23:19.1883234Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
2025-04-11T04:23:19.1883762Z 0.11s setup    tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
2025-04-11T04:23:19.1884153Z 0.11s setup    tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
2025-04-11T04:23:19.1884586Z 0.11s setup    tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
2025-04-11T04:23:19.1884978Z 0.11s setup    tests/test_infer/test_batch_bucket.py::test_bucket
2025-04-11T04:23:19.1885442Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
2025-04-11T04:23:19.1886005Z 0.11s setup    tests/test_infer/test_continuous_batching.py::test_continuous_batching
2025-04-11T04:23:19.1886397Z 0.11s setup    tests/test_fp8/test_fp8_hook.py::test_fp8_hook
2025-04-11T04:23:19.1886745Z 0.11s setup    tests/test_fp8/test_fp8_allgather.py::test_all_gather
2025-04-11T04:23:19.1887120Z 0.11s setup    tests/test_device/test_init_logical_pg.py::test_logical_pg
2025-04-11T04:23:19.1887558Z 0.11s setup    tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
2025-04-11T04:23:19.1888085Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
2025-04-11T04:23:19.1888575Z 0.11s setup    tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
2025-04-11T04:23:19.1889003Z 0.11s setup    tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
2025-04-11T04:23:19.1889516Z 0.11s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.1890120Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T04:23:19.1890627Z 0.11s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
2025-04-11T04:23:19.1891040Z 0.11s setup    tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
2025-04-11T04:23:19.1891542Z 0.11s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
2025-04-11T04:23:19.1892107Z 0.11s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_layers
2025-04-11T04:23:19.1892754Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
2025-04-11T04:23:19.1893274Z 0.11s setup    tests/test_shardformer/test_shard_utils.py::test_release_layer
2025-04-11T04:23:19.1893718Z 0.11s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
2025-04-11T04:23:19.1908708Z 0.11s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.1909435Z 0.10s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.1910018Z 0.10s setup    tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
2025-04-11T04:23:19.1910609Z 0.10s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
2025-04-11T04:23:19.1911144Z 0.10s setup    tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
2025-04-11T04:23:19.1911633Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T04:23:19.1912131Z 0.10s setup    tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
2025-04-11T04:23:19.1912695Z 0.10s setup    tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
2025-04-11T04:23:19.1913168Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
2025-04-11T04:23:19.1913659Z 0.10s setup    tests/test_shardformer/test_model/test_shard_command.py::test_command
2025-04-11T04:23:19.1914083Z 0.10s setup    tests/test_shardformer/test_model/test_shard_llama.py::test_llama
2025-04-11T04:23:19.1914565Z 0.10s setup    tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
2025-04-11T04:23:19.1914966Z 0.10s setup    tests/test_shardformer/test_model/test_shard_sam.py::test_sam
2025-04-11T04:23:19.1915388Z 0.10s setup    tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
2025-04-11T04:23:19.1915813Z 0.10s setup    tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
2025-04-11T04:23:19.1916221Z 0.10s setup    tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
2025-04-11T04:23:19.1916663Z 0.10s setup    tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
2025-04-11T04:23:19.1917140Z 0.10s setup    tests/test_shardformer/test_model/test_shard_t5.py::test_t5
2025-04-11T04:23:19.1917532Z 0.10s setup    tests/test_shardformer/test_model/test_shard_vit.py::test_vit
2025-04-11T04:23:19.1918025Z 0.10s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
2025-04-11T04:23:19.1918551Z 0.10s setup    tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
2025-04-11T04:23:19.1919076Z 0.10s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_layers
2025-04-11T04:23:19.1919591Z 0.10s setup    tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
2025-04-11T04:23:19.1920048Z 0.10s setup    tests/test_infer/test_async_engine/test_request_tracer.py::test_request_tracer
2025-04-11T04:23:19.1920605Z 0.10s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
2025-04-11T04:23:19.1921190Z 0.10s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_micro_batch
2025-04-11T04:23:19.1921736Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
2025-04-11T04:23:19.1922342Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
2025-04-11T04:23:19.1922922Z 0.10s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_merge_batch
2025-04-11T04:23:19.1923499Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
2025-04-11T04:23:19.1924145Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T04:23:19.1924619Z 0.10s setup    tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
2025-04-11T04:23:19.1925056Z 0.10s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
2025-04-11T04:23:19.1925542Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
2025-04-11T04:23:19.1926046Z 0.10s setup    tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
2025-04-11T04:23:19.1926565Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
2025-04-11T04:23:19.1927052Z 0.10s setup    tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
2025-04-11T04:23:19.1927468Z 0.10s setup    tests/test_tensor/test_shape_consistency.py::test_shape_consistency
2025-04-11T04:23:19.1927991Z 0.10s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
2025-04-11T04:23:19.1928498Z 0.10s setup    tests/test_tensor/test_shape_consistency_apply.py::test_apply
2025-04-11T04:23:19.1928939Z 0.10s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
2025-04-11T04:23:19.1929463Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-7]
2025-04-11T04:23:19.1930052Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T04:23:19.1930575Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T04:23:19.1931207Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
2025-04-11T04:23:19.1931652Z 0.10s call     tests/test_lazy/test_ops.py::test_lazy_ops
2025-04-11T04:23:19.1932148Z 0.10s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
2025-04-11T04:23:19.1932811Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
2025-04-11T04:23:19.1933448Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
2025-04-11T04:23:19.1933923Z 0.10s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
2025-04-11T04:23:19.1934445Z 0.10s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
2025-04-11T04:23:19.1934938Z 0.10s call     tests/test_fp8/test_fp8_hook.py::test_fp8_hook
2025-04-11T04:23:19.1935314Z 0.10s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T04:23:19.1935798Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T04:23:19.1936338Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T04:23:19.1936869Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T04:23:19.1937460Z 0.10s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
2025-04-11T04:23:19.1938128Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
2025-04-11T04:23:19.1938727Z 0.10s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
2025-04-11T04:23:19.1939244Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
2025-04-11T04:23:19.1939777Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
2025-04-11T04:23:19.1940369Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T04:23:19.1940920Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
2025-04-11T04:23:19.1941467Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
2025-04-11T04:23:19.1941961Z 0.09s call     tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
2025-04-11T04:23:19.1942455Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
2025-04-11T04:23:19.1942910Z 0.09s setup    tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
2025-04-11T04:23:19.1944053Z 0.09s call     tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
2025-04-11T04:23:19.1944484Z 0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
2025-04-11T04:23:19.1944972Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
2025-04-11T04:23:19.1945521Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
2025-04-11T04:23:19.1946140Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
2025-04-11T04:23:19.1946681Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T04:23:19.1947231Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
2025-04-11T04:23:19.1947775Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T04:23:19.1948385Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
2025-04-11T04:23:19.1948976Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
2025-04-11T04:23:19.1949506Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T04:23:19.1949964Z 0.09s setup    tests/test_booster/test_accelerator.py::test_accelerator
2025-04-11T04:23:19.1950327Z 0.09s call     tests/test_infer/test_batch_bucket.py::test_bucket
2025-04-11T04:23:19.1950763Z 0.09s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
2025-04-11T04:23:19.1951287Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
2025-04-11T04:23:19.1951879Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
2025-04-11T04:23:19.1952421Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
2025-04-11T04:23:19.1952961Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T04:23:19.1953527Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
2025-04-11T04:23:19.1954109Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
2025-04-11T04:23:19.1954653Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
2025-04-11T04:23:19.1955180Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T04:23:19.1955653Z 0.09s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
2025-04-11T04:23:19.1956115Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T04:23:19.1956578Z 0.09s call     tests/test_shardformer/test_shard_utils.py::test_release_layer
2025-04-11T04:23:19.1957094Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T04:23:19.1957643Z 0.09s setup    tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
2025-04-11T04:23:19.1958182Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
2025-04-11T04:23:19.1958724Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T04:23:19.1959280Z 0.09s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
2025-04-11T04:23:19.1959845Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
2025-04-11T04:23:19.1960398Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
2025-04-11T04:23:19.1960944Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
2025-04-11T04:23:19.1961470Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
2025-04-11T04:23:19.1961994Z 0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T04:23:19.1962602Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
2025-04-11T04:23:19.1963152Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T04:23:19.1963691Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
2025-04-11T04:23:19.1964361Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
2025-04-11T04:23:19.1964974Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
2025-04-11T04:23:19.1965532Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
2025-04-11T04:23:19.1966088Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
2025-04-11T04:23:19.1966579Z 0.09s call     tests/test_shardformer/test_model/test_shard_bert.py::test_bert
2025-04-11T04:23:19.1967115Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T04:23:19.1967671Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
2025-04-11T04:23:19.1968231Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
2025-04-11T04:23:19.1968791Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
2025-04-11T04:23:19.1969350Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
2025-04-11T04:23:19.1969905Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
2025-04-11T04:23:19.1970491Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
2025-04-11T04:23:19.1971075Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
2025-04-11T04:23:19.1971628Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
2025-04-11T04:23:19.1972109Z 0.09s call     tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
2025-04-11T04:23:19.1972596Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
2025-04-11T04:23:19.1973270Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
2025-04-11T04:23:19.1973899Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
2025-04-11T04:23:19.1974459Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
2025-04-11T04:23:19.1975025Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
2025-04-11T04:23:19.1975615Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
2025-04-11T04:23:19.1976195Z 0.09s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
2025-04-11T04:23:19.1976780Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
2025-04-11T04:23:19.1977349Z 0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
2025-04-11T04:23:19.1977957Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
2025-04-11T04:23:19.1978599Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
2025-04-11T04:23:19.1979270Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
2025-04-11T04:23:19.1979540Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
2025-04-11T04:23:19.1979780Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T04:23:19.1980138Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
2025-04-11T04:23:19.1980383Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
2025-04-11T04:23:19.1980654Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
2025-04-11T04:23:19.1980929Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
2025-04-11T04:23:19.1981214Z 0.09s call     tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
2025-04-11T04:23:19.1981497Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
2025-04-11T04:23:19.1981765Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
2025-04-11T04:23:19.1982068Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
2025-04-11T04:23:19.1982343Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
2025-04-11T04:23:19.1982613Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
2025-04-11T04:23:19.1982888Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
2025-04-11T04:23:19.1983158Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
2025-04-11T04:23:19.1983455Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
2025-04-11T04:23:19.1983699Z 0.09s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
2025-04-11T04:23:19.1983990Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
2025-04-11T04:23:19.1984288Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
2025-04-11T04:23:19.1984601Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
2025-04-11T04:23:19.1984894Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
2025-04-11T04:23:19.1985146Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
2025-04-11T04:23:19.1985389Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
2025-04-11T04:23:19.1985637Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
2025-04-11T04:23:19.1985885Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
2025-04-11T04:23:19.1986127Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
2025-04-11T04:23:19.1986374Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
2025-04-11T04:23:19.1986690Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
2025-04-11T04:23:19.1986981Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
2025-04-11T04:23:19.1987277Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
2025-04-11T04:23:19.1987638Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
2025-04-11T04:23:19.1987929Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
2025-04-11T04:23:19.1988229Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
2025-04-11T04:23:19.1988575Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
2025-04-11T04:23:19.1988926Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
2025-04-11T04:23:19.1989225Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
2025-04-11T04:23:19.1989509Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
2025-04-11T04:23:19.1989804Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
2025-04-11T04:23:19.1990101Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
2025-04-11T04:23:19.1990377Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
2025-04-11T04:23:19.1990632Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
2025-04-11T04:23:19.1990910Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
2025-04-11T04:23:19.1991204Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
2025-04-11T04:23:19.1991502Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
2025-04-11T04:23:19.1991792Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
2025-04-11T04:23:19.1992124Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
2025-04-11T04:23:19.1992423Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
2025-04-11T04:23:19.1992699Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
2025-04-11T04:23:19.1992976Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
2025-04-11T04:23:19.1993244Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
2025-04-11T04:23:19.1993523Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
2025-04-11T04:23:19.1993813Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
2025-04-11T04:23:19.1994099Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
2025-04-11T04:23:19.1994370Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
2025-04-11T04:23:19.1994702Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
2025-04-11T04:23:19.1994969Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
2025-04-11T04:23:19.1995209Z 0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
2025-04-11T04:23:19.1995532Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
2025-04-11T04:23:19.1995820Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
2025-04-11T04:23:19.1996069Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
2025-04-11T04:23:19.1996343Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
2025-04-11T04:23:19.1996669Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
2025-04-11T04:23:19.1996943Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
2025-04-11T04:23:19.1997216Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
2025-04-11T04:23:19.1997384Z 0.09s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
2025-04-11T04:23:19.1997661Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
2025-04-11T04:23:19.1997913Z 0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
2025-04-11T04:23:19.1998230Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
2025-04-11T04:23:19.1998521Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
2025-04-11T04:23:19.1998804Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
2025-04-11T04:23:19.1999092Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
2025-04-11T04:23:19.1999326Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T04:23:19.1999612Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T04:23:19.1999846Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T04:23:19.2000077Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T04:23:19.2000307Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T04:23:19.2000535Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
2025-04-11T04:23:19.2000815Z 0.09s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
2025-04-11T04:23:19.2001102Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2001329Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T04:23:19.2001624Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2001855Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T04:23:19.2002136Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T04:23:19.2002430Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2002654Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T04:23:19.2002948Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T04:23:19.2003259Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
2025-04-11T04:23:19.2003561Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2003852Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2004177Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
2025-04-11T04:23:19.2004470Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2004765Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2005062Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2005353Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2005651Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2005942Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2006234Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2006520Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2006815Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2007102Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2007442Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2007733Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2008020Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2008314Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2008534Z 0.09s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
2025-04-11T04:23:19.2008824Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2009114Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2009406Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2009745Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2010036Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2010322Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2010612Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2010943Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2011225Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2011517Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2011850Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2012138Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2012436Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
2025-04-11T04:23:19.2012732Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2013023Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2013312Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2013600Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2013893Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2014180Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2014468Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2014760Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2015107Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2015398Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2015708Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
2025-04-11T04:23:19.2016003Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2016282Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2016576Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2016860Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2017029Z 0.09s setup    tests/test_infer/test_models/test_custom_model.py::test_model
2025-04-11T04:23:19.2017310Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2017650Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2017950Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2018230Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2018567Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2018852Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2019142Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2019427Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2019764Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2020047Z 0.09s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
2025-04-11T04:23:19.2020340Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2020620Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
2025-04-11T04:23:19.2020901Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
2025-04-11T04:23:19.2021190Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2021479Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
2025-04-11T04:23:19.2021752Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
2025-04-11T04:23:19.2022035Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2022311Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
2025-04-11T04:23:19.2022595Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2022917Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
2025-04-11T04:23:19.2023208Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2023503Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2023774Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
2025-04-11T04:23:19.2024046Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
2025-04-11T04:23:19.2024325Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
2025-04-11T04:23:19.2024606Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
2025-04-11T04:23:19.2024875Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
2025-04-11T04:23:19.2025200Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
2025-04-11T04:23:19.2025474Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
2025-04-11T04:23:19.2025760Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2026053Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2026374Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
2025-04-11T04:23:19.2026650Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
2025-04-11T04:23:19.2026916Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
2025-04-11T04:23:19.2027255Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2027546Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2027819Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
2025-04-11T04:23:19.2028107Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2028390Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2028731Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
2025-04-11T04:23:19.2029013Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2029288Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
2025-04-11T04:23:19.2029571Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2029751Z 0.09s setup    tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
2025-04-11T04:23:19.2030039Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2030387Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2030676Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2030885Z 0.09s setup    tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
2025-04-11T04:23:19.2031170Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2031454Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2031737Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2032020Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2032305Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2032587Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2032933Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2033214Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2033503Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2033807Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
2025-04-11T04:23:19.2034094Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2034383Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2034676Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2035014Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2035301Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2035591Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2035877Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2036165Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2036447Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2036756Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
2025-04-11T04:23:19.2037042Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2037334Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2037619Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2037913Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2038261Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2038550Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2038839Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2039059Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
2025-04-11T04:23:19.2039346Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2039634Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2039924Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2040209Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2040483Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
2025-04-11T04:23:19.2040961Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2041243Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2041524Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2041862Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2042148Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2042432Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2042787Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2043072Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2043355Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2043637Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2043919Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2044209Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2044494Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2044785Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2045072Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2045354Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2045636Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2045971Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2046258Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2046549Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2046828Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2047111Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2047422Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
2025-04-11T04:23:19.2047706Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2048014Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
2025-04-11T04:23:19.2048320Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
2025-04-11T04:23:19.2048679Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
2025-04-11T04:23:19.2048981Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
2025-04-11T04:23:19.2049271Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2049622Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-7]
2025-04-11T04:23:19.2049926Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
2025-04-11T04:23:19.2050229Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
2025-04-11T04:23:19.2050587Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
2025-04-11T04:23:19.2050887Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
2025-04-11T04:23:19.2051195Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
2025-04-11T04:23:19.2051500Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
2025-04-11T04:23:19.2051665Z 0.09s call     tests/test_booster/test_accelerator.py::test_accelerator
2025-04-11T04:23:19.2051969Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
2025-04-11T04:23:19.2052281Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
2025-04-11T04:23:19.2052590Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
2025-04-11T04:23:19.2052902Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
2025-04-11T04:23:19.2053206Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
2025-04-11T04:23:19.2053511Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
2025-04-11T04:23:19.2053878Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
2025-04-11T04:23:19.2054185Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
2025-04-11T04:23:19.2054492Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
2025-04-11T04:23:19.2054797Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
2025-04-11T04:23:19.2055105Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
2025-04-11T04:23:19.2055414Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
2025-04-11T04:23:19.2055725Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
2025-04-11T04:23:19.2056027Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
2025-04-11T04:23:19.2056396Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
2025-04-11T04:23:19.2056698Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
2025-04-11T04:23:19.2057000Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
2025-04-11T04:23:19.2057323Z 0.09s setup    tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
2025-04-11T04:23:19.2057627Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
2025-04-11T04:23:19.2057931Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
2025-04-11T04:23:19.2058233Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
2025-04-11T04:23:19.2058531Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
2025-04-11T04:23:19.2058837Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
2025-04-11T04:23:19.2059126Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2059429Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
2025-04-11T04:23:19.2059736Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
2025-04-11T04:23:19.2060037Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
2025-04-11T04:23:19.2060344Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
2025-04-11T04:23:19.2060644Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
2025-04-11T04:23:19.2060933Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2061238Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
2025-04-11T04:23:19.2061590Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
2025-04-11T04:23:19.2061894Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
2025-04-11T04:23:19.2062191Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
2025-04-11T04:23:19.2062492Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
2025-04-11T04:23:19.2062795Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
2025-04-11T04:23:19.2063100Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
2025-04-11T04:23:19.2063400Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
2025-04-11T04:23:19.2063702Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
2025-04-11T04:23:19.2064000Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
2025-04-11T04:23:19.2064356Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
2025-04-11T04:23:19.2064653Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
2025-04-11T04:23:19.2064962Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
2025-04-11T04:23:19.2065304Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2065609Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
2025-04-11T04:23:19.2065915Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
2025-04-11T04:23:19.2066243Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2066548Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
2025-04-11T04:23:19.2066847Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
2025-04-11T04:23:19.2067150Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
2025-04-11T04:23:19.2067448Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
2025-04-11T04:23:19.2067752Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
2025-04-11T04:23:19.2068054Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
2025-04-11T04:23:19.2068353Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
2025-04-11T04:23:19.2068697Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
2025-04-11T04:23:19.2068996Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
2025-04-11T04:23:19.2069301Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
2025-04-11T04:23:19.2069658Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
2025-04-11T04:23:19.2069978Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
2025-04-11T04:23:19.2070280Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
2025-04-11T04:23:19.2070584Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
2025-04-11T04:23:19.2070881Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
2025-04-11T04:23:19.2071179Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2071487Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
2025-04-11T04:23:19.2071776Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2072134Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
2025-04-11T04:23:19.2072440Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
2025-04-11T04:23:19.2072737Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
2025-04-11T04:23:19.2073097Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
2025-04-11T04:23:19.2073375Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
2025-04-11T04:23:19.2073672Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
2025-04-11T04:23:19.2073970Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2074304Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2074605Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
2025-04-11T04:23:19.2074910Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
2025-04-11T04:23:19.2075214Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
2025-04-11T04:23:19.2075513Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
2025-04-11T04:23:19.2075819Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
2025-04-11T04:23:19.2076117Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
2025-04-11T04:23:19.2076408Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2076709Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
2025-04-11T04:23:19.2076999Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2077334Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2077620Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2077906Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
2025-04-11T04:23:19.2078194Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2078482Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2078769Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2079071Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
2025-04-11T04:23:19.2079356Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2079639Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2079975Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2080261Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2080541Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2080849Z 0.09s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
2025-04-11T04:23:19.2081145Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2081425Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2081713Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2082044Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2082335Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2082605Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
2025-04-11T04:23:19.2082895Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2083182Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2083464Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2083749Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2084049Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
2025-04-11T04:23:19.2084338Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2084625Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2084957Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2085240Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2085528Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2085820Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2086104Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2086383Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2086677Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2086967Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2087251Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2087519Z 0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
2025-04-11T04:23:19.2087799Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2088077Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
2025-04-11T04:23:19.2088411Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2088695Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2088984Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2089266Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2089598Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2089883Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2090147Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
2025-04-11T04:23:19.2090433Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2090713Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2090993Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2091278Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2091562Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2091813Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
2025-04-11T04:23:19.2092091Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2092361Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
2025-04-11T04:23:19.2092689Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2092978Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2093261Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2093551Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2093830Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2094116Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2094343Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
2025-04-11T04:23:19.2094629Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2094916Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2095186Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
2025-04-11T04:23:19.2095471Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2095757Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2096091Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2096341Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
2025-04-11T04:23:19.2096626Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2096905Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2097232Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2097521Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2097804Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2098091Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2098386Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2098667Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2098950Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2099178Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
2025-04-11T04:23:19.2099459Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2099743Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2100024Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2100330Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
2025-04-11T04:23:19.2100579Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
2025-04-11T04:23:19.2100861Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2101146Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2101429Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2101718Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2101999Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2102283Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2102455Z 0.09s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
2025-04-11T04:23:19.2102810Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2103086Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2103310Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
2025-04-11T04:23:19.2103610Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
2025-04-11T04:23:19.2103894Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2104181Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2104393Z 0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
2025-04-11T04:23:19.2104728Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2105008Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2105289Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2105570Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2105850Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2106101Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
2025-04-11T04:23:19.2106350Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
2025-04-11T04:23:19.2106634Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2106916Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2107138Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
2025-04-11T04:23:19.2107420Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2107667Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
2025-04-11T04:23:19.2108003Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2108257Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
2025-04-11T04:23:19.2108578Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2108859Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2109141Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2109395Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
2025-04-11T04:23:19.2109642Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
2025-04-11T04:23:19.2109925Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2110149Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
2025-04-11T04:23:19.2110493Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2110780Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2111057Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2111361Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
2025-04-11T04:23:19.2111581Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
2025-04-11T04:23:19.2111892Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
2025-04-11T04:23:19.2112142Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
2025-04-11T04:23:19.2112489Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2112737Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
2025-04-11T04:23:19.2113020Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2113303Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2113522Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
2025-04-11T04:23:19.2113809Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2114057Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
2025-04-11T04:23:19.2114308Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
2025-04-11T04:23:19.2114593Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2114844Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
2025-04-11T04:23:19.2115133Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2115500Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2115789Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2116043Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
2025-04-11T04:23:19.2116334Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2116553Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
2025-04-11T04:23:19.2116737Z 0.09s call     tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
2025-04-11T04:23:19.2117022Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2117238Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
2025-04-11T04:23:19.2117524Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2117810Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2118145Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2118334Z 0.09s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
2025-04-11T04:23:19.2118552Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
2025-04-11T04:23:19.2118884Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2119100Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
2025-04-11T04:23:19.2119350Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
2025-04-11T04:23:19.2119604Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
2025-04-11T04:23:19.2119958Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
2025-04-11T04:23:19.2120232Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
2025-04-11T04:23:19.2120540Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
2025-04-11T04:23:19.2120848Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
2025-04-11T04:23:19.2121067Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
2025-04-11T04:23:19.2121315Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
2025-04-11T04:23:19.2121620Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
2025-04-11T04:23:19.2121841Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
2025-04-11T04:23:19.2122059Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
2025-04-11T04:23:19.2122364Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
2025-04-11T04:23:19.2122651Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2122863Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
2025-04-11T04:23:19.2123223Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
2025-04-11T04:23:19.2123528Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
2025-04-11T04:23:19.2123837Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
2025-04-11T04:23:19.2124120Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2124401Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
2025-04-11T04:23:19.2124708Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
2025-04-11T04:23:19.2124981Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
2025-04-11T04:23:19.2125287Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
2025-04-11T04:23:19.2125638Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
2025-04-11T04:23:19.2125935Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
2025-04-11T04:23:19.2126229Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
2025-04-11T04:23:19.2126456Z 0.09s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
2025-04-11T04:23:19.2126737Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2127022Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2127320Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
2025-04-11T04:23:19.2127674Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
2025-04-11T04:23:19.2127971Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
2025-04-11T04:23:19.2128255Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2128554Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
2025-04-11T04:23:19.2128715Z 0.09s call     tests/test_shardformer/test_model/test_shard_sam.py::test_sam
2025-04-11T04:23:19.2128974Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
2025-04-11T04:23:19.2129255Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2129561Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
2025-04-11T04:23:19.2129858Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
2025-04-11T04:23:19.2130114Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
2025-04-11T04:23:19.2130410Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
2025-04-11T04:23:19.2130660Z 0.09s setup    tests/test_infer/test_config_and_struct.py::test_config_and_inference
2025-04-11T04:23:19.2130912Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
2025-04-11T04:23:19.2131202Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2131459Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
2025-04-11T04:23:19.2131760Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
2025-04-11T04:23:19.2132018Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
2025-04-11T04:23:19.2132271Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
2025-04-11T04:23:19.2132525Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
2025-04-11T04:23:19.2132813Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2133113Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
2025-04-11T04:23:19.2133398Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2133683Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2133864Z 0.09s setup    tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T04:23:19.2134163Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
2025-04-11T04:23:19.2134456Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2134703Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
2025-04-11T04:23:19.2134954Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
2025-04-11T04:23:19.2135181Z 0.09s call     tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
2025-04-11T04:23:19.2135458Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
2025-04-11T04:23:19.2135757Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
2025-04-11T04:23:19.2136059Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
2025-04-11T04:23:19.2136346Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2136631Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2136917Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2137091Z 0.09s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
2025-04-11T04:23:19.2137343Z 0.08s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
2025-04-11T04:23:19.2137623Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2137909Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2138209Z 0.08s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
2025-04-11T04:23:19.2138518Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
2025-04-11T04:23:19.2138738Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
2025-04-11T04:23:19.2138997Z 0.08s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
2025-04-11T04:23:19.2139268Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
2025-04-11T04:23:19.2139572Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
2025-04-11T04:23:19.2139842Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
2025-04-11T04:23:19.2140110Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
2025-04-11T04:23:19.2140412Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
2025-04-11T04:23:19.2140751Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2141036Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2141314Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2141661Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2141945Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2142244Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
2025-04-11T04:23:19.2142511Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
2025-04-11T04:23:19.2142845Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2143113Z 0.08s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.2143300Z 0.08s call     tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
2025-04-11T04:23:19.2143532Z 0.08s setup    tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
2025-04-11T04:23:19.2143799Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
2025-04-11T04:23:19.2144071Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
2025-04-11T04:23:19.2144356Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2144675Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
2025-04-11T04:23:19.2144975Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
2025-04-11T04:23:19.2145243Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
2025-04-11T04:23:19.2145521Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2145829Z 0.08s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
2025-04-11T04:23:19.2146096Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
2025-04-11T04:23:19.2146367Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
2025-04-11T04:23:19.2146659Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2146958Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-32]
2025-04-11T04:23:19.2147245Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2147418Z 0.08s call     tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
2025-04-11T04:23:19.2147710Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2147986Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
2025-04-11T04:23:19.2148311Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
2025-04-11T04:23:19.2148627Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2148898Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
2025-04-11T04:23:19.2149258Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
2025-04-11T04:23:19.2149428Z 0.08s setup    tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
2025-04-11T04:23:19.2149703Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
2025-04-11T04:23:19.2150000Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
2025-04-11T04:23:19.2150324Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
2025-04-11T04:23:19.2150586Z 0.08s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.2150762Z 0.08s call     tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
2025-04-11T04:23:19.2151045Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2151337Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2151608Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
2025-04-11T04:23:19.2151910Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
2025-04-11T04:23:19.2152178Z 0.08s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
2025-04-11T04:23:19.2152459Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2152678Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
2025-04-11T04:23:19.2152945Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
2025-04-11T04:23:19.2153212Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
2025-04-11T04:23:19.2153562Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2153840Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
2025-04-11T04:23:19.2154128Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2154400Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
2025-04-11T04:23:19.2154584Z 0.08s call     tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
2025-04-11T04:23:19.2154877Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2155146Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
2025-04-11T04:23:19.2155430Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2155714Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
2025-04-11T04:23:19.2156038Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
2025-04-11T04:23:19.2156319Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2156604Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
2025-04-11T04:23:19.2156945Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2157230Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2157528Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-7]
2025-04-11T04:23:19.2157800Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
2025-04-11T04:23:19.2158161Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
2025-04-11T04:23:19.2158429Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
2025-04-11T04:23:19.2158712Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2158985Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
2025-04-11T04:23:19.2159273Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2159577Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
2025-04-11T04:23:19.2159881Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-32]
2025-04-11T04:23:19.2160154Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
2025-04-11T04:23:19.2160420Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
2025-04-11T04:23:19.2160692Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
2025-04-11T04:23:19.2161158Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
2025-04-11T04:23:19.2161437Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
2025-04-11T04:23:19.2161745Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
2025-04-11T04:23:19.2162053Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
2025-04-11T04:23:19.2162338Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2162610Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
2025-04-11T04:23:19.2162899Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2163206Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
2025-04-11T04:23:19.2163515Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-7]
2025-04-11T04:23:19.2163866Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-32]
2025-04-11T04:23:19.2164153Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2164454Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-7]
2025-04-11T04:23:19.2164803Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-7]
2025-04-11T04:23:19.2165097Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2165385Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2165670Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2166022Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
2025-04-11T04:23:19.2166325Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-32]
2025-04-11T04:23:19.2166542Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
2025-04-11T04:23:19.2166830Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2167106Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
2025-04-11T04:23:19.2167420Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
2025-04-11T04:23:19.2167722Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
2025-04-11T04:23:19.2168026Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-32]
2025-04-11T04:23:19.2168241Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
2025-04-11T04:23:19.2168520Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
2025-04-11T04:23:19.2168873Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-32]
2025-04-11T04:23:19.2169164Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2169452Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2169769Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
2025-04-11T04:23:19.2170072Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
2025-04-11T04:23:19.2170379Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-32]
2025-04-11T04:23:19.2170689Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-32]
2025-04-11T04:23:19.2170976Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2171266Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2171625Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
2025-04-11T04:23:19.2171918Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
2025-04-11T04:23:19.2172218Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
2025-04-11T04:23:19.2172556Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2172839Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2173120Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2173424Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-32]
2025-04-11T04:23:19.2173775Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-32]
2025-04-11T04:23:19.2174071Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2174347Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
2025-04-11T04:23:19.2174513Z 0.08s call     tests/test_shardformer/test_model/test_shard_vit.py::test_vit
2025-04-11T04:23:19.2174812Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
2025-04-11T04:23:19.2175102Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2175322Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
2025-04-11T04:23:19.2175631Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
2025-04-11T04:23:19.2175930Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
2025-04-11T04:23:19.2176241Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-32]
2025-04-11T04:23:19.2176518Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2176880Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-32]
2025-04-11T04:23:19.2177169Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2177454Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2177737Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2178030Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-7]
2025-04-11T04:23:19.2178316Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2178584Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
2025-04-11T04:23:19.2178770Z 0.08s call     tests/test_shardformer/test_model/test_shard_command.py::test_command
2025-04-11T04:23:19.2179064Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-7]
2025-04-11T04:23:19.2179400Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
2025-04-11T04:23:19.2179685Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2179986Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-7]
2025-04-11T04:23:19.2180341Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-7]
2025-04-11T04:23:19.2180640Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-7]
2025-04-11T04:23:19.2180926Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2181275Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-32]
2025-04-11T04:23:19.2181580Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-32]
2025-04-11T04:23:19.2181875Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-7]
2025-04-11T04:23:19.2182167Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2182462Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-7]
2025-04-11T04:23:19.2182749Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2183029Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2183330Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
2025-04-11T04:23:19.2183607Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
2025-04-11T04:23:19.2183910Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
2025-04-11T04:23:19.2184202Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
2025-04-11T04:23:19.2184552Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
2025-04-11T04:23:19.2184860Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-32]
2025-04-11T04:23:19.2185078Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
2025-04-11T04:23:19.2185379Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-7]
2025-04-11T04:23:19.2185663Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2185950Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2186256Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-32]
2025-04-11T04:23:19.2186562Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
2025-04-11T04:23:19.2186914Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
2025-04-11T04:23:19.2187217Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
2025-04-11T04:23:19.2187517Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-7]
2025-04-11T04:23:19.2187874Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2188172Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
2025-04-11T04:23:19.2188507Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
2025-04-11T04:23:19.2188809Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-7]
2025-04-11T04:23:19.2189167Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2189473Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
2025-04-11T04:23:19.2189757Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2189980Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
2025-04-11T04:23:19.2190264Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2190533Z 0.08s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
2025-04-11T04:23:19.2190810Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
2025-04-11T04:23:19.2191093Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2191392Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
2025-04-11T04:23:19.2191675Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
2025-04-11T04:23:19.2191963Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2192316Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
2025-04-11T04:23:19.2192622Z 0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-7]
2025-04-11T04:23:19.2192922Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
2025-04-11T04:23:19.2193222Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
2025-04-11T04:23:19.2193513Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
2025-04-11T04:23:19.2193796Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2194016Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
2025-04-11T04:23:19.2194308Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2194639Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
2025-04-11T04:23:19.2194928Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2195218Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
2025-04-11T04:23:19.2195381Z 0.08s call     tests/test_shardformer/test_model/test_shard_t5.py::test_t5
2025-04-11T04:23:19.2195741Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
2025-04-11T04:23:19.2196043Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
2025-04-11T04:23:19.2196329Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2196668Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
2025-04-11T04:23:19.2196975Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
2025-04-11T04:23:19.2197272Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
2025-04-11T04:23:19.2197563Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2197858Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
2025-04-11T04:23:19.2198155Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
2025-04-11T04:23:19.2198436Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2198737Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
2025-04-11T04:23:19.2199033Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
2025-04-11T04:23:19.2199317Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2199616Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
2025-04-11T04:23:19.2199963Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
2025-04-11T04:23:19.2200254Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
2025-04-11T04:23:19.2200539Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2200835Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2201122Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2201425Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
2025-04-11T04:23:19.2201707Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2202006Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
2025-04-11T04:23:19.2202337Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2202620Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2202923Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
2025-04-11T04:23:19.2203203Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2203429Z 0.08s call     tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
2025-04-11T04:23:19.2203715Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
2025-04-11T04:23:19.2204020Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
2025-04-11T04:23:19.2204348Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
2025-04-11T04:23:19.2204641Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
2025-04-11T04:23:19.2204921Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2205211Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
2025-04-11T04:23:19.2205393Z 0.08s call     tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
2025-04-11T04:23:19.2205676Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2205961Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
2025-04-11T04:23:19.2206243Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2206527Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2206828Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
2025-04-11T04:23:19.2207115Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2207465Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
2025-04-11T04:23:19.2207757Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
2025-04-11T04:23:19.2208038Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2208323Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2208605Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2208907Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
2025-04-11T04:23:19.2209208Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
2025-04-11T04:23:19.2209506Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
2025-04-11T04:23:19.2209682Z 0.08s call     tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
2025-04-11T04:23:19.2210029Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2210312Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
2025-04-11T04:23:19.2210528Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
2025-04-11T04:23:19.2210868Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2211171Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
2025-04-11T04:23:19.2211461Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
2025-04-11T04:23:19.2211676Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
2025-04-11T04:23:19.2212008Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2212312Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
2025-04-11T04:23:19.2212594Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
2025-04-11T04:23:19.2212811Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
2025-04-11T04:23:19.2213093Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2213381Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2213666Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
2025-04-11T04:23:19.2213971Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
2025-04-11T04:23:19.2214249Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
2025-04-11T04:23:19.2214532Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2214815Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2215156Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2215460Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
2025-04-11T04:23:19.2215745Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
2025-04-11T04:23:19.2216026Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
2025-04-11T04:23:19.2216324Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
2025-04-11T04:23:19.2216504Z 0.08s call     tests/test_shardformer/test_model/test_shard_llama.py::test_llama
2025-04-11T04:23:19.2216785Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2217087Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
2025-04-11T04:23:19.2217368Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2217715Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
2025-04-11T04:23:19.2217997Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
2025-04-11T04:23:19.2218302Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
2025-04-11T04:23:19.2218634Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
2025-04-11T04:23:19.2218920Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2219204Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
2025-04-11T04:23:19.2219476Z 0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
2025-04-11T04:23:19.2219814Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
2025-04-11T04:23:19.2220118Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
2025-04-11T04:23:19.2220404Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2220626Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
2025-04-11T04:23:19.2220912Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
2025-04-11T04:23:19.2221197Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
2025-04-11T04:23:19.2221512Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
2025-04-11T04:23:19.2221796Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
2025-04-11T04:23:19.2222087Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
2025-04-11T04:23:19.2222372Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
2025-04-11T04:23:19.2222718Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
2025-04-11T04:23:19.2223023Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
2025-04-11T04:23:19.2223238Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
2025-04-11T04:23:19.2223532Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
2025-04-11T04:23:19.2223815Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
2025-04-11T04:23:19.2224120Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
2025-04-11T04:23:19.2224406Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
2025-04-11T04:23:19.2224697Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
2025-04-11T04:23:19.2224993Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
2025-04-11T04:23:19.2225331Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2225605Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2225889Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
2025-04-11T04:23:19.2226222Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
2025-04-11T04:23:19.2226511Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
2025-04-11T04:23:19.2226812Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
2025-04-11T04:23:19.2227100Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
2025-04-11T04:23:19.2227432Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
2025-04-11T04:23:19.2227716Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
2025-04-11T04:23:19.2227937Z 0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
2025-04-11T04:23:19.2228220Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2228552Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
2025-04-11T04:23:19.2228835Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
2025-04-11T04:23:19.2229122Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
2025-04-11T04:23:19.2229405Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
2025-04-11T04:23:19.2229681Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
2025-04-11T04:23:19.2229976Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
2025-04-11T04:23:19.2230254Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
2025-04-11T04:23:19.2230617Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
2025-04-11T04:23:19.2230926Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
2025-04-11T04:23:19.2231216Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
2025-04-11T04:23:19.2231503Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
2025-04-11T04:23:19.2231809Z 0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
2025-04-11T04:23:19.2232093Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
2025-04-11T04:23:19.2232378Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
2025-04-11T04:23:19.2232655Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
2025-04-11T04:23:19.2232995Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2233281Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
2025-04-11T04:23:19.2233562Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
2025-04-11T04:23:19.2233903Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
2025-04-11T04:23:19.2234183Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
2025-04-11T04:23:19.2234468Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
2025-04-11T04:23:19.2234747Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
2025-04-11T04:23:19.2235080Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
2025-04-11T04:23:19.2235373Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
2025-04-11T04:23:19.2235656Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
2025-04-11T04:23:19.2235935Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
2025-04-11T04:23:19.2236220Z 0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
2025-04-11T04:23:19.2236376Z 0.08s setup    tests/test_device/test_device_mesh.py::test_device_mesh
2025-04-11T04:23:19.2236576Z 0.08s setup    tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
2025-04-11T04:23:19.2236589Z 
2025-04-11T04:23:19.2236754Z (1097 durations < 0.005s hidden.  Use -vv to show these durations.)
2025-04-11T04:23:19.2236879Z =========================== short test summary info ============================
2025-04-11T04:23:19.2237140Z FAILED tests/test_booster/test_accelerator.py::test_accelerator - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2237428Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2237569Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2237734Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2238116Z FAILED tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2238121Z 
2025-04-11T04:23:19.2238244Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2238350Z Traceback (most recent call last):
2025-04-11T04:23:19.2238660Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2238740Z     fn(i, *args)
2025-04-11T04:23:19.2239001Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
2025-04-11T04:23:19.2239104Z     check_3d_plugin(early_stop=early_stop)
2025-04-11T04:23:19.2239362Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2239458Z     partial_func(**kwargs)
2025-04-11T04:23:19.2239724Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
2025-04-11T04:23:19.2239886Z     err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:19.2240100Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2240253Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2240506Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2240610Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2240883Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2240980Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2241082Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2241414Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2241553Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2241714Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2242089Z FAILED tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2242096Z 
2025-04-11T04:23:19.2242215Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2242364Z Traceback (most recent call last):
2025-04-11T04:23:19.2242648Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2242727Z     fn(i, *args)
2025-04-11T04:23:19.2242999Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
2025-04-11T04:23:19.2243096Z     check_dataloader_sharding()
2025-04-11T04:23:19.2243405Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
2025-04-11T04:23:19.2243516Z     batch = next(iter(train_dataloader))[0].cuda()
2025-04-11T04:23:19.2243618Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2243893Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2244024Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2244186Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2244534Z FAILED tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2244538Z 
2025-04-11T04:23:19.2244658Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2244749Z Traceback (most recent call last):
2025-04-11T04:23:19.2245036Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2245115Z     fn(i, *args)
2025-04-11T04:23:19.2245445Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
2025-04-11T04:23:19.2245553Z     check_gemini_plugin(early_stop=early_stop)
2025-04-11T04:23:19.2245805Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2245900Z     partial_func(**kwargs)
2025-04-11T04:23:19.2246148Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2246237Z     partial_func(**kwargs)
2025-04-11T04:23:19.2246481Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2246574Z     partial_func(**kwargs)
2025-04-11T04:23:19.2246674Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2246971Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
2025-04-11T04:23:19.2247168Z     err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
2025-04-11T04:23:19.2247384Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2247483Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2247733Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2247893Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2248173Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2248270Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2248369Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2248647Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2248832Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2248986Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2249379Z FAILED tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2249384Z 
2025-04-11T04:23:19.2249501Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2249595Z Traceback (most recent call last):
2025-04-11T04:23:19.2249930Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2250009Z     fn(i, *args)
2025-04-11T04:23:19.2250299Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
2025-04-11T04:23:19.2250420Z     check_low_level_zero_plugin(early_stop=early_stop)
2025-04-11T04:23:19.2250673Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2250756Z     partial_func(**kwargs)
2025-04-11T04:23:19.2251095Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
2025-04-11T04:23:19.2251233Z     err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:19.2251445Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2251548Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2251797Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2251896Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2252169Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2252266Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2252359Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2252632Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2252814Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2252969Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2253340Z FAILED tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2253346Z 
2025-04-11T04:23:19.2253462Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2253560Z Traceback (most recent call last):
2025-04-11T04:23:19.2253839Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2253913Z     fn(i, *args)
2025-04-11T04:23:19.2254187Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
2025-04-11T04:23:19.2254280Z     check_torch_ddp_plugin()
2025-04-11T04:23:19.2254589Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
2025-04-11T04:23:19.2254712Z     run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:19.2254929Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2255074Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2255324Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2255420Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2255688Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2255786Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2255880Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2256209Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2256337Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2256497Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2256882Z FAILED tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2256888Z 
2025-04-11T04:23:19.2257004Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2257146Z Traceback (most recent call last):
2025-04-11T04:23:19.2257428Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2257507Z     fn(i, *args)
2025-04-11T04:23:19.2257781Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
2025-04-11T04:23:19.2257877Z     check_torch_fsdp_plugin()
2025-04-11T04:23:19.2258187Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
2025-04-11T04:23:19.2258304Z     run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T04:23:19.2258522Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2258617Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2258874Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2258970Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2259240Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2259333Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2259427Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2259706Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2259835Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2259996Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2260417Z FAILED tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2260422Z 
2025-04-11T04:23:19.2260542Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2260637Z Traceback (most recent call last):
2025-04-11T04:23:19.2260920Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2260996Z     fn(i, *args)
2025-04-11T04:23:19.2261257Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
2025-04-11T04:23:19.2261342Z     exam_state_dict()
2025-04-11T04:23:19.2261553Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2261648Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2261897Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2261992Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2262253Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2262345Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2262492Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2262770Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2262903Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2263056Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2263426Z FAILED tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2263496Z 
2025-04-11T04:23:19.2263610Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2263700Z Traceback (most recent call last):
2025-04-11T04:23:19.2263983Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2264056Z     fn(i, *args)
2025-04-11T04:23:19.2264342Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
2025-04-11T04:23:19.2264433Z     exam_torch_load_from_gemini()
2025-04-11T04:23:19.2264695Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2264789Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2265038Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2265135Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2265401Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2265497Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2265590Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2265867Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2265994Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2266149Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2266469Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2266741Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2266869Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2267021Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2267374Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2267694Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2267822Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2267973Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2268322Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2268619Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2268743Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2268897Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2269245Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2269521Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2269642Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2269848Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2270193Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2270459Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2270582Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2270782Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2271156Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2271425Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2271549Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2271697Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2272130Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2272404Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2272524Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2272675Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2273065Z FAILED tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2273069Z 
2025-04-11T04:23:19.2273188Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2273280Z Traceback (most recent call last):
2025-04-11T04:23:19.2273570Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2273646Z     fn(i, *args)
2025-04-11T04:23:19.2273962Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
2025-04-11T04:23:19.2274042Z     exam_state_dict()
2025-04-11T04:23:19.2274295Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2274386Z     partial_func(**kwargs)
2025-04-11T04:23:19.2274636Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2274723Z     partial_func(**kwargs)
2025-04-11T04:23:19.2275023Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2275111Z     partial_func(**kwargs)
2025-04-11T04:23:19.2275206Z   [Previous line repeated 3 more times]
2025-04-11T04:23:19.2275416Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2275514Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2275762Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2275857Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2276123Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2276219Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2276317Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2276587Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2276715Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2276868Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2277209Z FAILED tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2277531Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2277660Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2277807Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2278234Z FAILED tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2278291Z 
2025-04-11T04:23:19.2278407Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2278497Z Traceback (most recent call last):
2025-04-11T04:23:19.2278787Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2278862Z     fn(i, *args)
2025-04-11T04:23:19.2279169Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
2025-04-11T04:23:19.2279257Z     exam_from_pretrained()
2025-04-11T04:23:19.2279607Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2279702Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2279952Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2280047Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2280318Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2280413Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2280506Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2280781Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2280908Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2281060Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2281369Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2281632Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2281758Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2281908Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2282210Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2282526Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2282657Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2282805Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2283100Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2283376Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2283498Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2283646Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2283942Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2284215Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2284337Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2284482Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2284811Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2285079Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2285206Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2285354Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2285786Z FAILED tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2285790Z 
2025-04-11T04:23:19.2285906Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2286006Z Traceback (most recent call last):
2025-04-11T04:23:19.2286293Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2286370Z     fn(i, *args)
2025-04-11T04:23:19.2286648Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
2025-04-11T04:23:19.2286790Z     check_torch_ddp_checkpointIO()
2025-04-11T04:23:19.2287044Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2287128Z     partial_func(**kwargs)
2025-04-11T04:23:19.2287378Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2287463Z     partial_func(**kwargs)
2025-04-11T04:23:19.2287706Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2287791Z     partial_func(**kwargs)
2025-04-11T04:23:19.2287891Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2288218Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
2025-04-11T04:23:19.2288456Z     model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
2025-04-11T04:23:19.2288661Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T04:23:19.2288848Z     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T04:23:19.2289115Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
2025-04-11T04:23:19.2289216Z     model = model.to(get_current_device())
2025-04-11T04:23:19.2289479Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.2289573Z     return self._apply(convert)
2025-04-11T04:23:19.2289888Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2289976Z     module._apply(fn)
2025-04-11T04:23:19.2290241Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2290335Z     param_applied = fn(param)
2025-04-11T04:23:19.2290604Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.2290814Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.2290914Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2291187Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2291320Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2291471Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2291841Z FAILED tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2291845Z 
2025-04-11T04:23:19.2291958Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2292104Z Traceback (most recent call last):
2025-04-11T04:23:19.2292387Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2292462Z     fn(i, *args)
2025-04-11T04:23:19.2292738Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
2025-04-11T04:23:19.2292824Z     check_torch_fsdp_ckpt()
2025-04-11T04:23:19.2293129Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2293211Z     partial_func(**kwargs)
2025-04-11T04:23:19.2293520Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
2025-04-11T04:23:19.2293714Z     fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
2025-04-11T04:23:19.2293913Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T04:23:19.2294105Z     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T04:23:19.2294419Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
2025-04-11T04:23:19.2294648Z     fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
2025-04-11T04:23:19.2294908Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
2025-04-11T04:23:19.2295023Z     self.module = FSDP(module, *args, **kwargs)
2025-04-11T04:23:19.2295385Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
2025-04-11T04:23:19.2295481Z     _init_param_handle_from_module(
2025-04-11T04:23:19.2295854Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
2025-04-11T04:23:19.2295943Z     _move_module_to_device(
2025-04-11T04:23:19.2296292Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
2025-04-11T04:23:19.2296468Z     _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
2025-04-11T04:23:19.2296813Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
2025-04-11T04:23:19.2296929Z     param.data = param.to(device_from_device_id)
2025-04-11T04:23:19.2297030Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2297310Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2297489Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2297649Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2297960Z FAILED tests/test_device/test_init_logical_pg.py::test_logical_pg - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2297967Z 
2025-04-11T04:23:19.2298086Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2298178Z Traceback (most recent call last):
2025-04-11T04:23:19.2298461Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2298536Z     fn(i, *args)
2025-04-11T04:23:19.2298773Z   File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
2025-04-11T04:23:19.2298896Z     tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
2025-04-11T04:23:19.2298990Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2299267Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2299394Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2299551Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2299919Z FAILED tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2299923Z 
2025-04-11T04:23:19.2300039Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2300129Z Traceback (most recent call last):
2025-04-11T04:23:19.2300409Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2300553Z     fn(i, *args)
2025-04-11T04:23:19.2300778Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
2025-04-11T04:23:19.2300861Z     check_all2all()
2025-04-11T04:23:19.2301078Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2301171Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2301424Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2301517Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2301840Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2301936Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2302032Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2302302Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2302434Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2302587Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2302877Z FAILED tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2302885Z 
2025-04-11T04:23:19.2302997Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2303086Z Traceback (most recent call last):
2025-04-11T04:23:19.2303365Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2303442Z     fn(i, *args)
2025-04-11T04:23:19.2303666Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
2025-04-11T04:23:19.2303744Z     check_4gpu()
2025-04-11T04:23:19.2303954Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2304052Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2304303Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2304398Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2304728Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2304827Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2304919Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2305193Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2305325Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2305478Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2305803Z FAILED tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2305810Z 
2025-04-11T04:23:19.2305923Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2306014Z Traceback (most recent call last):
2025-04-11T04:23:19.2306290Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2306368Z     fn(i, *args)
2025-04-11T04:23:19.2306600Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
2025-04-11T04:23:19.2306675Z     check_4gpu()
2025-04-11T04:23:19.2306944Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2307037Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2307293Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2307384Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2307655Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2307795Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2307887Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2308165Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2308291Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2308479Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2308775Z FAILED tests/test_fp8/test_fp8_allgather.py::test_all_gather - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2308779Z 
2025-04-11T04:23:19.2308947Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2309040Z Traceback (most recent call last):
2025-04-11T04:23:19.2309319Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2309396Z     fn(i, *args)
2025-04-11T04:23:19.2309621Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
2025-04-11T04:23:19.2309702Z     check_4gpu()
2025-04-11T04:23:19.2309913Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2310007Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2310257Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2310345Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2310625Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2310718Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2310815Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2311089Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2311219Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2311375Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2311670Z FAILED tests/test_fp8/test_fp8_allreduce.py::test_all_reduce - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2311677Z 
2025-04-11T04:23:19.2311843Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2311935Z Traceback (most recent call last):
2025-04-11T04:23:19.2312220Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2312295Z     fn(i, *args)
2025-04-11T04:23:19.2312520Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
2025-04-11T04:23:19.2312595Z     check_4gpu()
2025-04-11T04:23:19.2312843Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2312933Z     partial_func(**kwargs)
2025-04-11T04:23:19.2313145Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2313238Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2313487Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2313583Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2313850Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2313942Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2314093Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2314367Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2314498Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2314652Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2314871Z FAILED tests/test_fp8/test_fp8_cast.py::test_fp8_cast - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2315197Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2315326Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2315476Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2315763Z FAILED tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2315769Z 
2025-04-11T04:23:19.2315885Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2316020Z Traceback (most recent call last):
2025-04-11T04:23:19.2316302Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2316375Z     fn(i, *args)
2025-04-11T04:23:19.2316610Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
2025-04-11T04:23:19.2316686Z     run_model()
2025-04-11T04:23:19.2316897Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2316991Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2317239Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2317332Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2317597Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2317692Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2317786Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2318063Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2318190Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2318341Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2318558Z FAILED tests/test_fp8/test_fp8_hook.py::test_fp8_hook - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2318823Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2319001Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2319154Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2319407Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2319684Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2319808Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2319960Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2320214Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2320483Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2320605Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2320760Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2321008Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2321328Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2321455Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2321603Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2321863Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2322182Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2322306Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2322453Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2322779Z FAILED tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2322783Z 
2025-04-11T04:23:19.2322898Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2322991Z Traceback (most recent call last):
2025-04-11T04:23:19.2323325Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2323402Z     fn(i, *args)
2025-04-11T04:23:19.2323635Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
2025-04-11T04:23:19.2323712Z     check_4gpu()
2025-04-11T04:23:19.2323926Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2324018Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2324270Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2324366Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2324628Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2324723Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2324819Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2325093Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2325217Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2325366Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2325601Z FAILED tests/test_infer/test_batch_bucket.py::test_bucket - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2325868Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2325994Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2326190Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2326543Z FAILED tests/test_infer/test_continuous_batching.py::test_continuous_batching - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2326549Z 
2025-04-11T04:23:19.2326664Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2326758Z Traceback (most recent call last):
2025-04-11T04:23:19.2327036Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2327109Z     fn(i, *args)
2025-04-11T04:23:19.2327352Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
2025-04-11T04:23:19.2327445Z     check_inference_engine()
2025-04-11T04:23:19.2327698Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2327783Z     partial_func(**kwargs)
2025-04-11T04:23:19.2328034Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2328118Z     partial_func(**kwargs)
2025-04-11T04:23:19.2328364Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2328515Z     partial_func(**kwargs)
2025-04-11T04:23:19.2328612Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2328894Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
2025-04-11T04:23:19.2329054Z     model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
2025-04-11T04:23:19.2329334Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2329478Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2329737Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2329854Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2330119Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2330203Z     module._apply(fn)
2025-04-11T04:23:19.2330466Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2330597Z     module._apply(fn)
2025-04-11T04:23:19.2330857Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2330949Z     param_applied = fn(param)
2025-04-11T04:23:19.2331219Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2331333Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2331434Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2331715Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2331846Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2332000Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2332234Z FAILED tests/test_infer/test_drafter.py::test_drafter[5] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2332508Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2332632Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2332785Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2333010Z FAILED tests/test_infer/test_drafter.py::test_spec_dec - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2333277Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2333400Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2333603Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2333922Z FAILED tests/test_infer/test_kvcache_manager.py::test_cache_manager - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2333928Z 
2025-04-11T04:23:19.2334041Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2334138Z Traceback (most recent call last):
2025-04-11T04:23:19.2334417Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2334494Z     fn(i, *args)
2025-04-11T04:23:19.2334722Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
2025-04-11T04:23:19.2334812Z     check_cache_manager()
2025-04-11T04:23:19.2335061Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2335143Z     partial_func(**kwargs)
2025-04-11T04:23:19.2335401Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
2025-04-11T04:23:19.2335552Z     cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T04:23:19.2335877Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T04:23:19.2336035Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:19.2336331Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T04:23:19.2336545Z     k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:19.2336690Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2336974Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2337098Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2337255Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2337622Z FAILED tests/test_infer/test_request_handler.py::test_running_list_and_request_handler - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2337628Z 
2025-04-11T04:23:19.2337744Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2337880Z Traceback (most recent call last):
2025-04-11T04:23:19.2338172Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2338245Z     fn(i, *args)
2025-04-11T04:23:19.2338472Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
2025-04-11T04:23:19.2338564Z     check_request_handler()
2025-04-11T04:23:19.2338824Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
2025-04-11T04:23:19.2338985Z     request_handler = RequestHandler(inference_config, model_config)
2025-04-11T04:23:19.2339234Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
2025-04-11T04:23:19.2339324Z     self._init_cache(model_config)
2025-04-11T04:23:19.2339586Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
2025-04-11T04:23:19.2339762Z     self.cache_manager = KVCacheManager(self.inference_config, model_config)
2025-04-11T04:23:19.2340019Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T04:23:19.2340172Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T04:23:19.2340467Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T04:23:19.2340674Z     k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T04:23:19.2340828Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2341102Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2341230Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2341384Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2341675Z FAILED tests/test_infer/test_streamingllm.py::test_engine - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2341679Z 
2025-04-11T04:23:19.2341796Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2341886Z Traceback (most recent call last):
2025-04-11T04:23:19.2342169Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2342244Z     fn(i, *args)
2025-04-11T04:23:19.2342470Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
2025-04-11T04:23:19.2342564Z     ret[rank] = func_to_run(**kwargs)
2025-04-11T04:23:19.2342809Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
2025-04-11T04:23:19.2342887Z     ).cuda()
2025-04-11T04:23:19.2343221Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2343323Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2343583Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2343693Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2343956Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2344090Z     module._apply(fn)
2025-04-11T04:23:19.2344354Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2344435Z     module._apply(fn)
2025-04-11T04:23:19.2344701Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2344792Z     param_applied = fn(param)
2025-04-11T04:23:19.2345061Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2345218Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2345317Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2345595Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2345722Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2345881Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2346239Z FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2346511Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2346635Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2346788Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2347159Z FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2347425Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2347553Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2347704Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2348039Z FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2348355Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2348532Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2348680Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2349012Z FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2349278Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2349400Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2349551Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2349888Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2350159Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2350281Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2350430Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2350827Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2351094Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2351219Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2351367Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2351766Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2352040Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2352167Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2352313Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2352704Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2352974Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2353099Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2353245Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2353575Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2353848Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2353972Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2354122Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2354466Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2354737Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2354860Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2355010Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2355339Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2355678Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2355810Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2355955Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2356293Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2356561Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2356687Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2356832Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2357171Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2357439Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2357560Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2357712Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2358100Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2358371Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2358492Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2358641Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2359028Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2359300Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2359422Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2359568Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2359912Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2360231Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2360360Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2360507Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2360846Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2361113Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2361240Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2361388Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2361721Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2361993Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2362115Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2362263Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2362604Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2362877Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2363055Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2363205Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2363543Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2363814Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2363940Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2364086Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2364422Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2364691Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2364818Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2364965Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2365301Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2365629Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2365751Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2365902Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2366273Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2366544Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2366668Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2366816Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2367148Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2367466Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2367592Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2367737Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2368072Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2368340Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2368466Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2368610Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2368946Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2369215Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2369336Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2369483Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2369812Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2370083Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2370255Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2370409Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2370746Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2371018Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2371142Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2371286Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2371628Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2371897Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2372024Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2372169Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2372505Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2372829Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2372954Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2373099Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2373432Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2373754Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2373878Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2374028Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2374363Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2374681Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2374805Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2374954Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2375290Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2375560Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2375689Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2375833Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2376177Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2376448Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2376572Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2376718Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2377055Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2377324Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2377444Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2377642Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2377976Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2378254Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2378373Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2378523Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2378860Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2379130Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2379250Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2379397Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2379733Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2380053Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2380177Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2380321Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2380659Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2380979Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2381103Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2381251Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2381588Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2381915Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2382039Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2382188Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2382499Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2382770Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2382890Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2383041Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2383350Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2383621Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2383746Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2383892Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2384200Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2384470Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2384596Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2384794Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2385109Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2385378Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2385502Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2385652Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2385960Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2386233Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2386353Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2386501Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2386810Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2387080Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2387252Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2387398Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2387706Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2387970Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2388147Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2388293Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2388654Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2388919Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2389045Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2389249Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2389558Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2389831Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2389956Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2390105Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2390413Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2390683Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2390806Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2390955Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2391266Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2391533Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2391659Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2391804Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2392174Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2392443Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2392568Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2392717Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2393029Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2393297Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2393417Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2393570Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2393877Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2394148Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2394270Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2394482Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2394788Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2395052Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2395176Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2395466Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2395780Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2396053Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2396179Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2396327Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2396736Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2397007Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2397128Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2397280Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2397634Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2397900Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2398021Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2398171Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2398524Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2398791Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2398911Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2399058Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2399411Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2399723Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2399851Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2399998Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2400329Z FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2400593Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2400716Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2400862Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2401182Z FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2401451Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2401571Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2401719Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2402185Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2402461Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2402581Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2402782Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2403182Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2403449Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2403571Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2403717Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2404164Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2404432Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2404556Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2404703Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2405103Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2405369Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2405491Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2405643Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2406039Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2406310Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2406430Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2406580Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2407043Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2407317Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2407439Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2407586Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2407983Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2408248Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2408376Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2408521Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2408923Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2409185Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2409368Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2409518Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2409911Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2410182Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2410369Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2410518Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2410912Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2411180Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2411302Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2411503Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2411898Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2412171Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2412296Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2412442Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2412845Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2413112Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2413240Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2413390Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2413785Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2414053Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2414180Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2414326Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2414774Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2415046Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2415170Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2415319Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2415709Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2415979Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2416102Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2416252Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2416649Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2416916Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2417092Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2417238Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2417636Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2417952Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2418078Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2418223Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2418627Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2418892Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2419058Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2419211Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2419606Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2419879Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2420000Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2420151Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2420549Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2420820Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2420944Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2421090Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2421496Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2421766Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2421890Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2422085Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2422495Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2422765Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2422889Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2423035Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2423431Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2423701Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2423823Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2423972Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2424374Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2424699Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2424819Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2424969Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2425368Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2425691Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2425814Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2425959Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2426358Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2426675Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2426804Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2426950Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2427346Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2427613Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2427740Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2427885Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2428276Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2428573Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2428695Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2428842Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2429238Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2429510Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2429689Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2429842Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2430241Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2430509Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2430634Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2430779Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2431179Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2431446Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2431571Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2431716Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2432179Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2432445Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2432566Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2432717Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2433169Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2433443Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2433565Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2433716Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2434166Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2434435Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2434557Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2434703Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2435114Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2435381Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2435505Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2435649Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2436052Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2436316Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2436440Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2436588Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2436978Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2437296Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2437418Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2437571Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2437967Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2438235Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2438356Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2438506Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2438894Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2439161Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2439280Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2439486Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2439887Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2440152Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2440278Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2440471Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2440871Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2441137Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2441262Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2441409Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2441845Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2442120Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2442244Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2442392Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2442784Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2443050Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2443175Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2443323Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2443719Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2443985Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2444111Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2444258Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2444706Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2444980Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2445106Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2445254Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2445657Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2445926Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2446048Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2446200Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2446598Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2446869Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2447044Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2447195Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2447592Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2447861Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2448032Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2448177Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2448576Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2448839Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2448966Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2449158Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2449562Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2449827Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2449952Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2450098Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2450489Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2450759Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2450885Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2451034Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2451431Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2451702Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2451822Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2451973Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2452423Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2452699Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2452825Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2452971Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2453376Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2453643Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2453769Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2453917Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2454315Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2454633Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2454758Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2454903Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2455298Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2455624Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2455744Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2455895Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2456289Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2456614Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2456736Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2456885Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2457283Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2457552Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2457679Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2457827Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2458227Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2458500Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2458622Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2458769Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2459172Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2459440Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2459559Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2459756Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2460158Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2460430Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2460550Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2460701Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2461100Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2461376Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2461498Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2461644Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2462044Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2462360Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2462483Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2462629Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2463028Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2463345Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2463474Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2463620Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2464015Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2464334Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2464458Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2464606Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2465010Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2465283Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2465403Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2465548Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2465946Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2466209Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2466332Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2466476Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2466881Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2467212Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2467338Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2467485Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2467890Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2468157Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2468281Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2468465Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2468872Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2469145Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2469266Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2469415Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2469870Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2470146Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2470268Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2470470Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2470868Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2471134Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2471258Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2471406Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2471852Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2472119Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2472242Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2472389Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2472790Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2473059Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2473179Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2473330Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2473726Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2473995Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2474117Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2474266Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2474722Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2474996Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2475118Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2475265Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2475668Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2475932Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2476058Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2476202Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2476603Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2476870Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2477048Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2477195Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2477590Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2477863Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2478033Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2478182Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2478585Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2478855Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2478978Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2479190Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2479591Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2479857Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2479985Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2480129Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2480539Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2480806Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2480935Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2481082Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2481486Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2481755Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2481881Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2482027Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2482477Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2482751Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2482874Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2483024Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2483429Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2483699Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2483822Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2483971Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2484385Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2484654Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2484831Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2484977Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2485381Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2485697Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2485821Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2485967Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2486374Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2486644Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2486810Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2486963Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2487357Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2487630Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2487751Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2487902Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2488301Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2488574Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2488695Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2488841Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2489242Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2489509Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2489633Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2489828Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2490236Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2490505Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2490629Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2490775Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2491171Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2491443Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2491565Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2491714Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2492117Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2492440Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2492560Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2492709Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2493109Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2493430Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2493555Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2493700Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2494105Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2494430Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2494562Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2494706Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2495100Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2495368Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2495491Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2495637Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2496022Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2496296Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2496417Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2496565Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2496947Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2497218Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2497387Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2497538Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2497924Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2498194Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2498319Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2498465Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2498853Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2499118Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2499244Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2499390Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2499831Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2500101Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2500223Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2500375Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2500808Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2501084Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2501206Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2501355Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2501791Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2502061Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2502180Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2502323Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2502702Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2502965Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2503088Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2503232Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2503612Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2503877Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2503999Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2504141Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2504516Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2504833Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2504957Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2505106Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2505491Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2505762Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2505882Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2506031Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2506414Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2506682Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2506806Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2506952Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2507393Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2507656Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2507779Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2507977Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2508359Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2508675Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2508797Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2508946Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2509389Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2509660Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2509780Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2509929Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2510310Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2510583Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2510703Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2510851Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2511239Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2511505Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2511632Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2511779Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2512165Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2512483Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2512611Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2512759Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2513145Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2513417Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2513537Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2513688Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2514072Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2514343Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2514464Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2514760Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2515152Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2515418Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2515544Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2515748Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2516136Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2516403Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2516526Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2516673Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2517111Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2517380Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2517509Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2517654Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2518037Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2518309Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2518430Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2518578Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2518965Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2519235Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2519359Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2519505Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2519936Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2520203Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2520329Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2520473Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2520859Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2521122Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2521247Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2521390Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2521776Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2522040Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2522215Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2522364Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2522754Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2523020Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2523205Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2523354Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2523737Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2524006Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2524129Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2524322Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2524719Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2524984Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2525109Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2525254Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2525642Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2525908Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2526035Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2526184Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2526568Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2526840Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2526964Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2527112Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2527541Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2527818Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2527940Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2528092Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2528473Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2528741Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2528867Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2529014Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2529400Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2529669Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2529844Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2529993Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2530377Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2530643Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2530813Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2530961Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2531339Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2531608Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2531775Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2531928Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2532311Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2532581Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2532703Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2532847Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2533226Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2533494Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2533623Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2533769Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2534148Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2534414Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2534536Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2534681Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2535107Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2535380Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2535506Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2535656Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2536032Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2536304Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2536425Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2536572Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2536948Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2537262Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2537389Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2537533Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2537919Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2538233Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2538358Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2538506Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2538891Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2539158Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2539323Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2539473Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2539851Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2540125Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2540244Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2540395Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2540773Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2541044Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2541165Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2541313Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2541697Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2541962Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2542084Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2542282Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2542667Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2542939Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2543064Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2543210Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2543598Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2543868Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2543990Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2544142Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2544529Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2544856Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2544976Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2545122Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2545506Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2545826Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2545948Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2546094Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2546484Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2546797Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2546923Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2547069Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2547455Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2547722Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2547847Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2547990Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2548370Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2548717Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2548837Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2548986Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2549367Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2549638Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2549814Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2549967Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2550345Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2550611Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2550735Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2550881Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2551265Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2551530Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2551656Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2551802Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2552187Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2552526Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2552646Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2552794Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2553186Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2553524Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2553648Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2553796Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2554175Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2554496Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2554618Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2554763Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2555157Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2555420Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2555547Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2555690Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2556070Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2556340Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2556461Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2556606Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2556984Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2557303Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2557426Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2557577Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2557960Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2558232Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2558354Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2558503Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2558884Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2559154Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2559279Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2559423Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2559860Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2560129Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2560252Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2560397Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2560834Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2561104Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2561225Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2561375Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2561804Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2562077Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2562196Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2562346Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2562729Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2563002Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2563123Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2563268Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2563652Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2563920Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2564042Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2564188Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2564572Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2564890Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2565017Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2565162Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2565546Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2565815Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2565936Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2566090Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2566469Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2566744Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2566864Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2567013Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2567444Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2567713Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2567833Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2568031Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2568414Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2568677Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2568801Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2568947Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2569368Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2569633Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2569755Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2569903Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2570279Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2570546Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2570667Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2570816Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2571192Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2571463Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2571582Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2571737Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2572118Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2572433Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2572560Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2572708Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2573093Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2573358Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2573482Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2573629Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2574015Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2574281Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2574400Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2574602Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2574988Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2575262Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2575437Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2575585Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2575975Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2576246Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2576367Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2576515Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2576942Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2577212Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2577337Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2577482Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2577870Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2578136Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2578261Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2578414Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2578793Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2579062Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2579183Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2579332Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2579776Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2580050Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2580173Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2580324Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2580702Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2580970Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2581095Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2581240Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2581626Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2581890Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2582065Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2582211Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2582593Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2582859Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2583027Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2583174Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2583557Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2583826Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2583947Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2584143Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2584527Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2584796Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2584918Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2585063Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2585453Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2585718Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2585843Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2585990Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2586372Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2586634Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2586758Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2586902Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2587334Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2587600Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2587726Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2587876Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2588250Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2588556Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2588680Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2588830Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2589208Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2589475Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2589656Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2589802Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2590183Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2590503Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2590626Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2590770Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2591155Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2591420Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2591597Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2591745Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2592124Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2592397Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2592517Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2592666Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2593051Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2593324Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2593446Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2593596Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2593974Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2594242Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2594365Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2594570Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2594955Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2595224Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2595349Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2595495Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2595873Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2596145Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2596264Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2596418Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2596798Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2597122Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2597246Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2597398Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2597776Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2598099Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2598221Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2598368Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2598757Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2599078Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2599205Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2599352Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2599729Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2599998Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2600122Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2600271Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2600649Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2600920Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2601041Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2601191Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2601569Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2601838Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2601960Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2602157Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2602542Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2602812Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2602936Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2603083Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2603461Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2603729Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2603853Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2604002Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2604386Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2604709Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2604830Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2604978Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2605359Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2605682Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2605803Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2605953Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2606337Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2606670Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2606795Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2606941Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2607326Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2607595Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2607722Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2607869Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2608254Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2608525Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2608649Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2608798Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2609179Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2609449Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2609619Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2609771Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2610150Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2610426Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2610545Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2610695Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2611072Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2611338Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2611459Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2611605Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2611987Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2612310Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2612436Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2612581Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2613009Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2613279Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2613407Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2613554Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2613995Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2614267Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2614387Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2614539Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2614923Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2615195Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2615315Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2615465Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2615851Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2616115Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2616239Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2616383Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2616766Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2617086Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2617212Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2617356Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2617745Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2618016Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2618138Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2618286Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2618668Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2618939Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2619060Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2619209Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2619640Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2619909Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2620030Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2620174Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2620611Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2620882Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2621006Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2621152Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2621592Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2621864Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2621987Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2622135Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2622515Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2622788Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2622907Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2623055Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2623441Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2623709Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2623829Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2623977Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2624356Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2624674Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2624802Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2624949Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2625337Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2625602Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2625726Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2625873Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2626250Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2626516Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2626634Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2626849Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2627234Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2627508Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2627629Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2627826Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2628207Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2628511Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2628633Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2628779Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2629216Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2629486Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2629611Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2629760Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2630146Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2630410Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2630535Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2630683Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2631073Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2631340Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2631463Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2631612Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2632136Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2632414Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2632537Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2632687Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2633077Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2633345Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2633468Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2633612Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2634003Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2634267Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2634446Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2634594Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2634989Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2635257Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2635448Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2635596Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2635988Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2636264Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2636387Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2636585Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2636981Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2637251Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2637375Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2637522Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2637911Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2638177Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2638304Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2638451Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2638845Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2639112Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2639239Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2639384Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2639823Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2640092Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2640215Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2640365Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2640747Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2641015Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2641138Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2641286Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2641674Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2641948Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2642119Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2642267Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2642655Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2642918Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2643097Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2643244Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2643636Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2643903Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2644077Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2644226Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2644618Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2644887Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2645008Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2645158Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2645545Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2645816Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2645941Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2646090Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2646478Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2646746Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2646868Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2647012Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2647453Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2647722Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2647849Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2647994Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2648377Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2648644Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2648767Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2648915Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2649292Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2649617Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2649740Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2649890Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2650278Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2650597Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2650718Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2650870Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2651252Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2651567Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2651694Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2651839Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2652222Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2652487Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2652611Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2652758Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2653139Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2653407Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2653527Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2653678Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2654056Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2654327Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2654449Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2654648Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2655031Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2655306Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2655426Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2655572Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2655955Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2656222Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2656347Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2656493Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2656875Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2657197Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2657322Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2657467Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2657846Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2658169Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2658291Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2658441Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2658818Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2659139Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2659263Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2659412Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2659796Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2660063Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2660188Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2660331Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2660714Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2660983Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2661107Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2661251Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2661637Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2661908Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2662074Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2662228Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2662618Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2662896Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2663014Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2663163Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2663549Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2663820Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2663942Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2664089Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2664479Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2664810Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2664937Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2665082Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2665513Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2665889Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2666032Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2666176Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2666617Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2666891Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2667012Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2667165Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2667559Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2667832Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2667953Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2668101Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2668524Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2668798Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2668920Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2669067Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2669462Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2669781Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2669909Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2670052Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2670445Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2670711Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2670837Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2670983Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2671371Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2671647Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2671768Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2671917Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2672360Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2672631Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2672752Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2672958Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2673343Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2673613Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2673738Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2673884Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2674323Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2674594Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2674717Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2674866Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2675260Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2675528Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2675649Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2675800Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2676184Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2676450Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2676570Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2676721Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2677106Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2677423Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2677548Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2677696Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2678067Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2678332Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2678456Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2678605Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2678967Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2679236Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2679362Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2679561Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2679921Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2680192Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2680315Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2680512Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2680867Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2681138Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2681259Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2681410Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2681810Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2682080Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2682205Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2682353Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2682714Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2682981Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2683108Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2683256Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2683617Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2683882Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2684002Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2684153Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2684518Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2684839Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2684963Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2685112Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2685469Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2685741Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2685862Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2686008Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2686377Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2686645Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2686772Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2686974Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2687336Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2687601Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2687775Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2687922Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2688283Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2688554Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2688673Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2688827Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2689229Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2689500Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2689624Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2689773Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2690129Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2690394Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2690516Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2690665Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2691026Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2691292Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2691418Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2691562Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2691988Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2692257Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2692378Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2692528Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2692880Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2693149Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2693270Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2693418Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2693777Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2694043Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2694162Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2694363Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2694724Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2694989Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2695164Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2695309Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2695669Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2695932Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2696057Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2696201Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2696603Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2696879Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2697003Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2697154Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2697516Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2697788Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2697912Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2698062Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2698420Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2698690Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2698814Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2698960Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2699389Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2699658Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2699784Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2699933Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2700297Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2700562Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2700685Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2700837Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2701198Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2701467Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2701641Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2701792Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2702149Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2702414Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2702585Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2702733Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2703093Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2703356Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2703484Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2703674Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2704042Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2704310Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2704441Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2704588Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2704954Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2705225Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2705348Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2705498Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2705857Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2706125Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2706247Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2706397Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2706816Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2707088Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2707209Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2707356Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2707717Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2707979Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2708107Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2708252Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2708658Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2708927Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2709118Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2709267Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2709627Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2709897Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2710071Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2710221Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2710590Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2710857Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2710980Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2711180Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2711538Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2711804Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2711931Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2712078Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2712438Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2712705Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2712832Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2712982Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2713347Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2713614Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2713739Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2713890Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2714311Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2714587Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2714714Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2714867Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2715227Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2715498Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2715623Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2715771Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2716140Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2716406Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2716580Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2716728Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2717091Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2717357Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2717535Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2717680Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2718049Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2718322Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2718445Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2718661Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2719020Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2719290Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2719415Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2719565Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2719932Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2720197Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2720329Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2720476Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2720839Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2721104Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2721232Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2721376Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2721795Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2722066Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2722190Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2722341Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2722642Z FAILED tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2722912Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2723035Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2723187Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2723559Z FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2723828Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2724001Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2724147Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2724523Z FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2724788Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2724963Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2725108Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2725439Z FAILED tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2725704Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2725830Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2726019Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2726295Z FAILED tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2726565Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2726689Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2726837Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2727050Z FAILED tests/test_lazy/test_ops.py::test_lazy_ops - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2727321Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2727445Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2727592Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2727883Z FAILED tests/test_lora/test_lora.py::test_torch_ddp_lora - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2727888Z 
2025-04-11T04:23:19.2728004Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2728104Z Traceback (most recent call last):
2025-04-11T04:23:19.2728403Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2728486Z     fn(i, *args)
2025-04-11T04:23:19.2728694Z   File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
2025-04-11T04:23:19.2728778Z     run_lora_test()
2025-04-11T04:23:19.2729041Z   File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
2025-04-11T04:23:19.2729219Z     check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
2025-04-11T04:23:19.2729438Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2729537Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2729794Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2729887Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2730164Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2730263Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2730357Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2730633Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2730757Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2730909Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2731203Z FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2731475Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2731599Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2731747Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2731988Z FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type1] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2732307Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2732432Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2732581Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2732900Z FAILED tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2732906Z 
2025-04-11T04:23:19.2733023Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2733167Z Traceback (most recent call last):
2025-04-11T04:23:19.2733448Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2733525Z     fn(i, *args)
2025-04-11T04:23:19.2733755Z   File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
2025-04-11T04:23:19.2733845Z     check_moe_checkpoint()
2025-04-11T04:23:19.2734098Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2734183Z     partial_func(**kwargs)
2025-04-11T04:23:19.2734438Z   File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
2025-04-11T04:23:19.2734570Z     dist.broadcast_object_list(broadcast_objects, src=0)
2025-04-11T04:23:19.2734857Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2734953Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2735304Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
2025-04-11T04:23:19.2735518Z     tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T04:23:19.2735842Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
2025-04-11T04:23:19.2736046Z     tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T04:23:19.2736437Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
2025-04-11T04:23:19.2736577Z     byte_tensor = torch.ByteTensor(byte_storage).to(device)
2025-04-11T04:23:19.2736676Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2736956Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2737085Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2737236Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2737582Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2737852Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2737979Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2738130Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2738465Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2738794Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2738916Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2739066Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2739396Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2739721Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2739843Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2739993Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2740320Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2740637Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2740764Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2740910Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2741241Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2741511Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2741633Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2741780Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2742104Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2742372Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2742495Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2742646Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2742970Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2743239Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2743358Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2743557Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2743885Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2744157Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2744280Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2744427Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2744755Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2745025Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2745150Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2745295Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2745626Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2745894Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2746071Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2746217Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2746546Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2746817Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2746999Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2747149Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2747475Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2747748Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2748007Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2748162Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2748513Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2748783Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2748909Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2749055Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2749382Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2749645Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2749770Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2749914Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2750244Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2750511Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2750634Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2750783Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2751164Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2751435Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2751559Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2751709Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2752032Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2752301Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2752424Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2752569Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2752895Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2753161Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2753342Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2753490Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2753817Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2754083Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2754261Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2754404Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2754729Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2755001Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2755123Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2755325Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2755708Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2755976Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2756101Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2756251Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2756634Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2756902Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2757031Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2757178Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2757564Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2757834Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2757961Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2758108Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2758537Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2758807Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2758930Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2759081Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2759452Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2759722Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2759845Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2759994Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2760363Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2760634Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2760810Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2760957Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2761335Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2761602Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2761775Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2761921Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2762301Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2762568Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2762692Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2762884Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2763260Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2763533Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2763658Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2763810Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2764177Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2764447Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2764570Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2764721Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2765095Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2765360Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2765485Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2765630Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2766056Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2766323Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2766449Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2766593Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2766961Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2767226Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2767348Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2767497Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2767863Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2768132Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2768306Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2768456Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2768823Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2769093Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2769267Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2769413Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2769791Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2770058Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2770186Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2770391Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2770771Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2771040Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2771167Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2771312Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2771680Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2771951Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2772074Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2772225Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2772602Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2772872Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2772996Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2773144Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2773567Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2773839Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2773965Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2774111Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2774486Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2774754Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2774881Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2775027Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2775396Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2775664Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2775850Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2776003Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2776377Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2776647Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2776817Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2776965Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2777337Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2777604Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2777774Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2777921Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2778254Z FAILED tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2778259Z 
2025-04-11T04:23:19.2778377Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2778478Z Traceback (most recent call last):
2025-04-11T04:23:19.2778764Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2778844Z     fn(i, *args)
2025-04-11T04:23:19.2779091Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
2025-04-11T04:23:19.2779186Z     exam_dist_adafactor_base()
2025-04-11T04:23:19.2779438Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2779529Z     partial_func(**kwargs)
2025-04-11T04:23:19.2779782Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2779864Z     partial_func(**kwargs)
2025-04-11T04:23:19.2780148Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
2025-04-11T04:23:19.2780303Z     model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
2025-04-11T04:23:19.2780566Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.2780657Z     return self._apply(convert)
2025-04-11T04:23:19.2780972Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2781070Z     param_applied = fn(param)
2025-04-11T04:23:19.2781347Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.2781572Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.2781671Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2781949Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2782074Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2782227Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2782537Z FAILED tests/test_optimizer/test_dist_came.py::test_dist_came - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2782542Z 
2025-04-11T04:23:19.2782659Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2782756Z Traceback (most recent call last):
2025-04-11T04:23:19.2783038Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2783168Z     fn(i, *args)
2025-04-11T04:23:19.2783397Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
2025-04-11T04:23:19.2783539Z     exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
2025-04-11T04:23:19.2783795Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2783879Z     partial_func(**kwargs)
2025-04-11T04:23:19.2784227Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
2025-04-11T04:23:19.2784449Z     ) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
2025-04-11T04:23:19.2784764Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
2025-04-11T04:23:19.2784855Z     org_model = org_model.cuda()
2025-04-11T04:23:19.2785137Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2785281Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2785541Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2785655Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2785916Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2786003Z     module._apply(fn)
2025-04-11T04:23:19.2786262Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2786345Z     module._apply(fn)
2025-04-11T04:23:19.2786603Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2786692Z     param_applied = fn(param)
2025-04-11T04:23:19.2786958Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2787071Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2787171Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2787446Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2787575Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2787731Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2788040Z FAILED tests/test_optimizer/test_dist_galore.py::test_dist_galore - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2788048Z 
2025-04-11T04:23:19.2788212Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2788307Z Traceback (most recent call last):
2025-04-11T04:23:19.2788617Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2788695Z     fn(i, *args)
2025-04-11T04:23:19.2788953Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
2025-04-11T04:23:19.2789035Z     dist.barrier()
2025-04-11T04:23:19.2789324Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2789411Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2789722Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:19.2789825Z     work = default_pg.barrier(opts=opts)
2025-04-11T04:23:19.2789919Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2790197Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2790323Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2790479Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2790839Z FAILED tests/test_optimizer/test_dist_lamb.py::test_dist_lamb - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2790843Z 
2025-04-11T04:23:19.2790958Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2791054Z Traceback (most recent call last):
2025-04-11T04:23:19.2791332Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2791470Z     fn(i, *args)
2025-04-11T04:23:19.2791718Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
2025-04-11T04:23:19.2791805Z     run_dist_lamb_basic()
2025-04-11T04:23:19.2792056Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2792140Z     partial_func(**kwargs)
2025-04-11T04:23:19.2792388Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2792473Z     partial_func(**kwargs)
2025-04-11T04:23:19.2792771Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2792855Z     partial_func(**kwargs)
2025-04-11T04:23:19.2793075Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T04:23:19.2793171Z     get_accelerator().synchronize()
2025-04-11T04:23:19.2793426Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T04:23:19.2793523Z     torch.cuda.synchronize(device)
2025-04-11T04:23:19.2793797Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T04:23:19.2793895Z     return torch._C._cuda_synchronize()
2025-04-11T04:23:19.2793990Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2794267Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2794399Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2794550Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2794893Z FAILED tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2794897Z 
2025-04-11T04:23:19.2795011Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2795108Z Traceback (most recent call last):
2025-04-11T04:23:19.2795388Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2795466Z     fn(i, *args)
2025-04-11T04:23:19.2795762Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
2025-04-11T04:23:19.2795859Z     check_p2p_communication()
2025-04-11T04:23:19.2796149Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
2025-04-11T04:23:19.2796316Z     tensor = torch.ones(1, device=get_accelerator().get_current_device())
2025-04-11T04:23:19.2796415Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2796686Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2796815Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2796970Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2797318Z FAILED tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2797322Z 
2025-04-11T04:23:19.2797435Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2797525Z Traceback (most recent call last):
2025-04-11T04:23:19.2797807Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2797934Z     fn(i, *args)
2025-04-11T04:23:19.2798167Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
2025-04-11T04:23:19.2798251Z     check_stage_manager()
2025-04-11T04:23:19.2798512Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
2025-04-11T04:23:19.2798600Z     dist.barrier(group=group)
2025-04-11T04:23:19.2798940Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2799031Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2799341Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
2025-04-11T04:23:19.2799440Z     work = group.barrier(opts=opts)
2025-04-11T04:23:19.2799534Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2799811Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2799988Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2800140Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2800487Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2800491Z 
2025-04-11T04:23:19.2800605Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2800697Z Traceback (most recent call last):
2025-04-11T04:23:19.2800971Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2801047Z     fn(i, *args)
2025-04-11T04:23:19.2801304Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:19.2801398Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2801655Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2801765Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2802029Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2802109Z     module._apply(fn)
2025-04-11T04:23:19.2802367Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2802450Z     module._apply(fn)
2025-04-11T04:23:19.2802709Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2802797Z     param_applied = fn(param)
2025-04-11T04:23:19.2803127Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2803242Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2803337Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2803618Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2803744Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2803898Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2804245Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2804252Z 
2025-04-11T04:23:19.2804367Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2804456Z Traceback (most recent call last):
2025-04-11T04:23:19.2804736Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2804813Z     fn(i, *args)
2025-04-11T04:23:19.2805072Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:19.2805214Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2805477Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2805585Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2805852Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2805930Z     module._apply(fn)
2025-04-11T04:23:19.2806245Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2806323Z     module._apply(fn)
2025-04-11T04:23:19.2806585Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2806674Z     param_applied = fn(param)
2025-04-11T04:23:19.2806943Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2807050Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2807145Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2807476Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2807604Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2807760Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2808103Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2808106Z 
2025-04-11T04:23:19.2808219Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2808311Z Traceback (most recent call last):
2025-04-11T04:23:19.2808585Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2808663Z     fn(i, *args)
2025-04-11T04:23:19.2808915Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:19.2809012Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2809266Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2809373Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2809633Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2809715Z     module._apply(fn)
2025-04-11T04:23:19.2809978Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2810057Z     module._apply(fn)
2025-04-11T04:23:19.2810368Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2810458Z     param_applied = fn(param)
2025-04-11T04:23:19.2810726Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2810829Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2810926Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2811206Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2811331Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2811486Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2811830Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2811834Z 
2025-04-11T04:23:19.2811949Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2812040Z Traceback (most recent call last):
2025-04-11T04:23:19.2812316Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2812440Z     fn(i, *args)
2025-04-11T04:23:19.2812697Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T04:23:19.2812791Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2813055Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2813161Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2813475Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2813558Z     module._apply(fn)
2025-04-11T04:23:19.2813814Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2813895Z     module._apply(fn)
2025-04-11T04:23:19.2814157Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2814244Z     param_applied = fn(param)
2025-04-11T04:23:19.2814560Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2814667Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2814765Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2815041Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2815172Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2815329Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2815663Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2815669Z 
2025-04-11T04:23:19.2815786Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2815875Z Traceback (most recent call last):
2025-04-11T04:23:19.2816155Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2816231Z     fn(i, *args)
2025-04-11T04:23:19.2816496Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:19.2816597Z     examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:19.2816853Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:19.2816953Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2817209Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2817314Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2817621Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2817706Z     module._apply(fn)
2025-04-11T04:23:19.2817962Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2818041Z     module._apply(fn)
2025-04-11T04:23:19.2818306Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2818393Z     param_applied = fn(param)
2025-04-11T04:23:19.2818660Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2818766Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2818865Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2819140Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2819268Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2819424Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2819748Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2819803Z 
2025-04-11T04:23:19.2819925Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2820017Z Traceback (most recent call last):
2025-04-11T04:23:19.2820294Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2820368Z     fn(i, *args)
2025-04-11T04:23:19.2820627Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:19.2820784Z     examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:19.2821045Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:19.2821141Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2821397Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2821505Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2821811Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2821895Z     module._apply(fn)
2025-04-11T04:23:19.2822157Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2822235Z     module._apply(fn)
2025-04-11T04:23:19.2822491Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2822582Z     param_applied = fn(param)
2025-04-11T04:23:19.2822846Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2822951Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2823049Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2823323Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2823450Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2823606Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2823927Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2823931Z 
2025-04-11T04:23:19.2824046Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2824137Z Traceback (most recent call last):
2025-04-11T04:23:19.2824414Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2824488Z     fn(i, *args)
2025-04-11T04:23:19.2824799Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:19.2824906Z     examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:19.2825164Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:19.2825258Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2825517Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2825622Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2825884Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2825965Z     module._apply(fn)
2025-04-11T04:23:19.2826226Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2826303Z     module._apply(fn)
2025-04-11T04:23:19.2826565Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2826651Z     param_applied = fn(param)
2025-04-11T04:23:19.2826916Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2827073Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2827170Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2827456Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2827582Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2827739Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2828113Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2828117Z 
2025-04-11T04:23:19.2828233Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2828326Z Traceback (most recent call last):
2025-04-11T04:23:19.2828629Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2828705Z     fn(i, *args)
2025-04-11T04:23:19.2828962Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T04:23:19.2829132Z     examine_pp(num_microbatch, batch_size)
2025-04-11T04:23:19.2829394Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T04:23:19.2829488Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.2829744Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2829851Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2830109Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2830190Z     module._apply(fn)
2025-04-11T04:23:19.2830452Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2830529Z     module._apply(fn)
2025-04-11T04:23:19.2830791Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2830879Z     param_applied = fn(param)
2025-04-11T04:23:19.2831144Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2831246Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2831341Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2831623Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2831750Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2831906Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2832298Z FAILED tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2832302Z 
2025-04-11T04:23:19.2832421Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2832512Z Traceback (most recent call last):
2025-04-11T04:23:19.2832795Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2832869Z     fn(i, *args)
2025-04-11T04:23:19.2833145Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
2025-04-11T04:23:19.2833248Z     run_with_booster_moehybridplugin()
2025-04-11T04:23:19.2833503Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2833591Z     partial_func(**kwargs)
2025-04-11T04:23:19.2833929Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
2025-04-11T04:23:19.2834054Z     torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T04:23:19.2834332Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2834483Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2834750Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2834854Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2835119Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2835254Z     module._apply(fn)
2025-04-11T04:23:19.2835516Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2835604Z     param_applied = fn(param)
2025-04-11T04:23:19.2835871Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2835976Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2836070Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2836401Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2836528Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2836685Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2836968Z FAILED tests/test_shardformer/test_flash_attention.py::test_flash_attn_func - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2837239Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2837368Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2837521Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2837782Z FAILED tests/test_shardformer/test_shard_utils.py::test_release_layer - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2838047Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2838177Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2838325Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2838572Z FAILED tests/test_shardformer/test_with_torch_ddp.py::test_gpt2 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2838842Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2838966Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2839116Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2839528Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2839800Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2839925Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2840075Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2840438Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2840707Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2840832Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2840978Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2841338Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2841602Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2841780Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2841927Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2842315Z FAILED tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2842319Z 
2025-04-11T04:23:19.2842432Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2842590Z Traceback (most recent call last):
2025-04-11T04:23:19.2842873Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2842948Z     fn(i, *args)
2025-04-11T04:23:19.2843279Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
2025-04-11T04:23:19.2843411Z     pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
2025-04-11T04:23:19.2843509Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2843824Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2843953Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2844100Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2844428Z FAILED tests/test_shardformer/test_layer/test_dropout.py::test_dropout - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2844437Z 
2025-04-11T04:23:19.2844549Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2844638Z Traceback (most recent call last):
2025-04-11T04:23:19.2844920Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2844996Z     fn(i, *args)
2025-04-11T04:23:19.2845249Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
2025-04-11T04:23:19.2845341Z     check_dropout_parallel_input()
2025-04-11T04:23:19.2845654Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
2025-04-11T04:23:19.2845875Z     dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
2025-04-11T04:23:19.2846140Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
2025-04-11T04:23:19.2846361Z     return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
2025-04-11T04:23:19.2846596Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
2025-04-11T04:23:19.2846846Z     self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
2025-04-11T04:23:19.2847144Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
2025-04-11T04:23:19.2847330Z     is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
2025-04-11T04:23:19.2847639Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
2025-04-11T04:23:19.2847872Z     index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
2025-04-11T04:23:19.2847968Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2848241Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2848372Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2848524Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2848878Z FAILED tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2848883Z 
2025-04-11T04:23:19.2848997Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2849145Z Traceback (most recent call last):
2025-04-11T04:23:19.2849429Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2849505Z     fn(i, *args)
2025-04-11T04:23:19.2849767Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
2025-04-11T04:23:19.2849851Z     check_embedding_1d()
2025-04-11T04:23:19.2850102Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2850236Z     partial_func(**kwargs)
2025-04-11T04:23:19.2850517Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
2025-04-11T04:23:19.2850620Z     embedding = nn.Embedding(32, 128).cuda()
2025-04-11T04:23:19.2850877Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2850986Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2851290Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2851385Z     param_applied = fn(param)
2025-04-11T04:23:19.2851652Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2851761Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2851858Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2852132Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2852260Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2852416Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2852794Z FAILED tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2852801Z 
2025-04-11T04:23:19.2852912Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2853007Z Traceback (most recent call last):
2025-04-11T04:23:19.2853286Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2853361Z     fn(i, *args)
2025-04-11T04:23:19.2853661Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
2025-04-11T04:23:19.2853755Z     check_gpt2_qkv_fused_linear_1d()
2025-04-11T04:23:19.2854008Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2854092Z     partial_func(**kwargs)
2025-04-11T04:23:19.2854391Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2854479Z     partial_func(**kwargs)
2025-04-11T04:23:19.2854831Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
2025-04-11T04:23:19.2854965Z     check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
2025-04-11T04:23:19.2855299Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
2025-04-11T04:23:19.2855392Z     linear = Conv1D(192, 48).cuda()
2025-04-11T04:23:19.2855651Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2855759Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2856021Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2856115Z     param_applied = fn(param)
2025-04-11T04:23:19.2856382Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2856540Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2856634Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2856914Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2857045Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2857199Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2857611Z FAILED tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2857616Z 
2025-04-11T04:23:19.2857726Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2857819Z Traceback (most recent call last):
2025-04-11T04:23:19.2858093Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2858167Z     fn(i, *args)
2025-04-11T04:23:19.2858425Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
2025-04-11T04:23:19.2858509Z     check_layernorm()
2025-04-11T04:23:19.2858804Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2858889Z     partial_func(**kwargs)
2025-04-11T04:23:19.2859165Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
2025-04-11T04:23:19.2859266Z     norm = nn.LayerNorm(128, 0.00001).cuda()
2025-04-11T04:23:19.2859523Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2859630Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2859891Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2859983Z     param_applied = fn(param)
2025-04-11T04:23:19.2860249Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2860358Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2860452Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2860728Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2860858Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2861012Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2861340Z FAILED tests/test_shardformer/test_layer/test_linear_1d.py::test_linear - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2861344Z 
2025-04-11T04:23:19.2861503Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2861599Z Traceback (most recent call last):
2025-04-11T04:23:19.2861878Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2861958Z     fn(i, *args)
2025-04-11T04:23:19.2862241Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
2025-04-11T04:23:19.2862326Z     run_dist_linear_test()
2025-04-11T04:23:19.2862576Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2862658Z     partial_func(**kwargs)
2025-04-11T04:23:19.2862905Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2862990Z     partial_func(**kwargs)
2025-04-11T04:23:19.2863234Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2863317Z     partial_func(**kwargs)
2025-04-11T04:23:19.2863607Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
2025-04-11T04:23:19.2863749Z     check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
2025-04-11T04:23:19.2864086Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T04:23:19.2864179Z     linear = nn.Linear(32, 128).cuda()
2025-04-11T04:23:19.2864436Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2864540Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2864947Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2865036Z     param_applied = fn(param)
2025-04-11T04:23:19.2865308Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2865409Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2865508Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2865782Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2865960Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2866115Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2866482Z FAILED tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2866491Z 
2025-04-11T04:23:19.2866605Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2866696Z Traceback (most recent call last):
2025-04-11T04:23:19.2866974Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2867048Z     fn(i, *args)
2025-04-11T04:23:19.2867338Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
2025-04-11T04:23:19.2867422Z     check_linear_1d_col()
2025-04-11T04:23:19.2867671Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2867756Z     partial_func(**kwargs)
2025-04-11T04:23:19.2868066Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T04:23:19.2868160Z     linear = nn.Linear(8, 80).cuda()
2025-04-11T04:23:19.2868481Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2868591Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2868852Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2868941Z     param_applied = fn(param)
2025-04-11T04:23:19.2869262Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2869367Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2869469Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2869745Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2869876Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2870029Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2870371Z FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2870377Z 
2025-04-11T04:23:19.2870489Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2870579Z Traceback (most recent call last):
2025-04-11T04:23:19.2870862Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2870935Z     fn(i, *args)
2025-04-11T04:23:19.2871224Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
2025-04-11T04:23:19.2871361Z     check_packed_seq()
2025-04-11T04:23:19.2871611Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2871695Z     partial_func(**kwargs)
2025-04-11T04:23:19.2871943Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2872031Z     partial_func(**kwargs)
2025-04-11T04:23:19.2872331Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2872417Z     partial_func(**kwargs)
2025-04-11T04:23:19.2872509Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2872793Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
2025-04-11T04:23:19.2872961Z     padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
2025-04-11T04:23:19.2873056Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2873384Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2873513Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2873671Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2874016Z FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2874022Z 
2025-04-11T04:23:19.2874136Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2874228Z Traceback (most recent call last):
2025-04-11T04:23:19.2874513Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2874589Z     fn(i, *args)
2025-04-11T04:23:19.2874875Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
2025-04-11T04:23:19.2874972Z     check_ring_attn(inner_ring_size=2)
2025-04-11T04:23:19.2875220Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2875305Z     partial_func(**kwargs)
2025-04-11T04:23:19.2875549Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2875633Z     partial_func(**kwargs)
2025-04-11T04:23:19.2875880Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2875960Z     partial_func(**kwargs)
2025-04-11T04:23:19.2876056Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2876381Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
2025-04-11T04:23:19.2876587Z     qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
2025-04-11T04:23:19.2876682Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2876962Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2877088Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2877240Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2877624Z FAILED tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2877631Z 
2025-04-11T04:23:19.2877742Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2877835Z Traceback (most recent call last):
2025-04-11T04:23:19.2878122Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2878198Z     fn(i, *args)
2025-04-11T04:23:19.2878514Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
2025-04-11T04:23:19.2878654Z     run_seq_parallel_attn()
2025-04-11T04:23:19.2878905Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2878986Z     partial_func(**kwargs)
2025-04-11T04:23:19.2879230Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2879312Z     partial_func(**kwargs)
2025-04-11T04:23:19.2879607Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2879688Z     partial_func(**kwargs)
2025-04-11T04:23:19.2879786Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2880108Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
2025-04-11T04:23:19.2880252Z     seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
2025-04-11T04:23:19.2880563Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
2025-04-11T04:23:19.2880741Z     x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
2025-04-11T04:23:19.2880843Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2881118Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2881249Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2881404Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2881805Z FAILED tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2881812Z 
2025-04-11T04:23:19.2881927Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2882018Z Traceback (most recent call last):
2025-04-11T04:23:19.2882310Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2882388Z     fn(i, *args)
2025-04-11T04:23:19.2882697Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
2025-04-11T04:23:19.2882786Z     check_vocab_embedding_1d()
2025-04-11T04:23:19.2883036Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2883119Z     partial_func(**kwargs)
2025-04-11T04:23:19.2883465Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
2025-04-11T04:23:19.2883578Z     embedding = nn.Embedding(128, 32).to("cuda")
2025-04-11T04:23:19.2883887Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.2883983Z     return self._apply(convert)
2025-04-11T04:23:19.2884247Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2884339Z     param_applied = fn(param)
2025-04-11T04:23:19.2884609Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.2884826Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.2884926Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2885204Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2885337Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2885493Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2885760Z FAILED tests/test_shardformer/test_model/test_shard_bert.py::test_bert - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2886029Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2886226Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2886377Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2886646Z FAILED tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2886917Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2887090Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2887240Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2887505Z FAILED tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2887777Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2887900Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2888095Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2888378Z FAILED tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2888642Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2888768Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2888915Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2889194Z FAILED tests/test_shardformer/test_model/test_shard_command.py::test_command - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2889460Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2889584Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2889733Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2890087Z FAILED tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2890091Z 
2025-04-11T04:23:19.2890209Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2890300Z Traceback (most recent call last):
2025-04-11T04:23:19.2890587Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2890661Z     fn(i, *args)
2025-04-11T04:23:19.2890957Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
2025-04-11T04:23:19.2891089Z     run_deepseek_test()
2025-04-11T04:23:19.2891343Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2891429Z     partial_func(**kwargs)
2025-04-11T04:23:19.2891732Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
2025-04-11T04:23:19.2891823Z     run_deepseek_commom(config)
2025-04-11T04:23:19.2892117Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
2025-04-11T04:23:19.2892320Z     torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
2025-04-11T04:23:19.2892601Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2892699Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2892958Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2893066Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2893332Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2893463Z     module._apply(fn)
2025-04-11T04:23:19.2893730Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2893821Z     param_applied = fn(param)
2025-04-11T04:23:19.2894088Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2894193Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2894338Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2894618Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2894746Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2894900Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2895267Z FAILED tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2895273Z 
2025-04-11T04:23:19.2895436Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2895528Z Traceback (most recent call last):
2025-04-11T04:23:19.2895806Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2895884Z     fn(i, *args)
2025-04-11T04:23:19.2896188Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
2025-04-11T04:23:19.2896276Z     run_deepseek_v3_test()
2025-04-11T04:23:19.2896525Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2896611Z     partial_func(**kwargs)
2025-04-11T04:23:19.2896924Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
2025-04-11T04:23:19.2897012Z     check_forward_backward(
2025-04-11T04:23:19.2897332Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
2025-04-11T04:23:19.2897598Z     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
2025-04-11T04:23:19.2897893Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
2025-04-11T04:23:19.2897987Z     org_model = org_model.cuda()
2025-04-11T04:23:19.2898266Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2898360Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2898672Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2898781Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2899051Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2899142Z     module._apply(fn)
2025-04-11T04:23:19.2899406Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2899493Z     module._apply(fn)
2025-04-11T04:23:19.2899757Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2899856Z     param_applied = fn(param)
2025-04-11T04:23:19.2900127Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2900236Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2900339Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2900621Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2900755Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2900958Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2901240Z FAILED tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2901506Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2901633Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2901833Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2902097Z FAILED tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2902374Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2902497Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2902648Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2902960Z FAILED tests/test_shardformer/test_model/test_shard_llama.py::test_llama - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2903229Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2903350Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2903498Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2903779Z FAILED tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2904042Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2904171Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2904317Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2904661Z FAILED tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2904669Z 
2025-04-11T04:23:19.2904782Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2904877Z Traceback (most recent call last):
2025-04-11T04:23:19.2905156Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2905230Z     fn(i, *args)
2025-04-11T04:23:19.2905512Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
2025-04-11T04:23:19.2905591Z     run_mixtral_test()
2025-04-11T04:23:19.2905891Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2905975Z     partial_func(**kwargs)
2025-04-11T04:23:19.2906273Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
2025-04-11T04:23:19.2906361Z     run_mixtral_commom(config)
2025-04-11T04:23:19.2906658Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
2025-04-11T04:23:19.2906784Z     torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T04:23:19.2907062Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2907160Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2907422Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2907531Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2907799Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2907879Z     module._apply(fn)
2025-04-11T04:23:19.2908143Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2908281Z     param_applied = fn(param)
2025-04-11T04:23:19.2908581Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2908684Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2908782Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2909056Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2909240Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2909392Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2909668Z FAILED tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2909940Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2910066Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2910269Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2910536Z FAILED tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2910803Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2910927Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2911072Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2911330Z FAILED tests/test_shardformer/test_model/test_shard_sam.py::test_sam - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2911596Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2911720Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2911869Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2912122Z FAILED tests/test_shardformer/test_model/test_shard_t5.py::test_t5 - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2912388Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2912511Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2912659Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2912910Z FAILED tests/test_shardformer/test_model/test_shard_vit.py::test_vit - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2913248Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2913375Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2913525Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2913806Z FAILED tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper - RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2914080Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2914203Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2914350Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2914659Z FAILED tests/test_tensor/test_comm_spec_apply.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2914663Z 
2025-04-11T04:23:19.2914778Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2914877Z Traceback (most recent call last):
2025-04-11T04:23:19.2915161Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2915238Z     fn(i, *args)
2025-04-11T04:23:19.2915474Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
2025-04-11T04:23:19.2915644Z     check_all_gather(device_mesh, rank)
2025-04-11T04:23:19.2915893Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
2025-04-11T04:23:19.2916009Z     sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T04:23:19.2916108Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2916381Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2916560Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2916710Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2917032Z FAILED tests/test_tensor/test_padded_tensor.py::test_padded_tensor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2917036Z 
2025-04-11T04:23:19.2917151Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2917244Z Traceback (most recent call last):
2025-04-11T04:23:19.2917579Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2917656Z     fn(i, *args)
2025-04-11T04:23:19.2917913Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
2025-04-11T04:23:19.2918025Z     original_tensor = torch.rand(32, 64).to("cuda")
2025-04-11T04:23:19.2918124Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2918392Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2918518Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2918671Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2918992Z FAILED tests/test_tensor/test_shape_consistency_apply.py::test_apply - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2918998Z 
2025-04-11T04:23:19.2919113Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2919206Z Traceback (most recent call last):
2025-04-11T04:23:19.2919487Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2919561Z     fn(i, *args)
2025-04-11T04:23:19.2919826Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
2025-04-11T04:23:19.2919997Z     tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
2025-04-11T04:23:19.2920091Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2920413Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2920542Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2920698Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2921025Z FAILED tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2921031Z 
2025-04-11T04:23:19.2921146Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2921238Z Traceback (most recent call last):
2025-04-11T04:23:19.2921515Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2921594Z     fn(i, *args)
2025-04-11T04:23:19.2921847Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
2025-04-11T04:23:19.2921958Z     check_all_gather(process_group_dict, rank)
2025-04-11T04:23:19.2922224Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
2025-04-11T04:23:19.2922343Z     sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T04:23:19.2922437Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2922764Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2922888Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2923038Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2923354Z FAILED tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2923407Z 
2025-04-11T04:23:19.2923518Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2923611Z Traceback (most recent call last):
2025-04-11T04:23:19.2923884Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2923962Z     fn(i, *args)
2025-04-11T04:23:19.2924212Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
2025-04-11T04:23:19.2924308Z     test_model = TestModel(8, 8).to("cuda")
2025-04-11T04:23:19.2924621Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.2924712Z     return self._apply(convert)
2025-04-11T04:23:19.2924976Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2925056Z     module._apply(fn)
2025-04-11T04:23:19.2925315Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2925405Z     param_applied = fn(param)
2025-04-11T04:23:19.2925669Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.2925884Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.2925977Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2926253Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2926380Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2926532Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2926901Z FAILED tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2926905Z 
2025-04-11T04:23:19.2927022Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2927112Z Traceback (most recent call last):
2025-04-11T04:23:19.2927386Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2927464Z     fn(i, *args)
2025-04-11T04:23:19.2927839Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
2025-04-11T04:23:19.2927964Z     original_tensor = torch.rand(global_shape).cuda()
2025-04-11T04:23:19.2928060Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2928348Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2928473Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2928625Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2928963Z FAILED tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2928969Z 
2025-04-11T04:23:19.2929080Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2929172Z Traceback (most recent call last):
2025-04-11T04:23:19.2929451Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2929528Z     fn(i, *args)
2025-04-11T04:23:19.2929776Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
2025-04-11T04:23:19.2929911Z     exam_chunk_memory()
2025-04-11T04:23:19.2930167Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2930251Z     partial_func(**kwargs)
2025-04-11T04:23:19.2930499Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2930580Z     partial_func(**kwargs)
2025-04-11T04:23:19.2930896Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
2025-04-11T04:23:19.2930993Z     chunk_manager = ChunkManager(config)
2025-04-11T04:23:19.2931231Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T04:23:19.2931478Z     self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:19.2931572Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2931851Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2932024Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2932182Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2932512Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2932518Z 
2025-04-11T04:23:19.2932633Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2932723Z Traceback (most recent call last):
2025-04-11T04:23:19.2933000Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2933079Z     fn(i, *args)
2025-04-11T04:23:19.2933310Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:19.2933394Z     exam_chunk_basic()
2025-04-11T04:23:19.2933641Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2933728Z     partial_func(**kwargs)
2025-04-11T04:23:19.2933968Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2934048Z     partial_func(**kwargs)
2025-04-11T04:23:19.2934292Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2934375Z     partial_func(**kwargs)
2025-04-11T04:23:19.2934473Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2934723Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:19.2934856Z     my_chunk = Chunk(
2025-04-11T04:23:19.2935090Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:19.2935288Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:19.2935391Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2935668Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2935796Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2935947Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2936279Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2936284Z 
2025-04-11T04:23:19.2936396Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2936489Z Traceback (most recent call last):
2025-04-11T04:23:19.2936769Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2936844Z     fn(i, *args)
2025-04-11T04:23:19.2937081Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:19.2937212Z     exam_chunk_basic()
2025-04-11T04:23:19.2937462Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2937545Z     partial_func(**kwargs)
2025-04-11T04:23:19.2937792Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2937921Z     partial_func(**kwargs)
2025-04-11T04:23:19.2938162Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2938248Z     partial_func(**kwargs)
2025-04-11T04:23:19.2938342Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2938600Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:19.2938678Z     my_chunk = Chunk(
2025-04-11T04:23:19.2938911Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:19.2939165Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:19.2939263Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2939544Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2939670Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2939827Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2940159Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2940163Z 
2025-04-11T04:23:19.2940279Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2940370Z Traceback (most recent call last):
2025-04-11T04:23:19.2940650Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2940728Z     fn(i, *args)
2025-04-11T04:23:19.2940960Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T04:23:19.2941041Z     exam_chunk_basic()
2025-04-11T04:23:19.2941287Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2941371Z     partial_func(**kwargs)
2025-04-11T04:23:19.2941612Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2941696Z     partial_func(**kwargs)
2025-04-11T04:23:19.2941940Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2942070Z     partial_func(**kwargs)
2025-04-11T04:23:19.2942168Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.2942420Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T04:23:19.2942504Z     my_chunk = Chunk(
2025-04-11T04:23:19.2942735Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T04:23:19.2942924Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T04:23:19.2943024Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2943300Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2943432Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2943586Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2943936Z FAILED tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2943940Z 
2025-04-11T04:23:19.2944052Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2944144Z Traceback (most recent call last):
2025-04-11T04:23:19.2944478Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2944552Z     fn(i, *args)
2025-04-11T04:23:19.2944797Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
2025-04-11T04:23:19.2944879Z     exam_gemini_grad_acc()
2025-04-11T04:23:19.2945127Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2945259Z     partial_func(**kwargs)
2025-04-11T04:23:19.2945504Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2945586Z     partial_func(**kwargs)
2025-04-11T04:23:19.2945826Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2945911Z     partial_func(**kwargs)
2025-04-11T04:23:19.2946004Z   [Previous line repeated 4 more times]
2025-04-11T04:23:19.2946280Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
2025-04-11T04:23:19.2946421Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2946702Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2946794Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2947052Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2947166Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2947424Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2947508Z     module._apply(fn)
2025-04-11T04:23:19.2947766Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2947849Z     module._apply(fn)
2025-04-11T04:23:19.2948106Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2948196Z     param_applied = fn(param)
2025-04-11T04:23:19.2948491Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2948597Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2948696Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2948976Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2949112Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2949329Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2949652Z FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2949662Z 
2025-04-11T04:23:19.2949783Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2949876Z Traceback (most recent call last):
2025-04-11T04:23:19.2950162Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2950240Z     fn(i, *args)
2025-04-11T04:23:19.2950493Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T04:23:19.2950582Z     exam_grad_clipping()
2025-04-11T04:23:19.2950831Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2950921Z     partial_func(**kwargs)
2025-04-11T04:23:19.2951166Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2951255Z     partial_func(**kwargs)
2025-04-11T04:23:19.2951498Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2951639Z     partial_func(**kwargs)
2025-04-11T04:23:19.2951735Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2952006Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T04:23:19.2952106Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2952389Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2952544Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2952804Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2952917Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2953179Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2953259Z     module._apply(fn)
2025-04-11T04:23:19.2953523Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2953607Z     module._apply(fn)
2025-04-11T04:23:19.2953920Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2954011Z     param_applied = fn(param)
2025-04-11T04:23:19.2954282Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2954390Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2954490Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2954775Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2954906Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2955067Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2955388Z FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2955394Z 
2025-04-11T04:23:19.2955515Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2955609Z Traceback (most recent call last):
2025-04-11T04:23:19.2955893Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2955967Z     fn(i, *args)
2025-04-11T04:23:19.2956210Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T04:23:19.2956305Z     exam_grad_clipping()
2025-04-11T04:23:19.2956551Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2956640Z     partial_func(**kwargs)
2025-04-11T04:23:19.2956932Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2957021Z     partial_func(**kwargs)
2025-04-11T04:23:19.2957452Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2957584Z     partial_func(**kwargs)
2025-04-11T04:23:19.2957746Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2958136Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T04:23:19.2958264Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2958613Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2958797Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2959122Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2959259Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2959585Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2959733Z     module._apply(fn)
2025-04-11T04:23:19.2960090Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2960200Z     module._apply(fn)
2025-04-11T04:23:19.2960492Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2960642Z     param_applied = fn(param)
2025-04-11T04:23:19.2960937Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2961152Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2961294Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2961636Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2961795Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2961981Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2962419Z FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2962424Z 
2025-04-11T04:23:19.2962578Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2962742Z Traceback (most recent call last):
2025-04-11T04:23:19.2963144Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2963285Z     fn(i, *args)
2025-04-11T04:23:19.2963568Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T04:23:19.2963706Z     exam_inference()
2025-04-11T04:23:19.2964003Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2964116Z     partial_func(**kwargs)
2025-04-11T04:23:19.2964425Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2964549Z     partial_func(**kwargs)
2025-04-11T04:23:19.2964843Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2964965Z     partial_func(**kwargs)
2025-04-11T04:23:19.2965290Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T04:23:19.2965417Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2965728Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2965893Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2966168Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2966412Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2966704Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2966851Z     module._apply(fn)
2025-04-11T04:23:19.2967142Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2967269Z     module._apply(fn)
2025-04-11T04:23:19.2967573Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2967703Z     param_applied = fn(param)
2025-04-11T04:23:19.2968115Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2968251Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2968410Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2968707Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2968922Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2969109Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2969536Z FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2969575Z 
2025-04-11T04:23:19.2969720Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2969844Z Traceback (most recent call last):
2025-04-11T04:23:19.2970181Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2970361Z     fn(i, *args)
2025-04-11T04:23:19.2970662Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T04:23:19.2970771Z     exam_inference()
2025-04-11T04:23:19.2971077Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2971175Z     partial_func(**kwargs)
2025-04-11T04:23:19.2971454Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2971608Z     partial_func(**kwargs)
2025-04-11T04:23:19.2971922Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2972064Z     partial_func(**kwargs)
2025-04-11T04:23:19.2972344Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T04:23:19.2972497Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2972881Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2972998Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2973311Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2973443Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2973755Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2973880Z     module._apply(fn)
2025-04-11T04:23:19.2974216Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2974322Z     module._apply(fn)
2025-04-11T04:23:19.2974642Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2974756Z     param_applied = fn(param)
2025-04-11T04:23:19.2975051Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2975234Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2975356Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2975742Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2975902Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2976110Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2976457Z FAILED tests/test_zero/test_gemini/test_optim.py::test_optim[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2976462Z 
2025-04-11T04:23:19.2976645Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2976765Z Traceback (most recent call last):
2025-04-11T04:23:19.2977076Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2977222Z     fn(i, *args)
2025-04-11T04:23:19.2977469Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
2025-04-11T04:23:19.2977692Z     exam_model_step()
2025-04-11T04:23:19.2977971Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2978112Z     partial_func(**kwargs)
2025-04-11T04:23:19.2978395Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2978556Z     partial_func(**kwargs)
2025-04-11T04:23:19.2978858Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.2978978Z     partial_func(**kwargs)
2025-04-11T04:23:19.2979127Z   [Previous line repeated 2 more times]
2025-04-11T04:23:19.2987911Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
2025-04-11T04:23:19.2988200Z     torch_model = model_builder().cuda()
2025-04-11T04:23:19.2988596Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T04:23:19.2988702Z     return super().cuda(*args, **kwargs)
2025-04-11T04:23:19.2988980Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.2989098Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2989372Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2989525Z     module._apply(fn)
2025-04-11T04:23:19.2989791Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.2989875Z     module._apply(fn)
2025-04-11T04:23:19.2990135Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.2990230Z     param_applied = fn(param)
2025-04-11T04:23:19.2990509Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.2990619Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.2990727Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2991016Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2991156Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2991318Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2991649Z FAILED tests/test_zero/test_gemini/test_search.py::test_search[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2991655Z 
2025-04-11T04:23:19.2991781Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2991878Z Traceback (most recent call last):
2025-04-11T04:23:19.2992167Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2992245Z     fn(i, *args)
2025-04-11T04:23:19.2992492Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T04:23:19.2992640Z     exam_chunk_manager()
2025-04-11T04:23:19.2992900Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T04:23:19.2992999Z     chunk_manager = init_chunk_manager(
2025-04-11T04:23:19.2993267Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T04:23:19.2993353Z     dist.barrier()
2025-04-11T04:23:19.2993647Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2993743Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2994061Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:19.2994166Z     work = default_pg.barrier(opts=opts)
2025-04-11T04:23:19.2994270Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2994556Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2994697Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2994863Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2995243Z FAILED tests/test_zero/test_gemini/test_search.py::test_search[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2995248Z 
2025-04-11T04:23:19.2995365Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2995462Z Traceback (most recent call last):
2025-04-11T04:23:19.2995746Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2995877Z     fn(i, *args)
2025-04-11T04:23:19.2996116Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T04:23:19.2996200Z     exam_chunk_manager()
2025-04-11T04:23:19.2996463Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T04:23:19.2996553Z     chunk_manager = init_chunk_manager(
2025-04-11T04:23:19.2996820Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T04:23:19.2996903Z     dist.barrier()
2025-04-11T04:23:19.2997243Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T04:23:19.2997338Z     return func(*args, **kwargs)
2025-04-11T04:23:19.2997649Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T04:23:19.2997750Z     work = default_pg.barrier(opts=opts)
2025-04-11T04:23:19.2997848Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.2998127Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.2998256Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.2998416Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.2998770Z FAILED tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.2998778Z 
2025-04-11T04:23:19.2998894Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.2998991Z Traceback (most recent call last):
2025-04-11T04:23:19.2999272Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.2999350Z     fn(i, *args)
2025-04-11T04:23:19.2999615Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
2025-04-11T04:23:19.2999698Z     exam_state_dict()
2025-04-11T04:23:19.2999956Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3000043Z     partial_func(**kwargs)
2025-04-11T04:23:19.3000363Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3000450Z     partial_func(**kwargs)
2025-04-11T04:23:19.3000698Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3000780Z     partial_func(**kwargs)
2025-04-11T04:23:19.3000877Z   [Previous line repeated 1 more time]
2025-04-11T04:23:19.3001160Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
2025-04-11T04:23:19.3001405Z     model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
2025-04-11T04:23:19.3001644Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
2025-04-11T04:23:19.3001740Z     self.chunk_manager = ChunkManager(
2025-04-11T04:23:19.3001981Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T04:23:19.3002230Z     self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T04:23:19.3002329Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3002660Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3002791Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3002949Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3003266Z FAILED tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3003316Z 
2025-04-11T04:23:19.3003436Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3003527Z Traceback (most recent call last):
2025-04-11T04:23:19.3003809Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3003887Z     fn(i, *args)
2025-04-11T04:23:19.3004123Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
2025-04-11T04:23:19.3004212Z     check_all_gather_2d()
2025-04-11T04:23:19.3004477Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
2025-04-11T04:23:19.3004654Z     tensor = torch.rand(128, device=get_current_device())
2025-04-11T04:23:19.3004752Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3005026Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3005153Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3005309Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3005656Z FAILED tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3005664Z 
2025-04-11T04:23:19.3005778Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3005870Z Traceback (most recent call last):
2025-04-11T04:23:19.3006151Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3006230Z     fn(i, *args)
2025-04-11T04:23:19.3006472Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
2025-04-11T04:23:19.3006567Z     exam_zero_1_grad_acc(sync=True)
2025-04-11T04:23:19.3006838Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
2025-04-11T04:23:19.3006930Z     zero_model = zero_model.to(device)
2025-04-11T04:23:19.3007187Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T04:23:19.3007278Z     return self._apply(convert)
2025-04-11T04:23:19.3007590Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.3007675Z     module._apply(fn)
2025-04-11T04:23:19.3007942Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.3008035Z     param_applied = fn(param)
2025-04-11T04:23:19.3008307Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T04:23:19.3008524Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T04:23:19.3008621Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3008909Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3009039Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3009196Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3009518Z FAILED tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3009523Z 
2025-04-11T04:23:19.3009640Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3009781Z Traceback (most recent call last):
2025-04-11T04:23:19.3010055Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3010133Z     fn(i, *args)
2025-04-11T04:23:19.3010376Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
2025-04-11T04:23:19.3010473Z     exam_mem_leak(world_size=world_size)
2025-04-11T04:23:19.3010770Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
2025-04-11T04:23:19.3010861Z     zero_model = MlpModel().cuda()
2025-04-11T04:23:19.3011121Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.3011230Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3011498Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.3011581Z     module._apply(fn)
2025-04-11T04:23:19.3011892Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.3011984Z     param_applied = fn(param)
2025-04-11T04:23:19.3012255Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.3012361Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3012458Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3012740Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3012865Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3013022Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3013339Z FAILED tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3013345Z 
2025-04-11T04:23:19.3013459Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3013551Z Traceback (most recent call last):
2025-04-11T04:23:19.3013826Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3013903Z     fn(i, *args)
2025-04-11T04:23:19.3014141Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
2025-04-11T04:23:19.3014234Z     exam_zero_1_torch_ddp()
2025-04-11T04:23:19.3014483Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3014571Z     partial_func(**kwargs)
2025-04-11T04:23:19.3014881Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3014968Z     partial_func(**kwargs)
2025-04-11T04:23:19.3015215Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3015298Z     partial_func(**kwargs)
2025-04-11T04:23:19.3015580Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
2025-04-11T04:23:19.3015683Z     torch_model = MlpModel().cuda().to(dtype)
2025-04-11T04:23:19.3015949Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.3016054Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3016316Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.3016397Z     module._apply(fn)
2025-04-11T04:23:19.3016656Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.3016748Z     param_applied = fn(param)
2025-04-11T04:23:19.3017008Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.3017163Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3017260Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3017543Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3017669Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3017823Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3018191Z FAILED tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T04:23:19.3018196Z 
2025-04-11T04:23:19.3018308Z -- Process 0 terminated with the following error:
2025-04-11T04:23:19.3018406Z Traceback (most recent call last):
2025-04-11T04:23:19.3018681Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T04:23:19.3018758Z     fn(i, *args)
2025-04-11T04:23:19.3019004Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
2025-04-11T04:23:19.3019145Z     exam_zero_1_torch_ddp_ckpt()
2025-04-11T04:23:19.3019397Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T04:23:19.3019480Z     partial_func(**kwargs)
2025-04-11T04:23:19.3019774Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
2025-04-11T04:23:19.3019869Z     torch_model = MlpModel().cuda()
2025-04-11T04:23:19.3020133Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T04:23:19.3020240Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3020503Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T04:23:19.3020586Z     module._apply(fn)
2025-04-11T04:23:19.3020851Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T04:23:19.3020945Z     param_applied = fn(param)
2025-04-11T04:23:19.3021213Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T04:23:19.3021320Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T04:23:19.3021414Z RuntimeError: CUDA error: out of memory
2025-04-11T04:23:19.3021690Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T04:23:19.3021818Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T04:23:19.3021972Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:19.3022207Z = 573 failed, 74 passed, 195 skipped, 23 deselected, 91 warnings in 663.23s (0:11:03) =
2025-04-11T04:23:20.1932420Z ##[error]Process completed with exit code 1.
2025-04-11T04:23:20.2002290Z Post job cleanup.
2025-04-11T04:23:20.2005516Z ##[command]/usr/bin/docker exec  920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f sh -c "cat /etc/*release | grep ^ID"
2025-04-11T04:23:20.4089417Z [command]/usr/bin/git version
2025-04-11T04:23:20.4117884Z git version 2.25.1
2025-04-11T04:23:20.4151590Z Temporarily overriding HOME='/__w/_temp/84be0375-2aba-41cf-b9a0-021db580aa02' before making global git config changes
2025-04-11T04:23:20.4152176Z Adding repository directory to the temporary git global config as a safe directory
2025-04-11T04:23:20.4154964Z [command]/usr/bin/git config --global --add safe.directory /__w/ColossalAI/ColossalAI
2025-04-11T04:23:20.4179165Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-11T04:23:20.4200985Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-11T04:23:20.4369085Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-11T04:23:20.4383118Z http.https://github.com/.extraheader
2025-04-11T04:23:20.4390564Z [command]/usr/bin/git config --local --unset-all http.https://github.com/.extraheader
2025-04-11T04:23:20.4411337Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-11T04:23:20.4684277Z Post job cleanup.
2025-04-11T04:23:20.4687441Z ##[command]/usr/bin/docker exec  920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f sh -c "cat /etc/*release | grep ^ID"
2025-04-11T04:23:20.6778645Z Stop and remove container: 7d6bacf453dd49bc8aea383939f91c8e_imagecloudluchentechcomhpcaitechpytorchcuda2221210_f16ff6
2025-04-11T04:23:20.6783385Z ##[command]/usr/bin/docker rm --force 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
2025-04-11T04:23:21.7125449Z 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
2025-04-11T04:23:21.7154928Z Remove container network: github_network_94998f4534db421f86753cfbbb2319dd
2025-04-11T04:23:21.7158651Z ##[command]/usr/bin/docker network rm github_network_94998f4534db421f86753cfbbb2319dd
2025-04-11T04:23:21.8175419Z github_network_94998f4534db421f86753cfbbb2319dd
2025-04-11T04:23:21.8227247Z Cleaning up orphan processes
